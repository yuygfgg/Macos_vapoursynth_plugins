diff --git a/.gitignore b/.gitignore
index f1e3d20..20cfe23 100644
--- a/.gitignore
+++ b/.gitignore
@@ -250,3 +250,8 @@ paket-files/
 # JetBrains Rider
 .idea/
 *.sln.iml
+
+# MacOS Finder
+.DS_Store
+.cache/*
+.vscode/*
diff --git a/include/Block.h b/include/Block.h
index 23accce..034e68c 100644
--- a/include/Block.h
+++ b/include/Block.h
@@ -436,7 +436,7 @@ public:
         double distMul = double(1) / MSE2SSE;
         dist_type thSSE = static_cast<dist_type>(thMSE * MSE2SSE);
 
-#if defined(__SSE2__)
+#if defined(__SSE2__) || defined(__ARM_NEON__)
         static const ptrdiff_t simd_step = 8;
         const ptrdiff_t simd_residue = Width() % simd_step;
         const ptrdiff_t simd_width = Width() - simd_residue;
@@ -494,6 +494,56 @@ public:
                         dist += ssum_f32[0] + ssum_f32[1] + ssum_f32[2] + ssum_f32[3];
                     }
 
+                    if (simd_residue > 0)
+                    {
+                        auto refp = refp0 + simd_width;
+                        auto srcp = srcp0 + simd_width;
+
+                        for (PCType y = 0; y < Height(); ++y)
+                        {
+                            for (const auto upper = refp + simd_residue; refp < upper; ++refp, ++srcp)
+                            {
+                                dist_type temp = static_cast<dist_type>(*refp) - static_cast<dist_type>(*srcp);
+                                dist += temp * temp;
+                            }
+
+                            refp += simd_width;
+                            srcp += src_stride2;
+                        }
+                    }
+#elif defined(__ARM_NEON__)
+                    if (simd_width > 0)
+                    {
+                        auto refp = refp0;
+                        auto srcp = srcp0;
+
+                        float32x4_t ssum = vdupq_n_f32(0.0f);
+
+                        for (PCType y = 0; y < Height(); ++y)
+                        {
+                            for (const auto upper = refp + simd_width; refp < upper; refp += simd_step, srcp += simd_step)
+                            {
+                                const float32x4_t r1 = vld1q_f32(refp);
+                                const float32x4_t r2 = vld1q_f32(refp + 4);
+                                const float32x4_t s1 = vld1q_f32(srcp);
+                                const float32x4_t s2 = vld1q_f32(srcp + 4);
+                                const float32x4_t d1 = vsubq_f32(r1, s1);
+                                const float32x4_t d2 = vsubq_f32(r2, s2);
+                                const float32x4_t d1sqr = vmulq_f32(d1, d1);
+                                const float32x4_t d2sqr = vmulq_f32(d2, d2);
+                                ssum = vaddq_f32(ssum, d1sqr);
+                                ssum = vaddq_f32(ssum, d2sqr);
+                            }
+
+                            refp += simd_residue;
+                            srcp += src_stride1;
+                        }
+
+                        alignas(16) FLType ssum_f32[4];
+                        vst1q_f32(ssum_f32, ssum);
+                        dist += ssum_f32[0] + ssum_f32[1] + ssum_f32[2] + ssum_f32[3];
+                    }
+
                     if (simd_residue > 0)
                     {
                         auto refp = refp0 + simd_width;
@@ -562,7 +612,7 @@ public:
         size_t index = match_code.size();
         match_code.resize(index + search_pos.size());
 
-#if defined(__SSE2__)
+#if defined(__SSE2__) || defined(__ARM_NEON__)
         static const ptrdiff_t simd_step = 8;
         const ptrdiff_t simd_residue = Width() % simd_step;
         const ptrdiff_t simd_width = Width() - simd_residue;
@@ -625,6 +675,50 @@ public:
                         dist += temp * temp;
                     }
 
+                    refp += simd_width;
+                    srcp += src_stride2;
+                }
+            }
+#elif defined(__ARM_NEON__)
+            if (simd_width > 0) {
+                auto refp = refp0;
+                auto srcp = srcp0;
+
+                float32x4_t ssum = vdupq_n_f32(0.0f);
+
+                for (PCType y = 0; y < Height(); ++y) {
+                    for (const auto upper = refp + simd_width; refp < upper; refp += simd_step, srcp += simd_step) {
+                        const float32x4_t r1 = vld1q_f32(refp);
+                        const float32x4_t r2 = vld1q_f32(refp + 4);
+                        const float32x4_t s1 = vld1q_f32(srcp);
+                        const float32x4_t s2 = vld1q_f32(srcp + 4);
+                        const float32x4_t d1 = vsubq_f32(r1, s1);
+                        const float32x4_t d2 = vsubq_f32(r2, s2);
+                        const float32x4_t d1sqr = vmulq_f32(d1, d1);
+                        const float32x4_t d2sqr = vmulq_f32(d2, d2);
+                        ssum = vaddq_f32(ssum, d1sqr);
+                        ssum = vaddq_f32(ssum, d2sqr);
+                    }
+
+                    refp += simd_residue;
+                    srcp += src_stride1;
+                }
+
+                alignas(16) FLType ssum_f32[4];
+                vst1q_f32(ssum_f32, ssum);
+                dist += ssum_f32[0] + ssum_f32[1] + ssum_f32[2] + ssum_f32[3];
+            }
+
+            if (simd_residue > 0) {
+                auto refp = refp0 + simd_width;
+                auto srcp = srcp0 + simd_width;
+
+                for (PCType y = 0; y < Height(); ++y) {
+                    for (const auto upper = refp + simd_residue; refp < upper; ++refp, ++srcp) {
+                        dist_type temp = static_cast<dist_type>(*refp) - static_cast<dist_type>(*srcp);
+                        dist += temp * temp;
+                    }
+
                     refp += simd_width;
                     srcp += src_stride2;
                 }
@@ -790,7 +884,7 @@ public:
         PCType range, PCType step = 1) const
     {
         range = range / step * step;
-        
+
         PosCode new_search_pos((range / step * 2 + 1) * (range / step * 2 + 1));
         size_t index = 0;
         AddSearchPos(new_search_pos, index, ref_pos, src_height, src_width, range, step);
diff --git a/include/Helper.h b/include/Helper.h
index 2c18832..c698ad1 100644
--- a/include/Helper.h
+++ b/include/Helper.h
@@ -41,8 +41,9 @@
 ////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
 // Instruction intrinsics
 
-
-#if defined(__AVX2__) || defined(__AVX__)
+#if defined(__ARM_NEON__)
+#include <arm_neon.h>
+#elif defined(__AVX2__) || defined(__AVX__)
 #include <immintrin.h>
 #elif defined(__SSE4_2__)
 #include <nmmintrin.h>
@@ -807,4 +808,4 @@ protected:
 ////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
 
 
-#endif
\ No newline at end of file
+#endif
diff --git a/meson.build b/meson.build
index 66f9131..91262a6 100644
--- a/meson.build
+++ b/meson.build
@@ -1,10 +1,10 @@
 project('BM3D', 'cpp',
-  default_options : ['buildtype=release', 'b_ndebug=if-release', 'cpp_std=c++14'],
+  default_options : ['buildtype=release', 'b_ndebug=if-release', 'cpp_std=c++14', 'b_lto=true'],
   meson_version : '>=0.48.0',
   version : '8'
 )
 
-add_project_arguments('-Wno-unused-local-typedefs', language : 'cpp')
+add_project_arguments('-Wno-unused-local-typedefs', '-funroll-loops', language : 'cpp')
 
 sources = [
   'include/Block.h',
diff --git a/source/BM3D_Basic.cpp b/source/BM3D_Basic.cpp
index d205b57..0126050 100644
--- a/source/BM3D_Basic.cpp
+++ b/source/BM3D_Basic.cpp
@@ -118,6 +118,33 @@ void BM3D_Basic_Process::CollaborativeFilter(int plane,
 
     alignas(16) int32_t cmp_sum_i32[4];
     _mm_store_si128(reinterpret_cast<__m128i *>(cmp_sum_i32), cmp_sum);
+    retainedCoefs += cmp_sum_i32[0] + cmp_sum_i32[1] + cmp_sum_i32[2] + cmp_sum_i32[3];
+#elif defined(__ARM_NEON__)
+    static const uint32x4_t abs_mask = vdupq_n_u32(0x7FFFFFFF);
+    static const ptrdiff_t simd_step = 4;
+    const ptrdiff_t simd_residue = srcGroup.size() % simd_step;
+    const ptrdiff_t simd_width = srcGroup.size() - simd_residue;
+
+    int32x4_t cmp_sum = vdupq_n_s32(0);
+
+    for (const auto upper1 = srcp + simd_width; srcp < upper1; srcp += simd_step, thrp += simd_step) {
+        const float32x4_t s1 = vld1q_f32(srcp);
+        const float32x4_t t1 = vld1q_f32(thrp);
+
+        const uint32x4_t s1abs = vandq_u32(vreinterpretq_u32_f32(s1), abs_mask);
+
+        const uint32x4_t cmp = vcgtq_f32(vreinterpretq_f32_u32(s1abs), t1);
+
+        const float32x4_t d1 = vbslq_f32(cmp, s1, vdupq_n_f32(0));
+
+        vst1q_f32(const_cast<float*>(srcp), d1);
+
+        cmp_sum = vsubq_s32(cmp_sum, vreinterpretq_s32_u32(cmp));
+    }
+
+    alignas(16) int32_t cmp_sum_i32[4];
+    vst1q_s32(cmp_sum_i32, cmp_sum);
+
     retainedCoefs += cmp_sum_i32[0] + cmp_sum_i32[1] + cmp_sum_i32[2] + cmp_sum_i32[3];
 #endif
 
diff --git a/source/BM3D_Final.cpp b/source/BM3D_Final.cpp
index 5f9acae..1684b94 100644
--- a/source/BM3D_Final.cpp
+++ b/source/BM3D_Final.cpp
@@ -101,6 +101,37 @@ void BM3D_Final_Process::CollaborativeFilter(int plane,
     alignas(16) FLType l2wiener_sum_f32[4];
     _mm_store_ps(l2wiener_sum_f32, l2wiener_sum);
     L2Wiener += l2wiener_sum_f32[0] + l2wiener_sum_f32[1] + l2wiener_sum_f32[2] + l2wiener_sum_f32[3];
+#elif defined(__ARM_NEON__)
+    static const ptrdiff_t simd_step = 4;
+    const ptrdiff_t simd_residue = srcGroup.size() % simd_step;
+    const ptrdiff_t simd_width = srcGroup.size() - simd_residue;
+
+    const float32x4_t sgm_sqr = vdupq_n_f32(sigmaSquare);
+    float32x4_t l2wiener_sum = vdupq_n_f32(0.0f);
+
+    for (const auto upper1 = srcp + simd_width; srcp < upper1; srcp += simd_step, refp += simd_step)
+    {
+        const float32x4_t s1 = vld1q_f32(srcp);
+        const float32x4_t r1 = vld1q_f32(refp);
+        const float32x4_t r1sqr = vmulq_f32(r1, r1);
+
+        const float32x4_t denom = vaddq_f32(r1sqr, sgm_sqr);
+        float32x4_t recip = vrecpeq_f32(denom);
+
+        recip = vmulq_f32(vrecpsq_f32(denom, recip), recip);
+
+        recip = vmulq_f32(vrecpsq_f32(denom, recip), recip);
+
+        const float32x4_t wiener = vmulq_f32(r1sqr, recip);
+
+        const float32x4_t d1 = vmulq_f32(s1, wiener);
+        vst1q_f32(srcp, d1);
+        l2wiener_sum = vaddq_f32(l2wiener_sum, vmulq_f32(wiener, wiener));
+    }
+
+    float l2wiener_sum_f32[4];
+    vst1q_f32(l2wiener_sum_f32, l2wiener_sum);
+    L2Wiener += l2wiener_sum_f32[0] + l2wiener_sum_f32[1] + l2wiener_sum_f32[2] + l2wiener_sum_f32[3];
 #endif
 
     for (; srcp < upper; ++srcp, ++refp)
diff --git a/source/VBM3D_Basic.cpp b/source/VBM3D_Basic.cpp
index fea20e7..6cc85c8 100644
--- a/source/VBM3D_Basic.cpp
+++ b/source/VBM3D_Basic.cpp
@@ -118,6 +118,33 @@ void VBM3D_Basic_Process::CollaborativeFilter(int plane,
 
     alignas(16) int32_t cmp_sum_i32[4];
     _mm_store_si128(reinterpret_cast<__m128i *>(cmp_sum_i32), cmp_sum);
+    retainedCoefs += cmp_sum_i32[0] + cmp_sum_i32[1] + cmp_sum_i32[2] + cmp_sum_i32[3];
+#elif defined(__ARM_NEON__)
+    static const uint32x4_t abs_mask = vdupq_n_u32(0x7FFFFFFF);
+    static const ptrdiff_t simd_step = 4;
+    const ptrdiff_t simd_residue = srcGroup.size() % simd_step;
+    const ptrdiff_t simd_width = srcGroup.size() - simd_residue;
+
+    int32x4_t cmp_sum = vdupq_n_s32(0);
+
+    for (const auto upper1 = srcp + simd_width; srcp < upper1; srcp += simd_step, thrp += simd_step) {
+        const float32x4_t s1 = vld1q_f32(srcp);
+        const float32x4_t t1 = vld1q_f32(thrp);
+
+        const uint32x4_t s1_abs = vandq_u32(vreinterpretq_u32_f32(s1), abs_mask);
+
+        const uint32x4_t cmp = vcgtq_f32(vreinterpretq_f32_u32(s1_abs), t1);
+
+        const float32x4_t d1 = vbslq_f32(cmp, s1, vdupq_n_f32(0));
+
+        vst1q_f32(const_cast<float*>(srcp), d1);
+
+        cmp_sum = vsubq_s32(cmp_sum, vreinterpretq_s32_u32(cmp));
+    }
+
+    alignas(16) int32_t cmp_sum_i32[4];
+    vst1q_s32(cmp_sum_i32, cmp_sum);
+
     retainedCoefs += cmp_sum_i32[0] + cmp_sum_i32[1] + cmp_sum_i32[2] + cmp_sum_i32[3];
 #endif
 
diff --git a/source/VBM3D_Final.cpp b/source/VBM3D_Final.cpp
index afd149d..01db9a8 100644
--- a/source/VBM3D_Final.cpp
+++ b/source/VBM3D_Final.cpp
@@ -100,6 +100,42 @@ void VBM3D_Final_Process::CollaborativeFilter(int plane,
 
     alignas(16) FLType l2wiener_sum_f32[4];
     _mm_store_ps(l2wiener_sum_f32, l2wiener_sum);
+    L2Wiener += l2wiener_sum_f32[0] + l2wiener_sum_f32[1] + l2wiener_sum_f32[2] + l2wiener_sum_f32[3];
+#elif defined(__ARM_NEON__)
+    static const ptrdiff_t simd_step = 4;
+    const ptrdiff_t simd_residue = srcGroup.size() % simd_step;
+    const ptrdiff_t simd_width = srcGroup.size() - simd_residue;
+
+    const float32x4_t sgm_sqr = vdupq_n_f32(sigmaSquare);
+    float32x4_t l2wiener_sum = vdupq_n_f32(0.0f);
+
+    for (const auto upper1 = srcp + simd_width; srcp < upper1; srcp += simd_step, refp += simd_step) {
+        const float32x4_t s1 = vld1q_f32(srcp);
+        const float32x4_t r1 = vld1q_f32(refp);
+
+        const float32x4_t r1sqr = vmulq_f32(r1, r1);
+
+        const float32x4_t denom = vaddq_f32(r1sqr, sgm_sqr);
+        float32x4_t recip = vrecpeq_f32(denom);
+
+        // First refinement
+        recip = vmulq_f32(vrecpsq_f32(denom, recip), recip);
+
+        // Second refinement for higher precision
+        recip = vmulq_f32(vrecpsq_f32(denom, recip), recip);
+
+        const float32x4_t wiener = vmulq_f32(r1sqr, recip);
+
+        const float32x4_t d1 = vmulq_f32(s1, wiener);
+
+        vst1q_f32(const_cast<float*>(srcp), d1);
+
+        l2wiener_sum = vaddq_f32(l2wiener_sum, vmulq_f32(wiener, wiener));
+    }
+
+    float l2wiener_sum_f32[4];
+    vst1q_f32(l2wiener_sum_f32, l2wiener_sum);
+
     L2Wiener += l2wiener_sum_f32[0] + l2wiener_sum_f32[1] + l2wiener_sum_f32[2] + l2wiener_sum_f32[3];
 #endif
 
