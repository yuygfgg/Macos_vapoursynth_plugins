diff --git a/.DS_Store b/.DS_Store
new file mode 100644
index 0000000..224553f
Binary files /dev/null and b/.DS_Store differ
diff --git a/EEDI3/.DS_Store b/EEDI3/.DS_Store
new file mode 100644
index 0000000..89c7627
Binary files /dev/null and b/EEDI3/.DS_Store differ
diff --git a/EEDI3/vectorclass/LICENSE b/EEDI3/vectorclass/LICENSE
new file mode 100644
index 0000000..fd2deda
--- /dev/null
+++ b/EEDI3/vectorclass/LICENSE
@@ -0,0 +1,191 @@
+                                 Apache License
+                           Version 2.0, January 2004
+                        http://www.apache.org/licenses/
+
+   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
+
+   1. Definitions.
+
+      "License" shall mean the terms and conditions for use, reproduction,
+      and distribution as defined by Sections 1 through 9 of this document.
+
+      "Licensor" shall mean the copyright owner or entity authorized by
+      the copyright owner that is granting the License.
+
+      "Legal Entity" shall mean the union of the acting entity and all
+      other entities that control, are controlled by, or are under common
+      control with that entity. For the purposes of this definition,
+      "control" means (i) the power, direct or indirect, to cause the
+      direction or management of such entity, whether by contract or
+      otherwise, or (ii) ownership of fifty percent (50%) or more of the
+      outstanding shares, or (iii) beneficial ownership of such entity.
+
+      "You" (or "Your") shall mean an individual or Legal Entity
+      exercising permissions granted by this License.
+
+      "Source" form shall mean the preferred form for making modifications,
+      including but not limited to software source code, documentation
+      source, and configuration files.
+
+      "Object" form shall mean any form resulting from mechanical
+      transformation or translation of a Source form, including but
+      not limited to compiled object code, generated documentation,
+      and conversions to other media types.
+
+      "Work" shall mean the work of authorship, whether in Source or
+      Object form, made available under the License, as indicated by a
+      copyright notice that is included in or attached to the work
+      (an example is provided in the Appendix below).
+
+      "Derivative Works" shall mean any work, whether in Source or Object
+      form, that is based on (or derived from) the Work and for which the
+      editorial revisions, annotations, elaborations, or other modifications
+      represent, as a whole, an original work of authorship. For the purposes
+      of this License, Derivative Works shall not include works that remain
+      separable from, or merely link (or bind by name) to the interfaces of,
+      the Work and Derivative Works thereof.
+
+      "Contribution" shall mean any work of authorship, including
+      the original version of the Work and any modifications or additions
+      to that Work or Derivative Works thereof, that is intentionally
+      submitted to Licensor for inclusion in the Work by the copyright owner
+      or by an individual or Legal Entity authorized to submit on behalf of
+      the copyright owner. For the purposes of this definition, "submitted"
+      means any form of electronic, verbal, or written communication sent
+      to the Licensor or its representatives, including but not limited to
+      communication on electronic mailing lists, source code control systems,
+      and issue tracking systems that are managed by, or on behalf of, the
+      Licensor for the purpose of discussing and improving the Work, but
+      excluding communication that is conspicuously marked or otherwise
+      designated in writing by the copyright owner as "Not a Contribution."
+
+      "Contributor" shall mean Licensor and any individual or Legal Entity
+      on behalf of whom a Contribution has been received by Licensor and
+      subsequently incorporated within the Work.
+
+   2. Grant of Copyright License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      copyright license to reproduce, prepare Derivative Works of,
+      publicly display, publicly perform, sublicense, and distribute the
+      Work and such Derivative Works in Source or Object form.
+
+   3. Grant of Patent License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      (except as stated in this section) patent license to make, have made,
+      use, offer to sell, sell, import, and otherwise transfer the Work,
+      where such license applies only to those patent claims licensable
+      by such Contributor that are necessarily infringed by their
+      Contribution(s) alone or by combination of their Contribution(s)
+      with the Work to which such Contribution(s) was submitted. If You
+      institute patent litigation against any entity (including a
+      cross-claim or counterclaim in a lawsuit) alleging that the Work
+      or a Contribution incorporated within the Work constitutes direct
+      or contributory patent infringement, then any patent licenses
+      granted to You under this License for that Work shall terminate
+      as of the date such litigation is filed.
+
+   4. Redistribution. You may reproduce and distribute copies of the
+      Work or Derivative Works thereof in any medium, with or without
+      modifications, and in Source or Object form, provided that You
+      meet the following conditions:
+
+      (a) You must give any other recipients of the Work or
+          Derivative Works a copy of this License; and
+
+      (b) You must cause any modified files to carry prominent notices
+          stating that You changed the files; and
+
+      (c) You must retain, in the Source form of any Derivative Works
+          that You distribute, all copyright, patent, trademark, and
+          attribution notices from the Source form of the Work,
+          excluding those notices that do not pertain to any part of
+          the Derivative Works; and
+
+      (d) If the Work includes a "NOTICE" text file as part of its
+          distribution, then any Derivative Works that You distribute must
+          include a readable copy of the attribution notices contained
+          within such NOTICE file, excluding those notices that do not
+          pertain to any part of the Derivative Works, in at least one
+          of the following places: within a NOTICE text file distributed
+          as part of the Derivative Works; within the Source form or
+          documentation, if provided along with the Derivative Works; or,
+          within a display generated by the Derivative Works, if and
+          wherever such third-party notices normally appear. The contents
+          of the NOTICE file are for informational purposes only and
+          do not modify the License. You may add Your own attribution
+          notices within Derivative Works that You distribute, alongside
+          or as an addendum to the NOTICE text from the Work, provided
+          that such additional attribution notices cannot be construed
+          as modifying the License.
+
+      You may add Your own copyright statement to Your modifications and
+      may provide additional or different license terms and conditions
+      for use, reproduction, or distribution of Your modifications, or
+      for any such Derivative Works as a whole, provided Your use,
+      reproduction, and distribution of the Work otherwise complies with
+      the conditions stated in this License.
+
+   5. Submission of Contributions. Unless You explicitly state otherwise,
+      any Contribution intentionally submitted for inclusion in the Work
+      by You to the Licensor shall be under the terms and conditions of
+      this License, without any additional terms or conditions.
+      Notwithstanding the above, nothing herein shall supersede or modify
+      the terms of any separate license agreement you may have executed
+      with Licensor regarding such Contributions.
+
+   6. Trademarks. This License does not grant permission to use the trade
+      names, trademarks, service marks, or product names of the Licensor,
+      except as required for reasonable and customary use in describing the
+      origin of the Work and reproducing the content of the NOTICE file.
+
+   7. Disclaimer of Warranty. Unless required by applicable law or
+      agreed to in writing, Licensor provides the Work (and each
+      Contributor provides its Contributions) on an "AS IS" BASIS,
+      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+      implied, including, without limitation, any warranties or conditions
+      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
+      PARTICULAR PURPOSE. You are solely responsible for determining the
+      appropriateness of using or redistributing the Work and assume any
+      risks associated with Your exercise of permissions under this License.
+
+   8. Limitation of Liability. In no event and under no legal theory,
+      whether in tort (including negligence), contract, or otherwise,
+      unless required by applicable law (such as deliberate and grossly
+      negligent acts) or agreed to in writing, shall any Contributor be
+      liable to You for damages, including any direct, indirect, special,
+      incidental, or consequential damages of any character arising as a
+      result of this License or out of the use or inability to use the
+      Work (including but not limited to damages for loss of goodwill,
+      work stoppage, computer failure or malfunction, or any and all
+      other commercial damages or losses), even if such Contributor
+      has been advised of the possibility of such damages.
+
+   9. Accepting Warranty or Additional Liability. While redistributing
+      the Work or Derivative Works thereof, You may choose to offer,
+      and charge a fee for, acceptance of support, warranty, indemnity,
+      or other liability obligations and/or rights consistent with this
+      License. However, in accepting such obligations, You may act only
+      on Your own behalf and on Your sole responsibility, not on behalf
+      of any other Contributor, and only if You agree to indemnify,
+      defend, and hold each Contributor harmless for any liability
+      incurred by, or claims asserted against, such Contributor by reason
+      of your accepting any such warranty or additional liability.
+
+   END OF TERMS AND CONDITIONS
+
+  
+   Copyright 2012-2019 Agner Fog.
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+   Unless required by applicable law or agreed to in writing, software
+   distributed under the License is distributed on an "AS IS" BASIS,
+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+   See the License for the specific language governing permissions and
+   limitations under the License.
diff --git a/EEDI3/vectorclass/README.md b/EEDI3/vectorclass/README.md
new file mode 100644
index 0000000..d59b92f
--- /dev/null
+++ b/EEDI3/vectorclass/README.md
@@ -0,0 +1,14 @@
+# version2
+Vector Class Library, latest version
+
+This is a C++ class library for using the Single Instruction Multiple Data (SIMD) instructions to improve performance on modern microprocessors with the x86 or x86/64 instruction set on Windows, Linux, and Mac platforms. There are no plans to support ARM or other instruction sets.
+
+[Latest release](https://github.com/vectorclass/version2/releases)
+
+[Download manual](https://github.com/vectorclass/manual/raw/master/vcl_manual.pdf)
+
+[Add-on packages for particular applications](https://github.com/vectorclass/add-on)
+
+[Getting-started video.](https://www.youtube.com/watch?v=TKjYdLIMTrI) Video blogger Christopher Rose has made this nice video telling how to get started with the Vector Class Library.
+
+**Help:** You may ask for programming help on [StackOverflow](https://stackoverflow.com) using the tag vector-class-library.
diff --git a/EEDI3/vectorclass/instrset.h b/EEDI3/vectorclass/instrset.h
index 9578147..2040255 100644
--- a/EEDI3/vectorclass/instrset.h
+++ b/EEDI3/vectorclass/instrset.h
@@ -28,29 +28,7 @@
 // Find instruction set from compiler macros if INSTRSET not defined
 // Note: Most of these macros are not defined in Microsoft compilers
 #ifndef INSTRSET
-#if defined ( __AVX512F__ ) || defined ( __AVX512__ )
-#define INSTRSET 9
-#elif defined ( __AVX2__ )
-#define INSTRSET 8
-#elif defined ( __AVX__ )
-#define INSTRSET 7
-#elif defined ( __SSE4_2__ )
 #define INSTRSET 6
-#elif defined ( __SSE4_1__ )
-#define INSTRSET 5
-#elif defined ( __SSSE3__ )
-#define INSTRSET 4
-#elif defined ( __SSE3__ )
-#define INSTRSET 3
-#elif defined ( __SSE2__ ) || defined ( __x86_64__ )
-#define INSTRSET 2
-#elif defined ( __SSE__ )
-#define INSTRSET 1
-#elif defined ( _M_IX86_FP )           // Defined in MS compiler. 1: SSE, 2: SSE2
-#define INSTRSET _M_IX86_FP
-#else 
-#define INSTRSET 0
-#endif // instruction set defines
 #endif // INSTRSET
 
 // Include the appropriate header file for intrinsic functions
@@ -65,7 +43,7 @@
 #elif INSTRSET == 7
 #include <immintrin.h>                 // AVX
 #elif INSTRSET == 6
-#include <nmmintrin.h>                 // SSE4.2
+#include "sse2neon.h"                 // SSE4.2
 #elif INSTRSET == 5
 #include <smmintrin.h>                 // SSE4.1
 #elif INSTRSET == 4
@@ -160,10 +138,10 @@
 namespace VCL_NAMESPACE {
 #endif
     int  instrset_detect(void);                      // tells which instruction sets are supported
-    bool hasFMA3(void);                              // true if FMA3 instructions supported
-    bool hasFMA4(void);                              // true if FMA4 instructions supported
-    bool hasXOP(void);                               // true if XOP  instructions supported
-    bool hasAVX512ER(void);                          // true if AVX512ER instructions supported
+    // bool hasFMA3(void);                              // true if FMA3 instructions supported
+    // bool hasFMA4(void);                              // true if FMA4 instructions supported
+    // bool hasXOP(void);                               // true if XOP  instructions supported
+    // bool hasAVX512ER(void);                          // true if AVX512ER instructions supported
 #ifdef VCL_NAMESPACE
 }
 #endif
diff --git a/EEDI3/vectorclass/instrset_detect.cpp b/EEDI3/vectorclass/instrset_detect.cpp
index 600b9e1..add0ba7 100644
--- a/EEDI3/vectorclass/instrset_detect.cpp
+++ b/EEDI3/vectorclass/instrset_detect.cpp
@@ -1,14 +1,15 @@
 /**************************  instrset_detect.cpp   ****************************
 * Author:        Agner Fog
 * Date created:  2012-05-30
-* Last modified: 2017-05-02
-* Version:       1.28
-* Project:       vector classes
+* Last modified: 2022-07-20
+* Version:       2.02.00
+* Project:       vector class library
 * Description:
 * Functions for checking which instruction sets are supported.
 *
-* (c) Copyright 2012-2017 GNU General Public License http://www.gnu.org/licenses
-\*****************************************************************************/
+* (c) Copyright 2012-2022 Agner Fog.
+* Apache License version 2.0 or later.
+******************************************************************************/
 
 #include "instrset.h"
 
@@ -16,53 +17,22 @@
 namespace VCL_NAMESPACE {
 #endif
 
-// Define interface to cpuid instruction.
-// input:  eax = functionnumber, ecx = 0
-// output: eax = output[0], ebx = output[1], ecx = output[2], edx = output[3]
-static inline void cpuid (int output[4], int functionnumber) {	
-#if defined(__GNUC__) || defined(__clang__)              // use inline assembly, Gnu/AT&T syntax
-
-   int a, b, c, d;
-   __asm("cpuid" : "=a"(a),"=b"(b),"=c"(c),"=d"(d) : "a"(functionnumber),"c"(0) : );
-   output[0] = a;
-   output[1] = b;
-   output[2] = c;
-   output[3] = d;
-
-#elif defined (_MSC_VER) || defined (__INTEL_COMPILER)     // Microsoft or Intel compiler, intrin.h included
-
-    __cpuidex(output, functionnumber, 0);                  // intrinsic function for CPUID
-
-#else                                                      // unknown platform. try inline assembly with masm/intel syntax
-
-    __asm {
-        mov eax, functionnumber
-        xor ecx, ecx
-        cpuid;
-        mov esi, output
-        mov [esi],    eax
-        mov [esi+4],  ebx
-        mov [esi+8],  ecx
-        mov [esi+12], edx
-    }
-
-#endif
-}
 
+#ifdef __x86_64__
 // Define interface to xgetbv instruction
-static inline int64_t xgetbv (int ctr) {	
-#if (defined (_MSC_FULL_VER) && _MSC_FULL_VER >= 160040000) || (defined (__INTEL_COMPILER) && __INTEL_COMPILER >= 1200) // Microsoft or Intel compiler supporting _xgetbv intrinsic
+static inline uint64_t xgetbv (int ctr) {
+#if (defined (_MSC_FULL_VER) && _MSC_FULL_VER >= 160040000) || (defined (__INTEL_COMPILER) && __INTEL_COMPILER >= 1200)
+    // Microsoft or Intel compiler supporting _xgetbv intrinsic
 
-    return _xgetbv(ctr);                                   // intrinsic function for XGETBV
+    return uint64_t(_xgetbv(ctr));                    // intrinsic function for XGETBV
 
-#elif defined(__GNUC__)                                    // use inline assembly, Gnu/AT&T syntax
+#elif defined(__GNUC__) ||  defined (__clang__)       // use inline assembly, Gnu/AT&T syntax
 
    uint32_t a, d;
    __asm("xgetbv" : "=a"(a),"=d"(d) : "c"(ctr) : );
    return a | (uint64_t(d) << 32);
 
-#else  // #elif defined (_WIN32)                           // other compiler. try inline assembly with masm/intel/MS syntax
-
+#else  // #elif defined (_WIN32)                      // other compiler. try inline assembly with masm/intel/MS syntax
    uint32_t a, d;
     __asm {
         mov ecx, ctr
@@ -76,12 +46,12 @@ static inline int64_t xgetbv (int ctr) {
 
 #endif
 }
-
+#endif
 
 /* find supported instruction set
     return value:
     0           = 80386 instruction set
-    1  or above = SSE (XMM) supported by CPU (not testing for O.S. support)
+    1  or above = SSE (XMM) supported by CPU (not testing for OS support)
     2  or above = SSE2
     3  or above = SSE3
     4  or above = Supplementary SSE3 (SSSE3)
@@ -90,8 +60,7 @@ static inline int64_t xgetbv (int ctr) {
     7  or above = AVX supported by CPU and operating system
     8  or above = AVX2
     9  or above = AVX512F
-    10 or above = AVX512VL
-    11 or above = AVX512BW, AVX512DQ
+   10  or above = AVX512VL, AVX512BW, AVX512DQ
 */
 int instrset_detect(void) {
 
@@ -100,6 +69,11 @@ int instrset_detect(void) {
         return iset;                                       // called before
     }
     iset = 0;                                              // default value
+#if defined(__aarch64__) || defined(__arm__)
+    // Assume NEON support on ARM
+    iset = 6;  // Simulate support for SSE4.2 using NEON
+    return iset;
+#else
     int abcd[4] = {0,0,0,0};                               // cpuid results
     cpuid(abcd, 0);                                        // call cpuid function 0
     if (abcd[0] == 0) return iset;                         // no further cpuid function supported
@@ -131,54 +105,79 @@ int instrset_detect(void) {
     if ((abcd[1] & (1 << 16)) == 0) return iset;           // no AVX512
     cpuid(abcd, 0xD);                                      // call cpuid leaf 0xD for feature flags
     if ((abcd[0] & 0x60) != 0x60)   return iset;           // no AVX512
-    iset = 9; 
+    iset = 9;
     cpuid(abcd, 7);                                        // call cpuid leaf 7 for feature flags
     if ((abcd[1] & (1 << 31)) == 0) return iset;           // no AVX512VL
-    iset = 10; 
     if ((abcd[1] & 0x40020000) != 0x40020000) return iset; // no AVX512BW, AVX512DQ
-    iset = 11; 
+    iset = 10;
+#endif
     return iset;
 }
 
-// detect if CPU supports the FMA3 instruction set
-bool hasFMA3(void) {
-    if (instrset_detect() < 7) return false;               // must have AVX
-    int abcd[4];                                           // cpuid results
-    cpuid(abcd, 1);                                        // call cpuid function 1
-    return ((abcd[2] & (1 << 12)) != 0);                   // ecx bit 12 indicates FMA3
-}
-
-// detect if CPU supports the FMA4 instruction set
-bool hasFMA4(void) {
-    if (instrset_detect() < 7) return false;               // must have AVX
-    int abcd[4];                                           // cpuid results
-    cpuid(abcd, 0x80000001);                               // call cpuid function 0x80000001
-    return ((abcd[2] & (1 << 16)) != 0);                   // ecx bit 16 indicates FMA4
-}
 
-// detect if CPU supports the XOP instruction set
-bool hasXOP(void) {
-    if (instrset_detect() < 7) return false;               // must have AVX
-    int abcd[4];                                           // cpuid results
-    cpuid(abcd, 0x80000001);                               // call cpuid function 0x80000001
-    return ((abcd[2] & (1 << 11)) != 0);                   // ecx bit 11 indicates XOP
-}
-
-// detect if CPU supports the F16C instruction set
-bool hasF16C(void) {
-    if (instrset_detect() < 7) return false;               // must have AVX
-    int abcd[4];                                           // cpuid results
-    cpuid(abcd, 1);                                        // call cpuid function 1
-    return ((abcd[2] & (1 << 29)) != 0);                   // ecx bit 29 indicates F16C
-}
-
-// detect if CPU supports the AVX512ER instruction set
-bool hasAVX512ER(void) {
-    if (instrset_detect() < 9) return false;               // must have AVX512F
-    int abcd[4];                                           // cpuid results
-    cpuid(abcd, 7);                                        // call cpuid function 7
-    return ((abcd[1] & (1 << 27)) != 0);                   // ebx bit 27 indicates AVX512ER
-}
+// // detect if CPU supports the FMA3 instruction set
+// bool hasFMA3(void) {
+//     if (instrset_detect() < 7) return false;               // must have AVX
+//     int abcd[4];                                           // cpuid results
+//     cpuid(abcd, 1);                                        // call cpuid function 1
+//     return ((abcd[2] & (1 << 12)) != 0);                   // ecx bit 12 indicates FMA3
+// }
+
+// // detect if CPU supports the FMA4 instruction set
+// bool hasFMA4(void) {
+//     if (instrset_detect() < 7) return false;               // must have AVX
+//     int abcd[4];                                           // cpuid results
+//     cpuid(abcd, 0x80000001);                               // call cpuid function 0x80000001
+//     return ((abcd[2] & (1 << 16)) != 0);                   // ecx bit 16 indicates FMA4
+// }
+
+// // detect if CPU supports the XOP instruction set
+// bool hasXOP(void) {
+//     if (instrset_detect() < 7) return false;               // must have AVX
+//     int abcd[4];                                           // cpuid results
+//     cpuid(abcd, 0x80000001);                               // call cpuid function 0x80000001
+//     return ((abcd[2] & (1 << 11)) != 0);                   // ecx bit 11 indicates XOP
+// }
+
+// // detect if CPU supports the AVX512ER instruction set
+// bool hasAVX512ER(void) {
+//     if (instrset_detect() < 9) return false;               // must have AVX512F
+//     int abcd[4];                                           // cpuid results
+//     cpuid(abcd, 7);                                        // call cpuid function 7
+//     return ((abcd[1] & (1 << 27)) != 0);                   // ebx bit 27 indicates AVX512ER
+// }
+
+// // detect if CPU supports the AVX512VBMI instruction set
+// bool hasAVX512VBMI(void) {
+//     if (instrset_detect() < 10) return false;              // must have AVX512BW
+//     int abcd[4];                                           // cpuid results
+//     cpuid(abcd, 7);                                        // call cpuid function 7
+//     return ((abcd[2] & (1 << 1)) != 0);                    // ecx bit 1 indicates AVX512VBMI
+// }
+
+// // detect if CPU supports the AVX512VBMI2 instruction set
+// bool hasAVX512VBMI2(void) {
+//     if (instrset_detect() < 10) return false;              // must have AVX512BW
+//     int abcd[4];                                           // cpuid results
+//     cpuid(abcd, 7);                                        // call cpuid function 7
+//     return ((abcd[2] & (1 << 6)) != 0);                    // ecx bit 6 indicates AVX512VBMI2
+// }
+
+// // detect if CPU supports the F16C instruction set
+// bool hasF16C(void) {
+//     if (instrset_detect() < 7) return false;               // must have AVX
+//     int abcd[4];                                           // cpuid results
+//     cpuid(abcd, 1);                                        // call cpuid function 1
+//     return ((abcd[2] & (1 << 29)) != 0);                   // ecx bit 29 indicates F16C
+// }
+
+// // detect if CPU supports the AVX512_FP16 instruction set
+// bool hasAVX512FP16(void) {
+//     if (instrset_detect() < 10) return false;              // must have AVX512
+//     int abcd[4];                                           // cpuid results
+//     cpuid(abcd, 7);                                        // call cpuid function 1
+//     return ((abcd[3] & (1 << 23)) != 0);                   // edx bit 23 indicates AVX512_FP16
+// }
 
 
 #ifdef VCL_NAMESPACE
diff --git a/EEDI3/vectorclass/license.txt b/EEDI3/vectorclass/license.txt
deleted file mode 100644
index bc08fe2..0000000
--- a/EEDI3/vectorclass/license.txt
+++ /dev/null
@@ -1,619 +0,0 @@
-                    GNU GENERAL PUBLIC LICENSE
-                       Version 3, 29 June 2007
-
- Copyright (C) 2007 Free Software Foundation, Inc. <http://fsf.org/>
- Everyone is permitted to copy and distribute verbatim copies
- of this license document, but changing it is not allowed.
-
-                            Preamble
-
-  The GNU General Public License is a free, copyleft license for
-software and other kinds of works.
-
-  The licenses for most software and other practical works are designed
-to take away your freedom to share and change the works.  By contrast,
-the GNU General Public License is intended to guarantee your freedom to
-share and change all versions of a program--to make sure it remains free
-software for all its users.  We, the Free Software Foundation, use the
-GNU General Public License for most of our software; it applies also to
-any other work released this way by its authors.  You can apply it to
-your programs, too.
-
-  When we speak of free software, we are referring to freedom, not
-price.  Our General Public Licenses are designed to make sure that you
-have the freedom to distribute copies of free software (and charge for
-them if you wish), that you receive source code or can get it if you
-want it, that you can change the software or use pieces of it in new
-free programs, and that you know you can do these things.
-
-  To protect your rights, we need to prevent others from denying you
-these rights or asking you to surrender the rights.  Therefore, you have
-certain responsibilities if you distribute copies of the software, or if
-you modify it: responsibilities to respect the freedom of others.
-
-  For example, if you distribute copies of such a program, whether
-gratis or for a fee, you must pass on to the recipients the same
-freedoms that you received.  You must make sure that they, too, receive
-or can get the source code.  And you must show them these terms so they
-know their rights.
-
-  Developers that use the GNU GPL protect your rights with two steps:
-(1) assert copyright on the software, and (2) offer you this License
-giving you legal permission to copy, distribute and/or modify it.
-
-  For the developers' and authors' protection, the GPL clearly explains
-that there is no warranty for this free software.  For both users' and
-authors' sake, the GPL requires that modified versions be marked as
-changed, so that their problems will not be attributed erroneously to
-authors of previous versions.
-
-  Some devices are designed to deny users access to install or run
-modified versions of the software inside them, although the manufacturer
-can do so.  This is fundamentally incompatible with the aim of
-protecting users' freedom to change the software.  The systematic
-pattern of such abuse occurs in the area of products for individuals to
-use, which is precisely where it is most unacceptable.  Therefore, we
-have designed this version of the GPL to prohibit the practice for those
-products.  If such problems arise substantially in other domains, we
-stand ready to extend this provision to those domains in future versions
-of the GPL, as needed to protect the freedom of users.
-
-  Finally, every program is threatened constantly by software patents.
-States should not allow patents to restrict development and use of
-software on general-purpose computers, but in those that do, we wish to
-avoid the special danger that patents applied to a free program could
-make it effectively proprietary.  To prevent this, the GPL assures that
-patents cannot be used to render the program non-free.
-
-  The precise terms and conditions for copying, distribution and
-modification follow.
-
-                       TERMS AND CONDITIONS
-
-  0. Definitions.
-
-  "This License" refers to version 3 of the GNU General Public License.
-
-  "Copyright" also means copyright-like laws that apply to other kinds of
-works, such as semiconductor masks.
-
-  "The Program" refers to any copyrightable work licensed under this
-License.  Each licensee is addressed as "you".  "Licensees" and
-"recipients" may be individuals or organizations.
-
-  To "modify" a work means to copy from or adapt all or part of the work
-in a fashion requiring copyright permission, other than the making of an
-exact copy.  The resulting work is called a "modified version" of the
-earlier work or a work "based on" the earlier work.
-
-  A "covered work" means either the unmodified Program or a work based
-on the Program.
-
-  To "propagate" a work means to do anything with it that, without
-permission, would make you directly or secondarily liable for
-infringement under applicable copyright law, except executing it on a
-computer or modifying a private copy.  Propagation includes copying,
-distribution (with or without modification), making available to the
-public, and in some countries other activities as well.
-
-  To "convey" a work means any kind of propagation that enables other
-parties to make or receive copies.  Mere interaction with a user through
-a computer network, with no transfer of a copy, is not conveying.
-
-  An interactive user interface displays "Appropriate Legal Notices"
-to the extent that it includes a convenient and prominently visible
-feature that (1) displays an appropriate copyright notice, and (2)
-tells the user that there is no warranty for the work (except to the
-extent that warranties are provided), that licensees may convey the
-work under this License, and how to view a copy of this License.  If
-the interface presents a list of user commands or options, such as a
-menu, a prominent item in the list meets this criterion.
-
-  1. Source Code.
-
-  The "source code" for a work means the preferred form of the work
-for making modifications to it.  "Object code" means any non-source
-form of a work.
-
-  A "Standard Interface" means an interface that either is an official
-standard defined by a recognized standards body, or, in the case of
-interfaces specified for a particular programming language, one that
-is widely used among developers working in that language.
-
-  The "System Libraries" of an executable work include anything, other
-than the work as a whole, that (a) is included in the normal form of
-packaging a Major Component, but which is not part of that Major
-Component, and (b) serves only to enable use of the work with that
-Major Component, or to implement a Standard Interface for which an
-implementation is available to the public in source code form.  A
-"Major Component", in this context, means a major essential component
-(kernel, window system, and so on) of the specific operating system
-(if any) on which the executable work runs, or a compiler used to
-produce the work, or an object code interpreter used to run it.
-
-  The "Corresponding Source" for a work in object code form means all
-the source code needed to generate, install, and (for an executable
-work) run the object code and to modify the work, including scripts to
-control those activities.  However, it does not include the work's
-System Libraries, or general-purpose tools or generally available free
-programs which are used unmodified in performing those activities but
-which are not part of the work.  For example, Corresponding Source
-includes interface definition files associated with source files for
-the work, and the source code for shared libraries and dynamically
-linked subprograms that the work is specifically designed to require,
-such as by intimate data communication or control flow between those
-subprograms and other parts of the work.
-
-  The Corresponding Source need not include anything that users
-can regenerate automatically from other parts of the Corresponding
-Source.
-
-  The Corresponding Source for a work in source code form is that
-same work.
-
-  2. Basic Permissions.
-
-  All rights granted under this License are granted for the term of
-copyright on the Program, and are irrevocable provided the stated
-conditions are met.  This License explicitly affirms your unlimited
-permission to run the unmodified Program.  The output from running a
-covered work is covered by this License only if the output, given its
-content, constitutes a covered work.  This License acknowledges your
-rights of fair use or other equivalent, as provided by copyright law.
-
-  You may make, run and propagate covered works that you do not
-convey, without conditions so long as your license otherwise remains
-in force.  You may convey covered works to others for the sole purpose
-of having them make modifications exclusively for you, or provide you
-with facilities for running those works, provided that you comply with
-the terms of this License in conveying all material for which you do
-not control copyright.  Those thus making or running the covered works
-for you must do so exclusively on your behalf, under your direction
-and control, on terms that prohibit them from making any copies of
-your copyrighted material outside their relationship with you.
-
-  Conveying under any other circumstances is permitted solely under
-the conditions stated below.  Sublicensing is not allowed; section 10
-makes it unnecessary.
-
-  3. Protecting Users' Legal Rights From Anti-Circumvention Law.
-
-  No covered work shall be deemed part of an effective technological
-measure under any applicable law fulfilling obligations under article
-11 of the WIPO copyright treaty adopted on 20 December 1996, or
-similar laws prohibiting or restricting circumvention of such
-measures.
-
-  When you convey a covered work, you waive any legal power to forbid
-circumvention of technological measures to the extent such circumvention
-is effected by exercising rights under this License with respect to
-the covered work, and you disclaim any intention to limit operation or
-modification of the work as a means of enforcing, against the work's
-users, your or third parties' legal rights to forbid circumvention of
-technological measures.
-
-  4. Conveying Verbatim Copies.
-
-  You may convey verbatim copies of the Program's source code as you
-receive it, in any medium, provided that you conspicuously and
-appropriately publish on each copy an appropriate copyright notice;
-keep intact all notices stating that this License and any
-non-permissive terms added in accord with section 7 apply to the code;
-keep intact all notices of the absence of any warranty; and give all
-recipients a copy of this License along with the Program.
-
-  You may charge any price or no price for each copy that you convey,
-and you may offer support or warranty protection for a fee.
-
-  5. Conveying Modified Source Versions.
-
-  You may convey a work based on the Program, or the modifications to
-produce it from the Program, in the form of source code under the
-terms of section 4, provided that you also meet all of these conditions:
-
-    a) The work must carry prominent notices stating that you modified
-    it, and giving a relevant date.
-
-    b) The work must carry prominent notices stating that it is
-    released under this License and any conditions added under section
-    7.  This requirement modifies the requirement in section 4 to
-    "keep intact all notices".
-
-    c) You must license the entire work, as a whole, under this
-    License to anyone who comes into possession of a copy.  This
-    License will therefore apply, along with any applicable section 7
-    additional terms, to the whole of the work, and all its parts,
-    regardless of how they are packaged.  This License gives no
-    permission to license the work in any other way, but it does not
-    invalidate such permission if you have separately received it.
-
-    d) If the work has interactive user interfaces, each must display
-    Appropriate Legal Notices; however, if the Program has interactive
-    interfaces that do not display Appropriate Legal Notices, your
-    work need not make them do so.
-
-  A compilation of a covered work with other separate and independent
-works, which are not by their nature extensions of the covered work,
-and which are not combined with it such as to form a larger program,
-in or on a volume of a storage or distribution medium, is called an
-"aggregate" if the compilation and its resulting copyright are not
-used to limit the access or legal rights of the compilation's users
-beyond what the individual works permit.  Inclusion of a covered work
-in an aggregate does not cause this License to apply to the other
-parts of the aggregate.
-
-  6. Conveying Non-Source Forms.
-
-  You may convey a covered work in object code form under the terms
-of sections 4 and 5, provided that you also convey the
-machine-readable Corresponding Source under the terms of this License,
-in one of these ways:
-
-    a) Convey the object code in, or embodied in, a physical product
-    (including a physical distribution medium), accompanied by the
-    Corresponding Source fixed on a durable physical medium
-    customarily used for software interchange.
-
-    b) Convey the object code in, or embodied in, a physical product
-    (including a physical distribution medium), accompanied by a
-    written offer, valid for at least three years and valid for as
-    long as you offer spare parts or customer support for that product
-    model, to give anyone who possesses the object code either (1) a
-    copy of the Corresponding Source for all the software in the
-    product that is covered by this License, on a durable physical
-    medium customarily used for software interchange, for a price no
-    more than your reasonable cost of physically performing this
-    conveying of source, or (2) access to copy the
-    Corresponding Source from a network server at no charge.
-
-    c) Convey individual copies of the object code with a copy of the
-    written offer to provide the Corresponding Source.  This
-    alternative is allowed only occasionally and noncommercially, and
-    only if you received the object code with such an offer, in accord
-    with subsection 6b.
-
-    d) Convey the object code by offering access from a designated
-    place (gratis or for a charge), and offer equivalent access to the
-    Corresponding Source in the same way through the same place at no
-    further charge.  You need not require recipients to copy the
-    Corresponding Source along with the object code.  If the place to
-    copy the object code is a network server, the Corresponding Source
-    may be on a different server (operated by you or a third party)
-    that supports equivalent copying facilities, provided you maintain
-    clear directions next to the object code saying where to find the
-    Corresponding Source.  Regardless of what server hosts the
-    Corresponding Source, you remain obligated to ensure that it is
-    available for as long as needed to satisfy these requirements.
-
-    e) Convey the object code using peer-to-peer transmission, provided
-    you inform other peers where the object code and Corresponding
-    Source of the work are being offered to the general public at no
-    charge under subsection 6d.
-
-  A separable portion of the object code, whose source code is excluded
-from the Corresponding Source as a System Library, need not be
-included in conveying the object code work.
-
-  A "User Product" is either (1) a "consumer product", which means any
-tangible personal property which is normally used for personal, family,
-or household purposes, or (2) anything designed or sold for incorporation
-into a dwelling.  In determining whether a product is a consumer product,
-doubtful cases shall be resolved in favor of coverage.  For a particular
-product received by a particular user, "normally used" refers to a
-typical or common use of that class of product, regardless of the status
-of the particular user or of the way in which the particular user
-actually uses, or expects or is expected to use, the product.  A product
-is a consumer product regardless of whether the product has substantial
-commercial, industrial or non-consumer uses, unless such uses represent
-the only significant mode of use of the product.
-
-  "Installation Information" for a User Product means any methods,
-procedures, authorization keys, or other information required to install
-and execute modified versions of a covered work in that User Product from
-a modified version of its Corresponding Source.  The information must
-suffice to ensure that the continued functioning of the modified object
-code is in no case prevented or interfered with solely because
-modification has been made.
-
-  If you convey an object code work under this section in, or with, or
-specifically for use in, a User Product, and the conveying occurs as
-part of a transaction in which the right of possession and use of the
-User Product is transferred to the recipient in perpetuity or for a
-fixed term (regardless of how the transaction is characterized), the
-Corresponding Source conveyed under this section must be accompanied
-by the Installation Information.  But this requirement does not apply
-if neither you nor any third party retains the ability to install
-modified object code on the User Product (for example, the work has
-been installed in ROM).
-
-  The requirement to provide Installation Information does not include a
-requirement to continue to provide support service, warranty, or updates
-for a work that has been modified or installed by the recipient, or for
-the User Product in which it has been modified or installed.  Access to a
-network may be denied when the modification itself materially and
-adversely affects the operation of the network or violates the rules and
-protocols for communication across the network.
-
-  Corresponding Source conveyed, and Installation Information provided,
-in accord with this section must be in a format that is publicly
-documented (and with an implementation available to the public in
-source code form), and must require no special password or key for
-unpacking, reading or copying.
-
-  7. Additional Terms.
-
-  "Additional permissions" are terms that supplement the terms of this
-License by making exceptions from one or more of its conditions.
-Additional permissions that are applicable to the entire Program shall
-be treated as though they were included in this License, to the extent
-that they are valid under applicable law.  If additional permissions
-apply only to part of the Program, that part may be used separately
-under those permissions, but the entire Program remains governed by
-this License without regard to the additional permissions.
-
-  When you convey a copy of a covered work, you may at your option
-remove any additional permissions from that copy, or from any part of
-it.  (Additional permissions may be written to require their own
-removal in certain cases when you modify the work.)  You may place
-additional permissions on material, added by you to a covered work,
-for which you have or can give appropriate copyright permission.
-
-  Notwithstanding any other provision of this License, for material you
-add to a covered work, you may (if authorized by the copyright holders of
-that material) supplement the terms of this License with terms:
-
-    a) Disclaiming warranty or limiting liability differently from the
-    terms of sections 15 and 16 of this License; or
-
-    b) Requiring preservation of specified reasonable legal notices or
-    author attributions in that material or in the Appropriate Legal
-    Notices displayed by works containing it; or
-
-    c) Prohibiting misrepresentation of the origin of that material, or
-    requiring that modified versions of such material be marked in
-    reasonable ways as different from the original version; or
-
-    d) Limiting the use for publicity purposes of names of licensors or
-    authors of the material; or
-
-    e) Declining to grant rights under trademark law for use of some
-    trade names, trademarks, or service marks; or
-
-    f) Requiring indemnification of licensors and authors of that
-    material by anyone who conveys the material (or modified versions of
-    it) with contractual assumptions of liability to the recipient, for
-    any liability that these contractual assumptions directly impose on
-    those licensors and authors.
-
-  All other non-permissive additional terms are considered "further
-restrictions" within the meaning of section 10.  If the Program as you
-received it, or any part of it, contains a notice stating that it is
-governed by this License along with a term that is a further
-restriction, you may remove that term.  If a license document contains
-a further restriction but permits relicensing or conveying under this
-License, you may add to a covered work material governed by the terms
-of that license document, provided that the further restriction does
-not survive such relicensing or conveying.
-
-  If you add terms to a covered work in accord with this section, you
-must place, in the relevant source files, a statement of the
-additional terms that apply to those files, or a notice indicating
-where to find the applicable terms.
-
-  Additional terms, permissive or non-permissive, may be stated in the
-form of a separately written license, or stated as exceptions;
-the above requirements apply either way.
-
-  8. Termination.
-
-  You may not propagate or modify a covered work except as expressly
-provided under this License.  Any attempt otherwise to propagate or
-modify it is void, and will automatically terminate your rights under
-this License (including any patent licenses granted under the third
-paragraph of section 11).
-
-  However, if you cease all violation of this License, then your
-license from a particular copyright holder is reinstated (a)
-provisionally, unless and until the copyright holder explicitly and
-finally terminates your license, and (b) permanently, if the copyright
-holder fails to notify you of the violation by some reasonable means
-prior to 60 days after the cessation.
-
-  Moreover, your license from a particular copyright holder is
-reinstated permanently if the copyright holder notifies you of the
-violation by some reasonable means, this is the first time you have
-received notice of violation of this License (for any work) from that
-copyright holder, and you cure the violation prior to 30 days after
-your receipt of the notice.
-
-  Termination of your rights under this section does not terminate the
-licenses of parties who have received copies or rights from you under
-this License.  If your rights have been terminated and not permanently
-reinstated, you do not qualify to receive new licenses for the same
-material under section 10.
-
-  9. Acceptance Not Required for Having Copies.
-
-  You are not required to accept this License in order to receive or
-run a copy of the Program.  Ancillary propagation of a covered work
-occurring solely as a consequence of using peer-to-peer transmission
-to receive a copy likewise does not require acceptance.  However,
-nothing other than this License grants you permission to propagate or
-modify any covered work.  These actions infringe copyright if you do
-not accept this License.  Therefore, by modifying or propagating a
-covered work, you indicate your acceptance of this License to do so.
-
-  10. Automatic Licensing of Downstream Recipients.
-
-  Each time you convey a covered work, the recipient automatically
-receives a license from the original licensors, to run, modify and
-propagate that work, subject to this License.  You are not responsible
-for enforcing compliance by third parties with this License.
-
-  An "entity transaction" is a transaction transferring control of an
-organization, or substantially all assets of one, or subdividing an
-organization, or merging organizations.  If propagation of a covered
-work results from an entity transaction, each party to that
-transaction who receives a copy of the work also receives whatever
-licenses to the work the party's predecessor in interest had or could
-give under the previous paragraph, plus a right to possession of the
-Corresponding Source of the work from the predecessor in interest, if
-the predecessor has it or can get it with reasonable efforts.
-
-  You may not impose any further restrictions on the exercise of the
-rights granted or affirmed under this License.  For example, you may
-not impose a license fee, royalty, or other charge for exercise of
-rights granted under this License, and you may not initiate litigation
-(including a cross-claim or counterclaim in a lawsuit) alleging that
-any patent claim is infringed by making, using, selling, offering for
-sale, or importing the Program or any portion of it.
-
-  11. Patents.
-
-  A "contributor" is a copyright holder who authorizes use under this
-License of the Program or a work on which the Program is based.  The
-work thus licensed is called the contributor's "contributor version".
-
-  A contributor's "essential patent claims" are all patent claims
-owned or controlled by the contributor, whether already acquired or
-hereafter acquired, that would be infringed by some manner, permitted
-by this License, of making, using, or selling its contributor version,
-but do not include claims that would be infringed only as a
-consequence of further modification of the contributor version.  For
-purposes of this definition, "control" includes the right to grant
-patent sublicenses in a manner consistent with the requirements of
-this License.
-
-  Each contributor grants you a non-exclusive, worldwide, royalty-free
-patent license under the contributor's essential patent claims, to
-make, use, sell, offer for sale, import and otherwise run, modify and
-propagate the contents of its contributor version.
-
-  In the following three paragraphs, a "patent license" is any express
-agreement or commitment, however denominated, not to enforce a patent
-(such as an express permission to practice a patent or covenant not to
-sue for patent infringement).  To "grant" such a patent license to a
-party means to make such an agreement or commitment not to enforce a
-patent against the party.
-
-  If you convey a covered work, knowingly relying on a patent license,
-and the Corresponding Source of the work is not available for anyone
-to copy, free of charge and under the terms of this License, through a
-publicly available network server or other readily accessible means,
-then you must either (1) cause the Corresponding Source to be so
-available, or (2) arrange to deprive yourself of the benefit of the
-patent license for this particular work, or (3) arrange, in a manner
-consistent with the requirements of this License, to extend the patent
-license to downstream recipients.  "Knowingly relying" means you have
-actual knowledge that, but for the patent license, your conveying the
-covered work in a country, or your recipient's use of the covered work
-in a country, would infringe one or more identifiable patents in that
-country that you have reason to believe are valid.
-
-  If, pursuant to or in connection with a single transaction or
-arrangement, you convey, or propagate by procuring conveyance of, a
-covered work, and grant a patent license to some of the parties
-receiving the covered work authorizing them to use, propagate, modify
-or convey a specific copy of the covered work, then the patent license
-you grant is automatically extended to all recipients of the covered
-work and works based on it.
-
-  A patent license is "discriminatory" if it does not include within
-the scope of its coverage, prohibits the exercise of, or is
-conditioned on the non-exercise of one or more of the rights that are
-specifically granted under this License.  You may not convey a covered
-work if you are a party to an arrangement with a third party that is
-in the business of distributing software, under which you make payment
-to the third party based on the extent of your activity of conveying
-the work, and under which the third party grants, to any of the
-parties who would receive the covered work from you, a discriminatory
-patent license (a) in connection with copies of the covered work
-conveyed by you (or copies made from those copies), or (b) primarily
-for and in connection with specific products or compilations that
-contain the covered work, unless you entered into that arrangement,
-or that patent license was granted, prior to 28 March 2007.
-
-  Nothing in this License shall be construed as excluding or limiting
-any implied license or other defenses to infringement that may
-otherwise be available to you under applicable patent law.
-
-  12. No Surrender of Others' Freedom.
-
-  If conditions are imposed on you (whether by court order, agreement or
-otherwise) that contradict the conditions of this License, they do not
-excuse you from the conditions of this License.  If you cannot convey a
-covered work so as to satisfy simultaneously your obligations under this
-License and any other pertinent obligations, then as a consequence you may
-not convey it at all.  For example, if you agree to terms that obligate you
-to collect a royalty for further conveying from those to whom you convey
-the Program, the only way you could satisfy both those terms and this
-License would be to refrain entirely from conveying the Program.
-
-  13. Use with the GNU Affero General Public License.
-
-  Notwithstanding any other provision of this License, you have
-permission to link or combine any covered work with a work licensed
-under version 3 of the GNU Affero General Public License into a single
-combined work, and to convey the resulting work.  The terms of this
-License will continue to apply to the part which is the covered work,
-but the special requirements of the GNU Affero General Public License,
-section 13, concerning interaction through a network will apply to the
-combination as such.
-
-  14. Revised Versions of this License.
-
-  The Free Software Foundation may publish revised and/or new versions of
-the GNU General Public License from time to time.  Such new versions will
-be similar in spirit to the present version, but may differ in detail to
-address new problems or concerns.
-
-  Each version is given a distinguishing version number.  If the
-Program specifies that a certain numbered version of the GNU General
-Public License "or any later version" applies to it, you have the
-option of following the terms and conditions either of that numbered
-version or of any later version published by the Free Software
-Foundation.  If the Program does not specify a version number of the
-GNU General Public License, you may choose any version ever published
-by the Free Software Foundation.
-
-  If the Program specifies that a proxy can decide which future
-versions of the GNU General Public License can be used, that proxy's
-public statement of acceptance of a version permanently authorizes you
-to choose that version for the Program.
-
-  Later license versions may give you additional or different
-permissions.  However, no additional obligations are imposed on any
-author or copyright holder as a result of your choosing to follow a
-later version.
-
-  15. Disclaimer of Warranty.
-
-  THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY
-APPLICABLE LAW.  EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT
-HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM "AS IS" WITHOUT WARRANTY
-OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO,
-THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
-PURPOSE.  THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM
-IS WITH YOU.  SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF
-ALL NECESSARY SERVICING, REPAIR OR CORRECTION.
-
-  16. Limitation of Liability.
-
-  IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING
-WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS
-THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY
-GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE
-USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF
-DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD
-PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS),
-EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF
-SUCH DAMAGES.
-
-  17. Interpretation of Sections 15 and 16.
-
-  If the disclaimer of warranty and limitation of liability provided
-above cannot be given local legal effect according to their terms,
-reviewing courts shall apply local law that most closely approximates
-an absolute waiver of all civil liability in connection with the
-Program, unless a warranty or assumption of liability accompanies a
-copy of the Program in return for a fee.
diff --git a/EEDI3/vectorclass/sse2neon.h b/EEDI3/vectorclass/sse2neon.h
new file mode 100644
index 0000000..a3ba954
--- /dev/null
+++ b/EEDI3/vectorclass/sse2neon.h
@@ -0,0 +1,9429 @@
+#ifndef SSE2NEON_H
+#define SSE2NEON_H
+
+/*
+ * sse2neon is freely redistributable under the MIT License.
+ *
+ * Copyright (c) 2015-2024 SSE2NEON Contributors.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the "Software"), to deal
+ * in the Software without restriction, including without limitation the rights
+ * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+ * copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+ * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+// This header file provides a simple API translation layer
+// between SSE intrinsics to their corresponding Arm/Aarch64 NEON versions
+//
+// Contributors to this work are:
+//   John W. Ratcliff <jratcliffscarab@gmail.com>
+//   Brandon Rowlett <browlett@nvidia.com>
+//   Ken Fast <kfast@gdeb.com>
+//   Eric van Beurden <evanbeurden@nvidia.com>
+//   Alexander Potylitsin <apotylitsin@nvidia.com>
+//   Hasindu Gamaarachchi <hasindu2008@gmail.com>
+//   Jim Huang <jserv@ccns.ncku.edu.tw>
+//   Mark Cheng <marktwtn@gmail.com>
+//   Malcolm James MacLeod <malcolm@gulden.com>
+//   Devin Hussey (easyaspi314) <husseydevin@gmail.com>
+//   Sebastian Pop <spop@amazon.com>
+//   Developer Ecosystem Engineering <DeveloperEcosystemEngineering@apple.com>
+//   Danila Kutenin <danilak@google.com>
+//   Franois Turban (JishinMaster) <francois.turban@gmail.com>
+//   Pei-Hsuan Hung <afcidk@gmail.com>
+//   Yang-Hao Yuan <yuanyanghau@gmail.com>
+//   Syoyo Fujita <syoyo@lighttransport.com>
+//   Brecht Van Lommel <brecht@blender.org>
+//   Jonathan Hue <jhue@adobe.com>
+//   Cuda Chen <clh960524@gmail.com>
+//   Aymen Qader <aymen.qader@arm.com>
+//   Anthony Roberts <anthony.roberts@linaro.org>
+
+/* Tunable configurations */
+
+/* Enable precise implementation of math operations
+ * This would slow down the computation a bit, but gives consistent result with
+ * x86 SSE. (e.g. would solve a hole or NaN pixel in the rendering result)
+ */
+/* _mm_min|max_ps|ss|pd|sd */
+#ifndef SSE2NEON_PRECISE_MINMAX
+#define SSE2NEON_PRECISE_MINMAX (1) // always use precise implementation
+#endif
+/* _mm_rcp_ps */
+#ifndef SSE2NEON_PRECISE_DIV
+#define SSE2NEON_PRECISE_DIV (1) // always use precise implementation
+#endif
+/* _mm_sqrt_ps and _mm_rsqrt_ps */
+#ifndef SSE2NEON_PRECISE_SQRT
+#define SSE2NEON_PRECISE_SQRT (1) // always use precise implementation
+#endif
+/* _mm_dp_pd */
+#ifndef SSE2NEON_PRECISE_DP
+#define SSE2NEON_PRECISE_DP (1) // always use precise implementation
+#endif
+
+/* Enable inclusion of windows.h on MSVC platforms
+ * This makes _mm_clflush functional on windows, as there is no builtin.
+ */
+#ifndef SSE2NEON_INCLUDE_WINDOWS_H
+#define SSE2NEON_INCLUDE_WINDOWS_H (0)
+#endif
+
+/* compiler specific definitions */
+#if defined(__GNUC__) || defined(__clang__)
+#pragma push_macro("FORCE_INLINE")
+#pragma push_macro("ALIGN_STRUCT")
+#define FORCE_INLINE static inline __attribute__((always_inline))
+#define ALIGN_STRUCT(x) __attribute__((aligned(x)))
+#define _sse2neon_likely(x) __builtin_expect(!!(x), 1)
+#define _sse2neon_unlikely(x) __builtin_expect(!!(x), 0)
+#elif defined(_MSC_VER)
+#if _MSVC_TRADITIONAL
+#error Using the traditional MSVC preprocessor is not supported! Use /Zc:preprocessor instead.
+#endif
+#ifndef FORCE_INLINE
+#define FORCE_INLINE static inline
+#endif
+#ifndef ALIGN_STRUCT
+#define ALIGN_STRUCT(x) __declspec(align(x))
+#endif
+#define _sse2neon_likely(x) (x)
+#define _sse2neon_unlikely(x) (x)
+#else
+#pragma message("Macro name collisions may happen with unsupported compilers.")
+#endif
+
+
+#if defined(__GNUC__) && !defined(__clang__)
+#pragma push_macro("FORCE_INLINE_OPTNONE")
+#define FORCE_INLINE_OPTNONE static inline __attribute__((optimize("O0")))
+#elif defined(__clang__)
+#pragma push_macro("FORCE_INLINE_OPTNONE")
+#define FORCE_INLINE_OPTNONE static inline __attribute__((optnone))
+#else
+#define FORCE_INLINE_OPTNONE FORCE_INLINE
+#endif
+
+#if !defined(__clang__) && defined(__GNUC__) && __GNUC__ < 10
+#warning "GCC versions earlier than 10 are not supported."
+#endif
+
+/* C language does not allow initializing a variable with a function call. */
+#ifdef __cplusplus
+#define _sse2neon_const static const
+#else
+#define _sse2neon_const const
+#endif
+
+#include <stdint.h>
+#include <stdlib.h>
+#include <string.h>
+
+FORCE_INLINE double sse2neon_recast_u64_f64(uint64_t u64)
+{
+    double f64;
+    memcpy(&f64, &u64, sizeof(uint64_t));
+    return f64;
+}
+FORCE_INLINE int64_t sse2neon_recast_f64_s64(double f64)
+{
+    int64_t i64;
+    memcpy(&i64, &f64, sizeof(uint64_t));
+    return i64;
+}
+
+#if defined(_WIN32)
+/* Definitions for _mm_{malloc,free} are provided by <malloc.h>
+ * from both MinGW-w64 and MSVC.
+ */
+#define SSE2NEON_ALLOC_DEFINED
+#endif
+
+/* If using MSVC */
+#ifdef _MSC_VER
+#include <intrin.h>
+#if SSE2NEON_INCLUDE_WINDOWS_H
+#include <processthreadsapi.h>
+#include <windows.h>
+#endif
+
+#if !defined(__cplusplus)
+#error SSE2NEON only supports C++ compilation with this compiler
+#endif
+
+#ifdef SSE2NEON_ALLOC_DEFINED
+#include <malloc.h>
+#endif
+
+#if (defined(_M_AMD64) || defined(__x86_64__)) || \
+    (defined(_M_ARM64) || defined(__arm64__))
+#define SSE2NEON_HAS_BITSCAN64
+#endif
+#endif
+
+#if defined(__GNUC__) || defined(__clang__)
+#define _sse2neon_define0(type, s, body) \
+    __extension__({                      \
+        type _a = (s);                   \
+        body                             \
+    })
+#define _sse2neon_define1(type, s, body) \
+    __extension__({                      \
+        type _a = (s);                   \
+        body                             \
+    })
+#define _sse2neon_define2(type, a, b, body) \
+    __extension__({                         \
+        type _a = (a), _b = (b);            \
+        body                                \
+    })
+#define _sse2neon_return(ret) (ret)
+#else
+#define _sse2neon_define0(type, a, body) [=](type _a) { body }(a)
+#define _sse2neon_define1(type, a, body) [](type _a) { body }(a)
+#define _sse2neon_define2(type, a, b, body) \
+    [](type _a, type _b) { body }((a), (b))
+#define _sse2neon_return(ret) return ret
+#endif
+
+#define _sse2neon_init(...) \
+    {                       \
+        __VA_ARGS__         \
+    }
+
+/* Compiler barrier */
+#if defined(_MSC_VER) && !defined(__clang__)
+#define SSE2NEON_BARRIER() _ReadWriteBarrier()
+#else
+#define SSE2NEON_BARRIER()                     \
+    do {                                       \
+        __asm__ __volatile__("" ::: "memory"); \
+        (void) 0;                              \
+    } while (0)
+#endif
+
+/* Memory barriers
+ * __atomic_thread_fence does not include a compiler barrier; instead,
+ * the barrier is part of __atomic_load/__atomic_store's "volatile-like"
+ * semantics.
+ */
+#if defined(__STDC_VERSION__) && (__STDC_VERSION__ >= 201112L)
+#include <stdatomic.h>
+#endif
+
+FORCE_INLINE void _sse2neon_smp_mb(void)
+{
+    SSE2NEON_BARRIER();
+#if defined(__STDC_VERSION__) && (__STDC_VERSION__ >= 201112L) && \
+    !defined(__STDC_NO_ATOMICS__)
+    atomic_thread_fence(memory_order_seq_cst);
+#elif defined(__GNUC__) || defined(__clang__)
+    __atomic_thread_fence(__ATOMIC_SEQ_CST);
+#else /* MSVC */
+    __dmb(_ARM64_BARRIER_ISH);
+#endif
+}
+
+/* Architecture-specific build options */
+/* FIXME: #pragma GCC push_options is only available on GCC */
+#if defined(__GNUC__)
+#if defined(__arm__) && __ARM_ARCH == 7
+/* According to ARM C Language Extensions Architecture specification,
+ * __ARM_NEON is defined to a value indicating the Advanced SIMD (NEON)
+ * architecture supported.
+ */
+#if !defined(__ARM_NEON) || !defined(__ARM_NEON__)
+#error "You must enable NEON instructions (e.g. -mfpu=neon) to use SSE2NEON."
+#endif
+#if !defined(__clang__)
+#pragma GCC push_options
+#pragma GCC target("fpu=neon")
+#endif
+#elif defined(__aarch64__) || defined(_M_ARM64)
+#if !defined(__clang__) && !defined(_MSC_VER)
+#pragma GCC push_options
+#pragma GCC target("+simd")
+#endif
+#elif __ARM_ARCH == 8
+#if !defined(__ARM_NEON) || !defined(__ARM_NEON__)
+#error \
+    "You must enable NEON instructions (e.g. -mfpu=neon-fp-armv8) to use SSE2NEON."
+#endif
+#if !defined(__clang__) && !defined(_MSC_VER)
+#pragma GCC push_options
+#endif
+#else
+#error \
+    "Unsupported target. Must be either ARMv7-A+NEON or ARMv8-A \
+(you could try setting target explicitly with -march or -mcpu)"
+#endif
+#endif
+
+#include <arm_neon.h>
+#if (!defined(__aarch64__) && !defined(_M_ARM64)) && (__ARM_ARCH == 8)
+#if defined __has_include && __has_include(<arm_acle.h>)
+#include <arm_acle.h>
+#endif
+#endif
+
+/* Apple Silicon cache lines are double of what is commonly used by Intel, AMD
+ * and other Arm microarchitectures use.
+ * From sysctl -a on Apple M1:
+ * hw.cachelinesize: 128
+ */
+#if defined(__APPLE__) && (defined(__aarch64__) || defined(__arm64__))
+#define SSE2NEON_CACHELINE_SIZE 128
+#else
+#define SSE2NEON_CACHELINE_SIZE 64
+#endif
+
+/* Rounding functions require either Aarch64 instructions or libm fallback */
+#if !defined(__aarch64__) && !defined(_M_ARM64)
+#include <math.h>
+#endif
+
+/* On ARMv7, some registers, such as PMUSERENR and PMCCNTR, are read-only
+ * or even not accessible in user mode.
+ * To write or access to these registers in user mode,
+ * we have to perform syscall instead.
+ */
+#if (!defined(__aarch64__) && !defined(_M_ARM64))
+#include <sys/time.h>
+#endif
+
+/* "__has_builtin" can be used to query support for built-in functions
+ * provided by gcc/clang and other compilers that support it.
+ */
+#ifndef __has_builtin /* GCC prior to 10 or non-clang compilers */
+/* Compatibility with gcc <= 9 */
+#if defined(__GNUC__) && (__GNUC__ <= 9)
+#define __has_builtin(x) HAS##x
+#define HAS__builtin_popcount 1
+#define HAS__builtin_popcountll 1
+
+// __builtin_shuffle introduced in GCC 4.7.0
+#if (__GNUC__ >= 5) || ((__GNUC__ == 4) && (__GNUC_MINOR__ >= 7))
+#define HAS__builtin_shuffle 1
+#else
+#define HAS__builtin_shuffle 0
+#endif
+
+#define HAS__builtin_shufflevector 0
+#define HAS__builtin_nontemporal_store 0
+#else
+#define __has_builtin(x) 0
+#endif
+#endif
+
+/**
+ * MACRO for shuffle parameter for _mm_shuffle_ps().
+ * Argument fp3 is a digit[0123] that represents the fp from argument "b"
+ * of mm_shuffle_ps that will be placed in fp3 of result. fp2 is the same
+ * for fp2 in result. fp1 is a digit[0123] that represents the fp from
+ * argument "a" of mm_shuffle_ps that will be places in fp1 of result.
+ * fp0 is the same for fp0 of result.
+ */
+#define _MM_SHUFFLE(fp3, fp2, fp1, fp0) \
+    (((fp3) << 6) | ((fp2) << 4) | ((fp1) << 2) | ((fp0)))
+
+#if __has_builtin(__builtin_shufflevector)
+#define _sse2neon_shuffle(type, a, b, ...) \
+    __builtin_shufflevector(a, b, __VA_ARGS__)
+#elif __has_builtin(__builtin_shuffle)
+#define _sse2neon_shuffle(type, a, b, ...) \
+    __extension__({                        \
+        type tmp = {__VA_ARGS__};          \
+        __builtin_shuffle(a, b, tmp);      \
+    })
+#endif
+
+#ifdef _sse2neon_shuffle
+#define vshuffle_s16(a, b, ...) _sse2neon_shuffle(int16x4_t, a, b, __VA_ARGS__)
+#define vshuffleq_s16(a, b, ...) _sse2neon_shuffle(int16x8_t, a, b, __VA_ARGS__)
+#define vshuffle_s32(a, b, ...) _sse2neon_shuffle(int32x2_t, a, b, __VA_ARGS__)
+#define vshuffleq_s32(a, b, ...) _sse2neon_shuffle(int32x4_t, a, b, __VA_ARGS__)
+#define vshuffle_s64(a, b, ...) _sse2neon_shuffle(int64x1_t, a, b, __VA_ARGS__)
+#define vshuffleq_s64(a, b, ...) _sse2neon_shuffle(int64x2_t, a, b, __VA_ARGS__)
+#endif
+
+/* Rounding mode macros. */
+#define _MM_FROUND_TO_NEAREST_INT 0x00
+#define _MM_FROUND_TO_NEG_INF 0x01
+#define _MM_FROUND_TO_POS_INF 0x02
+#define _MM_FROUND_TO_ZERO 0x03
+#define _MM_FROUND_CUR_DIRECTION 0x04
+#define _MM_FROUND_NO_EXC 0x08
+#define _MM_FROUND_RAISE_EXC 0x00
+#define _MM_FROUND_NINT (_MM_FROUND_TO_NEAREST_INT | _MM_FROUND_RAISE_EXC)
+#define _MM_FROUND_FLOOR (_MM_FROUND_TO_NEG_INF | _MM_FROUND_RAISE_EXC)
+#define _MM_FROUND_CEIL (_MM_FROUND_TO_POS_INF | _MM_FROUND_RAISE_EXC)
+#define _MM_FROUND_TRUNC (_MM_FROUND_TO_ZERO | _MM_FROUND_RAISE_EXC)
+#define _MM_FROUND_RINT (_MM_FROUND_CUR_DIRECTION | _MM_FROUND_RAISE_EXC)
+#define _MM_FROUND_NEARBYINT (_MM_FROUND_CUR_DIRECTION | _MM_FROUND_NO_EXC)
+#define _MM_ROUND_NEAREST 0x0000
+#define _MM_ROUND_DOWN 0x2000
+#define _MM_ROUND_UP 0x4000
+#define _MM_ROUND_TOWARD_ZERO 0x6000
+/* Flush zero mode macros. */
+#define _MM_FLUSH_ZERO_MASK 0x8000
+#define _MM_FLUSH_ZERO_ON 0x8000
+#define _MM_FLUSH_ZERO_OFF 0x0000
+/* Denormals are zeros mode macros. */
+#define _MM_DENORMALS_ZERO_MASK 0x0040
+#define _MM_DENORMALS_ZERO_ON 0x0040
+#define _MM_DENORMALS_ZERO_OFF 0x0000
+
+/* indicate immediate constant argument in a given range */
+#define __constrange(a, b) const
+
+/* A few intrinsics accept traditional data types like ints or floats, but
+ * most operate on data types that are specific to SSE.
+ * If a vector type ends in d, it contains doubles, and if it does not have
+ * a suffix, it contains floats. An integer vector type can contain any type
+ * of integer, from chars to shorts to unsigned long longs.
+ */
+typedef int64x1_t __m64;
+typedef float32x4_t __m128; /* 128-bit vector containing 4 floats */
+// On ARM 32-bit architecture, the float64x2_t is not supported.
+// The data type __m128d should be represented in a different way for related
+// intrinsic conversion.
+#if defined(__aarch64__) || defined(_M_ARM64)
+typedef float64x2_t __m128d; /* 128-bit vector containing 2 doubles */
+#else
+typedef float32x4_t __m128d;
+#endif
+typedef int64x2_t __m128i; /* 128-bit vector containing integers */
+
+// Some intrinsics operate on unaligned data types.
+typedef int16_t ALIGN_STRUCT(1) unaligned_int16_t;
+typedef int32_t ALIGN_STRUCT(1) unaligned_int32_t;
+typedef int64_t ALIGN_STRUCT(1) unaligned_int64_t;
+
+// __int64 is defined in the Intrinsics Guide which maps to different datatype
+// in different data model
+#if !(defined(_WIN32) || defined(_WIN64) || defined(__int64))
+#if (defined(__x86_64__) || defined(__i386__))
+#define __int64 long long
+#else
+#define __int64 int64_t
+#endif
+#endif
+
+/* type-safe casting between types */
+
+#define vreinterpretq_m128_f16(x) vreinterpretq_f32_f16(x)
+#define vreinterpretq_m128_f32(x) (x)
+#define vreinterpretq_m128_f64(x) vreinterpretq_f32_f64(x)
+
+#define vreinterpretq_m128_u8(x) vreinterpretq_f32_u8(x)
+#define vreinterpretq_m128_u16(x) vreinterpretq_f32_u16(x)
+#define vreinterpretq_m128_u32(x) vreinterpretq_f32_u32(x)
+#define vreinterpretq_m128_u64(x) vreinterpretq_f32_u64(x)
+
+#define vreinterpretq_m128_s8(x) vreinterpretq_f32_s8(x)
+#define vreinterpretq_m128_s16(x) vreinterpretq_f32_s16(x)
+#define vreinterpretq_m128_s32(x) vreinterpretq_f32_s32(x)
+#define vreinterpretq_m128_s64(x) vreinterpretq_f32_s64(x)
+
+#define vreinterpretq_f16_m128(x) vreinterpretq_f16_f32(x)
+#define vreinterpretq_f32_m128(x) (x)
+#define vreinterpretq_f64_m128(x) vreinterpretq_f64_f32(x)
+
+#define vreinterpretq_u8_m128(x) vreinterpretq_u8_f32(x)
+#define vreinterpretq_u16_m128(x) vreinterpretq_u16_f32(x)
+#define vreinterpretq_u32_m128(x) vreinterpretq_u32_f32(x)
+#define vreinterpretq_u64_m128(x) vreinterpretq_u64_f32(x)
+
+#define vreinterpretq_s8_m128(x) vreinterpretq_s8_f32(x)
+#define vreinterpretq_s16_m128(x) vreinterpretq_s16_f32(x)
+#define vreinterpretq_s32_m128(x) vreinterpretq_s32_f32(x)
+#define vreinterpretq_s64_m128(x) vreinterpretq_s64_f32(x)
+
+#define vreinterpretq_m128i_s8(x) vreinterpretq_s64_s8(x)
+#define vreinterpretq_m128i_s16(x) vreinterpretq_s64_s16(x)
+#define vreinterpretq_m128i_s32(x) vreinterpretq_s64_s32(x)
+#define vreinterpretq_m128i_s64(x) (x)
+
+#define vreinterpretq_m128i_u8(x) vreinterpretq_s64_u8(x)
+#define vreinterpretq_m128i_u16(x) vreinterpretq_s64_u16(x)
+#define vreinterpretq_m128i_u32(x) vreinterpretq_s64_u32(x)
+#define vreinterpretq_m128i_u64(x) vreinterpretq_s64_u64(x)
+
+#define vreinterpretq_f32_m128i(x) vreinterpretq_f32_s64(x)
+#define vreinterpretq_f64_m128i(x) vreinterpretq_f64_s64(x)
+
+#define vreinterpretq_s8_m128i(x) vreinterpretq_s8_s64(x)
+#define vreinterpretq_s16_m128i(x) vreinterpretq_s16_s64(x)
+#define vreinterpretq_s32_m128i(x) vreinterpretq_s32_s64(x)
+#define vreinterpretq_s64_m128i(x) (x)
+
+#define vreinterpretq_u8_m128i(x) vreinterpretq_u8_s64(x)
+#define vreinterpretq_u16_m128i(x) vreinterpretq_u16_s64(x)
+#define vreinterpretq_u32_m128i(x) vreinterpretq_u32_s64(x)
+#define vreinterpretq_u64_m128i(x) vreinterpretq_u64_s64(x)
+
+#define vreinterpret_m64_s8(x) vreinterpret_s64_s8(x)
+#define vreinterpret_m64_s16(x) vreinterpret_s64_s16(x)
+#define vreinterpret_m64_s32(x) vreinterpret_s64_s32(x)
+#define vreinterpret_m64_s64(x) (x)
+
+#define vreinterpret_m64_u8(x) vreinterpret_s64_u8(x)
+#define vreinterpret_m64_u16(x) vreinterpret_s64_u16(x)
+#define vreinterpret_m64_u32(x) vreinterpret_s64_u32(x)
+#define vreinterpret_m64_u64(x) vreinterpret_s64_u64(x)
+
+#define vreinterpret_m64_f16(x) vreinterpret_s64_f16(x)
+#define vreinterpret_m64_f32(x) vreinterpret_s64_f32(x)
+#define vreinterpret_m64_f64(x) vreinterpret_s64_f64(x)
+
+#define vreinterpret_u8_m64(x) vreinterpret_u8_s64(x)
+#define vreinterpret_u16_m64(x) vreinterpret_u16_s64(x)
+#define vreinterpret_u32_m64(x) vreinterpret_u32_s64(x)
+#define vreinterpret_u64_m64(x) vreinterpret_u64_s64(x)
+
+#define vreinterpret_s8_m64(x) vreinterpret_s8_s64(x)
+#define vreinterpret_s16_m64(x) vreinterpret_s16_s64(x)
+#define vreinterpret_s32_m64(x) vreinterpret_s32_s64(x)
+#define vreinterpret_s64_m64(x) (x)
+
+#define vreinterpret_f32_m64(x) vreinterpret_f32_s64(x)
+
+#if defined(__aarch64__) || defined(_M_ARM64)
+#define vreinterpretq_m128d_s32(x) vreinterpretq_f64_s32(x)
+#define vreinterpretq_m128d_s64(x) vreinterpretq_f64_s64(x)
+
+#define vreinterpretq_m128d_u64(x) vreinterpretq_f64_u64(x)
+
+#define vreinterpretq_m128d_f32(x) vreinterpretq_f64_f32(x)
+#define vreinterpretq_m128d_f64(x) (x)
+
+#define vreinterpretq_s64_m128d(x) vreinterpretq_s64_f64(x)
+
+#define vreinterpretq_u32_m128d(x) vreinterpretq_u32_f64(x)
+#define vreinterpretq_u64_m128d(x) vreinterpretq_u64_f64(x)
+
+#define vreinterpretq_f64_m128d(x) (x)
+#define vreinterpretq_f32_m128d(x) vreinterpretq_f32_f64(x)
+#else
+#define vreinterpretq_m128d_s32(x) vreinterpretq_f32_s32(x)
+#define vreinterpretq_m128d_s64(x) vreinterpretq_f32_s64(x)
+
+#define vreinterpretq_m128d_u32(x) vreinterpretq_f32_u32(x)
+#define vreinterpretq_m128d_u64(x) vreinterpretq_f32_u64(x)
+
+#define vreinterpretq_m128d_f32(x) (x)
+
+#define vreinterpretq_s64_m128d(x) vreinterpretq_s64_f32(x)
+
+#define vreinterpretq_u32_m128d(x) vreinterpretq_u32_f32(x)
+#define vreinterpretq_u64_m128d(x) vreinterpretq_u64_f32(x)
+
+#define vreinterpretq_f32_m128d(x) (x)
+#endif
+
+// A struct is defined in this header file called 'SIMDVec' which can be used
+// by applications which attempt to access the contents of an __m128 struct
+// directly.  It is important to note that accessing the __m128 struct directly
+// is bad coding practice by Microsoft: @see:
+// https://learn.microsoft.com/en-us/cpp/cpp/m128
+//
+// However, some legacy source code may try to access the contents of an __m128
+// struct directly so the developer can use the SIMDVec as an alias for it.  Any
+// casting must be done manually by the developer, as you cannot cast or
+// otherwise alias the base NEON data type for intrinsic operations.
+//
+// union intended to allow direct access to an __m128 variable using the names
+// that the MSVC compiler provides.  This union should really only be used when
+// trying to access the members of the vector as integer values.  GCC/clang
+// allow native access to the float members through a simple array access
+// operator (in C since 4.6, in C++ since 4.8).
+//
+// Ideally direct accesses to SIMD vectors should not be used since it can cause
+// a performance hit.  If it really is needed however, the original __m128
+// variable can be aliased with a pointer to this union and used to access
+// individual components.  The use of this union should be hidden behind a macro
+// that is used throughout the codebase to access the members instead of always
+// declaring this type of variable.
+typedef union ALIGN_STRUCT(16) SIMDVec {
+    float m128_f32[4];     // as floats - DON'T USE. Added for convenience.
+    int8_t m128_i8[16];    // as signed 8-bit integers.
+    int16_t m128_i16[8];   // as signed 16-bit integers.
+    int32_t m128_i32[4];   // as signed 32-bit integers.
+    int64_t m128_i64[2];   // as signed 64-bit integers.
+    uint8_t m128_u8[16];   // as unsigned 8-bit integers.
+    uint16_t m128_u16[8];  // as unsigned 16-bit integers.
+    uint32_t m128_u32[4];  // as unsigned 32-bit integers.
+    uint64_t m128_u64[2];  // as unsigned 64-bit integers.
+} SIMDVec;
+
+// casting using SIMDVec
+#define vreinterpretq_nth_u64_m128i(x, n) (((SIMDVec *) &x)->m128_u64[n])
+#define vreinterpretq_nth_u32_m128i(x, n) (((SIMDVec *) &x)->m128_u32[n])
+#define vreinterpretq_nth_u8_m128i(x, n) (((SIMDVec *) &x)->m128_u8[n])
+
+/* SSE macros */
+#define _MM_GET_FLUSH_ZERO_MODE _sse2neon_mm_get_flush_zero_mode
+#define _MM_SET_FLUSH_ZERO_MODE _sse2neon_mm_set_flush_zero_mode
+#define _MM_GET_DENORMALS_ZERO_MODE _sse2neon_mm_get_denormals_zero_mode
+#define _MM_SET_DENORMALS_ZERO_MODE _sse2neon_mm_set_denormals_zero_mode
+
+// Function declaration
+// SSE
+FORCE_INLINE unsigned int _MM_GET_ROUNDING_MODE(void);
+FORCE_INLINE __m128 _mm_move_ss(__m128, __m128);
+FORCE_INLINE __m128 _mm_or_ps(__m128, __m128);
+FORCE_INLINE __m128 _mm_set_ps1(float);
+FORCE_INLINE __m128 _mm_setzero_ps(void);
+// SSE2
+FORCE_INLINE __m128i _mm_and_si128(__m128i, __m128i);
+FORCE_INLINE __m128i _mm_castps_si128(__m128);
+FORCE_INLINE __m128i _mm_cmpeq_epi32(__m128i, __m128i);
+FORCE_INLINE __m128i _mm_cvtps_epi32(__m128);
+FORCE_INLINE __m128d _mm_move_sd(__m128d, __m128d);
+FORCE_INLINE __m128i _mm_or_si128(__m128i, __m128i);
+FORCE_INLINE __m128i _mm_set_epi32(int, int, int, int);
+FORCE_INLINE __m128i _mm_set_epi64x(int64_t, int64_t);
+FORCE_INLINE __m128d _mm_set_pd(double, double);
+FORCE_INLINE __m128i _mm_set1_epi32(int);
+FORCE_INLINE __m128i _mm_setzero_si128(void);
+// SSE4.1
+FORCE_INLINE __m128d _mm_ceil_pd(__m128d);
+FORCE_INLINE __m128 _mm_ceil_ps(__m128);
+FORCE_INLINE __m128d _mm_floor_pd(__m128d);
+FORCE_INLINE __m128 _mm_floor_ps(__m128);
+FORCE_INLINE_OPTNONE __m128d _mm_round_pd(__m128d, int);
+FORCE_INLINE_OPTNONE __m128 _mm_round_ps(__m128, int);
+// SSE4.2
+FORCE_INLINE uint32_t _mm_crc32_u8(uint32_t, uint8_t);
+
+/* Backwards compatibility for compilers with lack of specific type support */
+
+// Older gcc does not define vld1q_u8_x4 type
+#if defined(__GNUC__) && !defined(__clang__) &&                        \
+    ((__GNUC__ <= 13 && defined(__arm__)) ||                           \
+     (__GNUC__ == 10 && __GNUC_MINOR__ < 3 && defined(__aarch64__)) || \
+     (__GNUC__ <= 9 && defined(__aarch64__)))
+FORCE_INLINE uint8x16x4_t _sse2neon_vld1q_u8_x4(const uint8_t *p)
+{
+    uint8x16x4_t ret;
+    ret.val[0] = vld1q_u8(p + 0);
+    ret.val[1] = vld1q_u8(p + 16);
+    ret.val[2] = vld1q_u8(p + 32);
+    ret.val[3] = vld1q_u8(p + 48);
+    return ret;
+}
+#else
+// Wraps vld1q_u8_x4
+FORCE_INLINE uint8x16x4_t _sse2neon_vld1q_u8_x4(const uint8_t *p)
+{
+    return vld1q_u8_x4(p);
+}
+#endif
+
+#if !defined(__aarch64__) && !defined(_M_ARM64)
+/* emulate vaddv u8 variant */
+FORCE_INLINE uint8_t _sse2neon_vaddv_u8(uint8x8_t v8)
+{
+    const uint64x1_t v1 = vpaddl_u32(vpaddl_u16(vpaddl_u8(v8)));
+    return vget_lane_u8(vreinterpret_u8_u64(v1), 0);
+}
+#else
+// Wraps vaddv_u8
+FORCE_INLINE uint8_t _sse2neon_vaddv_u8(uint8x8_t v8)
+{
+    return vaddv_u8(v8);
+}
+#endif
+
+#if !defined(__aarch64__) && !defined(_M_ARM64)
+/* emulate vaddvq u8 variant */
+FORCE_INLINE uint8_t _sse2neon_vaddvq_u8(uint8x16_t a)
+{
+    uint8x8_t tmp = vpadd_u8(vget_low_u8(a), vget_high_u8(a));
+    uint8_t res = 0;
+    for (int i = 0; i < 8; ++i)
+        res += tmp[i];
+    return res;
+}
+#else
+// Wraps vaddvq_u8
+FORCE_INLINE uint8_t _sse2neon_vaddvq_u8(uint8x16_t a)
+{
+    return vaddvq_u8(a);
+}
+#endif
+
+#if !defined(__aarch64__) && !defined(_M_ARM64)
+/* emulate vaddvq u16 variant */
+FORCE_INLINE uint16_t _sse2neon_vaddvq_u16(uint16x8_t a)
+{
+    uint32x4_t m = vpaddlq_u16(a);
+    uint64x2_t n = vpaddlq_u32(m);
+    uint64x1_t o = vget_low_u64(n) + vget_high_u64(n);
+
+    return vget_lane_u32((uint32x2_t) o, 0);
+}
+#else
+// Wraps vaddvq_u16
+FORCE_INLINE uint16_t _sse2neon_vaddvq_u16(uint16x8_t a)
+{
+    return vaddvq_u16(a);
+}
+#endif
+
+/* Function Naming Conventions
+ * The naming convention of SSE intrinsics is straightforward. A generic SSE
+ * intrinsic function is given as follows:
+ *   _mm_<name>_<data_type>
+ *
+ * The parts of this format are given as follows:
+ * 1. <name> describes the operation performed by the intrinsic
+ * 2. <data_type> identifies the data type of the function's primary arguments
+ *
+ * This last part, <data_type>, is a little complicated. It identifies the
+ * content of the input values, and can be set to any of the following values:
+ * + ps - vectors contain floats (ps stands for packed single-precision)
+ * + pd - vectors contain doubles (pd stands for packed double-precision)
+ * + epi8/epi16/epi32/epi64 - vectors contain 8-bit/16-bit/32-bit/64-bit
+ *                            signed integers
+ * + epu8/epu16/epu32/epu64 - vectors contain 8-bit/16-bit/32-bit/64-bit
+ *                            unsigned integers
+ * + si128 - unspecified 128-bit vector or 256-bit vector
+ * + m128/m128i/m128d - identifies input vector types when they are different
+ *                      than the type of the returned vector
+ *
+ * For example, _mm_setzero_ps. The _mm implies that the function returns
+ * a 128-bit vector. The _ps at the end implies that the argument vectors
+ * contain floats.
+ *
+ * A complete example: Byte Shuffle - pshufb (_mm_shuffle_epi8)
+ *   // Set packed 16-bit integers. 128 bits, 8 short, per 16 bits
+ *   __m128i v_in = _mm_setr_epi16(1, 2, 3, 4, 5, 6, 7, 8);
+ *   // Set packed 8-bit integers
+ *   // 128 bits, 16 chars, per 8 bits
+ *   __m128i v_perm = _mm_setr_epi8(1, 0,  2,  3, 8, 9, 10, 11,
+ *                                  4, 5, 12, 13, 6, 7, 14, 15);
+ *   // Shuffle packed 8-bit integers
+ *   __m128i v_out = _mm_shuffle_epi8(v_in, v_perm); // pshufb
+ */
+
+/* Constants for use with _mm_prefetch. */
+enum _mm_hint {
+    _MM_HINT_NTA = 0, /* load data to L1 and L2 cache, mark it as NTA */
+    _MM_HINT_T0 = 1,  /* load data to L1 and L2 cache */
+    _MM_HINT_T1 = 2,  /* load data to L2 cache only */
+    _MM_HINT_T2 = 3,  /* load data to L2 cache only, mark it as NTA */
+};
+
+// The bit field mapping to the FPCR(floating-point control register)
+typedef struct {
+    uint16_t res0;
+    uint8_t res1 : 6;
+    uint8_t bit22 : 1;
+    uint8_t bit23 : 1;
+    uint8_t bit24 : 1;
+    uint8_t res2 : 7;
+#if defined(__aarch64__) || defined(_M_ARM64)
+    uint32_t res3;
+#endif
+} fpcr_bitfield;
+
+// Takes the upper 64 bits of a and places it in the low end of the result
+// Takes the lower 64 bits of b and places it into the high end of the result.
+FORCE_INLINE __m128 _mm_shuffle_ps_1032(__m128 a, __m128 b)
+{
+    float32x2_t a32 = vget_high_f32(vreinterpretq_f32_m128(a));
+    float32x2_t b10 = vget_low_f32(vreinterpretq_f32_m128(b));
+    return vreinterpretq_m128_f32(vcombine_f32(a32, b10));
+}
+
+// takes the lower two 32-bit values from a and swaps them and places in high
+// end of result takes the higher two 32 bit values from b and swaps them and
+// places in low end of result.
+FORCE_INLINE __m128 _mm_shuffle_ps_2301(__m128 a, __m128 b)
+{
+    float32x2_t a01 = vrev64_f32(vget_low_f32(vreinterpretq_f32_m128(a)));
+    float32x2_t b23 = vrev64_f32(vget_high_f32(vreinterpretq_f32_m128(b)));
+    return vreinterpretq_m128_f32(vcombine_f32(a01, b23));
+}
+
+FORCE_INLINE __m128 _mm_shuffle_ps_0321(__m128 a, __m128 b)
+{
+    float32x2_t a21 = vget_high_f32(
+        vextq_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(a), 3));
+    float32x2_t b03 = vget_low_f32(
+        vextq_f32(vreinterpretq_f32_m128(b), vreinterpretq_f32_m128(b), 3));
+    return vreinterpretq_m128_f32(vcombine_f32(a21, b03));
+}
+
+FORCE_INLINE __m128 _mm_shuffle_ps_2103(__m128 a, __m128 b)
+{
+    float32x2_t a03 = vget_low_f32(
+        vextq_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(a), 3));
+    float32x2_t b21 = vget_high_f32(
+        vextq_f32(vreinterpretq_f32_m128(b), vreinterpretq_f32_m128(b), 3));
+    return vreinterpretq_m128_f32(vcombine_f32(a03, b21));
+}
+
+FORCE_INLINE __m128 _mm_shuffle_ps_1010(__m128 a, __m128 b)
+{
+    float32x2_t a10 = vget_low_f32(vreinterpretq_f32_m128(a));
+    float32x2_t b10 = vget_low_f32(vreinterpretq_f32_m128(b));
+    return vreinterpretq_m128_f32(vcombine_f32(a10, b10));
+}
+
+FORCE_INLINE __m128 _mm_shuffle_ps_1001(__m128 a, __m128 b)
+{
+    float32x2_t a01 = vrev64_f32(vget_low_f32(vreinterpretq_f32_m128(a)));
+    float32x2_t b10 = vget_low_f32(vreinterpretq_f32_m128(b));
+    return vreinterpretq_m128_f32(vcombine_f32(a01, b10));
+}
+
+FORCE_INLINE __m128 _mm_shuffle_ps_0101(__m128 a, __m128 b)
+{
+    float32x2_t a01 = vrev64_f32(vget_low_f32(vreinterpretq_f32_m128(a)));
+    float32x2_t b01 = vrev64_f32(vget_low_f32(vreinterpretq_f32_m128(b)));
+    return vreinterpretq_m128_f32(vcombine_f32(a01, b01));
+}
+
+// keeps the low 64 bits of b in the low and puts the high 64 bits of a in the
+// high
+FORCE_INLINE __m128 _mm_shuffle_ps_3210(__m128 a, __m128 b)
+{
+    float32x2_t a10 = vget_low_f32(vreinterpretq_f32_m128(a));
+    float32x2_t b32 = vget_high_f32(vreinterpretq_f32_m128(b));
+    return vreinterpretq_m128_f32(vcombine_f32(a10, b32));
+}
+
+FORCE_INLINE __m128 _mm_shuffle_ps_0011(__m128 a, __m128 b)
+{
+    float32x2_t a11 = vdup_lane_f32(vget_low_f32(vreinterpretq_f32_m128(a)), 1);
+    float32x2_t b00 = vdup_lane_f32(vget_low_f32(vreinterpretq_f32_m128(b)), 0);
+    return vreinterpretq_m128_f32(vcombine_f32(a11, b00));
+}
+
+FORCE_INLINE __m128 _mm_shuffle_ps_0022(__m128 a, __m128 b)
+{
+    float32x2_t a22 =
+        vdup_lane_f32(vget_high_f32(vreinterpretq_f32_m128(a)), 0);
+    float32x2_t b00 = vdup_lane_f32(vget_low_f32(vreinterpretq_f32_m128(b)), 0);
+    return vreinterpretq_m128_f32(vcombine_f32(a22, b00));
+}
+
+FORCE_INLINE __m128 _mm_shuffle_ps_2200(__m128 a, __m128 b)
+{
+    float32x2_t a00 = vdup_lane_f32(vget_low_f32(vreinterpretq_f32_m128(a)), 0);
+    float32x2_t b22 =
+        vdup_lane_f32(vget_high_f32(vreinterpretq_f32_m128(b)), 0);
+    return vreinterpretq_m128_f32(vcombine_f32(a00, b22));
+}
+
+FORCE_INLINE __m128 _mm_shuffle_ps_3202(__m128 a, __m128 b)
+{
+    float32_t a0 = vgetq_lane_f32(vreinterpretq_f32_m128(a), 0);
+    float32x2_t a22 =
+        vdup_lane_f32(vget_high_f32(vreinterpretq_f32_m128(a)), 0);
+    float32x2_t a02 = vset_lane_f32(a0, a22, 1); /* TODO: use vzip ?*/
+    float32x2_t b32 = vget_high_f32(vreinterpretq_f32_m128(b));
+    return vreinterpretq_m128_f32(vcombine_f32(a02, b32));
+}
+
+FORCE_INLINE __m128 _mm_shuffle_ps_1133(__m128 a, __m128 b)
+{
+    float32x2_t a33 =
+        vdup_lane_f32(vget_high_f32(vreinterpretq_f32_m128(a)), 1);
+    float32x2_t b11 = vdup_lane_f32(vget_low_f32(vreinterpretq_f32_m128(b)), 1);
+    return vreinterpretq_m128_f32(vcombine_f32(a33, b11));
+}
+
+FORCE_INLINE __m128 _mm_shuffle_ps_2010(__m128 a, __m128 b)
+{
+    float32x2_t a10 = vget_low_f32(vreinterpretq_f32_m128(a));
+    float32_t b2 = vgetq_lane_f32(vreinterpretq_f32_m128(b), 2);
+    float32x2_t b00 = vdup_lane_f32(vget_low_f32(vreinterpretq_f32_m128(b)), 0);
+    float32x2_t b20 = vset_lane_f32(b2, b00, 1);
+    return vreinterpretq_m128_f32(vcombine_f32(a10, b20));
+}
+
+FORCE_INLINE __m128 _mm_shuffle_ps_2001(__m128 a, __m128 b)
+{
+    float32x2_t a01 = vrev64_f32(vget_low_f32(vreinterpretq_f32_m128(a)));
+    float32_t b2 = vgetq_lane_f32(b, 2);
+    float32x2_t b00 = vdup_lane_f32(vget_low_f32(vreinterpretq_f32_m128(b)), 0);
+    float32x2_t b20 = vset_lane_f32(b2, b00, 1);
+    return vreinterpretq_m128_f32(vcombine_f32(a01, b20));
+}
+
+FORCE_INLINE __m128 _mm_shuffle_ps_2032(__m128 a, __m128 b)
+{
+    float32x2_t a32 = vget_high_f32(vreinterpretq_f32_m128(a));
+    float32_t b2 = vgetq_lane_f32(b, 2);
+    float32x2_t b00 = vdup_lane_f32(vget_low_f32(vreinterpretq_f32_m128(b)), 0);
+    float32x2_t b20 = vset_lane_f32(b2, b00, 1);
+    return vreinterpretq_m128_f32(vcombine_f32(a32, b20));
+}
+
+// For MSVC, we check only if it is ARM64, as every single ARM64 processor
+// supported by WoA has crypto extensions. If this changes in the future,
+// this can be verified via the runtime-only method of:
+// IsProcessorFeaturePresent(PF_ARM_V8_CRYPTO_INSTRUCTIONS_AVAILABLE)
+#if (defined(_M_ARM64) && !defined(__clang__)) || \
+    (defined(__ARM_FEATURE_CRYPTO) &&             \
+     (defined(__aarch64__) || __has_builtin(__builtin_arm_crypto_vmullp64)))
+// Wraps vmull_p64
+FORCE_INLINE uint64x2_t _sse2neon_vmull_p64(uint64x1_t _a, uint64x1_t _b)
+{
+    poly64_t a = vget_lane_p64(vreinterpret_p64_u64(_a), 0);
+    poly64_t b = vget_lane_p64(vreinterpret_p64_u64(_b), 0);
+#if defined(_MSC_VER) && !defined(__clang__)
+    __n64 a1 = {a}, b1 = {b};
+    return vreinterpretq_u64_p128(vmull_p64(a1, b1));
+#else
+    return vreinterpretq_u64_p128(vmull_p64(a, b));
+#endif
+}
+#else  // ARMv7 polyfill
+// ARMv7/some A64 lacks vmull_p64, but it has vmull_p8.
+//
+// vmull_p8 calculates 8 8-bit->16-bit polynomial multiplies, but we need a
+// 64-bit->128-bit polynomial multiply.
+//
+// It needs some work and is somewhat slow, but it is still faster than all
+// known scalar methods.
+//
+// Algorithm adapted to C from
+// https://www.workofard.com/2017/07/ghash-for-low-end-cores/, which is adapted
+// from "Fast Software Polynomial Multiplication on ARM Processors Using the
+// NEON Engine" by Danilo Camara, Conrado Gouvea, Julio Lopez and Ricardo Dahab
+// (https://hal.inria.fr/hal-01506572)
+static uint64x2_t _sse2neon_vmull_p64(uint64x1_t _a, uint64x1_t _b)
+{
+    poly8x8_t a = vreinterpret_p8_u64(_a);
+    poly8x8_t b = vreinterpret_p8_u64(_b);
+
+    // Masks
+    uint8x16_t k48_32 = vcombine_u8(vcreate_u8(0x0000ffffffffffff),
+                                    vcreate_u8(0x00000000ffffffff));
+    uint8x16_t k16_00 = vcombine_u8(vcreate_u8(0x000000000000ffff),
+                                    vcreate_u8(0x0000000000000000));
+
+    // Do the multiplies, rotating with vext to get all combinations
+    uint8x16_t d = vreinterpretq_u8_p16(vmull_p8(a, b));  // D = A0 * B0
+    uint8x16_t e =
+        vreinterpretq_u8_p16(vmull_p8(a, vext_p8(b, b, 1)));  // E = A0 * B1
+    uint8x16_t f =
+        vreinterpretq_u8_p16(vmull_p8(vext_p8(a, a, 1), b));  // F = A1 * B0
+    uint8x16_t g =
+        vreinterpretq_u8_p16(vmull_p8(a, vext_p8(b, b, 2)));  // G = A0 * B2
+    uint8x16_t h =
+        vreinterpretq_u8_p16(vmull_p8(vext_p8(a, a, 2), b));  // H = A2 * B0
+    uint8x16_t i =
+        vreinterpretq_u8_p16(vmull_p8(a, vext_p8(b, b, 3)));  // I = A0 * B3
+    uint8x16_t j =
+        vreinterpretq_u8_p16(vmull_p8(vext_p8(a, a, 3), b));  // J = A3 * B0
+    uint8x16_t k =
+        vreinterpretq_u8_p16(vmull_p8(a, vext_p8(b, b, 4)));  // L = A0 * B4
+
+    // Add cross products
+    uint8x16_t l = veorq_u8(e, f);  // L = E + F
+    uint8x16_t m = veorq_u8(g, h);  // M = G + H
+    uint8x16_t n = veorq_u8(i, j);  // N = I + J
+
+    // Interleave. Using vzip1 and vzip2 prevents Clang from emitting TBL
+    // instructions.
+#if defined(__aarch64__)
+    uint8x16_t lm_p0 = vreinterpretq_u8_u64(
+        vzip1q_u64(vreinterpretq_u64_u8(l), vreinterpretq_u64_u8(m)));
+    uint8x16_t lm_p1 = vreinterpretq_u8_u64(
+        vzip2q_u64(vreinterpretq_u64_u8(l), vreinterpretq_u64_u8(m)));
+    uint8x16_t nk_p0 = vreinterpretq_u8_u64(
+        vzip1q_u64(vreinterpretq_u64_u8(n), vreinterpretq_u64_u8(k)));
+    uint8x16_t nk_p1 = vreinterpretq_u8_u64(
+        vzip2q_u64(vreinterpretq_u64_u8(n), vreinterpretq_u64_u8(k)));
+#else
+    uint8x16_t lm_p0 = vcombine_u8(vget_low_u8(l), vget_low_u8(m));
+    uint8x16_t lm_p1 = vcombine_u8(vget_high_u8(l), vget_high_u8(m));
+    uint8x16_t nk_p0 = vcombine_u8(vget_low_u8(n), vget_low_u8(k));
+    uint8x16_t nk_p1 = vcombine_u8(vget_high_u8(n), vget_high_u8(k));
+#endif
+    // t0 = (L) (P0 + P1) << 8
+    // t1 = (M) (P2 + P3) << 16
+    uint8x16_t t0t1_tmp = veorq_u8(lm_p0, lm_p1);
+    uint8x16_t t0t1_h = vandq_u8(lm_p1, k48_32);
+    uint8x16_t t0t1_l = veorq_u8(t0t1_tmp, t0t1_h);
+
+    // t2 = (N) (P4 + P5) << 24
+    // t3 = (K) (P6 + P7) << 32
+    uint8x16_t t2t3_tmp = veorq_u8(nk_p0, nk_p1);
+    uint8x16_t t2t3_h = vandq_u8(nk_p1, k16_00);
+    uint8x16_t t2t3_l = veorq_u8(t2t3_tmp, t2t3_h);
+
+    // De-interleave
+#if defined(__aarch64__)
+    uint8x16_t t0 = vreinterpretq_u8_u64(
+        vuzp1q_u64(vreinterpretq_u64_u8(t0t1_l), vreinterpretq_u64_u8(t0t1_h)));
+    uint8x16_t t1 = vreinterpretq_u8_u64(
+        vuzp2q_u64(vreinterpretq_u64_u8(t0t1_l), vreinterpretq_u64_u8(t0t1_h)));
+    uint8x16_t t2 = vreinterpretq_u8_u64(
+        vuzp1q_u64(vreinterpretq_u64_u8(t2t3_l), vreinterpretq_u64_u8(t2t3_h)));
+    uint8x16_t t3 = vreinterpretq_u8_u64(
+        vuzp2q_u64(vreinterpretq_u64_u8(t2t3_l), vreinterpretq_u64_u8(t2t3_h)));
+#else
+    uint8x16_t t1 = vcombine_u8(vget_high_u8(t0t1_l), vget_high_u8(t0t1_h));
+    uint8x16_t t0 = vcombine_u8(vget_low_u8(t0t1_l), vget_low_u8(t0t1_h));
+    uint8x16_t t3 = vcombine_u8(vget_high_u8(t2t3_l), vget_high_u8(t2t3_h));
+    uint8x16_t t2 = vcombine_u8(vget_low_u8(t2t3_l), vget_low_u8(t2t3_h));
+#endif
+    // Shift the cross products
+    uint8x16_t t0_shift = vextq_u8(t0, t0, 15);  // t0 << 8
+    uint8x16_t t1_shift = vextq_u8(t1, t1, 14);  // t1 << 16
+    uint8x16_t t2_shift = vextq_u8(t2, t2, 13);  // t2 << 24
+    uint8x16_t t3_shift = vextq_u8(t3, t3, 12);  // t3 << 32
+
+    // Accumulate the products
+    uint8x16_t cross1 = veorq_u8(t0_shift, t1_shift);
+    uint8x16_t cross2 = veorq_u8(t2_shift, t3_shift);
+    uint8x16_t mix = veorq_u8(d, cross1);
+    uint8x16_t r = veorq_u8(mix, cross2);
+    return vreinterpretq_u64_u8(r);
+}
+#endif  // ARMv7 polyfill
+
+// C equivalent:
+//   __m128i _mm_shuffle_epi32_default(__m128i a,
+//                                     __constrange(0, 255) int imm) {
+//       __m128i ret;
+//       ret[0] = a[imm        & 0x3];   ret[1] = a[(imm >> 2) & 0x3];
+//       ret[2] = a[(imm >> 4) & 0x03];  ret[3] = a[(imm >> 6) & 0x03];
+//       return ret;
+//   }
+#define _mm_shuffle_epi32_default(a, imm)                                   \
+    vreinterpretq_m128i_s32(vsetq_lane_s32(                                 \
+        vgetq_lane_s32(vreinterpretq_s32_m128i(a), ((imm) >> 6) & 0x3),     \
+        vsetq_lane_s32(                                                     \
+            vgetq_lane_s32(vreinterpretq_s32_m128i(a), ((imm) >> 4) & 0x3), \
+            vsetq_lane_s32(vgetq_lane_s32(vreinterpretq_s32_m128i(a),       \
+                                          ((imm) >> 2) & 0x3),              \
+                           vmovq_n_s32(vgetq_lane_s32(                      \
+                               vreinterpretq_s32_m128i(a), (imm) & (0x3))), \
+                           1),                                              \
+            2),                                                             \
+        3))
+
+// Takes the upper 64 bits of a and places it in the low end of the result
+// Takes the lower 64 bits of a and places it into the high end of the result.
+FORCE_INLINE __m128i _mm_shuffle_epi_1032(__m128i a)
+{
+    int32x2_t a32 = vget_high_s32(vreinterpretq_s32_m128i(a));
+    int32x2_t a10 = vget_low_s32(vreinterpretq_s32_m128i(a));
+    return vreinterpretq_m128i_s32(vcombine_s32(a32, a10));
+}
+
+// takes the lower two 32-bit values from a and swaps them and places in low end
+// of result takes the higher two 32 bit values from a and swaps them and places
+// in high end of result.
+FORCE_INLINE __m128i _mm_shuffle_epi_2301(__m128i a)
+{
+    int32x2_t a01 = vrev64_s32(vget_low_s32(vreinterpretq_s32_m128i(a)));
+    int32x2_t a23 = vrev64_s32(vget_high_s32(vreinterpretq_s32_m128i(a)));
+    return vreinterpretq_m128i_s32(vcombine_s32(a01, a23));
+}
+
+// rotates the least significant 32 bits into the most significant 32 bits, and
+// shifts the rest down
+FORCE_INLINE __m128i _mm_shuffle_epi_0321(__m128i a)
+{
+    return vreinterpretq_m128i_s32(
+        vextq_s32(vreinterpretq_s32_m128i(a), vreinterpretq_s32_m128i(a), 1));
+}
+
+// rotates the most significant 32 bits into the least significant 32 bits, and
+// shifts the rest up
+FORCE_INLINE __m128i _mm_shuffle_epi_2103(__m128i a)
+{
+    return vreinterpretq_m128i_s32(
+        vextq_s32(vreinterpretq_s32_m128i(a), vreinterpretq_s32_m128i(a), 3));
+}
+
+// gets the lower 64 bits of a, and places it in the upper 64 bits
+// gets the lower 64 bits of a and places it in the lower 64 bits
+FORCE_INLINE __m128i _mm_shuffle_epi_1010(__m128i a)
+{
+    int32x2_t a10 = vget_low_s32(vreinterpretq_s32_m128i(a));
+    return vreinterpretq_m128i_s32(vcombine_s32(a10, a10));
+}
+
+// gets the lower 64 bits of a, swaps the 0 and 1 elements, and places it in the
+// lower 64 bits gets the lower 64 bits of a, and places it in the upper 64 bits
+FORCE_INLINE __m128i _mm_shuffle_epi_1001(__m128i a)
+{
+    int32x2_t a01 = vrev64_s32(vget_low_s32(vreinterpretq_s32_m128i(a)));
+    int32x2_t a10 = vget_low_s32(vreinterpretq_s32_m128i(a));
+    return vreinterpretq_m128i_s32(vcombine_s32(a01, a10));
+}
+
+// gets the lower 64 bits of a, swaps the 0 and 1 elements and places it in the
+// upper 64 bits gets the lower 64 bits of a, swaps the 0 and 1 elements, and
+// places it in the lower 64 bits
+FORCE_INLINE __m128i _mm_shuffle_epi_0101(__m128i a)
+{
+    int32x2_t a01 = vrev64_s32(vget_low_s32(vreinterpretq_s32_m128i(a)));
+    return vreinterpretq_m128i_s32(vcombine_s32(a01, a01));
+}
+
+FORCE_INLINE __m128i _mm_shuffle_epi_2211(__m128i a)
+{
+    int32x2_t a11 = vdup_lane_s32(vget_low_s32(vreinterpretq_s32_m128i(a)), 1);
+    int32x2_t a22 = vdup_lane_s32(vget_high_s32(vreinterpretq_s32_m128i(a)), 0);
+    return vreinterpretq_m128i_s32(vcombine_s32(a11, a22));
+}
+
+FORCE_INLINE __m128i _mm_shuffle_epi_0122(__m128i a)
+{
+    int32x2_t a22 = vdup_lane_s32(vget_high_s32(vreinterpretq_s32_m128i(a)), 0);
+    int32x2_t a01 = vrev64_s32(vget_low_s32(vreinterpretq_s32_m128i(a)));
+    return vreinterpretq_m128i_s32(vcombine_s32(a22, a01));
+}
+
+FORCE_INLINE __m128i _mm_shuffle_epi_3332(__m128i a)
+{
+    int32x2_t a32 = vget_high_s32(vreinterpretq_s32_m128i(a));
+    int32x2_t a33 = vdup_lane_s32(vget_high_s32(vreinterpretq_s32_m128i(a)), 1);
+    return vreinterpretq_m128i_s32(vcombine_s32(a32, a33));
+}
+
+#if defined(__aarch64__) || defined(_M_ARM64)
+#define _mm_shuffle_epi32_splat(a, imm) \
+    vreinterpretq_m128i_s32(vdupq_laneq_s32(vreinterpretq_s32_m128i(a), (imm)))
+#else
+#define _mm_shuffle_epi32_splat(a, imm) \
+    vreinterpretq_m128i_s32(            \
+        vdupq_n_s32(vgetq_lane_s32(vreinterpretq_s32_m128i(a), (imm))))
+#endif
+
+// NEON does not support a general purpose permute intrinsic.
+// Shuffle single-precision (32-bit) floating-point elements in a using the
+// control in imm8, and store the results in dst.
+//
+// C equivalent:
+//   __m128 _mm_shuffle_ps_default(__m128 a, __m128 b,
+//                                 __constrange(0, 255) int imm) {
+//       __m128 ret;
+//       ret[0] = a[imm        & 0x3];   ret[1] = a[(imm >> 2) & 0x3];
+//       ret[2] = b[(imm >> 4) & 0x03];  ret[3] = b[(imm >> 6) & 0x03];
+//       return ret;
+//   }
+//
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_shuffle_ps
+#define _mm_shuffle_ps_default(a, b, imm)                                      \
+    vreinterpretq_m128_f32(vsetq_lane_f32(                                     \
+        vgetq_lane_f32(vreinterpretq_f32_m128(b), ((imm) >> 6) & 0x3),         \
+        vsetq_lane_f32(                                                        \
+            vgetq_lane_f32(vreinterpretq_f32_m128(b), ((imm) >> 4) & 0x3),     \
+            vsetq_lane_f32(                                                    \
+                vgetq_lane_f32(vreinterpretq_f32_m128(a), ((imm) >> 2) & 0x3), \
+                vmovq_n_f32(                                                   \
+                    vgetq_lane_f32(vreinterpretq_f32_m128(a), (imm) & (0x3))), \
+                1),                                                            \
+            2),                                                                \
+        3))
+
+// Shuffle 16-bit integers in the low 64 bits of a using the control in imm8.
+// Store the results in the low 64 bits of dst, with the high 64 bits being
+// copied from a to dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_shufflelo_epi16
+#define _mm_shufflelo_epi16_function(a, imm)                                  \
+    _sse2neon_define1(                                                        \
+        __m128i, a, int16x8_t ret = vreinterpretq_s16_m128i(_a);              \
+        int16x4_t lowBits = vget_low_s16(ret);                                \
+        ret = vsetq_lane_s16(vget_lane_s16(lowBits, (imm) & (0x3)), ret, 0);  \
+        ret = vsetq_lane_s16(vget_lane_s16(lowBits, ((imm) >> 2) & 0x3), ret, \
+                             1);                                              \
+        ret = vsetq_lane_s16(vget_lane_s16(lowBits, ((imm) >> 4) & 0x3), ret, \
+                             2);                                              \
+        ret = vsetq_lane_s16(vget_lane_s16(lowBits, ((imm) >> 6) & 0x3), ret, \
+                             3);                                              \
+        _sse2neon_return(vreinterpretq_m128i_s16(ret));)
+
+// Shuffle 16-bit integers in the high 64 bits of a using the control in imm8.
+// Store the results in the high 64 bits of dst, with the low 64 bits being
+// copied from a to dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_shufflehi_epi16
+#define _mm_shufflehi_epi16_function(a, imm)                                   \
+    _sse2neon_define1(                                                         \
+        __m128i, a, int16x8_t ret = vreinterpretq_s16_m128i(_a);               \
+        int16x4_t highBits = vget_high_s16(ret);                               \
+        ret = vsetq_lane_s16(vget_lane_s16(highBits, (imm) & (0x3)), ret, 4);  \
+        ret = vsetq_lane_s16(vget_lane_s16(highBits, ((imm) >> 2) & 0x3), ret, \
+                             5);                                               \
+        ret = vsetq_lane_s16(vget_lane_s16(highBits, ((imm) >> 4) & 0x3), ret, \
+                             6);                                               \
+        ret = vsetq_lane_s16(vget_lane_s16(highBits, ((imm) >> 6) & 0x3), ret, \
+                             7);                                               \
+        _sse2neon_return(vreinterpretq_m128i_s16(ret));)
+
+/* MMX */
+
+//_mm_empty is a no-op on arm
+FORCE_INLINE void _mm_empty(void) {}
+
+/* SSE */
+
+// Add packed single-precision (32-bit) floating-point elements in a and b, and
+// store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_add_ps
+FORCE_INLINE __m128 _mm_add_ps(__m128 a, __m128 b)
+{
+    return vreinterpretq_m128_f32(
+        vaddq_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(b)));
+}
+
+// Add the lower single-precision (32-bit) floating-point element in a and b,
+// store the result in the lower element of dst, and copy the upper 3 packed
+// elements from a to the upper elements of dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_add_ss
+FORCE_INLINE __m128 _mm_add_ss(__m128 a, __m128 b)
+{
+    float32_t b0 = vgetq_lane_f32(vreinterpretq_f32_m128(b), 0);
+    float32x4_t value = vsetq_lane_f32(b0, vdupq_n_f32(0), 0);
+    // the upper values in the result must be the remnants of <a>.
+    return vreinterpretq_m128_f32(vaddq_f32(a, value));
+}
+
+// Compute the bitwise AND of packed single-precision (32-bit) floating-point
+// elements in a and b, and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_and_ps
+FORCE_INLINE __m128 _mm_and_ps(__m128 a, __m128 b)
+{
+    return vreinterpretq_m128_s32(
+        vandq_s32(vreinterpretq_s32_m128(a), vreinterpretq_s32_m128(b)));
+}
+
+// Compute the bitwise NOT of packed single-precision (32-bit) floating-point
+// elements in a and then AND with b, and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_andnot_ps
+FORCE_INLINE __m128 _mm_andnot_ps(__m128 a, __m128 b)
+{
+    return vreinterpretq_m128_s32(
+        vbicq_s32(vreinterpretq_s32_m128(b),
+                  vreinterpretq_s32_m128(a)));  // *NOTE* argument swap
+}
+
+// Average packed unsigned 16-bit integers in a and b, and store the results in
+// dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_avg_pu16
+FORCE_INLINE __m64 _mm_avg_pu16(__m64 a, __m64 b)
+{
+    return vreinterpret_m64_u16(
+        vrhadd_u16(vreinterpret_u16_m64(a), vreinterpret_u16_m64(b)));
+}
+
+// Average packed unsigned 8-bit integers in a and b, and store the results in
+// dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_avg_pu8
+FORCE_INLINE __m64 _mm_avg_pu8(__m64 a, __m64 b)
+{
+    return vreinterpret_m64_u8(
+        vrhadd_u8(vreinterpret_u8_m64(a), vreinterpret_u8_m64(b)));
+}
+
+// Compare packed single-precision (32-bit) floating-point elements in a and b
+// for equality, and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cmpeq_ps
+FORCE_INLINE __m128 _mm_cmpeq_ps(__m128 a, __m128 b)
+{
+    return vreinterpretq_m128_u32(
+        vceqq_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(b)));
+}
+
+// Compare the lower single-precision (32-bit) floating-point elements in a and
+// b for equality, store the result in the lower element of dst, and copy the
+// upper 3 packed elements from a to the upper elements of dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cmpeq_ss
+FORCE_INLINE __m128 _mm_cmpeq_ss(__m128 a, __m128 b)
+{
+    return _mm_move_ss(a, _mm_cmpeq_ps(a, b));
+}
+
+// Compare packed single-precision (32-bit) floating-point elements in a and b
+// for greater-than-or-equal, and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cmpge_ps
+FORCE_INLINE __m128 _mm_cmpge_ps(__m128 a, __m128 b)
+{
+    return vreinterpretq_m128_u32(
+        vcgeq_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(b)));
+}
+
+// Compare the lower single-precision (32-bit) floating-point elements in a and
+// b for greater-than-or-equal, store the result in the lower element of dst,
+// and copy the upper 3 packed elements from a to the upper elements of dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cmpge_ss
+FORCE_INLINE __m128 _mm_cmpge_ss(__m128 a, __m128 b)
+{
+    return _mm_move_ss(a, _mm_cmpge_ps(a, b));
+}
+
+// Compare packed single-precision (32-bit) floating-point elements in a and b
+// for greater-than, and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cmpgt_ps
+FORCE_INLINE __m128 _mm_cmpgt_ps(__m128 a, __m128 b)
+{
+    return vreinterpretq_m128_u32(
+        vcgtq_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(b)));
+}
+
+// Compare the lower single-precision (32-bit) floating-point elements in a and
+// b for greater-than, store the result in the lower element of dst, and copy
+// the upper 3 packed elements from a to the upper elements of dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cmpgt_ss
+FORCE_INLINE __m128 _mm_cmpgt_ss(__m128 a, __m128 b)
+{
+    return _mm_move_ss(a, _mm_cmpgt_ps(a, b));
+}
+
+// Compare packed single-precision (32-bit) floating-point elements in a and b
+// for less-than-or-equal, and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cmple_ps
+FORCE_INLINE __m128 _mm_cmple_ps(__m128 a, __m128 b)
+{
+    return vreinterpretq_m128_u32(
+        vcleq_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(b)));
+}
+
+// Compare the lower single-precision (32-bit) floating-point elements in a and
+// b for less-than-or-equal, store the result in the lower element of dst, and
+// copy the upper 3 packed elements from a to the upper elements of dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cmple_ss
+FORCE_INLINE __m128 _mm_cmple_ss(__m128 a, __m128 b)
+{
+    return _mm_move_ss(a, _mm_cmple_ps(a, b));
+}
+
+// Compare packed single-precision (32-bit) floating-point elements in a and b
+// for less-than, and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cmplt_ps
+FORCE_INLINE __m128 _mm_cmplt_ps(__m128 a, __m128 b)
+{
+    return vreinterpretq_m128_u32(
+        vcltq_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(b)));
+}
+
+// Compare the lower single-precision (32-bit) floating-point elements in a and
+// b for less-than, store the result in the lower element of dst, and copy the
+// upper 3 packed elements from a to the upper elements of dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cmplt_ss
+FORCE_INLINE __m128 _mm_cmplt_ss(__m128 a, __m128 b)
+{
+    return _mm_move_ss(a, _mm_cmplt_ps(a, b));
+}
+
+// Compare packed single-precision (32-bit) floating-point elements in a and b
+// for not-equal, and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cmpneq_ps
+FORCE_INLINE __m128 _mm_cmpneq_ps(__m128 a, __m128 b)
+{
+    return vreinterpretq_m128_u32(vmvnq_u32(
+        vceqq_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(b))));
+}
+
+// Compare the lower single-precision (32-bit) floating-point elements in a and
+// b for not-equal, store the result in the lower element of dst, and copy the
+// upper 3 packed elements from a to the upper elements of dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cmpneq_ss
+FORCE_INLINE __m128 _mm_cmpneq_ss(__m128 a, __m128 b)
+{
+    return _mm_move_ss(a, _mm_cmpneq_ps(a, b));
+}
+
+// Compare packed single-precision (32-bit) floating-point elements in a and b
+// for not-greater-than-or-equal, and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cmpnge_ps
+FORCE_INLINE __m128 _mm_cmpnge_ps(__m128 a, __m128 b)
+{
+    return vreinterpretq_m128_u32(vmvnq_u32(
+        vcgeq_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(b))));
+}
+
+// Compare the lower single-precision (32-bit) floating-point elements in a and
+// b for not-greater-than-or-equal, store the result in the lower element of
+// dst, and copy the upper 3 packed elements from a to the upper elements of
+// dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cmpnge_ss
+FORCE_INLINE __m128 _mm_cmpnge_ss(__m128 a, __m128 b)
+{
+    return _mm_move_ss(a, _mm_cmpnge_ps(a, b));
+}
+
+// Compare packed single-precision (32-bit) floating-point elements in a and b
+// for not-greater-than, and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cmpngt_ps
+FORCE_INLINE __m128 _mm_cmpngt_ps(__m128 a, __m128 b)
+{
+    return vreinterpretq_m128_u32(vmvnq_u32(
+        vcgtq_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(b))));
+}
+
+// Compare the lower single-precision (32-bit) floating-point elements in a and
+// b for not-greater-than, store the result in the lower element of dst, and
+// copy the upper 3 packed elements from a to the upper elements of dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cmpngt_ss
+FORCE_INLINE __m128 _mm_cmpngt_ss(__m128 a, __m128 b)
+{
+    return _mm_move_ss(a, _mm_cmpngt_ps(a, b));
+}
+
+// Compare packed single-precision (32-bit) floating-point elements in a and b
+// for not-less-than-or-equal, and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cmpnle_ps
+FORCE_INLINE __m128 _mm_cmpnle_ps(__m128 a, __m128 b)
+{
+    return vreinterpretq_m128_u32(vmvnq_u32(
+        vcleq_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(b))));
+}
+
+// Compare the lower single-precision (32-bit) floating-point elements in a and
+// b for not-less-than-or-equal, store the result in the lower element of dst,
+// and copy the upper 3 packed elements from a to the upper elements of dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cmpnle_ss
+FORCE_INLINE __m128 _mm_cmpnle_ss(__m128 a, __m128 b)
+{
+    return _mm_move_ss(a, _mm_cmpnle_ps(a, b));
+}
+
+// Compare packed single-precision (32-bit) floating-point elements in a and b
+// for not-less-than, and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cmpnlt_ps
+FORCE_INLINE __m128 _mm_cmpnlt_ps(__m128 a, __m128 b)
+{
+    return vreinterpretq_m128_u32(vmvnq_u32(
+        vcltq_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(b))));
+}
+
+// Compare the lower single-precision (32-bit) floating-point elements in a and
+// b for not-less-than, store the result in the lower element of dst, and copy
+// the upper 3 packed elements from a to the upper elements of dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cmpnlt_ss
+FORCE_INLINE __m128 _mm_cmpnlt_ss(__m128 a, __m128 b)
+{
+    return _mm_move_ss(a, _mm_cmpnlt_ps(a, b));
+}
+
+// Compare packed single-precision (32-bit) floating-point elements in a and b
+// to see if neither is NaN, and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cmpord_ps
+//
+// See also:
+// http://stackoverflow.com/questions/8627331/what-does-ordered-unordered-comparison-mean
+// http://stackoverflow.com/questions/29349621/neon-isnanval-intrinsics
+FORCE_INLINE __m128 _mm_cmpord_ps(__m128 a, __m128 b)
+{
+    // Note: NEON does not have ordered compare builtin
+    // Need to compare a eq a and b eq b to check for NaN
+    // Do AND of results to get final
+    uint32x4_t ceqaa =
+        vceqq_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(a));
+    uint32x4_t ceqbb =
+        vceqq_f32(vreinterpretq_f32_m128(b), vreinterpretq_f32_m128(b));
+    return vreinterpretq_m128_u32(vandq_u32(ceqaa, ceqbb));
+}
+
+// Compare the lower single-precision (32-bit) floating-point elements in a and
+// b to see if neither is NaN, store the result in the lower element of dst, and
+// copy the upper 3 packed elements from a to the upper elements of dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cmpord_ss
+FORCE_INLINE __m128 _mm_cmpord_ss(__m128 a, __m128 b)
+{
+    return _mm_move_ss(a, _mm_cmpord_ps(a, b));
+}
+
+// Compare packed single-precision (32-bit) floating-point elements in a and b
+// to see if either is NaN, and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cmpunord_ps
+FORCE_INLINE __m128 _mm_cmpunord_ps(__m128 a, __m128 b)
+{
+    uint32x4_t f32a =
+        vceqq_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(a));
+    uint32x4_t f32b =
+        vceqq_f32(vreinterpretq_f32_m128(b), vreinterpretq_f32_m128(b));
+    return vreinterpretq_m128_u32(vmvnq_u32(vandq_u32(f32a, f32b)));
+}
+
+// Compare the lower single-precision (32-bit) floating-point elements in a and
+// b to see if either is NaN, store the result in the lower element of dst, and
+// copy the upper 3 packed elements from a to the upper elements of dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cmpunord_ss
+FORCE_INLINE __m128 _mm_cmpunord_ss(__m128 a, __m128 b)
+{
+    return _mm_move_ss(a, _mm_cmpunord_ps(a, b));
+}
+
+// Compare the lower single-precision (32-bit) floating-point element in a and b
+// for equality, and return the boolean result (0 or 1).
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_comieq_ss
+FORCE_INLINE int _mm_comieq_ss(__m128 a, __m128 b)
+{
+    uint32x4_t a_eq_b =
+        vceqq_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(b));
+    return vgetq_lane_u32(a_eq_b, 0) & 0x1;
+}
+
+// Compare the lower single-precision (32-bit) floating-point element in a and b
+// for greater-than-or-equal, and return the boolean result (0 or 1).
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_comige_ss
+FORCE_INLINE int _mm_comige_ss(__m128 a, __m128 b)
+{
+    uint32x4_t a_ge_b =
+        vcgeq_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(b));
+    return vgetq_lane_u32(a_ge_b, 0) & 0x1;
+}
+
+// Compare the lower single-precision (32-bit) floating-point element in a and b
+// for greater-than, and return the boolean result (0 or 1).
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_comigt_ss
+FORCE_INLINE int _mm_comigt_ss(__m128 a, __m128 b)
+{
+    uint32x4_t a_gt_b =
+        vcgtq_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(b));
+    return vgetq_lane_u32(a_gt_b, 0) & 0x1;
+}
+
+// Compare the lower single-precision (32-bit) floating-point element in a and b
+// for less-than-or-equal, and return the boolean result (0 or 1).
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_comile_ss
+FORCE_INLINE int _mm_comile_ss(__m128 a, __m128 b)
+{
+    uint32x4_t a_le_b =
+        vcleq_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(b));
+    return vgetq_lane_u32(a_le_b, 0) & 0x1;
+}
+
+// Compare the lower single-precision (32-bit) floating-point element in a and b
+// for less-than, and return the boolean result (0 or 1).
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_comilt_ss
+FORCE_INLINE int _mm_comilt_ss(__m128 a, __m128 b)
+{
+    uint32x4_t a_lt_b =
+        vcltq_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(b));
+    return vgetq_lane_u32(a_lt_b, 0) & 0x1;
+}
+
+// Compare the lower single-precision (32-bit) floating-point element in a and b
+// for not-equal, and return the boolean result (0 or 1).
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_comineq_ss
+FORCE_INLINE int _mm_comineq_ss(__m128 a, __m128 b)
+{
+    return !_mm_comieq_ss(a, b);
+}
+
+// Convert packed signed 32-bit integers in b to packed single-precision
+// (32-bit) floating-point elements, store the results in the lower 2 elements
+// of dst, and copy the upper 2 packed elements from a to the upper elements of
+// dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cvt_pi2ps
+FORCE_INLINE __m128 _mm_cvt_pi2ps(__m128 a, __m64 b)
+{
+    return vreinterpretq_m128_f32(
+        vcombine_f32(vcvt_f32_s32(vreinterpret_s32_m64(b)),
+                     vget_high_f32(vreinterpretq_f32_m128(a))));
+}
+
+// Convert packed single-precision (32-bit) floating-point elements in a to
+// packed 32-bit integers, and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cvt_ps2pi
+FORCE_INLINE __m64 _mm_cvt_ps2pi(__m128 a)
+{
+#if (defined(__aarch64__) || defined(_M_ARM64)) || \
+    defined(__ARM_FEATURE_DIRECTED_ROUNDING)
+    return vreinterpret_m64_s32(
+        vget_low_s32(vcvtnq_s32_f32(vrndiq_f32(vreinterpretq_f32_m128(a)))));
+#else
+    return vreinterpret_m64_s32(vcvt_s32_f32(vget_low_f32(
+        vreinterpretq_f32_m128(_mm_round_ps(a, _MM_FROUND_CUR_DIRECTION)))));
+#endif
+}
+
+// Convert the signed 32-bit integer b to a single-precision (32-bit)
+// floating-point element, store the result in the lower element of dst, and
+// copy the upper 3 packed elements from a to the upper elements of dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cvt_si2ss
+FORCE_INLINE __m128 _mm_cvt_si2ss(__m128 a, int b)
+{
+    return vreinterpretq_m128_f32(
+        vsetq_lane_f32((float) b, vreinterpretq_f32_m128(a), 0));
+}
+
+// Convert the lower single-precision (32-bit) floating-point element in a to a
+// 32-bit integer, and store the result in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cvt_ss2si
+FORCE_INLINE int _mm_cvt_ss2si(__m128 a)
+{
+#if (defined(__aarch64__) || defined(_M_ARM64)) || \
+    defined(__ARM_FEATURE_DIRECTED_ROUNDING)
+    return vgetq_lane_s32(vcvtnq_s32_f32(vrndiq_f32(vreinterpretq_f32_m128(a))),
+                          0);
+#else
+    float32_t data = vgetq_lane_f32(
+        vreinterpretq_f32_m128(_mm_round_ps(a, _MM_FROUND_CUR_DIRECTION)), 0);
+    return (int32_t) data;
+#endif
+}
+
+// Convert packed 16-bit integers in a to packed single-precision (32-bit)
+// floating-point elements, and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cvtpi16_ps
+FORCE_INLINE __m128 _mm_cvtpi16_ps(__m64 a)
+{
+    return vreinterpretq_m128_f32(
+        vcvtq_f32_s32(vmovl_s16(vreinterpret_s16_m64(a))));
+}
+
+// Convert packed 32-bit integers in b to packed single-precision (32-bit)
+// floating-point elements, store the results in the lower 2 elements of dst,
+// and copy the upper 2 packed elements from a to the upper elements of dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cvtpi32_ps
+FORCE_INLINE __m128 _mm_cvtpi32_ps(__m128 a, __m64 b)
+{
+    return vreinterpretq_m128_f32(
+        vcombine_f32(vcvt_f32_s32(vreinterpret_s32_m64(b)),
+                     vget_high_f32(vreinterpretq_f32_m128(a))));
+}
+
+// Convert packed signed 32-bit integers in a to packed single-precision
+// (32-bit) floating-point elements, store the results in the lower 2 elements
+// of dst, then convert the packed signed 32-bit integers in b to
+// single-precision (32-bit) floating-point element, and store the results in
+// the upper 2 elements of dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cvtpi32x2_ps
+FORCE_INLINE __m128 _mm_cvtpi32x2_ps(__m64 a, __m64 b)
+{
+    return vreinterpretq_m128_f32(vcvtq_f32_s32(
+        vcombine_s32(vreinterpret_s32_m64(a), vreinterpret_s32_m64(b))));
+}
+
+// Convert the lower packed 8-bit integers in a to packed single-precision
+// (32-bit) floating-point elements, and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cvtpi8_ps
+FORCE_INLINE __m128 _mm_cvtpi8_ps(__m64 a)
+{
+    return vreinterpretq_m128_f32(vcvtq_f32_s32(
+        vmovl_s16(vget_low_s16(vmovl_s8(vreinterpret_s8_m64(a))))));
+}
+
+// Convert packed single-precision (32-bit) floating-point elements in a to
+// packed 16-bit integers, and store the results in dst. Note: this intrinsic
+// will generate 0x7FFF, rather than 0x8000, for input values between 0x7FFF and
+// 0x7FFFFFFF.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cvtps_pi16
+FORCE_INLINE __m64 _mm_cvtps_pi16(__m128 a)
+{
+    return vreinterpret_m64_s16(
+        vqmovn_s32(vreinterpretq_s32_m128i(_mm_cvtps_epi32(a))));
+}
+
+// Convert packed single-precision (32-bit) floating-point elements in a to
+// packed 32-bit integers, and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cvtps_pi32
+#define _mm_cvtps_pi32(a) _mm_cvt_ps2pi(a)
+
+// Convert packed single-precision (32-bit) floating-point elements in a to
+// packed 8-bit integers, and store the results in lower 4 elements of dst.
+// Note: this intrinsic will generate 0x7F, rather than 0x80, for input values
+// between 0x7F and 0x7FFFFFFF.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cvtps_pi8
+FORCE_INLINE __m64 _mm_cvtps_pi8(__m128 a)
+{
+    return vreinterpret_m64_s8(vqmovn_s16(
+        vcombine_s16(vreinterpret_s16_m64(_mm_cvtps_pi16(a)), vdup_n_s16(0))));
+}
+
+// Convert packed unsigned 16-bit integers in a to packed single-precision
+// (32-bit) floating-point elements, and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cvtpu16_ps
+FORCE_INLINE __m128 _mm_cvtpu16_ps(__m64 a)
+{
+    return vreinterpretq_m128_f32(
+        vcvtq_f32_u32(vmovl_u16(vreinterpret_u16_m64(a))));
+}
+
+// Convert the lower packed unsigned 8-bit integers in a to packed
+// single-precision (32-bit) floating-point elements, and store the results in
+// dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cvtpu8_ps
+FORCE_INLINE __m128 _mm_cvtpu8_ps(__m64 a)
+{
+    return vreinterpretq_m128_f32(vcvtq_f32_u32(
+        vmovl_u16(vget_low_u16(vmovl_u8(vreinterpret_u8_m64(a))))));
+}
+
+// Convert the signed 32-bit integer b to a single-precision (32-bit)
+// floating-point element, store the result in the lower element of dst, and
+// copy the upper 3 packed elements from a to the upper elements of dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cvtsi32_ss
+#define _mm_cvtsi32_ss(a, b) _mm_cvt_si2ss(a, b)
+
+// Convert the signed 64-bit integer b to a single-precision (32-bit)
+// floating-point element, store the result in the lower element of dst, and
+// copy the upper 3 packed elements from a to the upper elements of dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cvtsi64_ss
+FORCE_INLINE __m128 _mm_cvtsi64_ss(__m128 a, int64_t b)
+{
+    return vreinterpretq_m128_f32(
+        vsetq_lane_f32((float) b, vreinterpretq_f32_m128(a), 0));
+}
+
+// Copy the lower single-precision (32-bit) floating-point element of a to dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cvtss_f32
+FORCE_INLINE float _mm_cvtss_f32(__m128 a)
+{
+    return vgetq_lane_f32(vreinterpretq_f32_m128(a), 0);
+}
+
+// Convert the lower single-precision (32-bit) floating-point element in a to a
+// 32-bit integer, and store the result in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cvtss_si32
+#define _mm_cvtss_si32(a) _mm_cvt_ss2si(a)
+
+// Convert the lower single-precision (32-bit) floating-point element in a to a
+// 64-bit integer, and store the result in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cvtss_si64
+FORCE_INLINE int64_t _mm_cvtss_si64(__m128 a)
+{
+#if (defined(__aarch64__) || defined(_M_ARM64)) || \
+    defined(__ARM_FEATURE_DIRECTED_ROUNDING)
+    return (int64_t) vgetq_lane_f32(vrndiq_f32(vreinterpretq_f32_m128(a)), 0);
+#else
+    float32_t data = vgetq_lane_f32(
+        vreinterpretq_f32_m128(_mm_round_ps(a, _MM_FROUND_CUR_DIRECTION)), 0);
+    return (int64_t) data;
+#endif
+}
+
+// Convert packed single-precision (32-bit) floating-point elements in a to
+// packed 32-bit integers with truncation, and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cvtt_ps2pi
+FORCE_INLINE __m64 _mm_cvtt_ps2pi(__m128 a)
+{
+    return vreinterpret_m64_s32(
+        vget_low_s32(vcvtq_s32_f32(vreinterpretq_f32_m128(a))));
+}
+
+// Convert the lower single-precision (32-bit) floating-point element in a to a
+// 32-bit integer with truncation, and store the result in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cvtt_ss2si
+FORCE_INLINE int _mm_cvtt_ss2si(__m128 a)
+{
+    return vgetq_lane_s32(vcvtq_s32_f32(vreinterpretq_f32_m128(a)), 0);
+}
+
+// Convert packed single-precision (32-bit) floating-point elements in a to
+// packed 32-bit integers with truncation, and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cvttps_pi32
+#define _mm_cvttps_pi32(a) _mm_cvtt_ps2pi(a)
+
+// Convert the lower single-precision (32-bit) floating-point element in a to a
+// 32-bit integer with truncation, and store the result in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cvttss_si32
+#define _mm_cvttss_si32(a) _mm_cvtt_ss2si(a)
+
+// Convert the lower single-precision (32-bit) floating-point element in a to a
+// 64-bit integer with truncation, and store the result in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cvttss_si64
+FORCE_INLINE int64_t _mm_cvttss_si64(__m128 a)
+{
+    return (int64_t) vgetq_lane_f32(vreinterpretq_f32_m128(a), 0);
+}
+
+// Divide packed single-precision (32-bit) floating-point elements in a by
+// packed elements in b, and store the results in dst.
+// Due to ARMv7-A NEON's lack of a precise division intrinsic, we implement
+// division by multiplying a by b's reciprocal before using the Newton-Raphson
+// method to approximate the results.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_div_ps
+FORCE_INLINE __m128 _mm_div_ps(__m128 a, __m128 b)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    return vreinterpretq_m128_f32(
+        vdivq_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(b)));
+#else
+    float32x4_t recip = vrecpeq_f32(vreinterpretq_f32_m128(b));
+    recip = vmulq_f32(recip, vrecpsq_f32(recip, vreinterpretq_f32_m128(b)));
+    // Additional Netwon-Raphson iteration for accuracy
+    recip = vmulq_f32(recip, vrecpsq_f32(recip, vreinterpretq_f32_m128(b)));
+    return vreinterpretq_m128_f32(vmulq_f32(vreinterpretq_f32_m128(a), recip));
+#endif
+}
+
+// Divide the lower single-precision (32-bit) floating-point element in a by the
+// lower single-precision (32-bit) floating-point element in b, store the result
+// in the lower element of dst, and copy the upper 3 packed elements from a to
+// the upper elements of dst.
+// Warning: ARMv7-A does not produce the same result compared to Intel and not
+// IEEE-compliant.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_div_ss
+FORCE_INLINE __m128 _mm_div_ss(__m128 a, __m128 b)
+{
+    float32_t value =
+        vgetq_lane_f32(vreinterpretq_f32_m128(_mm_div_ps(a, b)), 0);
+    return vreinterpretq_m128_f32(
+        vsetq_lane_f32(value, vreinterpretq_f32_m128(a), 0));
+}
+
+// Extract a 16-bit integer from a, selected with imm8, and store the result in
+// the lower element of dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_extract_pi16
+#define _mm_extract_pi16(a, imm) \
+    (int32_t) vget_lane_u16(vreinterpret_u16_m64(a), (imm))
+
+// Free aligned memory that was allocated with _mm_malloc.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_free
+#if !defined(SSE2NEON_ALLOC_DEFINED)
+FORCE_INLINE void _mm_free(void *addr)
+{
+    free(addr);
+}
+#endif
+
+FORCE_INLINE uint64_t _sse2neon_get_fpcr(void)
+{
+    uint64_t value;
+#if defined(_MSC_VER) && !defined(__clang__)
+    value = _ReadStatusReg(ARM64_FPCR);
+#else
+    __asm__ __volatile__("mrs %0, FPCR" : "=r"(value)); /* read */
+#endif
+    return value;
+}
+
+FORCE_INLINE void _sse2neon_set_fpcr(uint64_t value)
+{
+#if defined(_MSC_VER) && !defined(__clang__)
+    _WriteStatusReg(ARM64_FPCR, value);
+#else
+    __asm__ __volatile__("msr FPCR, %0" ::"r"(value));  /* write */
+#endif
+}
+
+// Macro: Get the flush zero bits from the MXCSR control and status register.
+// The flush zero may contain any of the following flags: _MM_FLUSH_ZERO_ON or
+// _MM_FLUSH_ZERO_OFF
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_MM_GET_FLUSH_ZERO_MODE
+FORCE_INLINE unsigned int _sse2neon_mm_get_flush_zero_mode(void)
+{
+    union {
+        fpcr_bitfield field;
+#if defined(__aarch64__) || defined(_M_ARM64)
+        uint64_t value;
+#else
+        uint32_t value;
+#endif
+    } r;
+
+#if defined(__aarch64__) || defined(_M_ARM64)
+    r.value = _sse2neon_get_fpcr();
+#else
+    __asm__ __volatile__("vmrs %0, FPSCR" : "=r"(r.value)); /* read */
+#endif
+
+    return r.field.bit24 ? _MM_FLUSH_ZERO_ON : _MM_FLUSH_ZERO_OFF;
+}
+
+// Macro: Get the rounding mode bits from the MXCSR control and status register.
+// The rounding mode may contain any of the following flags: _MM_ROUND_NEAREST,
+// _MM_ROUND_DOWN, _MM_ROUND_UP, _MM_ROUND_TOWARD_ZERO
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_MM_GET_ROUNDING_MODE
+FORCE_INLINE unsigned int _MM_GET_ROUNDING_MODE(void)
+{
+    union {
+        fpcr_bitfield field;
+#if defined(__aarch64__) || defined(_M_ARM64)
+        uint64_t value;
+#else
+        uint32_t value;
+#endif
+    } r;
+
+#if defined(__aarch64__) || defined(_M_ARM64)
+    r.value = _sse2neon_get_fpcr();
+#else
+    __asm__ __volatile__("vmrs %0, FPSCR" : "=r"(r.value)); /* read */
+#endif
+
+    if (r.field.bit22) {
+        return r.field.bit23 ? _MM_ROUND_TOWARD_ZERO : _MM_ROUND_UP;
+    } else {
+        return r.field.bit23 ? _MM_ROUND_DOWN : _MM_ROUND_NEAREST;
+    }
+}
+
+// Copy a to dst, and insert the 16-bit integer i into dst at the location
+// specified by imm8.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_insert_pi16
+#define _mm_insert_pi16(a, b, imm) \
+    vreinterpret_m64_s16(vset_lane_s16((b), vreinterpret_s16_m64(a), (imm)))
+
+// Load 128-bits (composed of 4 packed single-precision (32-bit) floating-point
+// elements) from memory into dst. mem_addr must be aligned on a 16-byte
+// boundary or a general-protection exception may be generated.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_load_ps
+FORCE_INLINE __m128 _mm_load_ps(const float *p)
+{
+    return vreinterpretq_m128_f32(vld1q_f32(p));
+}
+
+// Load a single-precision (32-bit) floating-point element from memory into all
+// elements of dst.
+//
+//   dst[31:0] := MEM[mem_addr+31:mem_addr]
+//   dst[63:32] := MEM[mem_addr+31:mem_addr]
+//   dst[95:64] := MEM[mem_addr+31:mem_addr]
+//   dst[127:96] := MEM[mem_addr+31:mem_addr]
+//
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_load_ps1
+#define _mm_load_ps1 _mm_load1_ps
+
+// Load a single-precision (32-bit) floating-point element from memory into the
+// lower of dst, and zero the upper 3 elements. mem_addr does not need to be
+// aligned on any particular boundary.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_load_ss
+FORCE_INLINE __m128 _mm_load_ss(const float *p)
+{
+    return vreinterpretq_m128_f32(vsetq_lane_f32(*p, vdupq_n_f32(0), 0));
+}
+
+// Load a single-precision (32-bit) floating-point element from memory into all
+// elements of dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_load1_ps
+FORCE_INLINE __m128 _mm_load1_ps(const float *p)
+{
+    return vreinterpretq_m128_f32(vld1q_dup_f32(p));
+}
+
+// Load 2 single-precision (32-bit) floating-point elements from memory into the
+// upper 2 elements of dst, and copy the lower 2 elements from a to dst.
+// mem_addr does not need to be aligned on any particular boundary.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_loadh_pi
+FORCE_INLINE __m128 _mm_loadh_pi(__m128 a, __m64 const *p)
+{
+    return vreinterpretq_m128_f32(
+        vcombine_f32(vget_low_f32(a), vld1_f32((const float32_t *) p)));
+}
+
+// Load 2 single-precision (32-bit) floating-point elements from memory into the
+// lower 2 elements of dst, and copy the upper 2 elements from a to dst.
+// mem_addr does not need to be aligned on any particular boundary.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_loadl_pi
+FORCE_INLINE __m128 _mm_loadl_pi(__m128 a, __m64 const *p)
+{
+    return vreinterpretq_m128_f32(
+        vcombine_f32(vld1_f32((const float32_t *) p), vget_high_f32(a)));
+}
+
+// Load 4 single-precision (32-bit) floating-point elements from memory into dst
+// in reverse order. mem_addr must be aligned on a 16-byte boundary or a
+// general-protection exception may be generated.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_loadr_ps
+FORCE_INLINE __m128 _mm_loadr_ps(const float *p)
+{
+    float32x4_t v = vrev64q_f32(vld1q_f32(p));
+    return vreinterpretq_m128_f32(vextq_f32(v, v, 2));
+}
+
+// Load 128-bits (composed of 4 packed single-precision (32-bit) floating-point
+// elements) from memory into dst. mem_addr does not need to be aligned on any
+// particular boundary.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_loadu_ps
+FORCE_INLINE __m128 _mm_loadu_ps(const float *p)
+{
+    // for neon, alignment doesn't matter, so _mm_load_ps and _mm_loadu_ps are
+    // equivalent for neon
+    return vreinterpretq_m128_f32(vld1q_f32(p));
+}
+
+// Load unaligned 16-bit integer from memory into the first element of dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_loadu_si16
+FORCE_INLINE __m128i _mm_loadu_si16(const void *p)
+{
+    return vreinterpretq_m128i_s16(
+        vsetq_lane_s16(*(const unaligned_int16_t *) p, vdupq_n_s16(0), 0));
+}
+
+// Load unaligned 64-bit integer from memory into the first element of dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_loadu_si64
+FORCE_INLINE __m128i _mm_loadu_si64(const void *p)
+{
+    return vreinterpretq_m128i_s64(
+        vsetq_lane_s64(*(const unaligned_int64_t *) p, vdupq_n_s64(0), 0));
+}
+
+// Allocate size bytes of memory, aligned to the alignment specified in align,
+// and return a pointer to the allocated memory. _mm_free should be used to free
+// memory that is allocated with _mm_malloc.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_malloc
+#if !defined(SSE2NEON_ALLOC_DEFINED)
+FORCE_INLINE void *_mm_malloc(size_t size, size_t align)
+{
+    void *ptr;
+    if (align == 1)
+        return malloc(size);
+    if (align == 2 || (sizeof(void *) == 8 && align == 4))
+        align = sizeof(void *);
+    if (!posix_memalign(&ptr, align, size))
+        return ptr;
+    return NULL;
+}
+#endif
+
+// Conditionally store 8-bit integer elements from a into memory using mask
+// (elements are not stored when the highest bit is not set in the corresponding
+// element) and a non-temporal memory hint.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_maskmove_si64
+FORCE_INLINE void _mm_maskmove_si64(__m64 a, __m64 mask, char *mem_addr)
+{
+    int8x8_t shr_mask = vshr_n_s8(vreinterpret_s8_m64(mask), 7);
+    __m128 b = _mm_load_ps((const float *) mem_addr);
+    int8x8_t masked =
+        vbsl_s8(vreinterpret_u8_s8(shr_mask), vreinterpret_s8_m64(a),
+                vreinterpret_s8_u64(vget_low_u64(vreinterpretq_u64_m128(b))));
+    vst1_s8((int8_t *) mem_addr, masked);
+}
+
+// Conditionally store 8-bit integer elements from a into memory using mask
+// (elements are not stored when the highest bit is not set in the corresponding
+// element) and a non-temporal memory hint.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_m_maskmovq
+#define _m_maskmovq(a, mask, mem_addr) _mm_maskmove_si64(a, mask, mem_addr)
+
+// Compare packed signed 16-bit integers in a and b, and store packed maximum
+// values in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_max_pi16
+FORCE_INLINE __m64 _mm_max_pi16(__m64 a, __m64 b)
+{
+    return vreinterpret_m64_s16(
+        vmax_s16(vreinterpret_s16_m64(a), vreinterpret_s16_m64(b)));
+}
+
+// Compare packed single-precision (32-bit) floating-point elements in a and b,
+// and store packed maximum values in dst. dst does not follow the IEEE Standard
+// for Floating-Point Arithmetic (IEEE 754) maximum value when inputs are NaN or
+// signed-zero values.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_max_ps
+FORCE_INLINE __m128 _mm_max_ps(__m128 a, __m128 b)
+{
+#if SSE2NEON_PRECISE_MINMAX
+    float32x4_t _a = vreinterpretq_f32_m128(a);
+    float32x4_t _b = vreinterpretq_f32_m128(b);
+    return vreinterpretq_m128_f32(vbslq_f32(vcgtq_f32(_a, _b), _a, _b));
+#else
+    return vreinterpretq_m128_f32(
+        vmaxq_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(b)));
+#endif
+}
+
+// Compare packed unsigned 8-bit integers in a and b, and store packed maximum
+// values in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_max_pu8
+FORCE_INLINE __m64 _mm_max_pu8(__m64 a, __m64 b)
+{
+    return vreinterpret_m64_u8(
+        vmax_u8(vreinterpret_u8_m64(a), vreinterpret_u8_m64(b)));
+}
+
+// Compare the lower single-precision (32-bit) floating-point elements in a and
+// b, store the maximum value in the lower element of dst, and copy the upper 3
+// packed elements from a to the upper element of dst. dst does not follow the
+// IEEE Standard for Floating-Point Arithmetic (IEEE 754) maximum value when
+// inputs are NaN or signed-zero values.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_max_ss
+FORCE_INLINE __m128 _mm_max_ss(__m128 a, __m128 b)
+{
+    float32_t value = vgetq_lane_f32(_mm_max_ps(a, b), 0);
+    return vreinterpretq_m128_f32(
+        vsetq_lane_f32(value, vreinterpretq_f32_m128(a), 0));
+}
+
+// Compare packed signed 16-bit integers in a and b, and store packed minimum
+// values in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_min_pi16
+FORCE_INLINE __m64 _mm_min_pi16(__m64 a, __m64 b)
+{
+    return vreinterpret_m64_s16(
+        vmin_s16(vreinterpret_s16_m64(a), vreinterpret_s16_m64(b)));
+}
+
+// Compare packed single-precision (32-bit) floating-point elements in a and b,
+// and store packed minimum values in dst. dst does not follow the IEEE Standard
+// for Floating-Point Arithmetic (IEEE 754) minimum value when inputs are NaN or
+// signed-zero values.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_min_ps
+FORCE_INLINE __m128 _mm_min_ps(__m128 a, __m128 b)
+{
+#if SSE2NEON_PRECISE_MINMAX
+    float32x4_t _a = vreinterpretq_f32_m128(a);
+    float32x4_t _b = vreinterpretq_f32_m128(b);
+    return vreinterpretq_m128_f32(vbslq_f32(vcltq_f32(_a, _b), _a, _b));
+#else
+    return vreinterpretq_m128_f32(
+        vminq_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(b)));
+#endif
+}
+
+// Compare packed unsigned 8-bit integers in a and b, and store packed minimum
+// values in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_min_pu8
+FORCE_INLINE __m64 _mm_min_pu8(__m64 a, __m64 b)
+{
+    return vreinterpret_m64_u8(
+        vmin_u8(vreinterpret_u8_m64(a), vreinterpret_u8_m64(b)));
+}
+
+// Compare the lower single-precision (32-bit) floating-point elements in a and
+// b, store the minimum value in the lower element of dst, and copy the upper 3
+// packed elements from a to the upper element of dst. dst does not follow the
+// IEEE Standard for Floating-Point Arithmetic (IEEE 754) minimum value when
+// inputs are NaN or signed-zero values.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_min_ss
+FORCE_INLINE __m128 _mm_min_ss(__m128 a, __m128 b)
+{
+    float32_t value = vgetq_lane_f32(_mm_min_ps(a, b), 0);
+    return vreinterpretq_m128_f32(
+        vsetq_lane_f32(value, vreinterpretq_f32_m128(a), 0));
+}
+
+// Move the lower single-precision (32-bit) floating-point element from b to the
+// lower element of dst, and copy the upper 3 packed elements from a to the
+// upper elements of dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_move_ss
+FORCE_INLINE __m128 _mm_move_ss(__m128 a, __m128 b)
+{
+    return vreinterpretq_m128_f32(
+        vsetq_lane_f32(vgetq_lane_f32(vreinterpretq_f32_m128(b), 0),
+                       vreinterpretq_f32_m128(a), 0));
+}
+
+// Move the upper 2 single-precision (32-bit) floating-point elements from b to
+// the lower 2 elements of dst, and copy the upper 2 elements from a to the
+// upper 2 elements of dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_movehl_ps
+FORCE_INLINE __m128 _mm_movehl_ps(__m128 a, __m128 b)
+{
+#if defined(aarch64__)
+    return vreinterpretq_m128_u64(
+        vzip2q_u64(vreinterpretq_u64_m128(b), vreinterpretq_u64_m128(a)));
+#else
+    float32x2_t a32 = vget_high_f32(vreinterpretq_f32_m128(a));
+    float32x2_t b32 = vget_high_f32(vreinterpretq_f32_m128(b));
+    return vreinterpretq_m128_f32(vcombine_f32(b32, a32));
+#endif
+}
+
+// Move the lower 2 single-precision (32-bit) floating-point elements from b to
+// the upper 2 elements of dst, and copy the lower 2 elements from a to the
+// lower 2 elements of dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_movelh_ps
+FORCE_INLINE __m128 _mm_movelh_ps(__m128 __A, __m128 __B)
+{
+    float32x2_t a10 = vget_low_f32(vreinterpretq_f32_m128(__A));
+    float32x2_t b10 = vget_low_f32(vreinterpretq_f32_m128(__B));
+    return vreinterpretq_m128_f32(vcombine_f32(a10, b10));
+}
+
+// Create mask from the most significant bit of each 8-bit element in a, and
+// store the result in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_movemask_pi8
+FORCE_INLINE int _mm_movemask_pi8(__m64 a)
+{
+    uint8x8_t input = vreinterpret_u8_m64(a);
+#if defined(__aarch64__) || defined(_M_ARM64)
+    static const int8_t shift[8] = {0, 1, 2, 3, 4, 5, 6, 7};
+    uint8x8_t tmp = vshr_n_u8(input, 7);
+    return vaddv_u8(vshl_u8(tmp, vld1_s8(shift)));
+#else
+    // Refer the implementation of `_mm_movemask_epi8`
+    uint16x4_t high_bits = vreinterpret_u16_u8(vshr_n_u8(input, 7));
+    uint32x2_t paired16 =
+        vreinterpret_u32_u16(vsra_n_u16(high_bits, high_bits, 7));
+    uint8x8_t paired32 =
+        vreinterpret_u8_u32(vsra_n_u32(paired16, paired16, 14));
+    return vget_lane_u8(paired32, 0) | ((int) vget_lane_u8(paired32, 4) << 4);
+#endif
+}
+
+// Set each bit of mask dst based on the most significant bit of the
+// corresponding packed single-precision (32-bit) floating-point element in a.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_movemask_ps
+FORCE_INLINE int _mm_movemask_ps(__m128 a)
+{
+    uint32x4_t input = vreinterpretq_u32_m128(a);
+#if defined(__aarch64__) || defined(_M_ARM64)
+    static const int32_t shift[4] = {0, 1, 2, 3};
+    uint32x4_t tmp = vshrq_n_u32(input, 31);
+    return vaddvq_u32(vshlq_u32(tmp, vld1q_s32(shift)));
+#else
+    // Uses the exact same method as _mm_movemask_epi8, see that for details.
+    // Shift out everything but the sign bits with a 32-bit unsigned shift
+    // right.
+    uint64x2_t high_bits = vreinterpretq_u64_u32(vshrq_n_u32(input, 31));
+    // Merge the two pairs together with a 64-bit unsigned shift right + add.
+    uint8x16_t paired =
+        vreinterpretq_u8_u64(vsraq_n_u64(high_bits, high_bits, 31));
+    // Extract the result.
+    return vgetq_lane_u8(paired, 0) | (vgetq_lane_u8(paired, 8) << 2);
+#endif
+}
+
+// Multiply packed single-precision (32-bit) floating-point elements in a and b,
+// and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_mul_ps
+FORCE_INLINE __m128 _mm_mul_ps(__m128 a, __m128 b)
+{
+    return vreinterpretq_m128_f32(
+        vmulq_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(b)));
+}
+
+// Multiply the lower single-precision (32-bit) floating-point element in a and
+// b, store the result in the lower element of dst, and copy the upper 3 packed
+// elements from a to the upper elements of dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_mul_ss
+FORCE_INLINE __m128 _mm_mul_ss(__m128 a, __m128 b)
+{
+    return _mm_move_ss(a, _mm_mul_ps(a, b));
+}
+
+// Multiply the packed unsigned 16-bit integers in a and b, producing
+// intermediate 32-bit integers, and store the high 16 bits of the intermediate
+// integers in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_mulhi_pu16
+FORCE_INLINE __m64 _mm_mulhi_pu16(__m64 a, __m64 b)
+{
+    return vreinterpret_m64_u16(vshrn_n_u32(
+        vmull_u16(vreinterpret_u16_m64(a), vreinterpret_u16_m64(b)), 16));
+}
+
+// Compute the bitwise OR of packed single-precision (32-bit) floating-point
+// elements in a and b, and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_or_ps
+FORCE_INLINE __m128 _mm_or_ps(__m128 a, __m128 b)
+{
+    return vreinterpretq_m128_s32(
+        vorrq_s32(vreinterpretq_s32_m128(a), vreinterpretq_s32_m128(b)));
+}
+
+// Average packed unsigned 8-bit integers in a and b, and store the results in
+// dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_m_pavgb
+#define _m_pavgb(a, b) _mm_avg_pu8(a, b)
+
+// Average packed unsigned 16-bit integers in a and b, and store the results in
+// dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_m_pavgw
+#define _m_pavgw(a, b) _mm_avg_pu16(a, b)
+
+// Extract a 16-bit integer from a, selected with imm8, and store the result in
+// the lower element of dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_m_pextrw
+#define _m_pextrw(a, imm) _mm_extract_pi16(a, imm)
+
+// Copy a to dst, and insert the 16-bit integer i into dst at the location
+// specified by imm8.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=m_pinsrw
+#define _m_pinsrw(a, i, imm) _mm_insert_pi16(a, i, imm)
+
+// Compare packed signed 16-bit integers in a and b, and store packed maximum
+// values in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_m_pmaxsw
+#define _m_pmaxsw(a, b) _mm_max_pi16(a, b)
+
+// Compare packed unsigned 8-bit integers in a and b, and store packed maximum
+// values in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_m_pmaxub
+#define _m_pmaxub(a, b) _mm_max_pu8(a, b)
+
+// Compare packed signed 16-bit integers in a and b, and store packed minimum
+// values in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_m_pminsw
+#define _m_pminsw(a, b) _mm_min_pi16(a, b)
+
+// Compare packed unsigned 8-bit integers in a and b, and store packed minimum
+// values in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_m_pminub
+#define _m_pminub(a, b) _mm_min_pu8(a, b)
+
+// Create mask from the most significant bit of each 8-bit element in a, and
+// store the result in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_m_pmovmskb
+#define _m_pmovmskb(a) _mm_movemask_pi8(a)
+
+// Multiply the packed unsigned 16-bit integers in a and b, producing
+// intermediate 32-bit integers, and store the high 16 bits of the intermediate
+// integers in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_m_pmulhuw
+#define _m_pmulhuw(a, b) _mm_mulhi_pu16(a, b)
+
+// Fetch the line of data from memory that contains address p to a location in
+// the cache hierarchy specified by the locality hint i.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_prefetch
+FORCE_INLINE void _mm_prefetch(char const *p, int i)
+{
+    (void) i;
+#if defined(_MSC_VER) && !defined(__clang__)
+    switch (i) {
+    case _MM_HINT_NTA:
+        __prefetch2(p, 1);
+        break;
+    case _MM_HINT_T0:
+        __prefetch2(p, 0);
+        break;
+    case _MM_HINT_T1:
+        __prefetch2(p, 2);
+        break;
+    case _MM_HINT_T2:
+        __prefetch2(p, 4);
+        break;
+    }
+#else
+    switch (i) {
+    case _MM_HINT_NTA:
+        __builtin_prefetch(p, 0, 0);
+        break;
+    case _MM_HINT_T0:
+        __builtin_prefetch(p, 0, 3);
+        break;
+    case _MM_HINT_T1:
+        __builtin_prefetch(p, 0, 2);
+        break;
+    case _MM_HINT_T2:
+        __builtin_prefetch(p, 0, 1);
+        break;
+    }
+#endif
+}
+
+// Compute the absolute differences of packed unsigned 8-bit integers in a and
+// b, then horizontally sum each consecutive 8 differences to produce four
+// unsigned 16-bit integers, and pack these unsigned 16-bit integers in the low
+// 16 bits of dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=m_psadbw
+#define _m_psadbw(a, b) _mm_sad_pu8(a, b)
+
+// Shuffle 16-bit integers in a using the control in imm8, and store the results
+// in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_m_pshufw
+#define _m_pshufw(a, imm) _mm_shuffle_pi16(a, imm)
+
+// Compute the approximate reciprocal of packed single-precision (32-bit)
+// floating-point elements in a, and store the results in dst. The maximum
+// relative error for this approximation is less than 1.5*2^-12.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_rcp_ps
+FORCE_INLINE __m128 _mm_rcp_ps(__m128 in)
+{
+    float32x4_t recip = vrecpeq_f32(vreinterpretq_f32_m128(in));
+    recip = vmulq_f32(recip, vrecpsq_f32(recip, vreinterpretq_f32_m128(in)));
+#if SSE2NEON_PRECISE_DIV
+    // Additional Netwon-Raphson iteration for accuracy
+    recip = vmulq_f32(recip, vrecpsq_f32(recip, vreinterpretq_f32_m128(in)));
+#endif
+    return vreinterpretq_m128_f32(recip);
+}
+
+// Compute the approximate reciprocal of the lower single-precision (32-bit)
+// floating-point element in a, store the result in the lower element of dst,
+// and copy the upper 3 packed elements from a to the upper elements of dst. The
+// maximum relative error for this approximation is less than 1.5*2^-12.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_rcp_ss
+FORCE_INLINE __m128 _mm_rcp_ss(__m128 a)
+{
+    return _mm_move_ss(a, _mm_rcp_ps(a));
+}
+
+// Compute the approximate reciprocal square root of packed single-precision
+// (32-bit) floating-point elements in a, and store the results in dst. The
+// maximum relative error for this approximation is less than 1.5*2^-12.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_rsqrt_ps
+FORCE_INLINE __m128 _mm_rsqrt_ps(__m128 in)
+{
+    float32x4_t out = vrsqrteq_f32(vreinterpretq_f32_m128(in));
+
+    // Generate masks for detecting whether input has any 0.0f/-0.0f
+    // (which becomes positive/negative infinity by IEEE-754 arithmetic rules).
+    const uint32x4_t pos_inf = vdupq_n_u32(0x7F800000);
+    const uint32x4_t neg_inf = vdupq_n_u32(0xFF800000);
+    const uint32x4_t has_pos_zero =
+        vceqq_u32(pos_inf, vreinterpretq_u32_f32(out));
+    const uint32x4_t has_neg_zero =
+        vceqq_u32(neg_inf, vreinterpretq_u32_f32(out));
+
+    out = vmulq_f32(
+        out, vrsqrtsq_f32(vmulq_f32(vreinterpretq_f32_m128(in), out), out));
+#if SSE2NEON_PRECISE_SQRT
+    // Additional Netwon-Raphson iteration for accuracy
+    out = vmulq_f32(
+        out, vrsqrtsq_f32(vmulq_f32(vreinterpretq_f32_m128(in), out), out));
+#endif
+
+    // Set output vector element to infinity/negative-infinity if
+    // the corresponding input vector element is 0.0f/-0.0f.
+    out = vbslq_f32(has_pos_zero, (float32x4_t) pos_inf, out);
+    out = vbslq_f32(has_neg_zero, (float32x4_t) neg_inf, out);
+
+    return vreinterpretq_m128_f32(out);
+}
+
+// Compute the approximate reciprocal square root of the lower single-precision
+// (32-bit) floating-point element in a, store the result in the lower element
+// of dst, and copy the upper 3 packed elements from a to the upper elements of
+// dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_rsqrt_ss
+FORCE_INLINE __m128 _mm_rsqrt_ss(__m128 in)
+{
+    return vsetq_lane_f32(vgetq_lane_f32(_mm_rsqrt_ps(in), 0), in, 0);
+}
+
+// Compute the absolute differences of packed unsigned 8-bit integers in a and
+// b, then horizontally sum each consecutive 8 differences to produce four
+// unsigned 16-bit integers, and pack these unsigned 16-bit integers in the low
+// 16 bits of dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_sad_pu8
+FORCE_INLINE __m64 _mm_sad_pu8(__m64 a, __m64 b)
+{
+    uint64x1_t t = vpaddl_u32(vpaddl_u16(
+        vpaddl_u8(vabd_u8(vreinterpret_u8_m64(a), vreinterpret_u8_m64(b)))));
+    return vreinterpret_m64_u16(
+        vset_lane_u16((int) vget_lane_u64(t, 0), vdup_n_u16(0), 0));
+}
+
+// Macro: Set the flush zero bits of the MXCSR control and status register to
+// the value in unsigned 32-bit integer a. The flush zero may contain any of the
+// following flags: _MM_FLUSH_ZERO_ON or _MM_FLUSH_ZERO_OFF
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_MM_SET_FLUSH_ZERO_MODE
+FORCE_INLINE void _sse2neon_mm_set_flush_zero_mode(unsigned int flag)
+{
+    // AArch32 Advanced SIMD arithmetic always uses the Flush-to-zero setting,
+    // regardless of the value of the FZ bit.
+    union {
+        fpcr_bitfield field;
+#if defined(__aarch64__) || defined(_M_ARM64)
+        uint64_t value;
+#else
+        uint32_t value;
+#endif
+    } r;
+
+#if defined(__aarch64__) || defined(_M_ARM64)
+    r.value = _sse2neon_get_fpcr();
+#else
+    __asm__ __volatile__("vmrs %0, FPSCR" : "=r"(r.value)); /* read */
+#endif
+
+    r.field.bit24 = (flag & _MM_FLUSH_ZERO_MASK) == _MM_FLUSH_ZERO_ON;
+
+#if defined(__aarch64__) || defined(_M_ARM64)
+    _sse2neon_set_fpcr(r.value);
+#else
+    __asm__ __volatile__("vmsr FPSCR, %0" ::"r"(r));        /* write */
+#endif
+}
+
+// Set packed single-precision (32-bit) floating-point elements in dst with the
+// supplied values.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_set_ps
+FORCE_INLINE __m128 _mm_set_ps(float w, float z, float y, float x)
+{
+    float ALIGN_STRUCT(16) data[4] = {x, y, z, w};
+    return vreinterpretq_m128_f32(vld1q_f32(data));
+}
+
+// Broadcast single-precision (32-bit) floating-point value a to all elements of
+// dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_set_ps1
+FORCE_INLINE __m128 _mm_set_ps1(float _w)
+{
+    return vreinterpretq_m128_f32(vdupq_n_f32(_w));
+}
+
+// Macro: Set the rounding mode bits of the MXCSR control and status register to
+// the value in unsigned 32-bit integer a. The rounding mode may contain any of
+// the following flags: _MM_ROUND_NEAREST, _MM_ROUND_DOWN, _MM_ROUND_UP,
+// _MM_ROUND_TOWARD_ZERO
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_MM_SET_ROUNDING_MODE
+FORCE_INLINE_OPTNONE void _MM_SET_ROUNDING_MODE(int rounding)
+{
+    union {
+        fpcr_bitfield field;
+#if defined(__aarch64__) || defined(_M_ARM64)
+        uint64_t value;
+#else
+        uint32_t value;
+#endif
+    } r;
+
+#if defined(__aarch64__) || defined(_M_ARM64)
+    r.value = _sse2neon_get_fpcr();
+#else
+    __asm__ __volatile__("vmrs %0, FPSCR" : "=r"(r.value)); /* read */
+#endif
+
+    switch (rounding) {
+    case _MM_ROUND_TOWARD_ZERO:
+        r.field.bit22 = 1;
+        r.field.bit23 = 1;
+        break;
+    case _MM_ROUND_DOWN:
+        r.field.bit22 = 0;
+        r.field.bit23 = 1;
+        break;
+    case _MM_ROUND_UP:
+        r.field.bit22 = 1;
+        r.field.bit23 = 0;
+        break;
+    default:  //_MM_ROUND_NEAREST
+        r.field.bit22 = 0;
+        r.field.bit23 = 0;
+    }
+
+#if defined(__aarch64__) || defined(_M_ARM64)
+    _sse2neon_set_fpcr(r.value);
+#else
+    __asm__ __volatile__("vmsr FPSCR, %0" ::"r"(r));        /* write */
+#endif
+}
+
+// Copy single-precision (32-bit) floating-point element a to the lower element
+// of dst, and zero the upper 3 elements.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_set_ss
+FORCE_INLINE __m128 _mm_set_ss(float a)
+{
+    return vreinterpretq_m128_f32(vsetq_lane_f32(a, vdupq_n_f32(0), 0));
+}
+
+// Broadcast single-precision (32-bit) floating-point value a to all elements of
+// dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_set1_ps
+FORCE_INLINE __m128 _mm_set1_ps(float _w)
+{
+    return vreinterpretq_m128_f32(vdupq_n_f32(_w));
+}
+
+// Set the MXCSR control and status register with the value in unsigned 32-bit
+// integer a.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_setcsr
+// FIXME: _mm_setcsr() implementation supports changing the rounding mode only.
+FORCE_INLINE void _mm_setcsr(unsigned int a)
+{
+    _MM_SET_ROUNDING_MODE(a);
+}
+
+// Get the unsigned 32-bit value of the MXCSR control and status register.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_getcsr
+// FIXME: _mm_getcsr() implementation supports reading the rounding mode only.
+FORCE_INLINE unsigned int _mm_getcsr(void)
+{
+    return _MM_GET_ROUNDING_MODE();
+}
+
+// Set packed single-precision (32-bit) floating-point elements in dst with the
+// supplied values in reverse order.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_setr_ps
+FORCE_INLINE __m128 _mm_setr_ps(float w, float z, float y, float x)
+{
+    float ALIGN_STRUCT(16) data[4] = {w, z, y, x};
+    return vreinterpretq_m128_f32(vld1q_f32(data));
+}
+
+// Return vector of type __m128 with all elements set to zero.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_setzero_ps
+FORCE_INLINE __m128 _mm_setzero_ps(void)
+{
+    return vreinterpretq_m128_f32(vdupq_n_f32(0));
+}
+
+// Shuffle 16-bit integers in a using the control in imm8, and store the results
+// in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_shuffle_pi16
+#ifdef _sse2neon_shuffle
+#define _mm_shuffle_pi16(a, imm)                                       \
+    vreinterpret_m64_s16(vshuffle_s16(                                 \
+        vreinterpret_s16_m64(a), vreinterpret_s16_m64(a), (imm & 0x3), \
+        ((imm >> 2) & 0x3), ((imm >> 4) & 0x3), ((imm >> 6) & 0x3)))
+#else
+#define _mm_shuffle_pi16(a, imm)                                              \
+    _sse2neon_define1(                                                        \
+        __m64, a, int16x4_t ret;                                              \
+        ret = vmov_n_s16(                                                     \
+            vget_lane_s16(vreinterpret_s16_m64(_a), (imm) & (0x3)));          \
+        ret = vset_lane_s16(                                                  \
+            vget_lane_s16(vreinterpret_s16_m64(_a), ((imm) >> 2) & 0x3), ret, \
+            1);                                                               \
+        ret = vset_lane_s16(                                                  \
+            vget_lane_s16(vreinterpret_s16_m64(_a), ((imm) >> 4) & 0x3), ret, \
+            2);                                                               \
+        ret = vset_lane_s16(                                                  \
+            vget_lane_s16(vreinterpret_s16_m64(_a), ((imm) >> 6) & 0x3), ret, \
+            3);                                                               \
+        _sse2neon_return(vreinterpret_m64_s16(ret));)
+#endif
+
+// Perform a serializing operation on all store-to-memory instructions that were
+// issued prior to this instruction. Guarantees that every store instruction
+// that precedes, in program order, is globally visible before any store
+// instruction which follows the fence in program order.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_sfence
+FORCE_INLINE void _mm_sfence(void)
+{
+    _sse2neon_smp_mb();
+}
+
+// Perform a serializing operation on all load-from-memory and store-to-memory
+// instructions that were issued prior to this instruction. Guarantees that
+// every memory access that precedes, in program order, the memory fence
+// instruction is globally visible before any memory instruction which follows
+// the fence in program order.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_mfence
+FORCE_INLINE void _mm_mfence(void)
+{
+    _sse2neon_smp_mb();
+}
+
+// Perform a serializing operation on all load-from-memory instructions that
+// were issued prior to this instruction. Guarantees that every load instruction
+// that precedes, in program order, is globally visible before any load
+// instruction which follows the fence in program order.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_lfence
+FORCE_INLINE void _mm_lfence(void)
+{
+    _sse2neon_smp_mb();
+}
+
+// FORCE_INLINE __m128 _mm_shuffle_ps(__m128 a, __m128 b, __constrange(0,255)
+// int imm)
+#ifdef _sse2neon_shuffle
+#define _mm_shuffle_ps(a, b, imm)                                              \
+    __extension__({                                                            \
+        float32x4_t _input1 = vreinterpretq_f32_m128(a);                       \
+        float32x4_t _input2 = vreinterpretq_f32_m128(b);                       \
+        float32x4_t _shuf =                                                    \
+            vshuffleq_s32(_input1, _input2, (imm) & (0x3), ((imm) >> 2) & 0x3, \
+                          (((imm) >> 4) & 0x3) + 4, (((imm) >> 6) & 0x3) + 4); \
+        vreinterpretq_m128_f32(_shuf);                                         \
+    })
+#else  // generic
+#define _mm_shuffle_ps(a, b, imm)                            \
+    _sse2neon_define2(                                       \
+        __m128, a, b, __m128 ret; switch (imm) {             \
+            case _MM_SHUFFLE(1, 0, 3, 2):                    \
+                ret = _mm_shuffle_ps_1032(_a, _b);           \
+                break;                                       \
+            case _MM_SHUFFLE(2, 3, 0, 1):                    \
+                ret = _mm_shuffle_ps_2301(_a, _b);           \
+                break;                                       \
+            case _MM_SHUFFLE(0, 3, 2, 1):                    \
+                ret = _mm_shuffle_ps_0321(_a, _b);           \
+                break;                                       \
+            case _MM_SHUFFLE(2, 1, 0, 3):                    \
+                ret = _mm_shuffle_ps_2103(_a, _b);           \
+                break;                                       \
+            case _MM_SHUFFLE(1, 0, 1, 0):                    \
+                ret = _mm_movelh_ps(_a, _b);                 \
+                break;                                       \
+            case _MM_SHUFFLE(1, 0, 0, 1):                    \
+                ret = _mm_shuffle_ps_1001(_a, _b);           \
+                break;                                       \
+            case _MM_SHUFFLE(0, 1, 0, 1):                    \
+                ret = _mm_shuffle_ps_0101(_a, _b);           \
+                break;                                       \
+            case _MM_SHUFFLE(3, 2, 1, 0):                    \
+                ret = _mm_shuffle_ps_3210(_a, _b);           \
+                break;                                       \
+            case _MM_SHUFFLE(0, 0, 1, 1):                    \
+                ret = _mm_shuffle_ps_0011(_a, _b);           \
+                break;                                       \
+            case _MM_SHUFFLE(0, 0, 2, 2):                    \
+                ret = _mm_shuffle_ps_0022(_a, _b);           \
+                break;                                       \
+            case _MM_SHUFFLE(2, 2, 0, 0):                    \
+                ret = _mm_shuffle_ps_2200(_a, _b);           \
+                break;                                       \
+            case _MM_SHUFFLE(3, 2, 0, 2):                    \
+                ret = _mm_shuffle_ps_3202(_a, _b);           \
+                break;                                       \
+            case _MM_SHUFFLE(3, 2, 3, 2):                    \
+                ret = _mm_movehl_ps(_b, _a);                 \
+                break;                                       \
+            case _MM_SHUFFLE(1, 1, 3, 3):                    \
+                ret = _mm_shuffle_ps_1133(_a, _b);           \
+                break;                                       \
+            case _MM_SHUFFLE(2, 0, 1, 0):                    \
+                ret = _mm_shuffle_ps_2010(_a, _b);           \
+                break;                                       \
+            case _MM_SHUFFLE(2, 0, 0, 1):                    \
+                ret = _mm_shuffle_ps_2001(_a, _b);           \
+                break;                                       \
+            case _MM_SHUFFLE(2, 0, 3, 2):                    \
+                ret = _mm_shuffle_ps_2032(_a, _b);           \
+                break;                                       \
+            default:                                         \
+                ret = _mm_shuffle_ps_default(_a, _b, (imm)); \
+                break;                                       \
+        } _sse2neon_return(ret);)
+#endif
+
+// Compute the square root of packed single-precision (32-bit) floating-point
+// elements in a, and store the results in dst.
+// Due to ARMv7-A NEON's lack of a precise square root intrinsic, we implement
+// square root by multiplying input in with its reciprocal square root before
+// using the Newton-Raphson method to approximate the results.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_sqrt_ps
+FORCE_INLINE __m128 _mm_sqrt_ps(__m128 in)
+{
+#if (defined(__aarch64__) || defined(_M_ARM64)) && !SSE2NEON_PRECISE_SQRT
+    return vreinterpretq_m128_f32(vsqrtq_f32(vreinterpretq_f32_m128(in)));
+#else
+    float32x4_t recip = vrsqrteq_f32(vreinterpretq_f32_m128(in));
+
+    // Test for vrsqrteq_f32(0) -> positive infinity case.
+    // Change to zero, so that s * 1/sqrt(s) result is zero too.
+    const uint32x4_t pos_inf = vdupq_n_u32(0x7F800000);
+    const uint32x4_t div_by_zero =
+        vceqq_u32(pos_inf, vreinterpretq_u32_f32(recip));
+    recip = vreinterpretq_f32_u32(
+        vandq_u32(vmvnq_u32(div_by_zero), vreinterpretq_u32_f32(recip)));
+
+    recip = vmulq_f32(
+        vrsqrtsq_f32(vmulq_f32(recip, recip), vreinterpretq_f32_m128(in)),
+        recip);
+    // Additional Netwon-Raphson iteration for accuracy
+    recip = vmulq_f32(
+        vrsqrtsq_f32(vmulq_f32(recip, recip), vreinterpretq_f32_m128(in)),
+        recip);
+
+    // sqrt(s) = s * 1/sqrt(s)
+    return vreinterpretq_m128_f32(vmulq_f32(vreinterpretq_f32_m128(in), recip));
+#endif
+}
+
+// Compute the square root of the lower single-precision (32-bit) floating-point
+// element in a, store the result in the lower element of dst, and copy the
+// upper 3 packed elements from a to the upper elements of dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_sqrt_ss
+FORCE_INLINE __m128 _mm_sqrt_ss(__m128 in)
+{
+    float32_t value =
+        vgetq_lane_f32(vreinterpretq_f32_m128(_mm_sqrt_ps(in)), 0);
+    return vreinterpretq_m128_f32(
+        vsetq_lane_f32(value, vreinterpretq_f32_m128(in), 0));
+}
+
+// Store 128-bits (composed of 4 packed single-precision (32-bit) floating-point
+// elements) from a into memory. mem_addr must be aligned on a 16-byte boundary
+// or a general-protection exception may be generated.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_store_ps
+FORCE_INLINE void _mm_store_ps(float *p, __m128 a)
+{
+    vst1q_f32(p, vreinterpretq_f32_m128(a));
+}
+
+// Store the lower single-precision (32-bit) floating-point element from a into
+// 4 contiguous elements in memory. mem_addr must be aligned on a 16-byte
+// boundary or a general-protection exception may be generated.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_store_ps1
+FORCE_INLINE void _mm_store_ps1(float *p, __m128 a)
+{
+    float32_t a0 = vgetq_lane_f32(vreinterpretq_f32_m128(a), 0);
+    vst1q_f32(p, vdupq_n_f32(a0));
+}
+
+// Store the lower single-precision (32-bit) floating-point element from a into
+// memory. mem_addr does not need to be aligned on any particular boundary.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_store_ss
+FORCE_INLINE void _mm_store_ss(float *p, __m128 a)
+{
+    vst1q_lane_f32(p, vreinterpretq_f32_m128(a), 0);
+}
+
+// Store the lower single-precision (32-bit) floating-point element from a into
+// 4 contiguous elements in memory. mem_addr must be aligned on a 16-byte
+// boundary or a general-protection exception may be generated.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_store1_ps
+#define _mm_store1_ps _mm_store_ps1
+
+// Store the upper 2 single-precision (32-bit) floating-point elements from a
+// into memory.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_storeh_pi
+FORCE_INLINE void _mm_storeh_pi(__m64 *p, __m128 a)
+{
+    *p = vreinterpret_m64_f32(vget_high_f32(a));
+}
+
+// Store the lower 2 single-precision (32-bit) floating-point elements from a
+// into memory.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_storel_pi
+FORCE_INLINE void _mm_storel_pi(__m64 *p, __m128 a)
+{
+    *p = vreinterpret_m64_f32(vget_low_f32(a));
+}
+
+// Store 4 single-precision (32-bit) floating-point elements from a into memory
+// in reverse order. mem_addr must be aligned on a 16-byte boundary or a
+// general-protection exception may be generated.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_storer_ps
+FORCE_INLINE void _mm_storer_ps(float *p, __m128 a)
+{
+    float32x4_t tmp = vrev64q_f32(vreinterpretq_f32_m128(a));
+    float32x4_t rev = vextq_f32(tmp, tmp, 2);
+    vst1q_f32(p, rev);
+}
+
+// Store 128-bits (composed of 4 packed single-precision (32-bit) floating-point
+// elements) from a into memory. mem_addr does not need to be aligned on any
+// particular boundary.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_storeu_ps
+FORCE_INLINE void _mm_storeu_ps(float *p, __m128 a)
+{
+    vst1q_f32(p, vreinterpretq_f32_m128(a));
+}
+
+// Stores 16-bits of integer data a at the address p.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_storeu_si16
+FORCE_INLINE void _mm_storeu_si16(void *p, __m128i a)
+{
+    vst1q_lane_s16((int16_t *) p, vreinterpretq_s16_m128i(a), 0);
+}
+
+// Stores 64-bits of integer data a at the address p.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_storeu_si64
+FORCE_INLINE void _mm_storeu_si64(void *p, __m128i a)
+{
+    vst1q_lane_s64((int64_t *) p, vreinterpretq_s64_m128i(a), 0);
+}
+
+// Store 64-bits of integer data from a into memory using a non-temporal memory
+// hint.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_stream_pi
+FORCE_INLINE void _mm_stream_pi(__m64 *p, __m64 a)
+{
+    vst1_s64((int64_t *) p, vreinterpret_s64_m64(a));
+}
+
+// Store 128-bits (composed of 4 packed single-precision (32-bit) floating-
+// point elements) from a into memory using a non-temporal memory hint.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_stream_ps
+FORCE_INLINE void _mm_stream_ps(float *p, __m128 a)
+{
+#if __has_builtin(__builtin_nontemporal_store)
+    __builtin_nontemporal_store(a, (float32x4_t *) p);
+#else
+    vst1q_f32(p, vreinterpretq_f32_m128(a));
+#endif
+}
+
+// Subtract packed single-precision (32-bit) floating-point elements in b from
+// packed single-precision (32-bit) floating-point elements in a, and store the
+// results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_sub_ps
+FORCE_INLINE __m128 _mm_sub_ps(__m128 a, __m128 b)
+{
+    return vreinterpretq_m128_f32(
+        vsubq_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(b)));
+}
+
+// Subtract the lower single-precision (32-bit) floating-point element in b from
+// the lower single-precision (32-bit) floating-point element in a, store the
+// result in the lower element of dst, and copy the upper 3 packed elements from
+// a to the upper elements of dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_sub_ss
+FORCE_INLINE __m128 _mm_sub_ss(__m128 a, __m128 b)
+{
+    return _mm_move_ss(a, _mm_sub_ps(a, b));
+}
+
+// Macro: Transpose the 4x4 matrix formed by the 4 rows of single-precision
+// (32-bit) floating-point elements in row0, row1, row2, and row3, and store the
+// transposed matrix in these vectors (row0 now contains column 0, etc.).
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=MM_TRANSPOSE4_PS
+#define _MM_TRANSPOSE4_PS(row0, row1, row2, row3)         \
+    do {                                                  \
+        float32x4x2_t ROW01 = vtrnq_f32(row0, row1);      \
+        float32x4x2_t ROW23 = vtrnq_f32(row2, row3);      \
+        row0 = vcombine_f32(vget_low_f32(ROW01.val[0]),   \
+                            vget_low_f32(ROW23.val[0]));  \
+        row1 = vcombine_f32(vget_low_f32(ROW01.val[1]),   \
+                            vget_low_f32(ROW23.val[1]));  \
+        row2 = vcombine_f32(vget_high_f32(ROW01.val[0]),  \
+                            vget_high_f32(ROW23.val[0])); \
+        row3 = vcombine_f32(vget_high_f32(ROW01.val[1]),  \
+                            vget_high_f32(ROW23.val[1])); \
+    } while (0)
+
+// according to the documentation, these intrinsics behave the same as the
+// non-'u' versions.  We'll just alias them here.
+#define _mm_ucomieq_ss _mm_comieq_ss
+#define _mm_ucomige_ss _mm_comige_ss
+#define _mm_ucomigt_ss _mm_comigt_ss
+#define _mm_ucomile_ss _mm_comile_ss
+#define _mm_ucomilt_ss _mm_comilt_ss
+#define _mm_ucomineq_ss _mm_comineq_ss
+
+// Return vector of type __m128i with undefined elements.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=mm_undefined_si128
+FORCE_INLINE __m128i _mm_undefined_si128(void)
+{
+#if defined(__GNUC__) || defined(__clang__)
+#pragma GCC diagnostic push
+#pragma GCC diagnostic ignored "-Wuninitialized"
+#endif
+    __m128i a;
+#if defined(_MSC_VER)
+    a = _mm_setzero_si128();
+#endif
+    return a;
+#if defined(__GNUC__) || defined(__clang__)
+#pragma GCC diagnostic pop
+#endif
+}
+
+// Return vector of type __m128 with undefined elements.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_undefined_ps
+FORCE_INLINE __m128 _mm_undefined_ps(void)
+{
+#if defined(__GNUC__) || defined(__clang__)
+#pragma GCC diagnostic push
+#pragma GCC diagnostic ignored "-Wuninitialized"
+#endif
+    __m128 a;
+#if defined(_MSC_VER)
+    a = _mm_setzero_ps();
+#endif
+    return a;
+#if defined(__GNUC__) || defined(__clang__)
+#pragma GCC diagnostic pop
+#endif
+}
+
+// Unpack and interleave single-precision (32-bit) floating-point elements from
+// the high half a and b, and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_unpackhi_ps
+FORCE_INLINE __m128 _mm_unpackhi_ps(__m128 a, __m128 b)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    return vreinterpretq_m128_f32(
+        vzip2q_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(b)));
+#else
+    float32x2_t a1 = vget_high_f32(vreinterpretq_f32_m128(a));
+    float32x2_t b1 = vget_high_f32(vreinterpretq_f32_m128(b));
+    float32x2x2_t result = vzip_f32(a1, b1);
+    return vreinterpretq_m128_f32(vcombine_f32(result.val[0], result.val[1]));
+#endif
+}
+
+// Unpack and interleave single-precision (32-bit) floating-point elements from
+// the low half of a and b, and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_unpacklo_ps
+FORCE_INLINE __m128 _mm_unpacklo_ps(__m128 a, __m128 b)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    return vreinterpretq_m128_f32(
+        vzip1q_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(b)));
+#else
+    float32x2_t a1 = vget_low_f32(vreinterpretq_f32_m128(a));
+    float32x2_t b1 = vget_low_f32(vreinterpretq_f32_m128(b));
+    float32x2x2_t result = vzip_f32(a1, b1);
+    return vreinterpretq_m128_f32(vcombine_f32(result.val[0], result.val[1]));
+#endif
+}
+
+// Compute the bitwise XOR of packed single-precision (32-bit) floating-point
+// elements in a and b, and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_xor_ps
+FORCE_INLINE __m128 _mm_xor_ps(__m128 a, __m128 b)
+{
+    return vreinterpretq_m128_s32(
+        veorq_s32(vreinterpretq_s32_m128(a), vreinterpretq_s32_m128(b)));
+}
+
+/* SSE2 */
+
+// Add packed 16-bit integers in a and b, and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_add_epi16
+FORCE_INLINE __m128i _mm_add_epi16(__m128i a, __m128i b)
+{
+    return vreinterpretq_m128i_s16(
+        vaddq_s16(vreinterpretq_s16_m128i(a), vreinterpretq_s16_m128i(b)));
+}
+
+// Add packed 32-bit integers in a and b, and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_add_epi32
+FORCE_INLINE __m128i _mm_add_epi32(__m128i a, __m128i b)
+{
+    return vreinterpretq_m128i_s32(
+        vaddq_s32(vreinterpretq_s32_m128i(a), vreinterpretq_s32_m128i(b)));
+}
+
+// Add packed 64-bit integers in a and b, and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_add_epi64
+FORCE_INLINE __m128i _mm_add_epi64(__m128i a, __m128i b)
+{
+    return vreinterpretq_m128i_s64(
+        vaddq_s64(vreinterpretq_s64_m128i(a), vreinterpretq_s64_m128i(b)));
+}
+
+// Add packed 8-bit integers in a and b, and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_add_epi8
+FORCE_INLINE __m128i _mm_add_epi8(__m128i a, __m128i b)
+{
+    return vreinterpretq_m128i_s8(
+        vaddq_s8(vreinterpretq_s8_m128i(a), vreinterpretq_s8_m128i(b)));
+}
+
+// Add packed double-precision (64-bit) floating-point elements in a and b, and
+// store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_add_pd
+FORCE_INLINE __m128d _mm_add_pd(__m128d a, __m128d b)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    return vreinterpretq_m128d_f64(
+        vaddq_f64(vreinterpretq_f64_m128d(a), vreinterpretq_f64_m128d(b)));
+#else
+    double a0 =
+        sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(a), 0));
+    double a1 =
+        sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(a), 1));
+    double b0 =
+        sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(b), 0));
+    double b1 =
+        sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(b), 1));
+    double c[2];
+    c[0] = a0 + b0;
+    c[1] = a1 + b1;
+    return vld1q_f32((float32_t *) c);
+#endif
+}
+
+// Add the lower double-precision (64-bit) floating-point element in a and b,
+// store the result in the lower element of dst, and copy the upper element from
+// a to the upper element of dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_add_sd
+FORCE_INLINE __m128d _mm_add_sd(__m128d a, __m128d b)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    return _mm_move_sd(a, _mm_add_pd(a, b));
+#else
+    double a0, a1, b0;
+    a0 = sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(a), 0));
+    a1 = sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(a), 1));
+    b0 = sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(b), 0));
+    double c[2];
+    c[0] = a0 + b0;
+    c[1] = a1;
+    return vld1q_f32((float32_t *) c);
+#endif
+}
+
+// Add 64-bit integers a and b, and store the result in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_add_si64
+FORCE_INLINE __m64 _mm_add_si64(__m64 a, __m64 b)
+{
+    return vreinterpret_m64_s64(
+        vadd_s64(vreinterpret_s64_m64(a), vreinterpret_s64_m64(b)));
+}
+
+// Add packed signed 16-bit integers in a and b using saturation, and store the
+// results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_adds_epi16
+FORCE_INLINE __m128i _mm_adds_epi16(__m128i a, __m128i b)
+{
+    return vreinterpretq_m128i_s16(
+        vqaddq_s16(vreinterpretq_s16_m128i(a), vreinterpretq_s16_m128i(b)));
+}
+
+// Add packed signed 8-bit integers in a and b using saturation, and store the
+// results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_adds_epi8
+FORCE_INLINE __m128i _mm_adds_epi8(__m128i a, __m128i b)
+{
+    return vreinterpretq_m128i_s8(
+        vqaddq_s8(vreinterpretq_s8_m128i(a), vreinterpretq_s8_m128i(b)));
+}
+
+// Add packed unsigned 16-bit integers in a and b using saturation, and store
+// the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_adds_epu16
+FORCE_INLINE __m128i _mm_adds_epu16(__m128i a, __m128i b)
+{
+    return vreinterpretq_m128i_u16(
+        vqaddq_u16(vreinterpretq_u16_m128i(a), vreinterpretq_u16_m128i(b)));
+}
+
+// Add packed unsigned 8-bit integers in a and b using saturation, and store the
+// results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_adds_epu8
+FORCE_INLINE __m128i _mm_adds_epu8(__m128i a, __m128i b)
+{
+    return vreinterpretq_m128i_u8(
+        vqaddq_u8(vreinterpretq_u8_m128i(a), vreinterpretq_u8_m128i(b)));
+}
+
+// Compute the bitwise AND of packed double-precision (64-bit) floating-point
+// elements in a and b, and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_and_pd
+FORCE_INLINE __m128d _mm_and_pd(__m128d a, __m128d b)
+{
+    return vreinterpretq_m128d_s64(
+        vandq_s64(vreinterpretq_s64_m128d(a), vreinterpretq_s64_m128d(b)));
+}
+
+// Compute the bitwise AND of 128 bits (representing integer data) in a and b,
+// and store the result in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_and_si128
+FORCE_INLINE __m128i _mm_and_si128(__m128i a, __m128i b)
+{
+    return vreinterpretq_m128i_s32(
+        vandq_s32(vreinterpretq_s32_m128i(a), vreinterpretq_s32_m128i(b)));
+}
+
+// Compute the bitwise NOT of packed double-precision (64-bit) floating-point
+// elements in a and then AND with b, and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_andnot_pd
+FORCE_INLINE __m128d _mm_andnot_pd(__m128d a, __m128d b)
+{
+    // *NOTE* argument swap
+    return vreinterpretq_m128d_s64(
+        vbicq_s64(vreinterpretq_s64_m128d(b), vreinterpretq_s64_m128d(a)));
+}
+
+// Compute the bitwise NOT of 128 bits (representing integer data) in a and then
+// AND with b, and store the result in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_andnot_si128
+FORCE_INLINE __m128i _mm_andnot_si128(__m128i a, __m128i b)
+{
+    return vreinterpretq_m128i_s32(
+        vbicq_s32(vreinterpretq_s32_m128i(b),
+                  vreinterpretq_s32_m128i(a)));  // *NOTE* argument swap
+}
+
+// Average packed unsigned 16-bit integers in a and b, and store the results in
+// dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_avg_epu16
+FORCE_INLINE __m128i _mm_avg_epu16(__m128i a, __m128i b)
+{
+    return (__m128i) vrhaddq_u16(vreinterpretq_u16_m128i(a),
+                                 vreinterpretq_u16_m128i(b));
+}
+
+// Average packed unsigned 8-bit integers in a and b, and store the results in
+// dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_avg_epu8
+FORCE_INLINE __m128i _mm_avg_epu8(__m128i a, __m128i b)
+{
+    return vreinterpretq_m128i_u8(
+        vrhaddq_u8(vreinterpretq_u8_m128i(a), vreinterpretq_u8_m128i(b)));
+}
+
+// Shift a left by imm8 bytes while shifting in zeros, and store the results in
+// dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_bslli_si128
+#define _mm_bslli_si128(a, imm) _mm_slli_si128(a, imm)
+
+// Shift a right by imm8 bytes while shifting in zeros, and store the results in
+// dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_bsrli_si128
+#define _mm_bsrli_si128(a, imm) _mm_srli_si128(a, imm)
+
+// Cast vector of type __m128d to type __m128. This intrinsic is only used for
+// compilation and does not generate any instructions, thus it has zero latency.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_castpd_ps
+FORCE_INLINE __m128 _mm_castpd_ps(__m128d a)
+{
+    return vreinterpretq_m128_s64(vreinterpretq_s64_m128d(a));
+}
+
+// Cast vector of type __m128d to type __m128i. This intrinsic is only used for
+// compilation and does not generate any instructions, thus it has zero latency.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_castpd_si128
+FORCE_INLINE __m128i _mm_castpd_si128(__m128d a)
+{
+    return vreinterpretq_m128i_s64(vreinterpretq_s64_m128d(a));
+}
+
+// Cast vector of type __m128 to type __m128d. This intrinsic is only used for
+// compilation and does not generate any instructions, thus it has zero latency.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_castps_pd
+FORCE_INLINE __m128d _mm_castps_pd(__m128 a)
+{
+    return vreinterpretq_m128d_s32(vreinterpretq_s32_m128(a));
+}
+
+// Cast vector of type __m128 to type __m128i. This intrinsic is only used for
+// compilation and does not generate any instructions, thus it has zero latency.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_castps_si128
+FORCE_INLINE __m128i _mm_castps_si128(__m128 a)
+{
+    return vreinterpretq_m128i_s32(vreinterpretq_s32_m128(a));
+}
+
+// Cast vector of type __m128i to type __m128d. This intrinsic is only used for
+// compilation and does not generate any instructions, thus it has zero latency.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_castsi128_pd
+FORCE_INLINE __m128d _mm_castsi128_pd(__m128i a)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    return vreinterpretq_m128d_f64(vreinterpretq_f64_m128i(a));
+#else
+    return vreinterpretq_m128d_f32(vreinterpretq_f32_m128i(a));
+#endif
+}
+
+// Cast vector of type __m128i to type __m128. This intrinsic is only used for
+// compilation and does not generate any instructions, thus it has zero latency.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_castsi128_ps
+FORCE_INLINE __m128 _mm_castsi128_ps(__m128i a)
+{
+    return vreinterpretq_m128_s32(vreinterpretq_s32_m128i(a));
+}
+
+// Invalidate and flush the cache line that contains p from all levels of the
+// cache hierarchy.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_clflush
+#if defined(__APPLE__)
+#include <libkern/OSCacheControl.h>
+#endif
+FORCE_INLINE void _mm_clflush(void const *p)
+{
+    (void) p;
+
+    /* sys_icache_invalidate is supported since macOS 10.5.
+     * However, it does not work on non-jailbroken iOS devices, although the
+     * compilation is successful.
+     */
+#if defined(__APPLE__)
+    sys_icache_invalidate((void *) (uintptr_t) p, SSE2NEON_CACHELINE_SIZE);
+#elif defined(__GNUC__) || defined(__clang__)
+    uintptr_t ptr = (uintptr_t) p;
+    __builtin___clear_cache((char *) ptr,
+                            (char *) ptr + SSE2NEON_CACHELINE_SIZE);
+#elif (_MSC_VER) && SSE2NEON_INCLUDE_WINDOWS_H
+    FlushInstructionCache(GetCurrentProcess(), p, SSE2NEON_CACHELINE_SIZE);
+#endif
+}
+
+// Compare packed 16-bit integers in a and b for equality, and store the results
+// in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cmpeq_epi16
+FORCE_INLINE __m128i _mm_cmpeq_epi16(__m128i a, __m128i b)
+{
+    return vreinterpretq_m128i_u16(
+        vceqq_s16(vreinterpretq_s16_m128i(a), vreinterpretq_s16_m128i(b)));
+}
+
+// Compare packed 32-bit integers in a and b for equality, and store the results
+// in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cmpeq_epi32
+FORCE_INLINE __m128i _mm_cmpeq_epi32(__m128i a, __m128i b)
+{
+    return vreinterpretq_m128i_u32(
+        vceqq_s32(vreinterpretq_s32_m128i(a), vreinterpretq_s32_m128i(b)));
+}
+
+// Compare packed 8-bit integers in a and b for equality, and store the results
+// in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cmpeq_epi8
+FORCE_INLINE __m128i _mm_cmpeq_epi8(__m128i a, __m128i b)
+{
+    return vreinterpretq_m128i_u8(
+        vceqq_s8(vreinterpretq_s8_m128i(a), vreinterpretq_s8_m128i(b)));
+}
+
+// Compare packed double-precision (64-bit) floating-point elements in a and b
+// for equality, and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cmpeq_pd
+FORCE_INLINE __m128d _mm_cmpeq_pd(__m128d a, __m128d b)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    return vreinterpretq_m128d_u64(
+        vceqq_f64(vreinterpretq_f64_m128d(a), vreinterpretq_f64_m128d(b)));
+#else
+    // (a == b) -> (a_lo == b_lo) && (a_hi == b_hi)
+    uint32x4_t cmp =
+        vceqq_u32(vreinterpretq_u32_m128d(a), vreinterpretq_u32_m128d(b));
+    uint32x4_t swapped = vrev64q_u32(cmp);
+    return vreinterpretq_m128d_u32(vandq_u32(cmp, swapped));
+#endif
+}
+
+// Compare the lower double-precision (64-bit) floating-point elements in a and
+// b for equality, store the result in the lower element of dst, and copy the
+// upper element from a to the upper element of dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cmpeq_sd
+FORCE_INLINE __m128d _mm_cmpeq_sd(__m128d a, __m128d b)
+{
+    return _mm_move_sd(a, _mm_cmpeq_pd(a, b));
+}
+
+// Compare packed double-precision (64-bit) floating-point elements in a and b
+// for greater-than-or-equal, and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cmpge_pd
+FORCE_INLINE __m128d _mm_cmpge_pd(__m128d a, __m128d b)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    return vreinterpretq_m128d_u64(
+        vcgeq_f64(vreinterpretq_f64_m128d(a), vreinterpretq_f64_m128d(b)));
+#else
+    double a0 =
+        sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(a), 0));
+    double a1 =
+        sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(a), 1));
+    double b0 =
+        sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(b), 0));
+    double b1 =
+        sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(b), 1));
+    uint64_t d[2];
+    d[0] = a0 >= b0 ? ~UINT64_C(0) : UINT64_C(0);
+    d[1] = a1 >= b1 ? ~UINT64_C(0) : UINT64_C(0);
+
+    return vreinterpretq_m128d_u64(vld1q_u64(d));
+#endif
+}
+
+// Compare the lower double-precision (64-bit) floating-point elements in a and
+// b for greater-than-or-equal, store the result in the lower element of dst,
+// and copy the upper element from a to the upper element of dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cmpge_sd
+FORCE_INLINE __m128d _mm_cmpge_sd(__m128d a, __m128d b)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    return _mm_move_sd(a, _mm_cmpge_pd(a, b));
+#else
+    // expand "_mm_cmpge_pd()" to reduce unnecessary operations
+    double a0, b0;
+    a0 = sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(a), 0));
+    uint64_t a1 = vgetq_lane_u64(vreinterpretq_u64_m128d(a), 1);
+    b0 = sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(b), 0));
+    uint64_t d[2];
+    d[0] = a0 >= b0 ? ~UINT64_C(0) : UINT64_C(0);
+    d[1] = a1;
+
+    return vreinterpretq_m128d_u64(vld1q_u64(d));
+#endif
+}
+
+// Compare packed signed 16-bit integers in a and b for greater-than, and store
+// the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cmpgt_epi16
+FORCE_INLINE __m128i _mm_cmpgt_epi16(__m128i a, __m128i b)
+{
+    return vreinterpretq_m128i_u16(
+        vcgtq_s16(vreinterpretq_s16_m128i(a), vreinterpretq_s16_m128i(b)));
+}
+
+// Compare packed signed 32-bit integers in a and b for greater-than, and store
+// the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cmpgt_epi32
+FORCE_INLINE __m128i _mm_cmpgt_epi32(__m128i a, __m128i b)
+{
+    return vreinterpretq_m128i_u32(
+        vcgtq_s32(vreinterpretq_s32_m128i(a), vreinterpretq_s32_m128i(b)));
+}
+
+// Compare packed signed 8-bit integers in a and b for greater-than, and store
+// the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cmpgt_epi8
+FORCE_INLINE __m128i _mm_cmpgt_epi8(__m128i a, __m128i b)
+{
+    return vreinterpretq_m128i_u8(
+        vcgtq_s8(vreinterpretq_s8_m128i(a), vreinterpretq_s8_m128i(b)));
+}
+
+// Compare packed double-precision (64-bit) floating-point elements in a and b
+// for greater-than, and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cmpgt_pd
+FORCE_INLINE __m128d _mm_cmpgt_pd(__m128d a, __m128d b)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    return vreinterpretq_m128d_u64(
+        vcgtq_f64(vreinterpretq_f64_m128d(a), vreinterpretq_f64_m128d(b)));
+#else
+    double a0 =
+        sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(a), 0));
+    double a1 =
+        sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(a), 1));
+    double b0 =
+        sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(b), 0));
+    double b1 =
+        sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(b), 1));
+    uint64_t d[2];
+    d[0] = a0 > b0 ? ~UINT64_C(0) : UINT64_C(0);
+    d[1] = a1 > b1 ? ~UINT64_C(0) : UINT64_C(0);
+
+    return vreinterpretq_m128d_u64(vld1q_u64(d));
+#endif
+}
+
+// Compare the lower double-precision (64-bit) floating-point elements in a and
+// b for greater-than, store the result in the lower element of dst, and copy
+// the upper element from a to the upper element of dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cmpgt_sd
+FORCE_INLINE __m128d _mm_cmpgt_sd(__m128d a, __m128d b)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    return _mm_move_sd(a, _mm_cmpgt_pd(a, b));
+#else
+    // expand "_mm_cmpge_pd()" to reduce unnecessary operations
+    double a0, b0;
+    a0 = sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(a), 0));
+    uint64_t a1 = vgetq_lane_u64(vreinterpretq_u64_m128d(a), 1);
+    b0 = sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(b), 0));
+    uint64_t d[2];
+    d[0] = a0 > b0 ? ~UINT64_C(0) : UINT64_C(0);
+    d[1] = a1;
+
+    return vreinterpretq_m128d_u64(vld1q_u64(d));
+#endif
+}
+
+// Compare packed double-precision (64-bit) floating-point elements in a and b
+// for less-than-or-equal, and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cmple_pd
+FORCE_INLINE __m128d _mm_cmple_pd(__m128d a, __m128d b)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    return vreinterpretq_m128d_u64(
+        vcleq_f64(vreinterpretq_f64_m128d(a), vreinterpretq_f64_m128d(b)));
+#else
+    double a0 =
+        sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(a), 0));
+    double a1 =
+        sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(a), 1));
+    double b0 =
+        sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(b), 0));
+    double b1 =
+        sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(b), 1));
+    uint64_t d[2];
+    d[0] = a0 <= b0 ? ~UINT64_C(0) : UINT64_C(0);
+    d[1] = a1 <= b1 ? ~UINT64_C(0) : UINT64_C(0);
+
+    return vreinterpretq_m128d_u64(vld1q_u64(d));
+#endif
+}
+
+// Compare the lower double-precision (64-bit) floating-point elements in a and
+// b for less-than-or-equal, store the result in the lower element of dst, and
+// copy the upper element from a to the upper element of dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cmple_sd
+FORCE_INLINE __m128d _mm_cmple_sd(__m128d a, __m128d b)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    return _mm_move_sd(a, _mm_cmple_pd(a, b));
+#else
+    // expand "_mm_cmpge_pd()" to reduce unnecessary operations
+    double a0, b0;
+    a0 = sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(a), 0));
+    uint64_t a1 = vgetq_lane_u64(vreinterpretq_u64_m128d(a), 1);
+    b0 = sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(b), 0));
+    uint64_t d[2];
+    d[0] = a0 <= b0 ? ~UINT64_C(0) : UINT64_C(0);
+    d[1] = a1;
+
+    return vreinterpretq_m128d_u64(vld1q_u64(d));
+#endif
+}
+
+// Compare packed signed 16-bit integers in a and b for less-than, and store the
+// results in dst. Note: This intrinsic emits the pcmpgtw instruction with the
+// order of the operands switched.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cmplt_epi16
+FORCE_INLINE __m128i _mm_cmplt_epi16(__m128i a, __m128i b)
+{
+    return vreinterpretq_m128i_u16(
+        vcltq_s16(vreinterpretq_s16_m128i(a), vreinterpretq_s16_m128i(b)));
+}
+
+// Compare packed signed 32-bit integers in a and b for less-than, and store the
+// results in dst. Note: This intrinsic emits the pcmpgtd instruction with the
+// order of the operands switched.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cmplt_epi32
+FORCE_INLINE __m128i _mm_cmplt_epi32(__m128i a, __m128i b)
+{
+    return vreinterpretq_m128i_u32(
+        vcltq_s32(vreinterpretq_s32_m128i(a), vreinterpretq_s32_m128i(b)));
+}
+
+// Compare packed signed 8-bit integers in a and b for less-than, and store the
+// results in dst. Note: This intrinsic emits the pcmpgtb instruction with the
+// order of the operands switched.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cmplt_epi8
+FORCE_INLINE __m128i _mm_cmplt_epi8(__m128i a, __m128i b)
+{
+    return vreinterpretq_m128i_u8(
+        vcltq_s8(vreinterpretq_s8_m128i(a), vreinterpretq_s8_m128i(b)));
+}
+
+// Compare packed double-precision (64-bit) floating-point elements in a and b
+// for less-than, and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cmplt_pd
+FORCE_INLINE __m128d _mm_cmplt_pd(__m128d a, __m128d b)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    return vreinterpretq_m128d_u64(
+        vcltq_f64(vreinterpretq_f64_m128d(a), vreinterpretq_f64_m128d(b)));
+#else
+    double a0 =
+        sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(a), 0));
+    double a1 =
+        sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(a), 1));
+    double b0 =
+        sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(b), 0));
+    double b1 =
+        sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(b), 1));
+    uint64_t d[2];
+    d[0] = a0 < b0 ? ~UINT64_C(0) : UINT64_C(0);
+    d[1] = a1 < b1 ? ~UINT64_C(0) : UINT64_C(0);
+
+    return vreinterpretq_m128d_u64(vld1q_u64(d));
+#endif
+}
+
+// Compare the lower double-precision (64-bit) floating-point elements in a and
+// b for less-than, store the result in the lower element of dst, and copy the
+// upper element from a to the upper element of dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cmplt_sd
+FORCE_INLINE __m128d _mm_cmplt_sd(__m128d a, __m128d b)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    return _mm_move_sd(a, _mm_cmplt_pd(a, b));
+#else
+    double a0, b0;
+    a0 = sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(a), 0));
+    uint64_t a1 = vgetq_lane_u64(vreinterpretq_u64_m128d(a), 1);
+    b0 = sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(b), 0));
+    uint64_t d[2];
+    d[0] = a0 < b0 ? ~UINT64_C(0) : UINT64_C(0);
+    d[1] = a1;
+
+    return vreinterpretq_m128d_u64(vld1q_u64(d));
+#endif
+}
+
+// Compare packed double-precision (64-bit) floating-point elements in a and b
+// for not-equal, and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cmpneq_pd
+FORCE_INLINE __m128d _mm_cmpneq_pd(__m128d a, __m128d b)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    return vreinterpretq_m128d_s32(vmvnq_s32(vreinterpretq_s32_u64(
+        vceqq_f64(vreinterpretq_f64_m128d(a), vreinterpretq_f64_m128d(b)))));
+#else
+    // (a == b) -> (a_lo == b_lo) && (a_hi == b_hi)
+    uint32x4_t cmp =
+        vceqq_u32(vreinterpretq_u32_m128d(a), vreinterpretq_u32_m128d(b));
+    uint32x4_t swapped = vrev64q_u32(cmp);
+    return vreinterpretq_m128d_u32(vmvnq_u32(vandq_u32(cmp, swapped)));
+#endif
+}
+
+// Compare the lower double-precision (64-bit) floating-point elements in a and
+// b for not-equal, store the result in the lower element of dst, and copy the
+// upper element from a to the upper element of dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cmpneq_sd
+FORCE_INLINE __m128d _mm_cmpneq_sd(__m128d a, __m128d b)
+{
+    return _mm_move_sd(a, _mm_cmpneq_pd(a, b));
+}
+
+// Compare packed double-precision (64-bit) floating-point elements in a and b
+// for not-greater-than-or-equal, and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cmpnge_pd
+FORCE_INLINE __m128d _mm_cmpnge_pd(__m128d a, __m128d b)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    return vreinterpretq_m128d_u64(veorq_u64(
+        vcgeq_f64(vreinterpretq_f64_m128d(a), vreinterpretq_f64_m128d(b)),
+        vdupq_n_u64(UINT64_MAX)));
+#else
+    double a0 =
+        sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(a), 0));
+    double a1 =
+        sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(a), 1));
+    double b0 =
+        sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(b), 0));
+    double b1 =
+        sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(b), 1));
+    uint64_t d[2];
+    d[0] = !(a0 >= b0) ? ~UINT64_C(0) : UINT64_C(0);
+    d[1] = !(a1 >= b1) ? ~UINT64_C(0) : UINT64_C(0);
+
+    return vreinterpretq_m128d_u64(vld1q_u64(d));
+#endif
+}
+
+// Compare the lower double-precision (64-bit) floating-point elements in a and
+// b for not-greater-than-or-equal, store the result in the lower element of
+// dst, and copy the upper element from a to the upper element of dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cmpnge_sd
+FORCE_INLINE __m128d _mm_cmpnge_sd(__m128d a, __m128d b)
+{
+    return _mm_move_sd(a, _mm_cmpnge_pd(a, b));
+}
+
+// Compare packed double-precision (64-bit) floating-point elements in a and b
+// for not-greater-than, and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_cmpngt_pd
+FORCE_INLINE __m128d _mm_cmpngt_pd(__m128d a, __m128d b)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    return vreinterpretq_m128d_u64(veorq_u64(
+        vcgtq_f64(vreinterpretq_f64_m128d(a), vreinterpretq_f64_m128d(b)),
+        vdupq_n_u64(UINT64_MAX)));
+#else
+    double a0 =
+        sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(a), 0));
+    double a1 =
+        sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(a), 1));
+    double b0 =
+        sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(b), 0));
+    double b1 =
+        sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(b), 1));
+    uint64_t d[2];
+    d[0] = !(a0 > b0) ? ~UINT64_C(0) : UINT64_C(0);
+    d[1] = !(a1 > b1) ? ~UINT64_C(0) : UINT64_C(0);
+
+    return vreinterpretq_m128d_u64(vld1q_u64(d));
+#endif
+}
+
+// Compare the lower double-precision (64-bit) floating-point elements in a and
+// b for not-greater-than, store the result in the lower element of dst, and
+// copy the upper element from a to the upper element of dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cmpngt_sd
+FORCE_INLINE __m128d _mm_cmpngt_sd(__m128d a, __m128d b)
+{
+    return _mm_move_sd(a, _mm_cmpngt_pd(a, b));
+}
+
+// Compare packed double-precision (64-bit) floating-point elements in a and b
+// for not-less-than-or-equal, and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cmpnle_pd
+FORCE_INLINE __m128d _mm_cmpnle_pd(__m128d a, __m128d b)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    return vreinterpretq_m128d_u64(veorq_u64(
+        vcleq_f64(vreinterpretq_f64_m128d(a), vreinterpretq_f64_m128d(b)),
+        vdupq_n_u64(UINT64_MAX)));
+#else
+    double a0 =
+        sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(a), 0));
+    double a1 =
+        sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(a), 1));
+    double b0 =
+        sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(b), 0));
+    double b1 =
+        sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(b), 1));
+    uint64_t d[2];
+    d[0] = !(a0 <= b0) ? ~UINT64_C(0) : UINT64_C(0);
+    d[1] = !(a1 <= b1) ? ~UINT64_C(0) : UINT64_C(0);
+
+    return vreinterpretq_m128d_u64(vld1q_u64(d));
+#endif
+}
+
+// Compare the lower double-precision (64-bit) floating-point elements in a and
+// b for not-less-than-or-equal, store the result in the lower element of dst,
+// and copy the upper element from a to the upper element of dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cmpnle_sd
+FORCE_INLINE __m128d _mm_cmpnle_sd(__m128d a, __m128d b)
+{
+    return _mm_move_sd(a, _mm_cmpnle_pd(a, b));
+}
+
+// Compare packed double-precision (64-bit) floating-point elements in a and b
+// for not-less-than, and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cmpnlt_pd
+FORCE_INLINE __m128d _mm_cmpnlt_pd(__m128d a, __m128d b)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    return vreinterpretq_m128d_u64(veorq_u64(
+        vcltq_f64(vreinterpretq_f64_m128d(a), vreinterpretq_f64_m128d(b)),
+        vdupq_n_u64(UINT64_MAX)));
+#else
+    double a0 =
+        sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(a), 0));
+    double a1 =
+        sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(a), 1));
+    double b0 =
+        sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(b), 0));
+    double b1 =
+        sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(b), 1));
+    uint64_t d[2];
+    d[0] = !(a0 < b0) ? ~UINT64_C(0) : UINT64_C(0);
+    d[1] = !(a1 < b1) ? ~UINT64_C(0) : UINT64_C(0);
+
+    return vreinterpretq_m128d_u64(vld1q_u64(d));
+#endif
+}
+
+// Compare the lower double-precision (64-bit) floating-point elements in a and
+// b for not-less-than, store the result in the lower element of dst, and copy
+// the upper element from a to the upper element of dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cmpnlt_sd
+FORCE_INLINE __m128d _mm_cmpnlt_sd(__m128d a, __m128d b)
+{
+    return _mm_move_sd(a, _mm_cmpnlt_pd(a, b));
+}
+
+// Compare packed double-precision (64-bit) floating-point elements in a and b
+// to see if neither is NaN, and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cmpord_pd
+FORCE_INLINE __m128d _mm_cmpord_pd(__m128d a, __m128d b)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    // Excluding NaNs, any two floating point numbers can be compared.
+    uint64x2_t not_nan_a =
+        vceqq_f64(vreinterpretq_f64_m128d(a), vreinterpretq_f64_m128d(a));
+    uint64x2_t not_nan_b =
+        vceqq_f64(vreinterpretq_f64_m128d(b), vreinterpretq_f64_m128d(b));
+    return vreinterpretq_m128d_u64(vandq_u64(not_nan_a, not_nan_b));
+#else
+    double a0 =
+        sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(a), 0));
+    double a1 =
+        sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(a), 1));
+    double b0 =
+        sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(b), 0));
+    double b1 =
+        sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(b), 1));
+    uint64_t d[2];
+    d[0] = (a0 == a0 && b0 == b0) ? ~UINT64_C(0) : UINT64_C(0);
+    d[1] = (a1 == a1 && b1 == b1) ? ~UINT64_C(0) : UINT64_C(0);
+
+    return vreinterpretq_m128d_u64(vld1q_u64(d));
+#endif
+}
+
+// Compare the lower double-precision (64-bit) floating-point elements in a and
+// b to see if neither is NaN, store the result in the lower element of dst, and
+// copy the upper element from a to the upper element of dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cmpord_sd
+FORCE_INLINE __m128d _mm_cmpord_sd(__m128d a, __m128d b)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    return _mm_move_sd(a, _mm_cmpord_pd(a, b));
+#else
+    double a0, b0;
+    a0 = sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(a), 0));
+    uint64_t a1 = vgetq_lane_u64(vreinterpretq_u64_m128d(a), 1);
+    b0 = sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(b), 0));
+    uint64_t d[2];
+    d[0] = (a0 == a0 && b0 == b0) ? ~UINT64_C(0) : UINT64_C(0);
+    d[1] = a1;
+
+    return vreinterpretq_m128d_u64(vld1q_u64(d));
+#endif
+}
+
+// Compare packed double-precision (64-bit) floating-point elements in a and b
+// to see if either is NaN, and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cmpunord_pd
+FORCE_INLINE __m128d _mm_cmpunord_pd(__m128d a, __m128d b)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    // Two NaNs are not equal in comparison operation.
+    uint64x2_t not_nan_a =
+        vceqq_f64(vreinterpretq_f64_m128d(a), vreinterpretq_f64_m128d(a));
+    uint64x2_t not_nan_b =
+        vceqq_f64(vreinterpretq_f64_m128d(b), vreinterpretq_f64_m128d(b));
+    return vreinterpretq_m128d_s32(
+        vmvnq_s32(vreinterpretq_s32_u64(vandq_u64(not_nan_a, not_nan_b))));
+#else
+    double a0 =
+        sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(a), 0));
+    double a1 =
+        sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(a), 1));
+    double b0 =
+        sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(b), 0));
+    double b1 =
+        sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(b), 1));
+    uint64_t d[2];
+    d[0] = (a0 == a0 && b0 == b0) ? UINT64_C(0) : ~UINT64_C(0);
+    d[1] = (a1 == a1 && b1 == b1) ? UINT64_C(0) : ~UINT64_C(0);
+
+    return vreinterpretq_m128d_u64(vld1q_u64(d));
+#endif
+}
+
+// Compare the lower double-precision (64-bit) floating-point elements in a and
+// b to see if either is NaN, store the result in the lower element of dst, and
+// copy the upper element from a to the upper element of dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cmpunord_sd
+FORCE_INLINE __m128d _mm_cmpunord_sd(__m128d a, __m128d b)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    return _mm_move_sd(a, _mm_cmpunord_pd(a, b));
+#else
+    double a0, b0;
+    a0 = sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(a), 0));
+    uint64_t a1 = vgetq_lane_u64(vreinterpretq_u64_m128d(a), 1);
+    b0 = sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(b), 0));
+    uint64_t d[2];
+    d[0] = (a0 == a0 && b0 == b0) ? UINT64_C(0) : ~UINT64_C(0);
+    d[1] = a1;
+
+    return vreinterpretq_m128d_u64(vld1q_u64(d));
+#endif
+}
+
+// Compare the lower double-precision (64-bit) floating-point element in a and b
+// for greater-than-or-equal, and return the boolean result (0 or 1).
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_comige_sd
+FORCE_INLINE int _mm_comige_sd(__m128d a, __m128d b)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    return vgetq_lane_u64(vcgeq_f64(a, b), 0) & 0x1;
+#else
+    double a0, b0;
+    a0 = sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(a), 0));
+    b0 = sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(b), 0));
+    return a0 >= b0;
+#endif
+}
+
+// Compare the lower double-precision (64-bit) floating-point element in a and b
+// for greater-than, and return the boolean result (0 or 1).
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_comigt_sd
+FORCE_INLINE int _mm_comigt_sd(__m128d a, __m128d b)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    return vgetq_lane_u64(vcgtq_f64(a, b), 0) & 0x1;
+#else
+    double a0, b0;
+    a0 = sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(a), 0));
+    b0 = sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(b), 0));
+
+    return a0 > b0;
+#endif
+}
+
+// Compare the lower double-precision (64-bit) floating-point element in a and b
+// for less-than-or-equal, and return the boolean result (0 or 1).
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_comile_sd
+FORCE_INLINE int _mm_comile_sd(__m128d a, __m128d b)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    return vgetq_lane_u64(vcleq_f64(a, b), 0) & 0x1;
+#else
+    double a0, b0;
+    a0 = sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(a), 0));
+    b0 = sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(b), 0));
+
+    return a0 <= b0;
+#endif
+}
+
+// Compare the lower double-precision (64-bit) floating-point element in a and b
+// for less-than, and return the boolean result (0 or 1).
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_comilt_sd
+FORCE_INLINE int _mm_comilt_sd(__m128d a, __m128d b)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    return vgetq_lane_u64(vcltq_f64(a, b), 0) & 0x1;
+#else
+    double a0, b0;
+    a0 = sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(a), 0));
+    b0 = sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(b), 0));
+
+    return a0 < b0;
+#endif
+}
+
+// Compare the lower double-precision (64-bit) floating-point element in a and b
+// for equality, and return the boolean result (0 or 1).
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_comieq_sd
+FORCE_INLINE int _mm_comieq_sd(__m128d a, __m128d b)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    return vgetq_lane_u64(vceqq_f64(a, b), 0) & 0x1;
+#else
+    uint32x4_t a_not_nan =
+        vceqq_u32(vreinterpretq_u32_m128d(a), vreinterpretq_u32_m128d(a));
+    uint32x4_t b_not_nan =
+        vceqq_u32(vreinterpretq_u32_m128d(b), vreinterpretq_u32_m128d(b));
+    uint32x4_t a_and_b_not_nan = vandq_u32(a_not_nan, b_not_nan);
+    uint32x4_t a_eq_b =
+        vceqq_u32(vreinterpretq_u32_m128d(a), vreinterpretq_u32_m128d(b));
+    uint64x2_t and_results = vandq_u64(vreinterpretq_u64_u32(a_and_b_not_nan),
+                                       vreinterpretq_u64_u32(a_eq_b));
+    return vgetq_lane_u64(and_results, 0) & 0x1;
+#endif
+}
+
+// Compare the lower double-precision (64-bit) floating-point element in a and b
+// for not-equal, and return the boolean result (0 or 1).
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_comineq_sd
+FORCE_INLINE int _mm_comineq_sd(__m128d a, __m128d b)
+{
+    return !_mm_comieq_sd(a, b);
+}
+
+// Convert packed signed 32-bit integers in a to packed double-precision
+// (64-bit) floating-point elements, and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cvtepi32_pd
+FORCE_INLINE __m128d _mm_cvtepi32_pd(__m128i a)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    return vreinterpretq_m128d_f64(
+        vcvtq_f64_s64(vmovl_s32(vget_low_s32(vreinterpretq_s32_m128i(a)))));
+#else
+    double a0 = (double) vgetq_lane_s32(vreinterpretq_s32_m128i(a), 0);
+    double a1 = (double) vgetq_lane_s32(vreinterpretq_s32_m128i(a), 1);
+    return _mm_set_pd(a1, a0);
+#endif
+}
+
+// Convert packed signed 32-bit integers in a to packed single-precision
+// (32-bit) floating-point elements, and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cvtepi32_ps
+FORCE_INLINE __m128 _mm_cvtepi32_ps(__m128i a)
+{
+    return vreinterpretq_m128_f32(vcvtq_f32_s32(vreinterpretq_s32_m128i(a)));
+}
+
+// Convert packed double-precision (64-bit) floating-point elements in a to
+// packed 32-bit integers, and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cvtpd_epi32
+FORCE_INLINE_OPTNONE __m128i _mm_cvtpd_epi32(__m128d a)
+{
+// vrnd32xq_f64 not supported on clang
+#if defined(__ARM_FEATURE_FRINT) && !defined(__clang__)
+    float64x2_t rounded = vrnd32xq_f64(vreinterpretq_f64_m128d(a));
+    int64x2_t integers = vcvtq_s64_f64(rounded);
+    return vreinterpretq_m128i_s32(
+        vcombine_s32(vmovn_s64(integers), vdup_n_s32(0)));
+#else
+    __m128d rnd = _mm_round_pd(a, _MM_FROUND_CUR_DIRECTION);
+    double d0, d1;
+    d0 = sse2neon_recast_u64_f64(
+        vgetq_lane_u64(vreinterpretq_u64_m128d(rnd), 0));
+    d1 = sse2neon_recast_u64_f64(
+        vgetq_lane_u64(vreinterpretq_u64_m128d(rnd), 1));
+    return _mm_set_epi32(0, 0, (int32_t) d1, (int32_t) d0);
+#endif
+}
+
+// Convert packed double-precision (64-bit) floating-point elements in a to
+// packed 32-bit integers, and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cvtpd_pi32
+FORCE_INLINE_OPTNONE __m64 _mm_cvtpd_pi32(__m128d a)
+{
+    __m128d rnd = _mm_round_pd(a, _MM_FROUND_CUR_DIRECTION);
+    double d0, d1;
+    d0 = sse2neon_recast_u64_f64(
+        vgetq_lane_u64(vreinterpretq_u64_m128d(rnd), 0));
+    d1 = sse2neon_recast_u64_f64(
+        vgetq_lane_u64(vreinterpretq_u64_m128d(rnd), 1));
+    int32_t ALIGN_STRUCT(16) data[2] = {(int32_t) d0, (int32_t) d1};
+    return vreinterpret_m64_s32(vld1_s32(data));
+}
+
+// Convert packed double-precision (64-bit) floating-point elements in a to
+// packed single-precision (32-bit) floating-point elements, and store the
+// results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cvtpd_ps
+FORCE_INLINE __m128 _mm_cvtpd_ps(__m128d a)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    float32x2_t tmp = vcvt_f32_f64(vreinterpretq_f64_m128d(a));
+    return vreinterpretq_m128_f32(vcombine_f32(tmp, vdup_n_f32(0)));
+#else
+    double a0, a1;
+    a0 = sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(a), 0));
+    a1 = sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(a), 1));
+    return _mm_set_ps(0, 0, (float) a1, (float) a0);
+#endif
+}
+
+// Convert packed signed 32-bit integers in a to packed double-precision
+// (64-bit) floating-point elements, and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cvtpi32_pd
+FORCE_INLINE __m128d _mm_cvtpi32_pd(__m64 a)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    return vreinterpretq_m128d_f64(
+        vcvtq_f64_s64(vmovl_s32(vreinterpret_s32_m64(a))));
+#else
+    double a0 = (double) vget_lane_s32(vreinterpret_s32_m64(a), 0);
+    double a1 = (double) vget_lane_s32(vreinterpret_s32_m64(a), 1);
+    return _mm_set_pd(a1, a0);
+#endif
+}
+
+// Convert packed single-precision (32-bit) floating-point elements in a to
+// packed 32-bit integers, and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cvtps_epi32
+// *NOTE*. The default rounding mode on SSE is 'round to even', which ARMv7-A
+// does not support! It is supported on ARMv8-A however.
+FORCE_INLINE __m128i _mm_cvtps_epi32(__m128 a)
+{
+#if defined(__ARM_FEATURE_FRINT)
+    return vreinterpretq_m128i_s32(vcvtq_s32_f32(vrnd32xq_f32(a)));
+#elif (defined(__aarch64__) || defined(_M_ARM64)) || \
+    defined(__ARM_FEATURE_DIRECTED_ROUNDING)
+    switch (_MM_GET_ROUNDING_MODE()) {
+    case _MM_ROUND_NEAREST:
+        return vreinterpretq_m128i_s32(vcvtnq_s32_f32(a));
+    case _MM_ROUND_DOWN:
+        return vreinterpretq_m128i_s32(vcvtmq_s32_f32(a));
+    case _MM_ROUND_UP:
+        return vreinterpretq_m128i_s32(vcvtpq_s32_f32(a));
+    default:  // _MM_ROUND_TOWARD_ZERO
+        return vreinterpretq_m128i_s32(vcvtq_s32_f32(a));
+    }
+#else
+    float *f = (float *) &a;
+    switch (_MM_GET_ROUNDING_MODE()) {
+    case _MM_ROUND_NEAREST: {
+        uint32x4_t signmask = vdupq_n_u32(0x80000000);
+        float32x4_t half = vbslq_f32(signmask, vreinterpretq_f32_m128(a),
+                                     vdupq_n_f32(0.5f)); /* +/- 0.5 */
+        int32x4_t r_normal = vcvtq_s32_f32(vaddq_f32(
+            vreinterpretq_f32_m128(a), half)); /* round to integer: [a + 0.5]*/
+        int32x4_t r_trunc = vcvtq_s32_f32(
+            vreinterpretq_f32_m128(a)); /* truncate to integer: [a] */
+        int32x4_t plusone = vreinterpretq_s32_u32(vshrq_n_u32(
+            vreinterpretq_u32_s32(vnegq_s32(r_trunc)), 31)); /* 1 or 0 */
+        int32x4_t r_even = vbicq_s32(vaddq_s32(r_trunc, plusone),
+                                     vdupq_n_s32(1)); /* ([a] + {0,1}) & ~1 */
+        float32x4_t delta = vsubq_f32(
+            vreinterpretq_f32_m128(a),
+            vcvtq_f32_s32(r_trunc)); /* compute delta: delta = (a - [a]) */
+        uint32x4_t is_delta_half =
+            vceqq_f32(delta, half); /* delta == +/- 0.5 */
+        return vreinterpretq_m128i_s32(
+            vbslq_s32(is_delta_half, r_even, r_normal));
+    }
+    case _MM_ROUND_DOWN:
+        return _mm_set_epi32(floorf(f[3]), floorf(f[2]), floorf(f[1]),
+                             floorf(f[0]));
+    case _MM_ROUND_UP:
+        return _mm_set_epi32(ceilf(f[3]), ceilf(f[2]), ceilf(f[1]),
+                             ceilf(f[0]));
+    default:  // _MM_ROUND_TOWARD_ZERO
+        return _mm_set_epi32((int32_t) f[3], (int32_t) f[2], (int32_t) f[1],
+                             (int32_t) f[0]);
+    }
+#endif
+}
+
+// Convert packed single-precision (32-bit) floating-point elements in a to
+// packed double-precision (64-bit) floating-point elements, and store the
+// results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cvtps_pd
+FORCE_INLINE __m128d _mm_cvtps_pd(__m128 a)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    return vreinterpretq_m128d_f64(
+        vcvt_f64_f32(vget_low_f32(vreinterpretq_f32_m128(a))));
+#else
+    double a0 = (double) vgetq_lane_f32(vreinterpretq_f32_m128(a), 0);
+    double a1 = (double) vgetq_lane_f32(vreinterpretq_f32_m128(a), 1);
+    return _mm_set_pd(a1, a0);
+#endif
+}
+
+// Copy the lower double-precision (64-bit) floating-point element of a to dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cvtsd_f64
+FORCE_INLINE double _mm_cvtsd_f64(__m128d a)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    return (double) vgetq_lane_f64(vreinterpretq_f64_m128d(a), 0);
+#else
+    double _a =
+        sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(a), 0));
+    return _a;
+#endif
+}
+
+// Convert the lower double-precision (64-bit) floating-point element in a to a
+// 32-bit integer, and store the result in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cvtsd_si32
+FORCE_INLINE int32_t _mm_cvtsd_si32(__m128d a)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    return (int32_t) vgetq_lane_f64(vrndiq_f64(vreinterpretq_f64_m128d(a)), 0);
+#else
+    __m128d rnd = _mm_round_pd(a, _MM_FROUND_CUR_DIRECTION);
+    double ret = sse2neon_recast_u64_f64(
+        vgetq_lane_u64(vreinterpretq_u64_m128d(rnd), 0));
+    return (int32_t) ret;
+#endif
+}
+
+// Convert the lower double-precision (64-bit) floating-point element in a to a
+// 64-bit integer, and store the result in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cvtsd_si64
+FORCE_INLINE int64_t _mm_cvtsd_si64(__m128d a)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    return (int64_t) vgetq_lane_f64(vrndiq_f64(vreinterpretq_f64_m128d(a)), 0);
+#else
+    __m128d rnd = _mm_round_pd(a, _MM_FROUND_CUR_DIRECTION);
+    double ret = sse2neon_recast_u64_f64(
+        vgetq_lane_u64(vreinterpretq_u64_m128d(rnd), 0));
+    return (int64_t) ret;
+#endif
+}
+
+// Convert the lower double-precision (64-bit) floating-point element in a to a
+// 64-bit integer, and store the result in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cvtsd_si64x
+#define _mm_cvtsd_si64x _mm_cvtsd_si64
+
+// Convert the lower double-precision (64-bit) floating-point element in b to a
+// single-precision (32-bit) floating-point element, store the result in the
+// lower element of dst, and copy the upper 3 packed elements from a to the
+// upper elements of dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cvtsd_ss
+FORCE_INLINE __m128 _mm_cvtsd_ss(__m128 a, __m128d b)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    return vreinterpretq_m128_f32(vsetq_lane_f32(
+        vget_lane_f32(vcvt_f32_f64(vreinterpretq_f64_m128d(b)), 0),
+        vreinterpretq_f32_m128(a), 0));
+#else
+    double b0 =
+        sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(b), 0));
+    return vreinterpretq_m128_f32(
+        vsetq_lane_f32((float) b0, vreinterpretq_f32_m128(a), 0));
+#endif
+}
+
+// Copy the lower 32-bit integer in a to dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cvtsi128_si32
+FORCE_INLINE int _mm_cvtsi128_si32(__m128i a)
+{
+    return vgetq_lane_s32(vreinterpretq_s32_m128i(a), 0);
+}
+
+// Copy the lower 64-bit integer in a to dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cvtsi128_si64
+FORCE_INLINE int64_t _mm_cvtsi128_si64(__m128i a)
+{
+    return vgetq_lane_s64(vreinterpretq_s64_m128i(a), 0);
+}
+
+// Copy the lower 64-bit integer in a to dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cvtsi128_si64x
+#define _mm_cvtsi128_si64x(a) _mm_cvtsi128_si64(a)
+
+// Convert the signed 32-bit integer b to a double-precision (64-bit)
+// floating-point element, store the result in the lower element of dst, and
+// copy the upper element from a to the upper element of dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cvtsi32_sd
+FORCE_INLINE __m128d _mm_cvtsi32_sd(__m128d a, int32_t b)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    return vreinterpretq_m128d_f64(
+        vsetq_lane_f64((double) b, vreinterpretq_f64_m128d(a), 0));
+#else
+    int64_t _b = sse2neon_recast_f64_s64((double) b);
+    return vreinterpretq_m128d_s64(
+        vsetq_lane_s64(_b, vreinterpretq_s64_m128d(a), 0));
+#endif
+}
+
+// Copy the lower 64-bit integer in a to dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cvtsi128_si64x
+#define _mm_cvtsi128_si64x(a) _mm_cvtsi128_si64(a)
+
+// Copy 32-bit integer a to the lower elements of dst, and zero the upper
+// elements of dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cvtsi32_si128
+FORCE_INLINE __m128i _mm_cvtsi32_si128(int a)
+{
+    return vreinterpretq_m128i_s32(vsetq_lane_s32(a, vdupq_n_s32(0), 0));
+}
+
+// Convert the signed 64-bit integer b to a double-precision (64-bit)
+// floating-point element, store the result in the lower element of dst, and
+// copy the upper element from a to the upper element of dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cvtsi64_sd
+FORCE_INLINE __m128d _mm_cvtsi64_sd(__m128d a, int64_t b)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    return vreinterpretq_m128d_f64(
+        vsetq_lane_f64((double) b, vreinterpretq_f64_m128d(a), 0));
+#else
+    int64_t _b = sse2neon_recast_f64_s64((double) b);
+    return vreinterpretq_m128d_s64(
+        vsetq_lane_s64(_b, vreinterpretq_s64_m128d(a), 0));
+#endif
+}
+
+// Copy 64-bit integer a to the lower element of dst, and zero the upper
+// element.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cvtsi64_si128
+FORCE_INLINE __m128i _mm_cvtsi64_si128(int64_t a)
+{
+    return vreinterpretq_m128i_s64(vsetq_lane_s64(a, vdupq_n_s64(0), 0));
+}
+
+// Copy 64-bit integer a to the lower element of dst, and zero the upper
+// element.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cvtsi64x_si128
+#define _mm_cvtsi64x_si128(a) _mm_cvtsi64_si128(a)
+
+// Convert the signed 64-bit integer b to a double-precision (64-bit)
+// floating-point element, store the result in the lower element of dst, and
+// copy the upper element from a to the upper element of dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cvtsi64x_sd
+#define _mm_cvtsi64x_sd(a, b) _mm_cvtsi64_sd(a, b)
+
+// Convert the lower single-precision (32-bit) floating-point element in b to a
+// double-precision (64-bit) floating-point element, store the result in the
+// lower element of dst, and copy the upper element from a to the upper element
+// of dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cvtss_sd
+FORCE_INLINE __m128d _mm_cvtss_sd(__m128d a, __m128 b)
+{
+    double d = (double) vgetq_lane_f32(vreinterpretq_f32_m128(b), 0);
+#if defined(__aarch64__) || defined(_M_ARM64)
+    return vreinterpretq_m128d_f64(
+        vsetq_lane_f64(d, vreinterpretq_f64_m128d(a), 0));
+#else
+    return vreinterpretq_m128d_s64(vsetq_lane_s64(
+        sse2neon_recast_f64_s64(d), vreinterpretq_s64_m128d(a), 0));
+#endif
+}
+
+// Convert packed double-precision (64-bit) floating-point elements in a to
+// packed 32-bit integers with truncation, and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cvttpd_epi32
+FORCE_INLINE __m128i _mm_cvttpd_epi32(__m128d a)
+{
+    double a0, a1;
+    a0 = sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(a), 0));
+    a1 = sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(a), 1));
+    return _mm_set_epi32(0, 0, (int32_t) a1, (int32_t) a0);
+}
+
+// Convert packed double-precision (64-bit) floating-point elements in a to
+// packed 32-bit integers with truncation, and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cvttpd_pi32
+FORCE_INLINE_OPTNONE __m64 _mm_cvttpd_pi32(__m128d a)
+{
+    double a0, a1;
+    a0 = sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(a), 0));
+    a1 = sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(a), 1));
+    int32_t ALIGN_STRUCT(16) data[2] = {(int32_t) a0, (int32_t) a1};
+    return vreinterpret_m64_s32(vld1_s32(data));
+}
+
+// Convert packed single-precision (32-bit) floating-point elements in a to
+// packed 32-bit integers with truncation, and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cvttps_epi32
+FORCE_INLINE __m128i _mm_cvttps_epi32(__m128 a)
+{
+    return vreinterpretq_m128i_s32(vcvtq_s32_f32(vreinterpretq_f32_m128(a)));
+}
+
+// Convert the lower double-precision (64-bit) floating-point element in a to a
+// 32-bit integer with truncation, and store the result in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cvttsd_si32
+FORCE_INLINE int32_t _mm_cvttsd_si32(__m128d a)
+{
+    double _a =
+        sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(a), 0));
+    return (int32_t) _a;
+}
+
+// Convert the lower double-precision (64-bit) floating-point element in a to a
+// 64-bit integer with truncation, and store the result in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cvttsd_si64
+FORCE_INLINE int64_t _mm_cvttsd_si64(__m128d a)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    return vgetq_lane_s64(vcvtq_s64_f64(vreinterpretq_f64_m128d(a)), 0);
+#else
+    double _a =
+        sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(a), 0));
+    return (int64_t) _a;
+#endif
+}
+
+// Convert the lower double-precision (64-bit) floating-point element in a to a
+// 64-bit integer with truncation, and store the result in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cvttsd_si64x
+#define _mm_cvttsd_si64x(a) _mm_cvttsd_si64(a)
+
+// Divide packed double-precision (64-bit) floating-point elements in a by
+// packed elements in b, and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_div_pd
+FORCE_INLINE __m128d _mm_div_pd(__m128d a, __m128d b)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    return vreinterpretq_m128d_f64(
+        vdivq_f64(vreinterpretq_f64_m128d(a), vreinterpretq_f64_m128d(b)));
+#else
+    double a0 =
+        sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(a), 0));
+    double a1 =
+        sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(a), 1));
+    double b0 =
+        sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(b), 0));
+    double b1 =
+        sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(b), 1));
+    double c[2];
+    c[0] = a0 / b0;
+    c[1] = a1 / b1;
+    return vld1q_f32((float32_t *) c);
+#endif
+}
+
+// Divide the lower double-precision (64-bit) floating-point element in a by the
+// lower double-precision (64-bit) floating-point element in b, store the result
+// in the lower element of dst, and copy the upper element from a to the upper
+// element of dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_div_sd
+FORCE_INLINE __m128d _mm_div_sd(__m128d a, __m128d b)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    float64x2_t tmp =
+        vdivq_f64(vreinterpretq_f64_m128d(a), vreinterpretq_f64_m128d(b));
+    return vreinterpretq_m128d_f64(
+        vsetq_lane_f64(vgetq_lane_f64(vreinterpretq_f64_m128d(a), 1), tmp, 1));
+#else
+    return _mm_move_sd(a, _mm_div_pd(a, b));
+#endif
+}
+
+// Extract a 16-bit integer from a, selected with imm8, and store the result in
+// the lower element of dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_extract_epi16
+// FORCE_INLINE int _mm_extract_epi16(__m128i a, __constrange(0,8) int imm)
+#define _mm_extract_epi16(a, imm) \
+    vgetq_lane_u16(vreinterpretq_u16_m128i(a), (imm))
+
+// Copy a to dst, and insert the 16-bit integer i into dst at the location
+// specified by imm8.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_insert_epi16
+// FORCE_INLINE __m128i _mm_insert_epi16(__m128i a, int b,
+//                                       __constrange(0,8) int imm)
+#define _mm_insert_epi16(a, b, imm) \
+    vreinterpretq_m128i_s16(        \
+        vsetq_lane_s16((b), vreinterpretq_s16_m128i(a), (imm)))
+
+// Load 128-bits (composed of 2 packed double-precision (64-bit) floating-point
+// elements) from memory into dst. mem_addr must be aligned on a 16-byte
+// boundary or a general-protection exception may be generated.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_load_pd
+FORCE_INLINE __m128d _mm_load_pd(const double *p)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    return vreinterpretq_m128d_f64(vld1q_f64(p));
+#else
+    const float *fp = (const float *) p;
+    float ALIGN_STRUCT(16) data[4] = {fp[0], fp[1], fp[2], fp[3]};
+    return vreinterpretq_m128d_f32(vld1q_f32(data));
+#endif
+}
+
+// Load a double-precision (64-bit) floating-point element from memory into both
+// elements of dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_load_pd1
+#define _mm_load_pd1 _mm_load1_pd
+
+// Load a double-precision (64-bit) floating-point element from memory into the
+// lower of dst, and zero the upper element. mem_addr does not need to be
+// aligned on any particular boundary.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_load_sd
+FORCE_INLINE __m128d _mm_load_sd(const double *p)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    return vreinterpretq_m128d_f64(vsetq_lane_f64(*p, vdupq_n_f64(0), 0));
+#else
+    const float *fp = (const float *) p;
+    float ALIGN_STRUCT(16) data[4] = {fp[0], fp[1], 0, 0};
+    return vreinterpretq_m128d_f32(vld1q_f32(data));
+#endif
+}
+
+// Load 128-bits of integer data from memory into dst. mem_addr must be aligned
+// on a 16-byte boundary or a general-protection exception may be generated.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_load_si128
+FORCE_INLINE __m128i _mm_load_si128(const __m128i *p)
+{
+    return vreinterpretq_m128i_s32(vld1q_s32((const int32_t *) p));
+}
+
+// Load a double-precision (64-bit) floating-point element from memory into both
+// elements of dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_load1_pd
+FORCE_INLINE __m128d _mm_load1_pd(const double *p)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    return vreinterpretq_m128d_f64(vld1q_dup_f64(p));
+#else
+    return vreinterpretq_m128d_s64(vdupq_n_s64(*(const int64_t *) p));
+#endif
+}
+
+// Load a double-precision (64-bit) floating-point element from memory into the
+// upper element of dst, and copy the lower element from a to dst. mem_addr does
+// not need to be aligned on any particular boundary.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_loadh_pd
+FORCE_INLINE __m128d _mm_loadh_pd(__m128d a, const double *p)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    return vreinterpretq_m128d_f64(
+        vcombine_f64(vget_low_f64(vreinterpretq_f64_m128d(a)), vld1_f64(p)));
+#else
+    return vreinterpretq_m128d_f32(vcombine_f32(
+        vget_low_f32(vreinterpretq_f32_m128d(a)), vld1_f32((const float *) p)));
+#endif
+}
+
+// Load 64-bit integer from memory into the first element of dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_loadl_epi64
+FORCE_INLINE __m128i _mm_loadl_epi64(__m128i const *p)
+{
+    /* Load the lower 64 bits of the value pointed to by p into the
+     * lower 64 bits of the result, zeroing the upper 64 bits of the result.
+     */
+    return vreinterpretq_m128i_s32(
+        vcombine_s32(vld1_s32((int32_t const *) p), vcreate_s32(0)));
+}
+
+// Load a double-precision (64-bit) floating-point element from memory into the
+// lower element of dst, and copy the upper element from a to dst. mem_addr does
+// not need to be aligned on any particular boundary.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_loadl_pd
+FORCE_INLINE __m128d _mm_loadl_pd(__m128d a, const double *p)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    return vreinterpretq_m128d_f64(
+        vcombine_f64(vld1_f64(p), vget_high_f64(vreinterpretq_f64_m128d(a))));
+#else
+    return vreinterpretq_m128d_f32(
+        vcombine_f32(vld1_f32((const float *) p),
+                     vget_high_f32(vreinterpretq_f32_m128d(a))));
+#endif
+}
+
+// Load 2 double-precision (64-bit) floating-point elements from memory into dst
+// in reverse order. mem_addr must be aligned on a 16-byte boundary or a
+// general-protection exception may be generated.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_loadr_pd
+FORCE_INLINE __m128d _mm_loadr_pd(const double *p)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    float64x2_t v = vld1q_f64(p);
+    return vreinterpretq_m128d_f64(vextq_f64(v, v, 1));
+#else
+    int64x2_t v = vld1q_s64((const int64_t *) p);
+    return vreinterpretq_m128d_s64(vextq_s64(v, v, 1));
+#endif
+}
+
+// Loads two double-precision from unaligned memory, floating-point values.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_loadu_pd
+FORCE_INLINE __m128d _mm_loadu_pd(const double *p)
+{
+    return _mm_load_pd(p);
+}
+
+// Load 128-bits of integer data from memory into dst. mem_addr does not need to
+// be aligned on any particular boundary.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_loadu_si128
+FORCE_INLINE __m128i _mm_loadu_si128(const __m128i *p)
+{
+    return vreinterpretq_m128i_s32(vld1q_s32((const unaligned_int32_t *) p));
+}
+
+// Load unaligned 32-bit integer from memory into the first element of dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_loadu_si32
+FORCE_INLINE __m128i _mm_loadu_si32(const void *p)
+{
+    return vreinterpretq_m128i_s32(
+        vsetq_lane_s32(*(const unaligned_int32_t *) p, vdupq_n_s32(0), 0));
+}
+
+// Multiply packed signed 16-bit integers in a and b, producing intermediate
+// signed 32-bit integers. Horizontally add adjacent pairs of intermediate
+// 32-bit integers, and pack the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_madd_epi16
+FORCE_INLINE __m128i _mm_madd_epi16(__m128i a, __m128i b)
+{
+    int32x4_t low = vmull_s16(vget_low_s16(vreinterpretq_s16_m128i(a)),
+                              vget_low_s16(vreinterpretq_s16_m128i(b)));
+#if defined(__aarch64__) || defined(_M_ARM64)
+    int32x4_t high =
+        vmull_high_s16(vreinterpretq_s16_m128i(a), vreinterpretq_s16_m128i(b));
+
+    return vreinterpretq_m128i_s32(vpaddq_s32(low, high));
+#else
+    int32x4_t high = vmull_s16(vget_high_s16(vreinterpretq_s16_m128i(a)),
+                               vget_high_s16(vreinterpretq_s16_m128i(b)));
+
+    int32x2_t low_sum = vpadd_s32(vget_low_s32(low), vget_high_s32(low));
+    int32x2_t high_sum = vpadd_s32(vget_low_s32(high), vget_high_s32(high));
+
+    return vreinterpretq_m128i_s32(vcombine_s32(low_sum, high_sum));
+#endif
+}
+
+// Conditionally store 8-bit integer elements from a into memory using mask
+// (elements are not stored when the highest bit is not set in the corresponding
+// element) and a non-temporal memory hint. mem_addr does not need to be aligned
+// on any particular boundary.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_maskmoveu_si128
+FORCE_INLINE void _mm_maskmoveu_si128(__m128i a, __m128i mask, char *mem_addr)
+{
+    int8x16_t shr_mask = vshrq_n_s8(vreinterpretq_s8_m128i(mask), 7);
+    __m128 b = _mm_load_ps((const float *) mem_addr);
+    int8x16_t masked =
+        vbslq_s8(vreinterpretq_u8_s8(shr_mask), vreinterpretq_s8_m128i(a),
+                 vreinterpretq_s8_m128(b));
+    vst1q_s8((int8_t *) mem_addr, masked);
+}
+
+// Compare packed signed 16-bit integers in a and b, and store packed maximum
+// values in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_max_epi16
+FORCE_INLINE __m128i _mm_max_epi16(__m128i a, __m128i b)
+{
+    return vreinterpretq_m128i_s16(
+        vmaxq_s16(vreinterpretq_s16_m128i(a), vreinterpretq_s16_m128i(b)));
+}
+
+// Compare packed unsigned 8-bit integers in a and b, and store packed maximum
+// values in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_max_epu8
+FORCE_INLINE __m128i _mm_max_epu8(__m128i a, __m128i b)
+{
+    return vreinterpretq_m128i_u8(
+        vmaxq_u8(vreinterpretq_u8_m128i(a), vreinterpretq_u8_m128i(b)));
+}
+
+// Compare packed double-precision (64-bit) floating-point elements in a and b,
+// and store packed maximum values in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_max_pd
+FORCE_INLINE __m128d _mm_max_pd(__m128d a, __m128d b)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+#if SSE2NEON_PRECISE_MINMAX
+    float64x2_t _a = vreinterpretq_f64_m128d(a);
+    float64x2_t _b = vreinterpretq_f64_m128d(b);
+    return vreinterpretq_m128d_f64(vbslq_f64(vcgtq_f64(_a, _b), _a, _b));
+#else
+    return vreinterpretq_m128d_f64(
+        vmaxq_f64(vreinterpretq_f64_m128d(a), vreinterpretq_f64_m128d(b)));
+#endif
+#else
+    double a0 =
+        sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(a), 0));
+    double a1 =
+        sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(a), 1));
+    double b0 =
+        sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(b), 0));
+    double b1 =
+        sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(b), 1));
+    int64_t d[2];
+    d[0] = a0 > b0 ? sse2neon_recast_f64_s64(a0) : sse2neon_recast_f64_s64(b0);
+    d[1] = a1 > b1 ? sse2neon_recast_f64_s64(a1) : sse2neon_recast_f64_s64(b1);
+
+    return vreinterpretq_m128d_s64(vld1q_s64(d));
+#endif
+}
+
+// Compare the lower double-precision (64-bit) floating-point elements in a and
+// b, store the maximum value in the lower element of dst, and copy the upper
+// element from a to the upper element of dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_max_sd
+FORCE_INLINE __m128d _mm_max_sd(__m128d a, __m128d b)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    return _mm_move_sd(a, _mm_max_pd(a, b));
+#else
+    double a0, a1, b0;
+    a0 = sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(a), 0));
+    a1 = sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(a), 1));
+    b0 = sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(b), 0));
+    double c[2] = {a0 > b0 ? a0 : b0, a1};
+    return vreinterpretq_m128d_f32(vld1q_f32((float32_t *) c));
+#endif
+}
+
+// Compare packed signed 16-bit integers in a and b, and store packed minimum
+// values in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_min_epi16
+FORCE_INLINE __m128i _mm_min_epi16(__m128i a, __m128i b)
+{
+    return vreinterpretq_m128i_s16(
+        vminq_s16(vreinterpretq_s16_m128i(a), vreinterpretq_s16_m128i(b)));
+}
+
+// Compare packed unsigned 8-bit integers in a and b, and store packed minimum
+// values in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_min_epu8
+FORCE_INLINE __m128i _mm_min_epu8(__m128i a, __m128i b)
+{
+    return vreinterpretq_m128i_u8(
+        vminq_u8(vreinterpretq_u8_m128i(a), vreinterpretq_u8_m128i(b)));
+}
+
+// Compare packed double-precision (64-bit) floating-point elements in a and b,
+// and store packed minimum values in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_min_pd
+FORCE_INLINE __m128d _mm_min_pd(__m128d a, __m128d b)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+#if SSE2NEON_PRECISE_MINMAX
+    float64x2_t _a = vreinterpretq_f64_m128d(a);
+    float64x2_t _b = vreinterpretq_f64_m128d(b);
+    return vreinterpretq_m128d_f64(vbslq_f64(vcltq_f64(_a, _b), _a, _b));
+#else
+    return vreinterpretq_m128d_f64(
+        vminq_f64(vreinterpretq_f64_m128d(a), vreinterpretq_f64_m128d(b)));
+#endif
+#else
+    double a0 =
+        sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(a), 0));
+    double a1 =
+        sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(a), 1));
+    double b0 =
+        sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(b), 0));
+    double b1 =
+        sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(b), 1));
+    int64_t d[2];
+    d[0] = a0 < b0 ? sse2neon_recast_f64_s64(a0) : sse2neon_recast_f64_s64(b0);
+    d[1] = a1 < b1 ? sse2neon_recast_f64_s64(a1) : sse2neon_recast_f64_s64(b1);
+    return vreinterpretq_m128d_s64(vld1q_s64(d));
+#endif
+}
+
+// Compare the lower double-precision (64-bit) floating-point elements in a and
+// b, store the minimum value in the lower element of dst, and copy the upper
+// element from a to the upper element of dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_min_sd
+FORCE_INLINE __m128d _mm_min_sd(__m128d a, __m128d b)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    return _mm_move_sd(a, _mm_min_pd(a, b));
+#else
+    double a0, a1, b0;
+    a0 = sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(a), 0));
+    a1 = sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(a), 1));
+    b0 = sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(b), 0));
+    double c[2] = {a0 < b0 ? a0 : b0, a1};
+    return vreinterpretq_m128d_f32(vld1q_f32((float32_t *) c));
+#endif
+}
+
+// Copy the lower 64-bit integer in a to the lower element of dst, and zero the
+// upper element.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_move_epi64
+FORCE_INLINE __m128i _mm_move_epi64(__m128i a)
+{
+    return vreinterpretq_m128i_s64(
+        vsetq_lane_s64(0, vreinterpretq_s64_m128i(a), 1));
+}
+
+// Move the lower double-precision (64-bit) floating-point element from b to the
+// lower element of dst, and copy the upper element from a to the upper element
+// of dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_move_sd
+FORCE_INLINE __m128d _mm_move_sd(__m128d a, __m128d b)
+{
+    return vreinterpretq_m128d_f32(
+        vcombine_f32(vget_low_f32(vreinterpretq_f32_m128d(b)),
+                     vget_high_f32(vreinterpretq_f32_m128d(a))));
+}
+
+// Create mask from the most significant bit of each 8-bit element in a, and
+// store the result in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_movemask_epi8
+FORCE_INLINE int _mm_movemask_epi8(__m128i a)
+{
+    // Use increasingly wide shifts+adds to collect the sign bits
+    // together.
+    // Since the widening shifts would be rather confusing to follow in little
+    // endian, everything will be illustrated in big endian order instead. This
+    // has a different result - the bits would actually be reversed on a big
+    // endian machine.
+
+    // Starting input (only half the elements are shown):
+    // 89 ff 1d c0 00 10 99 33
+    uint8x16_t input = vreinterpretq_u8_m128i(a);
+
+    // Shift out everything but the sign bits with an unsigned shift right.
+    //
+    // Bytes of the vector::
+    // 89 ff 1d c0 00 10 99 33
+    // \  \  \  \  \  \  \  \    high_bits = (uint16x4_t)(input >> 7)
+    //  |  |  |  |  |  |  |  |
+    // 01 01 00 01 00 00 01 00
+    //
+    // Bits of first important lane(s):
+    // 10001001 (89)
+    // \______
+    //        |
+    // 00000001 (01)
+    uint16x8_t high_bits = vreinterpretq_u16_u8(vshrq_n_u8(input, 7));
+
+    // Merge the even lanes together with a 16-bit unsigned shift right + add.
+    // 'xx' represents garbage data which will be ignored in the final result.
+    // In the important bytes, the add functions like a binary OR.
+    //
+    // 01 01 00 01 00 00 01 00
+    //  \_ |  \_ |  \_ |  \_ |   paired16 = (uint32x4_t)(input + (input >> 7))
+    //    \|    \|    \|    \|
+    // xx 03 xx 01 xx 00 xx 02
+    //
+    // 00000001 00000001 (01 01)
+    //        \_______ |
+    //                \|
+    // xxxxxxxx xxxxxx11 (xx 03)
+    uint32x4_t paired16 =
+        vreinterpretq_u32_u16(vsraq_n_u16(high_bits, high_bits, 7));
+
+    // Repeat with a wider 32-bit shift + add.
+    // xx 03 xx 01 xx 00 xx 02
+    //     \____ |     \____ |  paired32 = (uint64x1_t)(paired16 + (paired16 >>
+    //     14))
+    //          \|          \|
+    // xx xx xx 0d xx xx xx 02
+    //
+    // 00000011 00000001 (03 01)
+    //        \\_____ ||
+    //         '----.\||
+    // xxxxxxxx xxxx1101 (xx 0d)
+    uint64x2_t paired32 =
+        vreinterpretq_u64_u32(vsraq_n_u32(paired16, paired16, 14));
+
+    // Last, an even wider 64-bit shift + add to get our result in the low 8 bit
+    // lanes. xx xx xx 0d xx xx xx 02
+    //            \_________ |   paired64 = (uint8x8_t)(paired32 + (paired32 >>
+    //            28))
+    //                      \|
+    // xx xx xx xx xx xx xx d2
+    //
+    // 00001101 00000010 (0d 02)
+    //     \   \___ |  |
+    //      '---.  \|  |
+    // xxxxxxxx 11010010 (xx d2)
+    uint8x16_t paired64 =
+        vreinterpretq_u8_u64(vsraq_n_u64(paired32, paired32, 28));
+
+    // Extract the low 8 bits from each 64-bit lane with 2 8-bit extracts.
+    // xx xx xx xx xx xx xx d2
+    //                      ||  return paired64[0]
+    //                      d2
+    // Note: Little endian would return the correct value 4b (01001011) instead.
+    return vgetq_lane_u8(paired64, 0) | ((int) vgetq_lane_u8(paired64, 8) << 8);
+}
+
+// Set each bit of mask dst based on the most significant bit of the
+// corresponding packed double-precision (64-bit) floating-point element in a.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_movemask_pd
+FORCE_INLINE int _mm_movemask_pd(__m128d a)
+{
+    uint64x2_t input = vreinterpretq_u64_m128d(a);
+    uint64x2_t high_bits = vshrq_n_u64(input, 63);
+    return (int) (vgetq_lane_u64(high_bits, 0) |
+                  (vgetq_lane_u64(high_bits, 1) << 1));
+}
+
+// Copy the lower 64-bit integer in a to dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_movepi64_pi64
+FORCE_INLINE __m64 _mm_movepi64_pi64(__m128i a)
+{
+    return vreinterpret_m64_s64(vget_low_s64(vreinterpretq_s64_m128i(a)));
+}
+
+// Copy the 64-bit integer a to the lower element of dst, and zero the upper
+// element.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_movpi64_epi64
+FORCE_INLINE __m128i _mm_movpi64_epi64(__m64 a)
+{
+    return vreinterpretq_m128i_s64(
+        vcombine_s64(vreinterpret_s64_m64(a), vdup_n_s64(0)));
+}
+
+// Multiply the low unsigned 32-bit integers from each packed 64-bit element in
+// a and b, and store the unsigned 64-bit results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_mul_epu32
+FORCE_INLINE __m128i _mm_mul_epu32(__m128i a, __m128i b)
+{
+    // vmull_u32 upcasts instead of masking, so we downcast.
+    uint32x2_t a_lo = vmovn_u64(vreinterpretq_u64_m128i(a));
+    uint32x2_t b_lo = vmovn_u64(vreinterpretq_u64_m128i(b));
+    return vreinterpretq_m128i_u64(vmull_u32(a_lo, b_lo));
+}
+
+// Multiply packed double-precision (64-bit) floating-point elements in a and b,
+// and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_mul_pd
+FORCE_INLINE __m128d _mm_mul_pd(__m128d a, __m128d b)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    return vreinterpretq_m128d_f64(
+        vmulq_f64(vreinterpretq_f64_m128d(a), vreinterpretq_f64_m128d(b)));
+#else
+    double a0 =
+        sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(a), 0));
+    double a1 =
+        sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(a), 1));
+    double b0 =
+        sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(b), 0));
+    double b1 =
+        sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(b), 1));
+    double c[2];
+    c[0] = a0 * b0;
+    c[1] = a1 * b1;
+    return vld1q_f32((float32_t *) c);
+#endif
+}
+
+// Multiply the lower double-precision (64-bit) floating-point element in a and
+// b, store the result in the lower element of dst, and copy the upper element
+// from a to the upper element of dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=mm_mul_sd
+FORCE_INLINE __m128d _mm_mul_sd(__m128d a, __m128d b)
+{
+    return _mm_move_sd(a, _mm_mul_pd(a, b));
+}
+
+// Multiply the low unsigned 32-bit integers from a and b, and store the
+// unsigned 64-bit result in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_mul_su32
+FORCE_INLINE __m64 _mm_mul_su32(__m64 a, __m64 b)
+{
+    return vreinterpret_m64_u64(vget_low_u64(
+        vmull_u32(vreinterpret_u32_m64(a), vreinterpret_u32_m64(b))));
+}
+
+// Multiply the packed signed 16-bit integers in a and b, producing intermediate
+// 32-bit integers, and store the high 16 bits of the intermediate integers in
+// dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_mulhi_epi16
+FORCE_INLINE __m128i _mm_mulhi_epi16(__m128i a, __m128i b)
+{
+    /* FIXME: issue with large values because of result saturation */
+    // int16x8_t ret = vqdmulhq_s16(vreinterpretq_s16_m128i(a),
+    // vreinterpretq_s16_m128i(b)); /* =2*a*b */ return
+    // vreinterpretq_m128i_s16(vshrq_n_s16(ret, 1));
+    int16x4_t a3210 = vget_low_s16(vreinterpretq_s16_m128i(a));
+    int16x4_t b3210 = vget_low_s16(vreinterpretq_s16_m128i(b));
+    int32x4_t ab3210 = vmull_s16(a3210, b3210); /* 3333222211110000 */
+    int16x4_t a7654 = vget_high_s16(vreinterpretq_s16_m128i(a));
+    int16x4_t b7654 = vget_high_s16(vreinterpretq_s16_m128i(b));
+    int32x4_t ab7654 = vmull_s16(a7654, b7654); /* 7777666655554444 */
+    uint16x8x2_t r =
+        vuzpq_u16(vreinterpretq_u16_s32(ab3210), vreinterpretq_u16_s32(ab7654));
+    return vreinterpretq_m128i_u16(r.val[1]);
+}
+
+// Multiply the packed unsigned 16-bit integers in a and b, producing
+// intermediate 32-bit integers, and store the high 16 bits of the intermediate
+// integers in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_mulhi_epu16
+FORCE_INLINE __m128i _mm_mulhi_epu16(__m128i a, __m128i b)
+{
+    uint16x4_t a3210 = vget_low_u16(vreinterpretq_u16_m128i(a));
+    uint16x4_t b3210 = vget_low_u16(vreinterpretq_u16_m128i(b));
+    uint32x4_t ab3210 = vmull_u16(a3210, b3210);
+#if defined(__aarch64__) || defined(_M_ARM64)
+    uint32x4_t ab7654 =
+        vmull_high_u16(vreinterpretq_u16_m128i(a), vreinterpretq_u16_m128i(b));
+    uint16x8_t r = vuzp2q_u16(vreinterpretq_u16_u32(ab3210),
+                              vreinterpretq_u16_u32(ab7654));
+    return vreinterpretq_m128i_u16(r);
+#else
+    uint16x4_t a7654 = vget_high_u16(vreinterpretq_u16_m128i(a));
+    uint16x4_t b7654 = vget_high_u16(vreinterpretq_u16_m128i(b));
+    uint32x4_t ab7654 = vmull_u16(a7654, b7654);
+    uint16x8x2_t r =
+        vuzpq_u16(vreinterpretq_u16_u32(ab3210), vreinterpretq_u16_u32(ab7654));
+    return vreinterpretq_m128i_u16(r.val[1]);
+#endif
+}
+
+// Multiply the packed 16-bit integers in a and b, producing intermediate 32-bit
+// integers, and store the low 16 bits of the intermediate integers in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_mullo_epi16
+FORCE_INLINE __m128i _mm_mullo_epi16(__m128i a, __m128i b)
+{
+    return vreinterpretq_m128i_s16(
+        vmulq_s16(vreinterpretq_s16_m128i(a), vreinterpretq_s16_m128i(b)));
+}
+
+// Compute the bitwise OR of packed double-precision (64-bit) floating-point
+// elements in a and b, and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=mm_or_pd
+FORCE_INLINE __m128d _mm_or_pd(__m128d a, __m128d b)
+{
+    return vreinterpretq_m128d_s64(
+        vorrq_s64(vreinterpretq_s64_m128d(a), vreinterpretq_s64_m128d(b)));
+}
+
+// Compute the bitwise OR of 128 bits (representing integer data) in a and b,
+// and store the result in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_or_si128
+FORCE_INLINE __m128i _mm_or_si128(__m128i a, __m128i b)
+{
+    return vreinterpretq_m128i_s32(
+        vorrq_s32(vreinterpretq_s32_m128i(a), vreinterpretq_s32_m128i(b)));
+}
+
+// Convert packed signed 16-bit integers from a and b to packed 8-bit integers
+// using signed saturation, and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_packs_epi16
+FORCE_INLINE __m128i _mm_packs_epi16(__m128i a, __m128i b)
+{
+    return vreinterpretq_m128i_s8(
+        vcombine_s8(vqmovn_s16(vreinterpretq_s16_m128i(a)),
+                    vqmovn_s16(vreinterpretq_s16_m128i(b))));
+}
+
+// Convert packed signed 32-bit integers from a and b to packed 16-bit integers
+// using signed saturation, and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_packs_epi32
+FORCE_INLINE __m128i _mm_packs_epi32(__m128i a, __m128i b)
+{
+    return vreinterpretq_m128i_s16(
+        vcombine_s16(vqmovn_s32(vreinterpretq_s32_m128i(a)),
+                     vqmovn_s32(vreinterpretq_s32_m128i(b))));
+}
+
+// Convert packed signed 16-bit integers from a and b to packed 8-bit integers
+// using unsigned saturation, and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_packus_epi16
+FORCE_INLINE __m128i _mm_packus_epi16(const __m128i a, const __m128i b)
+{
+    return vreinterpretq_m128i_u8(
+        vcombine_u8(vqmovun_s16(vreinterpretq_s16_m128i(a)),
+                    vqmovun_s16(vreinterpretq_s16_m128i(b))));
+}
+
+// Pause the processor. This is typically used in spin-wait loops and depending
+// on the x86 processor typical values are in the 40-100 cycle range. The
+// 'yield' instruction isn't a good fit because it's effectively a nop on most
+// Arm cores. Experience with several databases has shown has shown an 'isb' is
+// a reasonable approximation.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_pause
+FORCE_INLINE void _mm_pause(void)
+{
+#if defined(_MSC_VER) && !defined(__clang__)
+    __isb(_ARM64_BARRIER_SY);
+#else
+    __asm__ __volatile__("isb\n");
+#endif
+}
+
+// Compute the absolute differences of packed unsigned 8-bit integers in a and
+// b, then horizontally sum each consecutive 8 differences to produce two
+// unsigned 16-bit integers, and pack these unsigned 16-bit integers in the low
+// 16 bits of 64-bit elements in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_sad_epu8
+FORCE_INLINE __m128i _mm_sad_epu8(__m128i a, __m128i b)
+{
+    uint16x8_t t = vpaddlq_u8(vabdq_u8((uint8x16_t) a, (uint8x16_t) b));
+    return vreinterpretq_m128i_u64(vpaddlq_u32(vpaddlq_u16(t)));
+}
+
+// Set packed 16-bit integers in dst with the supplied values.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_set_epi16
+FORCE_INLINE __m128i _mm_set_epi16(short i7,
+                                   short i6,
+                                   short i5,
+                                   short i4,
+                                   short i3,
+                                   short i2,
+                                   short i1,
+                                   short i0)
+{
+    int16_t ALIGN_STRUCT(16) data[8] = {i0, i1, i2, i3, i4, i5, i6, i7};
+    return vreinterpretq_m128i_s16(vld1q_s16(data));
+}
+
+// Set packed 32-bit integers in dst with the supplied values.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_set_epi32
+FORCE_INLINE __m128i _mm_set_epi32(int i3, int i2, int i1, int i0)
+{
+    int32_t ALIGN_STRUCT(16) data[4] = {i0, i1, i2, i3};
+    return vreinterpretq_m128i_s32(vld1q_s32(data));
+}
+
+// Set packed 64-bit integers in dst with the supplied values.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_set_epi64
+FORCE_INLINE __m128i _mm_set_epi64(__m64 i1, __m64 i2)
+{
+    return _mm_set_epi64x(vget_lane_s64(i1, 0), vget_lane_s64(i2, 0));
+}
+
+// Set packed 64-bit integers in dst with the supplied values.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_set_epi64x
+FORCE_INLINE __m128i _mm_set_epi64x(int64_t i1, int64_t i2)
+{
+    return vreinterpretq_m128i_s64(
+        vcombine_s64(vcreate_s64(i2), vcreate_s64(i1)));
+}
+
+// Set packed 8-bit integers in dst with the supplied values.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_set_epi8
+FORCE_INLINE __m128i _mm_set_epi8(signed char b15,
+                                  signed char b14,
+                                  signed char b13,
+                                  signed char b12,
+                                  signed char b11,
+                                  signed char b10,
+                                  signed char b9,
+                                  signed char b8,
+                                  signed char b7,
+                                  signed char b6,
+                                  signed char b5,
+                                  signed char b4,
+                                  signed char b3,
+                                  signed char b2,
+                                  signed char b1,
+                                  signed char b0)
+{
+    int8_t ALIGN_STRUCT(16)
+        data[16] = {(int8_t) b0,  (int8_t) b1,  (int8_t) b2,  (int8_t) b3,
+                    (int8_t) b4,  (int8_t) b5,  (int8_t) b6,  (int8_t) b7,
+                    (int8_t) b8,  (int8_t) b9,  (int8_t) b10, (int8_t) b11,
+                    (int8_t) b12, (int8_t) b13, (int8_t) b14, (int8_t) b15};
+    return (__m128i) vld1q_s8(data);
+}
+
+// Set packed double-precision (64-bit) floating-point elements in dst with the
+// supplied values.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_set_pd
+FORCE_INLINE __m128d _mm_set_pd(double e1, double e0)
+{
+    double ALIGN_STRUCT(16) data[2] = {e0, e1};
+#if defined(__aarch64__) || defined(_M_ARM64)
+    return vreinterpretq_m128d_f64(vld1q_f64((float64_t *) data));
+#else
+    return vreinterpretq_m128d_f32(vld1q_f32((float32_t *) data));
+#endif
+}
+
+// Broadcast double-precision (64-bit) floating-point value a to all elements of
+// dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_set_pd1
+#define _mm_set_pd1 _mm_set1_pd
+
+// Copy double-precision (64-bit) floating-point element a to the lower element
+// of dst, and zero the upper element.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_set_sd
+FORCE_INLINE __m128d _mm_set_sd(double a)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    return vreinterpretq_m128d_f64(vsetq_lane_f64(a, vdupq_n_f64(0), 0));
+#else
+    return _mm_set_pd(0, a);
+#endif
+}
+
+// Broadcast 16-bit integer a to all elements of dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_set1_epi16
+FORCE_INLINE __m128i _mm_set1_epi16(short w)
+{
+    return vreinterpretq_m128i_s16(vdupq_n_s16(w));
+}
+
+// Broadcast 32-bit integer a to all elements of dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_set1_epi32
+FORCE_INLINE __m128i _mm_set1_epi32(int _i)
+{
+    return vreinterpretq_m128i_s32(vdupq_n_s32(_i));
+}
+
+// Broadcast 64-bit integer a to all elements of dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_set1_epi64
+FORCE_INLINE __m128i _mm_set1_epi64(__m64 _i)
+{
+    return vreinterpretq_m128i_s64(vdupq_lane_s64(_i, 0));
+}
+
+// Broadcast 64-bit integer a to all elements of dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_set1_epi64x
+FORCE_INLINE __m128i _mm_set1_epi64x(int64_t _i)
+{
+    return vreinterpretq_m128i_s64(vdupq_n_s64(_i));
+}
+
+// Broadcast 8-bit integer a to all elements of dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_set1_epi8
+FORCE_INLINE __m128i _mm_set1_epi8(signed char w)
+{
+    return vreinterpretq_m128i_s8(vdupq_n_s8(w));
+}
+
+// Broadcast double-precision (64-bit) floating-point value a to all elements of
+// dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_set1_pd
+FORCE_INLINE __m128d _mm_set1_pd(double d)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    return vreinterpretq_m128d_f64(vdupq_n_f64(d));
+#else
+    int64_t _d = sse2neon_recast_f64_s64(d);
+    return vreinterpretq_m128d_s64(vdupq_n_s64(_d));
+#endif
+}
+
+// Set packed 16-bit integers in dst with the supplied values in reverse order.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_setr_epi16
+FORCE_INLINE __m128i _mm_setr_epi16(short w0,
+                                    short w1,
+                                    short w2,
+                                    short w3,
+                                    short w4,
+                                    short w5,
+                                    short w6,
+                                    short w7)
+{
+    int16_t ALIGN_STRUCT(16) data[8] = {w0, w1, w2, w3, w4, w5, w6, w7};
+    return vreinterpretq_m128i_s16(vld1q_s16((int16_t *) data));
+}
+
+// Set packed 32-bit integers in dst with the supplied values in reverse order.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_setr_epi32
+FORCE_INLINE __m128i _mm_setr_epi32(int i3, int i2, int i1, int i0)
+{
+    int32_t ALIGN_STRUCT(16) data[4] = {i3, i2, i1, i0};
+    return vreinterpretq_m128i_s32(vld1q_s32(data));
+}
+
+// Set packed 64-bit integers in dst with the supplied values in reverse order.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_setr_epi64
+FORCE_INLINE __m128i _mm_setr_epi64(__m64 e1, __m64 e0)
+{
+    return vreinterpretq_m128i_s64(vcombine_s64(e1, e0));
+}
+
+// Set packed 8-bit integers in dst with the supplied values in reverse order.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_setr_epi8
+FORCE_INLINE __m128i _mm_setr_epi8(signed char b0,
+                                   signed char b1,
+                                   signed char b2,
+                                   signed char b3,
+                                   signed char b4,
+                                   signed char b5,
+                                   signed char b6,
+                                   signed char b7,
+                                   signed char b8,
+                                   signed char b9,
+                                   signed char b10,
+                                   signed char b11,
+                                   signed char b12,
+                                   signed char b13,
+                                   signed char b14,
+                                   signed char b15)
+{
+    int8_t ALIGN_STRUCT(16)
+        data[16] = {(int8_t) b0,  (int8_t) b1,  (int8_t) b2,  (int8_t) b3,
+                    (int8_t) b4,  (int8_t) b5,  (int8_t) b6,  (int8_t) b7,
+                    (int8_t) b8,  (int8_t) b9,  (int8_t) b10, (int8_t) b11,
+                    (int8_t) b12, (int8_t) b13, (int8_t) b14, (int8_t) b15};
+    return (__m128i) vld1q_s8(data);
+}
+
+// Set packed double-precision (64-bit) floating-point elements in dst with the
+// supplied values in reverse order.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_setr_pd
+FORCE_INLINE __m128d _mm_setr_pd(double e1, double e0)
+{
+    return _mm_set_pd(e0, e1);
+}
+
+// Return vector of type __m128d with all elements set to zero.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_setzero_pd
+FORCE_INLINE __m128d _mm_setzero_pd(void)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    return vreinterpretq_m128d_f64(vdupq_n_f64(0));
+#else
+    return vreinterpretq_m128d_f32(vdupq_n_f32(0));
+#endif
+}
+
+// Return vector of type __m128i with all elements set to zero.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_setzero_si128
+FORCE_INLINE __m128i _mm_setzero_si128(void)
+{
+    return vreinterpretq_m128i_s32(vdupq_n_s32(0));
+}
+
+// Shuffle 32-bit integers in a using the control in imm8, and store the results
+// in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_shuffle_epi32
+// FORCE_INLINE __m128i _mm_shuffle_epi32(__m128i a,
+//                                        __constrange(0,255) int imm)
+#if defined(_sse2neon_shuffle)
+#define _mm_shuffle_epi32(a, imm)                                            \
+    __extension__({                                                          \
+        int32x4_t _input = vreinterpretq_s32_m128i(a);                       \
+        int32x4_t _shuf =                                                    \
+            vshuffleq_s32(_input, _input, (imm) & (0x3), ((imm) >> 2) & 0x3, \
+                          ((imm) >> 4) & 0x3, ((imm) >> 6) & 0x3);           \
+        vreinterpretq_m128i_s32(_shuf);                                      \
+    })
+#else  // generic
+#define _mm_shuffle_epi32(a, imm)                           \
+    _sse2neon_define1(                                      \
+        __m128i, a, __m128i ret; switch (imm) {             \
+            case _MM_SHUFFLE(1, 0, 3, 2):                   \
+                ret = _mm_shuffle_epi_1032(_a);             \
+                break;                                      \
+            case _MM_SHUFFLE(2, 3, 0, 1):                   \
+                ret = _mm_shuffle_epi_2301(_a);             \
+                break;                                      \
+            case _MM_SHUFFLE(0, 3, 2, 1):                   \
+                ret = _mm_shuffle_epi_0321(_a);             \
+                break;                                      \
+            case _MM_SHUFFLE(2, 1, 0, 3):                   \
+                ret = _mm_shuffle_epi_2103(_a);             \
+                break;                                      \
+            case _MM_SHUFFLE(1, 0, 1, 0):                   \
+                ret = _mm_shuffle_epi_1010(_a);             \
+                break;                                      \
+            case _MM_SHUFFLE(1, 0, 0, 1):                   \
+                ret = _mm_shuffle_epi_1001(_a);             \
+                break;                                      \
+            case _MM_SHUFFLE(0, 1, 0, 1):                   \
+                ret = _mm_shuffle_epi_0101(_a);             \
+                break;                                      \
+            case _MM_SHUFFLE(2, 2, 1, 1):                   \
+                ret = _mm_shuffle_epi_2211(_a);             \
+                break;                                      \
+            case _MM_SHUFFLE(0, 1, 2, 2):                   \
+                ret = _mm_shuffle_epi_0122(_a);             \
+                break;                                      \
+            case _MM_SHUFFLE(3, 3, 3, 2):                   \
+                ret = _mm_shuffle_epi_3332(_a);             \
+                break;                                      \
+            case _MM_SHUFFLE(0, 0, 0, 0):                   \
+                ret = _mm_shuffle_epi32_splat(_a, 0);       \
+                break;                                      \
+            case _MM_SHUFFLE(1, 1, 1, 1):                   \
+                ret = _mm_shuffle_epi32_splat(_a, 1);       \
+                break;                                      \
+            case _MM_SHUFFLE(2, 2, 2, 2):                   \
+                ret = _mm_shuffle_epi32_splat(_a, 2);       \
+                break;                                      \
+            case _MM_SHUFFLE(3, 3, 3, 3):                   \
+                ret = _mm_shuffle_epi32_splat(_a, 3);       \
+                break;                                      \
+            default:                                        \
+                ret = _mm_shuffle_epi32_default(_a, (imm)); \
+                break;                                      \
+        } _sse2neon_return(ret);)
+#endif
+
+// Shuffle double-precision (64-bit) floating-point elements using the control
+// in imm8, and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_shuffle_pd
+#ifdef _sse2neon_shuffle
+#define _mm_shuffle_pd(a, b, imm8)                                            \
+    vreinterpretq_m128d_s64(                                                  \
+        vshuffleq_s64(vreinterpretq_s64_m128d(a), vreinterpretq_s64_m128d(b), \
+                      imm8 & 0x1, ((imm8 & 0x2) >> 1) + 2))
+#else
+#define _mm_shuffle_pd(a, b, imm8)                                     \
+    _mm_castsi128_pd(_mm_set_epi64x(                                   \
+        vgetq_lane_s64(vreinterpretq_s64_m128d(b), (imm8 & 0x2) >> 1), \
+        vgetq_lane_s64(vreinterpretq_s64_m128d(a), imm8 & 0x1)))
+#endif
+
+// FORCE_INLINE __m128i _mm_shufflehi_epi16(__m128i a,
+//                                          __constrange(0,255) int imm)
+#if defined(_sse2neon_shuffle)
+#define _mm_shufflehi_epi16(a, imm)                                           \
+    __extension__({                                                           \
+        int16x8_t _input = vreinterpretq_s16_m128i(a);                        \
+        int16x8_t _shuf =                                                     \
+            vshuffleq_s16(_input, _input, 0, 1, 2, 3, ((imm) & (0x3)) + 4,    \
+                          (((imm) >> 2) & 0x3) + 4, (((imm) >> 4) & 0x3) + 4, \
+                          (((imm) >> 6) & 0x3) + 4);                          \
+        vreinterpretq_m128i_s16(_shuf);                                       \
+    })
+#else  // generic
+#define _mm_shufflehi_epi16(a, imm) _mm_shufflehi_epi16_function((a), (imm))
+#endif
+
+// FORCE_INLINE __m128i _mm_shufflelo_epi16(__m128i a,
+//                                          __constrange(0,255) int imm)
+#if defined(_sse2neon_shuffle)
+#define _mm_shufflelo_epi16(a, imm)                                  \
+    __extension__({                                                  \
+        int16x8_t _input = vreinterpretq_s16_m128i(a);               \
+        int16x8_t _shuf = vshuffleq_s16(                             \
+            _input, _input, ((imm) & (0x3)), (((imm) >> 2) & 0x3),   \
+            (((imm) >> 4) & 0x3), (((imm) >> 6) & 0x3), 4, 5, 6, 7); \
+        vreinterpretq_m128i_s16(_shuf);                              \
+    })
+#else  // generic
+#define _mm_shufflelo_epi16(a, imm) _mm_shufflelo_epi16_function((a), (imm))
+#endif
+
+// Shift packed 16-bit integers in a left by count while shifting in zeros, and
+// store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_sll_epi16
+FORCE_INLINE __m128i _mm_sll_epi16(__m128i a, __m128i count)
+{
+    uint64_t c = vreinterpretq_nth_u64_m128i(count, 0);
+    if (_sse2neon_unlikely(c & ~15))
+        return _mm_setzero_si128();
+
+    int16x8_t vc = vdupq_n_s16((int16_t) c);
+    return vreinterpretq_m128i_s16(vshlq_s16(vreinterpretq_s16_m128i(a), vc));
+}
+
+// Shift packed 32-bit integers in a left by count while shifting in zeros, and
+// store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_sll_epi32
+FORCE_INLINE __m128i _mm_sll_epi32(__m128i a, __m128i count)
+{
+    uint64_t c = vreinterpretq_nth_u64_m128i(count, 0);
+    if (_sse2neon_unlikely(c & ~31))
+        return _mm_setzero_si128();
+
+    int32x4_t vc = vdupq_n_s32((int32_t) c);
+    return vreinterpretq_m128i_s32(vshlq_s32(vreinterpretq_s32_m128i(a), vc));
+}
+
+// Shift packed 64-bit integers in a left by count while shifting in zeros, and
+// store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_sll_epi64
+FORCE_INLINE __m128i _mm_sll_epi64(__m128i a, __m128i count)
+{
+    uint64_t c = vreinterpretq_nth_u64_m128i(count, 0);
+    if (_sse2neon_unlikely(c & ~63))
+        return _mm_setzero_si128();
+
+    int64x2_t vc = vdupq_n_s64((int64_t) c);
+    return vreinterpretq_m128i_s64(vshlq_s64(vreinterpretq_s64_m128i(a), vc));
+}
+
+// Shift packed 16-bit integers in a left by imm8 while shifting in zeros, and
+// store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_slli_epi16
+FORCE_INLINE __m128i _mm_slli_epi16(__m128i a, int imm)
+{
+    if (_sse2neon_unlikely(imm & ~15))
+        return _mm_setzero_si128();
+    return vreinterpretq_m128i_s16(
+        vshlq_s16(vreinterpretq_s16_m128i(a), vdupq_n_s16(imm)));
+}
+
+// Shift packed 32-bit integers in a left by imm8 while shifting in zeros, and
+// store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_slli_epi32
+FORCE_INLINE __m128i _mm_slli_epi32(__m128i a, int imm)
+{
+    if (_sse2neon_unlikely(imm & ~31))
+        return _mm_setzero_si128();
+    return vreinterpretq_m128i_s32(
+        vshlq_s32(vreinterpretq_s32_m128i(a), vdupq_n_s32(imm)));
+}
+
+// Shift packed 64-bit integers in a left by imm8 while shifting in zeros, and
+// store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_slli_epi64
+FORCE_INLINE __m128i _mm_slli_epi64(__m128i a, int imm)
+{
+    if (_sse2neon_unlikely(imm & ~63))
+        return _mm_setzero_si128();
+    return vreinterpretq_m128i_s64(
+        vshlq_s64(vreinterpretq_s64_m128i(a), vdupq_n_s64(imm)));
+}
+
+// Shift a left by imm8 bytes while shifting in zeros, and store the results in
+// dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_slli_si128
+#define _mm_slli_si128(a, imm)                                              \
+    _sse2neon_define1(                                                      \
+        __m128i, a, int8x16_t ret;                                          \
+        if (_sse2neon_unlikely(imm == 0)) ret = vreinterpretq_s8_m128i(_a); \
+        else if (_sse2neon_unlikely((imm) & ~15)) ret = vdupq_n_s8(0);      \
+        else ret = vextq_s8(vdupq_n_s8(0), vreinterpretq_s8_m128i(_a),      \
+                            ((imm <= 0 || imm > 15) ? 0 : (16 - imm)));     \
+        _sse2neon_return(vreinterpretq_m128i_s8(ret));)
+
+// Compute the square root of packed double-precision (64-bit) floating-point
+// elements in a, and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_sqrt_pd
+FORCE_INLINE __m128d _mm_sqrt_pd(__m128d a)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    return vreinterpretq_m128d_f64(vsqrtq_f64(vreinterpretq_f64_m128d(a)));
+#else
+    double a0, a1;
+    a0 = sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(a), 0));
+    a1 = sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(a), 1));
+    double _a0 = sqrt(a0);
+    double _a1 = sqrt(a1);
+    return _mm_set_pd(_a1, _a0);
+#endif
+}
+
+// Compute the square root of the lower double-precision (64-bit) floating-point
+// element in b, store the result in the lower element of dst, and copy the
+// upper element from a to the upper element of dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_sqrt_sd
+FORCE_INLINE __m128d _mm_sqrt_sd(__m128d a, __m128d b)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    return _mm_move_sd(a, _mm_sqrt_pd(b));
+#else
+    double _a, _b;
+    _a = sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(a), 1));
+    _b = sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(b), 0));
+    return _mm_set_pd(_a, sqrt(_b));
+#endif
+}
+
+// Shift packed 16-bit integers in a right by count while shifting in sign bits,
+// and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_sra_epi16
+FORCE_INLINE __m128i _mm_sra_epi16(__m128i a, __m128i count)
+{
+    int64_t c = vgetq_lane_s64(count, 0);
+    if (_sse2neon_unlikely(c & ~15))
+        return _mm_cmplt_epi16(a, _mm_setzero_si128());
+    return vreinterpretq_m128i_s16(
+        vshlq_s16((int16x8_t) a, vdupq_n_s16((int) -c)));
+}
+
+// Shift packed 32-bit integers in a right by count while shifting in sign bits,
+// and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_sra_epi32
+FORCE_INLINE __m128i _mm_sra_epi32(__m128i a, __m128i count)
+{
+    int64_t c = vgetq_lane_s64(count, 0);
+    if (_sse2neon_unlikely(c & ~31))
+        return _mm_cmplt_epi32(a, _mm_setzero_si128());
+    return vreinterpretq_m128i_s32(
+        vshlq_s32((int32x4_t) a, vdupq_n_s32((int) -c)));
+}
+
+// Shift packed 16-bit integers in a right by imm8 while shifting in sign
+// bits, and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_srai_epi16
+FORCE_INLINE __m128i _mm_srai_epi16(__m128i a, int imm)
+{
+    const int count = (imm & ~15) ? 15 : imm;
+    return (__m128i) vshlq_s16((int16x8_t) a, vdupq_n_s16(-count));
+}
+
+// Shift packed 32-bit integers in a right by imm8 while shifting in sign bits,
+// and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_srai_epi32
+// FORCE_INLINE __m128i _mm_srai_epi32(__m128i a, __constrange(0,255) int imm)
+#define _mm_srai_epi32(a, imm)                                                \
+    _sse2neon_define0(                                                        \
+        __m128i, a, __m128i ret; if (_sse2neon_unlikely((imm) == 0)) {        \
+            ret = _a;                                                         \
+        } else if (_sse2neon_likely(0 < (imm) && (imm) < 32)) {               \
+            ret = vreinterpretq_m128i_s32(                                    \
+                vshlq_s32(vreinterpretq_s32_m128i(_a), vdupq_n_s32(-(imm)))); \
+        } else {                                                              \
+            ret = vreinterpretq_m128i_s32(                                    \
+                vshrq_n_s32(vreinterpretq_s32_m128i(_a), 31));                \
+        } _sse2neon_return(ret);)
+
+// Shift packed 16-bit integers in a right by count while shifting in zeros, and
+// store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_srl_epi16
+FORCE_INLINE __m128i _mm_srl_epi16(__m128i a, __m128i count)
+{
+    uint64_t c = vreinterpretq_nth_u64_m128i(count, 0);
+    if (_sse2neon_unlikely(c & ~15))
+        return _mm_setzero_si128();
+
+    int16x8_t vc = vdupq_n_s16(-(int16_t) c);
+    return vreinterpretq_m128i_u16(vshlq_u16(vreinterpretq_u16_m128i(a), vc));
+}
+
+// Shift packed 32-bit integers in a right by count while shifting in zeros, and
+// store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_srl_epi32
+FORCE_INLINE __m128i _mm_srl_epi32(__m128i a, __m128i count)
+{
+    uint64_t c = vreinterpretq_nth_u64_m128i(count, 0);
+    if (_sse2neon_unlikely(c & ~31))
+        return _mm_setzero_si128();
+
+    int32x4_t vc = vdupq_n_s32(-(int32_t) c);
+    return vreinterpretq_m128i_u32(vshlq_u32(vreinterpretq_u32_m128i(a), vc));
+}
+
+// Shift packed 64-bit integers in a right by count while shifting in zeros, and
+// store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_srl_epi64
+FORCE_INLINE __m128i _mm_srl_epi64(__m128i a, __m128i count)
+{
+    uint64_t c = vreinterpretq_nth_u64_m128i(count, 0);
+    if (_sse2neon_unlikely(c & ~63))
+        return _mm_setzero_si128();
+
+    int64x2_t vc = vdupq_n_s64(-(int64_t) c);
+    return vreinterpretq_m128i_u64(vshlq_u64(vreinterpretq_u64_m128i(a), vc));
+}
+
+// Shift packed 16-bit integers in a right by imm8 while shifting in zeros, and
+// store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_srli_epi16
+#define _mm_srli_epi16(a, imm)                                                \
+    _sse2neon_define0(                                                        \
+        __m128i, a, __m128i ret; if (_sse2neon_unlikely((imm) & ~15)) {       \
+            ret = _mm_setzero_si128();                                        \
+        } else {                                                              \
+            ret = vreinterpretq_m128i_u16(                                    \
+                vshlq_u16(vreinterpretq_u16_m128i(_a), vdupq_n_s16(-(imm)))); \
+        } _sse2neon_return(ret);)
+
+// Shift packed 32-bit integers in a right by imm8 while shifting in zeros, and
+// store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_srli_epi32
+// FORCE_INLINE __m128i _mm_srli_epi32(__m128i a, __constrange(0,255) int imm)
+#define _mm_srli_epi32(a, imm)                                                \
+    _sse2neon_define0(                                                        \
+        __m128i, a, __m128i ret; if (_sse2neon_unlikely((imm) & ~31)) {       \
+            ret = _mm_setzero_si128();                                        \
+        } else {                                                              \
+            ret = vreinterpretq_m128i_u32(                                    \
+                vshlq_u32(vreinterpretq_u32_m128i(_a), vdupq_n_s32(-(imm)))); \
+        } _sse2neon_return(ret);)
+
+// Shift packed 64-bit integers in a right by imm8 while shifting in zeros, and
+// store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_srli_epi64
+#define _mm_srli_epi64(a, imm)                                                \
+    _sse2neon_define0(                                                        \
+        __m128i, a, __m128i ret; if (_sse2neon_unlikely((imm) & ~63)) {       \
+            ret = _mm_setzero_si128();                                        \
+        } else {                                                              \
+            ret = vreinterpretq_m128i_u64(                                    \
+                vshlq_u64(vreinterpretq_u64_m128i(_a), vdupq_n_s64(-(imm)))); \
+        } _sse2neon_return(ret);)
+
+// Shift a right by imm8 bytes while shifting in zeros, and store the results in
+// dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_srli_si128
+#define _mm_srli_si128(a, imm)                                         \
+    _sse2neon_define1(                                                 \
+        __m128i, a, int8x16_t ret;                                     \
+        if (_sse2neon_unlikely((imm) & ~15)) ret = vdupq_n_s8(0);      \
+        else ret = vextq_s8(vreinterpretq_s8_m128i(_a), vdupq_n_s8(0), \
+                            (imm > 15 ? 0 : imm));                     \
+        _sse2neon_return(vreinterpretq_m128i_s8(ret));)
+
+// Store 128-bits (composed of 2 packed double-precision (64-bit) floating-point
+// elements) from a into memory. mem_addr must be aligned on a 16-byte boundary
+// or a general-protection exception may be generated.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_store_pd
+FORCE_INLINE void _mm_store_pd(double *mem_addr, __m128d a)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    vst1q_f64((float64_t *) mem_addr, vreinterpretq_f64_m128d(a));
+#else
+    vst1q_f32((float32_t *) mem_addr, vreinterpretq_f32_m128d(a));
+#endif
+}
+
+// Store the lower double-precision (64-bit) floating-point element from a into
+// 2 contiguous elements in memory. mem_addr must be aligned on a 16-byte
+// boundary or a general-protection exception may be generated.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_store_pd1
+FORCE_INLINE void _mm_store_pd1(double *mem_addr, __m128d a)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    float64x1_t a_low = vget_low_f64(vreinterpretq_f64_m128d(a));
+    vst1q_f64((float64_t *) mem_addr,
+              vreinterpretq_f64_m128d(vcombine_f64(a_low, a_low)));
+#else
+    float32x2_t a_low = vget_low_f32(vreinterpretq_f32_m128d(a));
+    vst1q_f32((float32_t *) mem_addr,
+              vreinterpretq_f32_m128d(vcombine_f32(a_low, a_low)));
+#endif
+}
+
+// Store the lower double-precision (64-bit) floating-point element from a into
+// memory. mem_addr does not need to be aligned on any particular boundary.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=mm_store_sd
+FORCE_INLINE void _mm_store_sd(double *mem_addr, __m128d a)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    vst1_f64((float64_t *) mem_addr, vget_low_f64(vreinterpretq_f64_m128d(a)));
+#else
+    vst1_u64((uint64_t *) mem_addr, vget_low_u64(vreinterpretq_u64_m128d(a)));
+#endif
+}
+
+// Store 128-bits of integer data from a into memory. mem_addr must be aligned
+// on a 16-byte boundary or a general-protection exception may be generated.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_store_si128
+FORCE_INLINE void _mm_store_si128(__m128i *p, __m128i a)
+{
+    vst1q_s32((int32_t *) p, vreinterpretq_s32_m128i(a));
+}
+
+// Store the lower double-precision (64-bit) floating-point element from a into
+// 2 contiguous elements in memory. mem_addr must be aligned on a 16-byte
+// boundary or a general-protection exception may be generated.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#expand=9,526,5601&text=_mm_store1_pd
+#define _mm_store1_pd _mm_store_pd1
+
+// Store the upper double-precision (64-bit) floating-point element from a into
+// memory.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_storeh_pd
+FORCE_INLINE void _mm_storeh_pd(double *mem_addr, __m128d a)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    vst1_f64((float64_t *) mem_addr, vget_high_f64(vreinterpretq_f64_m128d(a)));
+#else
+    vst1_f32((float32_t *) mem_addr, vget_high_f32(vreinterpretq_f32_m128d(a)));
+#endif
+}
+
+// Store 64-bit integer from the first element of a into memory.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_storel_epi64
+FORCE_INLINE void _mm_storel_epi64(__m128i *a, __m128i b)
+{
+    vst1_u64((uint64_t *) a, vget_low_u64(vreinterpretq_u64_m128i(b)));
+}
+
+// Store the lower double-precision (64-bit) floating-point element from a into
+// memory.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_storel_pd
+FORCE_INLINE void _mm_storel_pd(double *mem_addr, __m128d a)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    vst1_f64((float64_t *) mem_addr, vget_low_f64(vreinterpretq_f64_m128d(a)));
+#else
+    vst1_f32((float32_t *) mem_addr, vget_low_f32(vreinterpretq_f32_m128d(a)));
+#endif
+}
+
+// Store 2 double-precision (64-bit) floating-point elements from a into memory
+// in reverse order. mem_addr must be aligned on a 16-byte boundary or a
+// general-protection exception may be generated.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_storer_pd
+FORCE_INLINE void _mm_storer_pd(double *mem_addr, __m128d a)
+{
+    float32x4_t f = vreinterpretq_f32_m128d(a);
+    _mm_store_pd(mem_addr, vreinterpretq_m128d_f32(vextq_f32(f, f, 2)));
+}
+
+// Store 128-bits (composed of 2 packed double-precision (64-bit) floating-point
+// elements) from a into memory. mem_addr does not need to be aligned on any
+// particular boundary.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_storeu_pd
+FORCE_INLINE void _mm_storeu_pd(double *mem_addr, __m128d a)
+{
+    _mm_store_pd(mem_addr, a);
+}
+
+// Store 128-bits of integer data from a into memory. mem_addr does not need to
+// be aligned on any particular boundary.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_storeu_si128
+FORCE_INLINE void _mm_storeu_si128(__m128i *p, __m128i a)
+{
+    vst1q_s32((int32_t *) p, vreinterpretq_s32_m128i(a));
+}
+
+// Store 32-bit integer from the first element of a into memory. mem_addr does
+// not need to be aligned on any particular boundary.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_storeu_si32
+FORCE_INLINE void _mm_storeu_si32(void *p, __m128i a)
+{
+    vst1q_lane_s32((int32_t *) p, vreinterpretq_s32_m128i(a), 0);
+}
+
+// Store 128-bits (composed of 2 packed double-precision (64-bit) floating-point
+// elements) from a into memory using a non-temporal memory hint. mem_addr must
+// be aligned on a 16-byte boundary or a general-protection exception may be
+// generated.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_stream_pd
+FORCE_INLINE void _mm_stream_pd(double *p, __m128d a)
+{
+#if __has_builtin(__builtin_nontemporal_store)
+    __builtin_nontemporal_store(a, (__m128d *) p);
+#elif defined(__aarch64__) || defined(_M_ARM64)
+    vst1q_f64(p, vreinterpretq_f64_m128d(a));
+#else
+    vst1q_s64((int64_t *) p, vreinterpretq_s64_m128d(a));
+#endif
+}
+
+// Store 128-bits of integer data from a into memory using a non-temporal memory
+// hint. mem_addr must be aligned on a 16-byte boundary or a general-protection
+// exception may be generated.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_stream_si128
+FORCE_INLINE void _mm_stream_si128(__m128i *p, __m128i a)
+{
+#if __has_builtin(__builtin_nontemporal_store)
+    __builtin_nontemporal_store(a, p);
+#else
+    vst1q_s64((int64_t *) p, vreinterpretq_s64_m128i(a));
+#endif
+}
+
+// Store 32-bit integer a into memory using a non-temporal hint to minimize
+// cache pollution. If the cache line containing address mem_addr is already in
+// the cache, the cache will be updated.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_stream_si32
+FORCE_INLINE void _mm_stream_si32(int *p, int a)
+{
+    vst1q_lane_s32((int32_t *) p, vdupq_n_s32(a), 0);
+}
+
+// Store 64-bit integer a into memory using a non-temporal hint to minimize
+// cache pollution. If the cache line containing address mem_addr is already in
+// the cache, the cache will be updated.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_stream_si64
+FORCE_INLINE void _mm_stream_si64(__int64 *p, __int64 a)
+{
+    vst1_s64((int64_t *) p, vdup_n_s64((int64_t) a));
+}
+
+// Subtract packed 16-bit integers in b from packed 16-bit integers in a, and
+// store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_sub_epi16
+FORCE_INLINE __m128i _mm_sub_epi16(__m128i a, __m128i b)
+{
+    return vreinterpretq_m128i_s16(
+        vsubq_s16(vreinterpretq_s16_m128i(a), vreinterpretq_s16_m128i(b)));
+}
+
+// Subtract packed 32-bit integers in b from packed 32-bit integers in a, and
+// store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_sub_epi32
+FORCE_INLINE __m128i _mm_sub_epi32(__m128i a, __m128i b)
+{
+    return vreinterpretq_m128i_s32(
+        vsubq_s32(vreinterpretq_s32_m128i(a), vreinterpretq_s32_m128i(b)));
+}
+
+// Subtract packed 64-bit integers in b from packed 64-bit integers in a, and
+// store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_sub_epi64
+FORCE_INLINE __m128i _mm_sub_epi64(__m128i a, __m128i b)
+{
+    return vreinterpretq_m128i_s64(
+        vsubq_s64(vreinterpretq_s64_m128i(a), vreinterpretq_s64_m128i(b)));
+}
+
+// Subtract packed 8-bit integers in b from packed 8-bit integers in a, and
+// store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_sub_epi8
+FORCE_INLINE __m128i _mm_sub_epi8(__m128i a, __m128i b)
+{
+    return vreinterpretq_m128i_s8(
+        vsubq_s8(vreinterpretq_s8_m128i(a), vreinterpretq_s8_m128i(b)));
+}
+
+// Subtract packed double-precision (64-bit) floating-point elements in b from
+// packed double-precision (64-bit) floating-point elements in a, and store the
+// results in dst.
+//  https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=mm_sub_pd
+FORCE_INLINE __m128d _mm_sub_pd(__m128d a, __m128d b)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    return vreinterpretq_m128d_f64(
+        vsubq_f64(vreinterpretq_f64_m128d(a), vreinterpretq_f64_m128d(b)));
+#else
+    double a0 =
+        sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(a), 0));
+    double a1 =
+        sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(a), 1));
+    double b0 =
+        sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(b), 0));
+    double b1 =
+        sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(b), 1));
+    double c[2];
+    c[0] = a0 - b0;
+    c[1] = a1 - b1;
+    return vld1q_f32((float32_t *) c);
+#endif
+}
+
+// Subtract the lower double-precision (64-bit) floating-point element in b from
+// the lower double-precision (64-bit) floating-point element in a, store the
+// result in the lower element of dst, and copy the upper element from a to the
+// upper element of dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_sub_sd
+FORCE_INLINE __m128d _mm_sub_sd(__m128d a, __m128d b)
+{
+    return _mm_move_sd(a, _mm_sub_pd(a, b));
+}
+
+// Subtract 64-bit integer b from 64-bit integer a, and store the result in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_sub_si64
+FORCE_INLINE __m64 _mm_sub_si64(__m64 a, __m64 b)
+{
+    return vreinterpret_m64_s64(
+        vsub_s64(vreinterpret_s64_m64(a), vreinterpret_s64_m64(b)));
+}
+
+// Subtract packed signed 16-bit integers in b from packed 16-bit integers in a
+// using saturation, and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_subs_epi16
+FORCE_INLINE __m128i _mm_subs_epi16(__m128i a, __m128i b)
+{
+    return vreinterpretq_m128i_s16(
+        vqsubq_s16(vreinterpretq_s16_m128i(a), vreinterpretq_s16_m128i(b)));
+}
+
+// Subtract packed signed 8-bit integers in b from packed 8-bit integers in a
+// using saturation, and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_subs_epi8
+FORCE_INLINE __m128i _mm_subs_epi8(__m128i a, __m128i b)
+{
+    return vreinterpretq_m128i_s8(
+        vqsubq_s8(vreinterpretq_s8_m128i(a), vreinterpretq_s8_m128i(b)));
+}
+
+// Subtract packed unsigned 16-bit integers in b from packed unsigned 16-bit
+// integers in a using saturation, and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_subs_epu16
+FORCE_INLINE __m128i _mm_subs_epu16(__m128i a, __m128i b)
+{
+    return vreinterpretq_m128i_u16(
+        vqsubq_u16(vreinterpretq_u16_m128i(a), vreinterpretq_u16_m128i(b)));
+}
+
+// Subtract packed unsigned 8-bit integers in b from packed unsigned 8-bit
+// integers in a using saturation, and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_subs_epu8
+FORCE_INLINE __m128i _mm_subs_epu8(__m128i a, __m128i b)
+{
+    return vreinterpretq_m128i_u8(
+        vqsubq_u8(vreinterpretq_u8_m128i(a), vreinterpretq_u8_m128i(b)));
+}
+
+#define _mm_ucomieq_sd _mm_comieq_sd
+#define _mm_ucomige_sd _mm_comige_sd
+#define _mm_ucomigt_sd _mm_comigt_sd
+#define _mm_ucomile_sd _mm_comile_sd
+#define _mm_ucomilt_sd _mm_comilt_sd
+#define _mm_ucomineq_sd _mm_comineq_sd
+
+// Return vector of type __m128d with undefined elements.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_undefined_pd
+FORCE_INLINE __m128d _mm_undefined_pd(void)
+{
+#if defined(__GNUC__) || defined(__clang__)
+#pragma GCC diagnostic push
+#pragma GCC diagnostic ignored "-Wuninitialized"
+#endif
+    __m128d a;
+#if defined(_MSC_VER) && !defined(__clang__)
+    a = _mm_setzero_pd();
+#endif
+    return a;
+#if defined(__GNUC__) || defined(__clang__)
+#pragma GCC diagnostic pop
+#endif
+}
+
+// Unpack and interleave 16-bit integers from the high half of a and b, and
+// store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_unpackhi_epi16
+FORCE_INLINE __m128i _mm_unpackhi_epi16(__m128i a, __m128i b)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    return vreinterpretq_m128i_s16(
+        vzip2q_s16(vreinterpretq_s16_m128i(a), vreinterpretq_s16_m128i(b)));
+#else
+    int16x4_t a1 = vget_high_s16(vreinterpretq_s16_m128i(a));
+    int16x4_t b1 = vget_high_s16(vreinterpretq_s16_m128i(b));
+    int16x4x2_t result = vzip_s16(a1, b1);
+    return vreinterpretq_m128i_s16(vcombine_s16(result.val[0], result.val[1]));
+#endif
+}
+
+// Unpack and interleave 32-bit integers from the high half of a and b, and
+// store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_unpackhi_epi32
+FORCE_INLINE __m128i _mm_unpackhi_epi32(__m128i a, __m128i b)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    return vreinterpretq_m128i_s32(
+        vzip2q_s32(vreinterpretq_s32_m128i(a), vreinterpretq_s32_m128i(b)));
+#else
+    int32x2_t a1 = vget_high_s32(vreinterpretq_s32_m128i(a));
+    int32x2_t b1 = vget_high_s32(vreinterpretq_s32_m128i(b));
+    int32x2x2_t result = vzip_s32(a1, b1);
+    return vreinterpretq_m128i_s32(vcombine_s32(result.val[0], result.val[1]));
+#endif
+}
+
+// Unpack and interleave 64-bit integers from the high half of a and b, and
+// store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_unpackhi_epi64
+FORCE_INLINE __m128i _mm_unpackhi_epi64(__m128i a, __m128i b)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    return vreinterpretq_m128i_s64(
+        vzip2q_s64(vreinterpretq_s64_m128i(a), vreinterpretq_s64_m128i(b)));
+#else
+    int64x1_t a_h = vget_high_s64(vreinterpretq_s64_m128i(a));
+    int64x1_t b_h = vget_high_s64(vreinterpretq_s64_m128i(b));
+    return vreinterpretq_m128i_s64(vcombine_s64(a_h, b_h));
+#endif
+}
+
+// Unpack and interleave 8-bit integers from the high half of a and b, and store
+// the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_unpackhi_epi8
+FORCE_INLINE __m128i _mm_unpackhi_epi8(__m128i a, __m128i b)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    return vreinterpretq_m128i_s8(
+        vzip2q_s8(vreinterpretq_s8_m128i(a), vreinterpretq_s8_m128i(b)));
+#else
+    int8x8_t a1 =
+        vreinterpret_s8_s16(vget_high_s16(vreinterpretq_s16_m128i(a)));
+    int8x8_t b1 =
+        vreinterpret_s8_s16(vget_high_s16(vreinterpretq_s16_m128i(b)));
+    int8x8x2_t result = vzip_s8(a1, b1);
+    return vreinterpretq_m128i_s8(vcombine_s8(result.val[0], result.val[1]));
+#endif
+}
+
+// Unpack and interleave double-precision (64-bit) floating-point elements from
+// the high half of a and b, and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_unpackhi_pd
+FORCE_INLINE __m128d _mm_unpackhi_pd(__m128d a, __m128d b)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    return vreinterpretq_m128d_f64(
+        vzip2q_f64(vreinterpretq_f64_m128d(a), vreinterpretq_f64_m128d(b)));
+#else
+    return vreinterpretq_m128d_s64(
+        vcombine_s64(vget_high_s64(vreinterpretq_s64_m128d(a)),
+                     vget_high_s64(vreinterpretq_s64_m128d(b))));
+#endif
+}
+
+// Unpack and interleave 16-bit integers from the low half of a and b, and store
+// the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_unpacklo_epi16
+FORCE_INLINE __m128i _mm_unpacklo_epi16(__m128i a, __m128i b)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    return vreinterpretq_m128i_s16(
+        vzip1q_s16(vreinterpretq_s16_m128i(a), vreinterpretq_s16_m128i(b)));
+#else
+    int16x4_t a1 = vget_low_s16(vreinterpretq_s16_m128i(a));
+    int16x4_t b1 = vget_low_s16(vreinterpretq_s16_m128i(b));
+    int16x4x2_t result = vzip_s16(a1, b1);
+    return vreinterpretq_m128i_s16(vcombine_s16(result.val[0], result.val[1]));
+#endif
+}
+
+// Unpack and interleave 32-bit integers from the low half of a and b, and store
+// the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_unpacklo_epi32
+FORCE_INLINE __m128i _mm_unpacklo_epi32(__m128i a, __m128i b)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    return vreinterpretq_m128i_s32(
+        vzip1q_s32(vreinterpretq_s32_m128i(a), vreinterpretq_s32_m128i(b)));
+#else
+    int32x2_t a1 = vget_low_s32(vreinterpretq_s32_m128i(a));
+    int32x2_t b1 = vget_low_s32(vreinterpretq_s32_m128i(b));
+    int32x2x2_t result = vzip_s32(a1, b1);
+    return vreinterpretq_m128i_s32(vcombine_s32(result.val[0], result.val[1]));
+#endif
+}
+
+// Unpack and interleave 64-bit integers from the low half of a and b, and store
+// the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_unpacklo_epi64
+FORCE_INLINE __m128i _mm_unpacklo_epi64(__m128i a, __m128i b)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    return vreinterpretq_m128i_s64(
+        vzip1q_s64(vreinterpretq_s64_m128i(a), vreinterpretq_s64_m128i(b)));
+#else
+    int64x1_t a_l = vget_low_s64(vreinterpretq_s64_m128i(a));
+    int64x1_t b_l = vget_low_s64(vreinterpretq_s64_m128i(b));
+    return vreinterpretq_m128i_s64(vcombine_s64(a_l, b_l));
+#endif
+}
+
+// Unpack and interleave 8-bit integers from the low half of a and b, and store
+// the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_unpacklo_epi8
+FORCE_INLINE __m128i _mm_unpacklo_epi8(__m128i a, __m128i b)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    return vreinterpretq_m128i_s8(
+        vzip1q_s8(vreinterpretq_s8_m128i(a), vreinterpretq_s8_m128i(b)));
+#else
+    int8x8_t a1 = vreinterpret_s8_s16(vget_low_s16(vreinterpretq_s16_m128i(a)));
+    int8x8_t b1 = vreinterpret_s8_s16(vget_low_s16(vreinterpretq_s16_m128i(b)));
+    int8x8x2_t result = vzip_s8(a1, b1);
+    return vreinterpretq_m128i_s8(vcombine_s8(result.val[0], result.val[1]));
+#endif
+}
+
+// Unpack and interleave double-precision (64-bit) floating-point elements from
+// the low half of a and b, and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_unpacklo_pd
+FORCE_INLINE __m128d _mm_unpacklo_pd(__m128d a, __m128d b)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    return vreinterpretq_m128d_f64(
+        vzip1q_f64(vreinterpretq_f64_m128d(a), vreinterpretq_f64_m128d(b)));
+#else
+    return vreinterpretq_m128d_s64(
+        vcombine_s64(vget_low_s64(vreinterpretq_s64_m128d(a)),
+                     vget_low_s64(vreinterpretq_s64_m128d(b))));
+#endif
+}
+
+// Compute the bitwise XOR of packed double-precision (64-bit) floating-point
+// elements in a and b, and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_xor_pd
+FORCE_INLINE __m128d _mm_xor_pd(__m128d a, __m128d b)
+{
+    return vreinterpretq_m128d_s64(
+        veorq_s64(vreinterpretq_s64_m128d(a), vreinterpretq_s64_m128d(b)));
+}
+
+// Compute the bitwise XOR of 128 bits (representing integer data) in a and b,
+// and store the result in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_xor_si128
+FORCE_INLINE __m128i _mm_xor_si128(__m128i a, __m128i b)
+{
+    return vreinterpretq_m128i_s32(
+        veorq_s32(vreinterpretq_s32_m128i(a), vreinterpretq_s32_m128i(b)));
+}
+
+/* SSE3 */
+
+// Alternatively add and subtract packed double-precision (64-bit)
+// floating-point elements in a to/from packed elements in b, and store the
+// results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_addsub_pd
+FORCE_INLINE __m128d _mm_addsub_pd(__m128d a, __m128d b)
+{
+    _sse2neon_const __m128d mask = _mm_set_pd(1.0f, -1.0f);
+#if defined(__aarch64__) || defined(_M_ARM64)
+    return vreinterpretq_m128d_f64(vfmaq_f64(vreinterpretq_f64_m128d(a),
+                                             vreinterpretq_f64_m128d(b),
+                                             vreinterpretq_f64_m128d(mask)));
+#else
+    return _mm_add_pd(_mm_mul_pd(b, mask), a);
+#endif
+}
+
+// Alternatively add and subtract packed single-precision (32-bit)
+// floating-point elements in a to/from packed elements in b, and store the
+// results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=addsub_ps
+FORCE_INLINE __m128 _mm_addsub_ps(__m128 a, __m128 b)
+{
+    _sse2neon_const __m128 mask = _mm_setr_ps(-1.0f, 1.0f, -1.0f, 1.0f);
+#if (defined(__aarch64__) || defined(_M_ARM64)) || \
+    defined(__ARM_FEATURE_FMA) /* VFPv4+ */
+    return vreinterpretq_m128_f32(vfmaq_f32(vreinterpretq_f32_m128(a),
+                                            vreinterpretq_f32_m128(mask),
+                                            vreinterpretq_f32_m128(b)));
+#else
+    return _mm_add_ps(_mm_mul_ps(b, mask), a);
+#endif
+}
+
+// Horizontally add adjacent pairs of double-precision (64-bit) floating-point
+// elements in a and b, and pack the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_hadd_pd
+FORCE_INLINE __m128d _mm_hadd_pd(__m128d a, __m128d b)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    return vreinterpretq_m128d_f64(
+        vpaddq_f64(vreinterpretq_f64_m128d(a), vreinterpretq_f64_m128d(b)));
+#else
+    double a0 =
+        sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(a), 0));
+    double a1 =
+        sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(a), 1));
+    double b0 =
+        sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(b), 0));
+    double b1 =
+        sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(b), 1));
+    double c[] = {a0 + a1, b0 + b1};
+    return vreinterpretq_m128d_u64(vld1q_u64((uint64_t *) c));
+#endif
+}
+
+// Horizontally add adjacent pairs of single-precision (32-bit) floating-point
+// elements in a and b, and pack the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_hadd_ps
+FORCE_INLINE __m128 _mm_hadd_ps(__m128 a, __m128 b)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    return vreinterpretq_m128_f32(
+        vpaddq_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(b)));
+#else
+    float32x2_t a10 = vget_low_f32(vreinterpretq_f32_m128(a));
+    float32x2_t a32 = vget_high_f32(vreinterpretq_f32_m128(a));
+    float32x2_t b10 = vget_low_f32(vreinterpretq_f32_m128(b));
+    float32x2_t b32 = vget_high_f32(vreinterpretq_f32_m128(b));
+    return vreinterpretq_m128_f32(
+        vcombine_f32(vpadd_f32(a10, a32), vpadd_f32(b10, b32)));
+#endif
+}
+
+// Horizontally subtract adjacent pairs of double-precision (64-bit)
+// floating-point elements in a and b, and pack the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_hsub_pd
+FORCE_INLINE __m128d _mm_hsub_pd(__m128d a, __m128d b)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    float64x2_t _a = vreinterpretq_f64_m128d(a);
+    float64x2_t _b = vreinterpretq_f64_m128d(b);
+    return vreinterpretq_m128d_f64(
+        vsubq_f64(vuzp1q_f64(_a, _b), vuzp2q_f64(_a, _b)));
+#else
+    double a0 =
+        sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(a), 0));
+    double a1 =
+        sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(a), 1));
+    double b0 =
+        sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(b), 0));
+    double b1 =
+        sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(b), 1));
+    double c[] = {a0 - a1, b0 - b1};
+    return vreinterpretq_m128d_u64(vld1q_u64((uint64_t *) c));
+#endif
+}
+
+// Horizontally subtract adjacent pairs of single-precision (32-bit)
+// floating-point elements in a and b, and pack the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_hsub_ps
+FORCE_INLINE __m128 _mm_hsub_ps(__m128 _a, __m128 _b)
+{
+    float32x4_t a = vreinterpretq_f32_m128(_a);
+    float32x4_t b = vreinterpretq_f32_m128(_b);
+#if defined(__aarch64__) || defined(_M_ARM64)
+    return vreinterpretq_m128_f32(
+        vsubq_f32(vuzp1q_f32(a, b), vuzp2q_f32(a, b)));
+#else
+    float32x4x2_t c = vuzpq_f32(a, b);
+    return vreinterpretq_m128_f32(vsubq_f32(c.val[0], c.val[1]));
+#endif
+}
+
+// Load 128-bits of integer data from unaligned memory into dst. This intrinsic
+// may perform better than _mm_loadu_si128 when the data crosses a cache line
+// boundary.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_lddqu_si128
+#define _mm_lddqu_si128 _mm_loadu_si128
+
+// Load a double-precision (64-bit) floating-point element from memory into both
+// elements of dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_loaddup_pd
+#define _mm_loaddup_pd _mm_load1_pd
+
+// Duplicate the low double-precision (64-bit) floating-point element from a,
+// and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_movedup_pd
+FORCE_INLINE __m128d _mm_movedup_pd(__m128d a)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    return vreinterpretq_m128d_f64(
+        vdupq_laneq_f64(vreinterpretq_f64_m128d(a), 0));
+#else
+    return vreinterpretq_m128d_u64(
+        vdupq_n_u64(vgetq_lane_u64(vreinterpretq_u64_m128d(a), 0)));
+#endif
+}
+
+// Duplicate odd-indexed single-precision (32-bit) floating-point elements
+// from a, and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_movehdup_ps
+FORCE_INLINE __m128 _mm_movehdup_ps(__m128 a)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    return vreinterpretq_m128_f32(
+        vtrn2q_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(a)));
+#elif defined(_sse2neon_shuffle)
+    return vreinterpretq_m128_f32(vshuffleq_s32(
+        vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(a), 1, 1, 3, 3));
+#else
+    float32_t a1 = vgetq_lane_f32(vreinterpretq_f32_m128(a), 1);
+    float32_t a3 = vgetq_lane_f32(vreinterpretq_f32_m128(a), 3);
+    float ALIGN_STRUCT(16) data[4] = {a1, a1, a3, a3};
+    return vreinterpretq_m128_f32(vld1q_f32(data));
+#endif
+}
+
+// Duplicate even-indexed single-precision (32-bit) floating-point elements
+// from a, and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_moveldup_ps
+FORCE_INLINE __m128 _mm_moveldup_ps(__m128 a)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    return vreinterpretq_m128_f32(
+        vtrn1q_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(a)));
+#elif defined(_sse2neon_shuffle)
+    return vreinterpretq_m128_f32(vshuffleq_s32(
+        vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(a), 0, 0, 2, 2));
+#else
+    float32_t a0 = vgetq_lane_f32(vreinterpretq_f32_m128(a), 0);
+    float32_t a2 = vgetq_lane_f32(vreinterpretq_f32_m128(a), 2);
+    float ALIGN_STRUCT(16) data[4] = {a0, a0, a2, a2};
+    return vreinterpretq_m128_f32(vld1q_f32(data));
+#endif
+}
+
+/* SSSE3 */
+
+// Compute the absolute value of packed signed 16-bit integers in a, and store
+// the unsigned results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_abs_epi16
+FORCE_INLINE __m128i _mm_abs_epi16(__m128i a)
+{
+    return vreinterpretq_m128i_s16(vabsq_s16(vreinterpretq_s16_m128i(a)));
+}
+
+// Compute the absolute value of packed signed 32-bit integers in a, and store
+// the unsigned results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_abs_epi32
+FORCE_INLINE __m128i _mm_abs_epi32(__m128i a)
+{
+    return vreinterpretq_m128i_s32(vabsq_s32(vreinterpretq_s32_m128i(a)));
+}
+
+// Compute the absolute value of packed signed 8-bit integers in a, and store
+// the unsigned results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_abs_epi8
+FORCE_INLINE __m128i _mm_abs_epi8(__m128i a)
+{
+    return vreinterpretq_m128i_s8(vabsq_s8(vreinterpretq_s8_m128i(a)));
+}
+
+// Compute the absolute value of packed signed 16-bit integers in a, and store
+// the unsigned results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_abs_pi16
+FORCE_INLINE __m64 _mm_abs_pi16(__m64 a)
+{
+    return vreinterpret_m64_s16(vabs_s16(vreinterpret_s16_m64(a)));
+}
+
+// Compute the absolute value of packed signed 32-bit integers in a, and store
+// the unsigned results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_abs_pi32
+FORCE_INLINE __m64 _mm_abs_pi32(__m64 a)
+{
+    return vreinterpret_m64_s32(vabs_s32(vreinterpret_s32_m64(a)));
+}
+
+// Compute the absolute value of packed signed 8-bit integers in a, and store
+// the unsigned results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_abs_pi8
+FORCE_INLINE __m64 _mm_abs_pi8(__m64 a)
+{
+    return vreinterpret_m64_s8(vabs_s8(vreinterpret_s8_m64(a)));
+}
+
+// Concatenate 16-byte blocks in a and b into a 32-byte temporary result, shift
+// the result right by imm8 bytes, and store the low 16 bytes in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_alignr_epi8
+#if defined(__GNUC__) && !defined(__clang__)
+#define _mm_alignr_epi8(a, b, imm)                                            \
+    __extension__({                                                           \
+        uint8x16_t _a = vreinterpretq_u8_m128i(a);                            \
+        uint8x16_t _b = vreinterpretq_u8_m128i(b);                            \
+        __m128i ret;                                                          \
+        if (_sse2neon_unlikely((imm) & ~31))                                  \
+            ret = vreinterpretq_m128i_u8(vdupq_n_u8(0));                      \
+        else if (imm >= 16)                                                   \
+            ret = _mm_srli_si128(a, imm >= 16 ? imm - 16 : 0);                \
+        else                                                                  \
+            ret =                                                             \
+                vreinterpretq_m128i_u8(vextq_u8(_b, _a, imm < 16 ? imm : 0)); \
+        ret;                                                                  \
+    })
+
+#else
+#define _mm_alignr_epi8(a, b, imm)                                          \
+    _sse2neon_define2(                                                      \
+        __m128i, a, b, uint8x16_t __a = vreinterpretq_u8_m128i(_a);         \
+        uint8x16_t __b = vreinterpretq_u8_m128i(_b); __m128i ret;           \
+        if (_sse2neon_unlikely((imm) & ~31)) ret =                          \
+            vreinterpretq_m128i_u8(vdupq_n_u8(0));                          \
+        else if (imm >= 16) ret =                                           \
+            _mm_srli_si128(_a, imm >= 16 ? imm - 16 : 0);                   \
+        else ret =                                                          \
+            vreinterpretq_m128i_u8(vextq_u8(__b, __a, imm < 16 ? imm : 0)); \
+        _sse2neon_return(ret);)
+
+#endif
+
+// Concatenate 8-byte blocks in a and b into a 16-byte temporary result, shift
+// the result right by imm8 bytes, and store the low 8 bytes in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_alignr_pi8
+#define _mm_alignr_pi8(a, b, imm)                                           \
+    _sse2neon_define2(                                                      \
+        __m64, a, b, __m64 ret; if (_sse2neon_unlikely((imm) >= 16)) {      \
+            ret = vreinterpret_m64_s8(vdup_n_s8(0));                        \
+        } else {                                                            \
+            uint8x8_t tmp_low;                                              \
+            uint8x8_t tmp_high;                                             \
+            if ((imm) >= 8) {                                               \
+                const int idx = (imm) -8;                                   \
+                tmp_low = vreinterpret_u8_m64(_a);                          \
+                tmp_high = vdup_n_u8(0);                                    \
+                ret = vreinterpret_m64_u8(vext_u8(tmp_low, tmp_high, idx)); \
+            } else {                                                        \
+                const int idx = (imm);                                      \
+                tmp_low = vreinterpret_u8_m64(_b);                          \
+                tmp_high = vreinterpret_u8_m64(_a);                         \
+                ret = vreinterpret_m64_u8(vext_u8(tmp_low, tmp_high, idx)); \
+            }                                                               \
+        } _sse2neon_return(ret);)
+
+// Horizontally add adjacent pairs of 16-bit integers in a and b, and pack the
+// signed 16-bit results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_hadd_epi16
+FORCE_INLINE __m128i _mm_hadd_epi16(__m128i _a, __m128i _b)
+{
+    int16x8_t a = vreinterpretq_s16_m128i(_a);
+    int16x8_t b = vreinterpretq_s16_m128i(_b);
+#if defined(__aarch64__) || defined(_M_ARM64)
+    return vreinterpretq_m128i_s16(vpaddq_s16(a, b));
+#else
+    return vreinterpretq_m128i_s16(
+        vcombine_s16(vpadd_s16(vget_low_s16(a), vget_high_s16(a)),
+                     vpadd_s16(vget_low_s16(b), vget_high_s16(b))));
+#endif
+}
+
+// Horizontally add adjacent pairs of 32-bit integers in a and b, and pack the
+// signed 32-bit results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_hadd_epi32
+FORCE_INLINE __m128i _mm_hadd_epi32(__m128i _a, __m128i _b)
+{
+    int32x4_t a = vreinterpretq_s32_m128i(_a);
+    int32x4_t b = vreinterpretq_s32_m128i(_b);
+#if defined(__aarch64__) || defined(_M_ARM64)
+    return vreinterpretq_m128i_s32(vpaddq_s32(a, b));
+#else
+    return vreinterpretq_m128i_s32(
+        vcombine_s32(vpadd_s32(vget_low_s32(a), vget_high_s32(a)),
+                     vpadd_s32(vget_low_s32(b), vget_high_s32(b))));
+#endif
+}
+
+// Horizontally add adjacent pairs of 16-bit integers in a and b, and pack the
+// signed 16-bit results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_hadd_pi16
+FORCE_INLINE __m64 _mm_hadd_pi16(__m64 a, __m64 b)
+{
+    return vreinterpret_m64_s16(
+        vpadd_s16(vreinterpret_s16_m64(a), vreinterpret_s16_m64(b)));
+}
+
+// Horizontally add adjacent pairs of 32-bit integers in a and b, and pack the
+// signed 32-bit results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_hadd_pi32
+FORCE_INLINE __m64 _mm_hadd_pi32(__m64 a, __m64 b)
+{
+    return vreinterpret_m64_s32(
+        vpadd_s32(vreinterpret_s32_m64(a), vreinterpret_s32_m64(b)));
+}
+
+// Horizontally add adjacent pairs of signed 16-bit integers in a and b using
+// saturation, and pack the signed 16-bit results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_hadds_epi16
+FORCE_INLINE __m128i _mm_hadds_epi16(__m128i _a, __m128i _b)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    int16x8_t a = vreinterpretq_s16_m128i(_a);
+    int16x8_t b = vreinterpretq_s16_m128i(_b);
+    return vreinterpretq_s64_s16(
+        vqaddq_s16(vuzp1q_s16(a, b), vuzp2q_s16(a, b)));
+#else
+    int32x4_t a = vreinterpretq_s32_m128i(_a);
+    int32x4_t b = vreinterpretq_s32_m128i(_b);
+    // Interleave using vshrn/vmovn
+    // [a0|a2|a4|a6|b0|b2|b4|b6]
+    // [a1|a3|a5|a7|b1|b3|b5|b7]
+    int16x8_t ab0246 = vcombine_s16(vmovn_s32(a), vmovn_s32(b));
+    int16x8_t ab1357 = vcombine_s16(vshrn_n_s32(a, 16), vshrn_n_s32(b, 16));
+    // Saturated add
+    return vreinterpretq_m128i_s16(vqaddq_s16(ab0246, ab1357));
+#endif
+}
+
+// Horizontally add adjacent pairs of signed 16-bit integers in a and b using
+// saturation, and pack the signed 16-bit results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_hadds_pi16
+FORCE_INLINE __m64 _mm_hadds_pi16(__m64 _a, __m64 _b)
+{
+    int16x4_t a = vreinterpret_s16_m64(_a);
+    int16x4_t b = vreinterpret_s16_m64(_b);
+#if defined(__aarch64__) || defined(_M_ARM64)
+    return vreinterpret_s64_s16(vqadd_s16(vuzp1_s16(a, b), vuzp2_s16(a, b)));
+#else
+    int16x4x2_t res = vuzp_s16(a, b);
+    return vreinterpret_s64_s16(vqadd_s16(res.val[0], res.val[1]));
+#endif
+}
+
+// Horizontally subtract adjacent pairs of 16-bit integers in a and b, and pack
+// the signed 16-bit results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_hsub_epi16
+FORCE_INLINE __m128i _mm_hsub_epi16(__m128i _a, __m128i _b)
+{
+    int16x8_t a = vreinterpretq_s16_m128i(_a);
+    int16x8_t b = vreinterpretq_s16_m128i(_b);
+#if defined(__aarch64__) || defined(_M_ARM64)
+    return vreinterpretq_m128i_s16(
+        vsubq_s16(vuzp1q_s16(a, b), vuzp2q_s16(a, b)));
+#else
+    int16x8x2_t c = vuzpq_s16(a, b);
+    return vreinterpretq_m128i_s16(vsubq_s16(c.val[0], c.val[1]));
+#endif
+}
+
+// Horizontally subtract adjacent pairs of 32-bit integers in a and b, and pack
+// the signed 32-bit results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_hsub_epi32
+FORCE_INLINE __m128i _mm_hsub_epi32(__m128i _a, __m128i _b)
+{
+    int32x4_t a = vreinterpretq_s32_m128i(_a);
+    int32x4_t b = vreinterpretq_s32_m128i(_b);
+#if defined(__aarch64__) || defined(_M_ARM64)
+    return vreinterpretq_m128i_s32(
+        vsubq_s32(vuzp1q_s32(a, b), vuzp2q_s32(a, b)));
+#else
+    int32x4x2_t c = vuzpq_s32(a, b);
+    return vreinterpretq_m128i_s32(vsubq_s32(c.val[0], c.val[1]));
+#endif
+}
+
+// Horizontally subtract adjacent pairs of 16-bit integers in a and b, and pack
+// the signed 16-bit results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_hsub_pi16
+FORCE_INLINE __m64 _mm_hsub_pi16(__m64 _a, __m64 _b)
+{
+    int16x4_t a = vreinterpret_s16_m64(_a);
+    int16x4_t b = vreinterpret_s16_m64(_b);
+#if defined(__aarch64__) || defined(_M_ARM64)
+    return vreinterpret_m64_s16(vsub_s16(vuzp1_s16(a, b), vuzp2_s16(a, b)));
+#else
+    int16x4x2_t c = vuzp_s16(a, b);
+    return vreinterpret_m64_s16(vsub_s16(c.val[0], c.val[1]));
+#endif
+}
+
+// Horizontally subtract adjacent pairs of 32-bit integers in a and b, and pack
+// the signed 32-bit results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=mm_hsub_pi32
+FORCE_INLINE __m64 _mm_hsub_pi32(__m64 _a, __m64 _b)
+{
+    int32x2_t a = vreinterpret_s32_m64(_a);
+    int32x2_t b = vreinterpret_s32_m64(_b);
+#if defined(__aarch64__) || defined(_M_ARM64)
+    return vreinterpret_m64_s32(vsub_s32(vuzp1_s32(a, b), vuzp2_s32(a, b)));
+#else
+    int32x2x2_t c = vuzp_s32(a, b);
+    return vreinterpret_m64_s32(vsub_s32(c.val[0], c.val[1]));
+#endif
+}
+
+// Horizontally subtract adjacent pairs of signed 16-bit integers in a and b
+// using saturation, and pack the signed 16-bit results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_hsubs_epi16
+FORCE_INLINE __m128i _mm_hsubs_epi16(__m128i _a, __m128i _b)
+{
+    int16x8_t a = vreinterpretq_s16_m128i(_a);
+    int16x8_t b = vreinterpretq_s16_m128i(_b);
+#if defined(__aarch64__) || defined(_M_ARM64)
+    return vreinterpretq_m128i_s16(
+        vqsubq_s16(vuzp1q_s16(a, b), vuzp2q_s16(a, b)));
+#else
+    int16x8x2_t c = vuzpq_s16(a, b);
+    return vreinterpretq_m128i_s16(vqsubq_s16(c.val[0], c.val[1]));
+#endif
+}
+
+// Horizontally subtract adjacent pairs of signed 16-bit integers in a and b
+// using saturation, and pack the signed 16-bit results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_hsubs_pi16
+FORCE_INLINE __m64 _mm_hsubs_pi16(__m64 _a, __m64 _b)
+{
+    int16x4_t a = vreinterpret_s16_m64(_a);
+    int16x4_t b = vreinterpret_s16_m64(_b);
+#if defined(__aarch64__) || defined(_M_ARM64)
+    return vreinterpret_m64_s16(vqsub_s16(vuzp1_s16(a, b), vuzp2_s16(a, b)));
+#else
+    int16x4x2_t c = vuzp_s16(a, b);
+    return vreinterpret_m64_s16(vqsub_s16(c.val[0], c.val[1]));
+#endif
+}
+
+// Vertically multiply each unsigned 8-bit integer from a with the corresponding
+// signed 8-bit integer from b, producing intermediate signed 16-bit integers.
+// Horizontally add adjacent pairs of intermediate signed 16-bit integers,
+// and pack the saturated results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_maddubs_epi16
+FORCE_INLINE __m128i _mm_maddubs_epi16(__m128i _a, __m128i _b)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    uint8x16_t a = vreinterpretq_u8_m128i(_a);
+    int8x16_t b = vreinterpretq_s8_m128i(_b);
+    int16x8_t tl = vmulq_s16(vreinterpretq_s16_u16(vmovl_u8(vget_low_u8(a))),
+                             vmovl_s8(vget_low_s8(b)));
+    int16x8_t th = vmulq_s16(vreinterpretq_s16_u16(vmovl_u8(vget_high_u8(a))),
+                             vmovl_s8(vget_high_s8(b)));
+    return vreinterpretq_m128i_s16(
+        vqaddq_s16(vuzp1q_s16(tl, th), vuzp2q_s16(tl, th)));
+#else
+    // This would be much simpler if x86 would choose to zero extend OR sign
+    // extend, not both. This could probably be optimized better.
+    uint16x8_t a = vreinterpretq_u16_m128i(_a);
+    int16x8_t b = vreinterpretq_s16_m128i(_b);
+
+    // Zero extend a
+    int16x8_t a_odd = vreinterpretq_s16_u16(vshrq_n_u16(a, 8));
+    int16x8_t a_even = vreinterpretq_s16_u16(vbicq_u16(a, vdupq_n_u16(0xff00)));
+
+    // Sign extend by shifting left then shifting right.
+    int16x8_t b_even = vshrq_n_s16(vshlq_n_s16(b, 8), 8);
+    int16x8_t b_odd = vshrq_n_s16(b, 8);
+
+    // multiply
+    int16x8_t prod1 = vmulq_s16(a_even, b_even);
+    int16x8_t prod2 = vmulq_s16(a_odd, b_odd);
+
+    // saturated add
+    return vreinterpretq_m128i_s16(vqaddq_s16(prod1, prod2));
+#endif
+}
+
+// Vertically multiply each unsigned 8-bit integer from a with the corresponding
+// signed 8-bit integer from b, producing intermediate signed 16-bit integers.
+// Horizontally add adjacent pairs of intermediate signed 16-bit integers, and
+// pack the saturated results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_maddubs_pi16
+FORCE_INLINE __m64 _mm_maddubs_pi16(__m64 _a, __m64 _b)
+{
+    uint16x4_t a = vreinterpret_u16_m64(_a);
+    int16x4_t b = vreinterpret_s16_m64(_b);
+
+    // Zero extend a
+    int16x4_t a_odd = vreinterpret_s16_u16(vshr_n_u16(a, 8));
+    int16x4_t a_even = vreinterpret_s16_u16(vand_u16(a, vdup_n_u16(0xff)));
+
+    // Sign extend by shifting left then shifting right.
+    int16x4_t b_even = vshr_n_s16(vshl_n_s16(b, 8), 8);
+    int16x4_t b_odd = vshr_n_s16(b, 8);
+
+    // multiply
+    int16x4_t prod1 = vmul_s16(a_even, b_even);
+    int16x4_t prod2 = vmul_s16(a_odd, b_odd);
+
+    // saturated add
+    return vreinterpret_m64_s16(vqadd_s16(prod1, prod2));
+}
+
+// Multiply packed signed 16-bit integers in a and b, producing intermediate
+// signed 32-bit integers. Shift right by 15 bits while rounding up, and store
+// the packed 16-bit integers in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_mulhrs_epi16
+FORCE_INLINE __m128i _mm_mulhrs_epi16(__m128i a, __m128i b)
+{
+    // Has issues due to saturation
+    // return vreinterpretq_m128i_s16(vqrdmulhq_s16(a, b));
+
+    // Multiply
+    int32x4_t mul_lo = vmull_s16(vget_low_s16(vreinterpretq_s16_m128i(a)),
+                                 vget_low_s16(vreinterpretq_s16_m128i(b)));
+    int32x4_t mul_hi = vmull_s16(vget_high_s16(vreinterpretq_s16_m128i(a)),
+                                 vget_high_s16(vreinterpretq_s16_m128i(b)));
+
+    // Rounding narrowing shift right
+    // narrow = (int16_t)((mul + 16384) >> 15);
+    int16x4_t narrow_lo = vrshrn_n_s32(mul_lo, 15);
+    int16x4_t narrow_hi = vrshrn_n_s32(mul_hi, 15);
+
+    // Join together
+    return vreinterpretq_m128i_s16(vcombine_s16(narrow_lo, narrow_hi));
+}
+
+// Multiply packed signed 16-bit integers in a and b, producing intermediate
+// signed 32-bit integers. Truncate each intermediate integer to the 18 most
+// significant bits, round by adding 1, and store bits [16:1] to dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_mulhrs_pi16
+FORCE_INLINE __m64 _mm_mulhrs_pi16(__m64 a, __m64 b)
+{
+    int32x4_t mul_extend =
+        vmull_s16((vreinterpret_s16_m64(a)), (vreinterpret_s16_m64(b)));
+
+    // Rounding narrowing shift right
+    return vreinterpret_m64_s16(vrshrn_n_s32(mul_extend, 15));
+}
+
+// Shuffle packed 8-bit integers in a according to shuffle control mask in the
+// corresponding 8-bit element of b, and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_shuffle_epi8
+FORCE_INLINE __m128i _mm_shuffle_epi8(__m128i a, __m128i b)
+{
+    int8x16_t tbl = vreinterpretq_s8_m128i(a);   // input a
+    uint8x16_t idx = vreinterpretq_u8_m128i(b);  // input b
+    uint8x16_t idx_masked =
+        vandq_u8(idx, vdupq_n_u8(0x8F));  // avoid using meaningless bits
+#if defined(__aarch64__) || defined(_M_ARM64)
+    return vreinterpretq_m128i_s8(vqtbl1q_s8(tbl, idx_masked));
+#elif defined(__GNUC__)
+    int8x16_t ret;
+    // %e and %f represent the even and odd D registers
+    // respectively.
+    __asm__ __volatile__(
+        "vtbl.8  %e[ret], {%e[tbl], %f[tbl]}, %e[idx]\n"
+        "vtbl.8  %f[ret], {%e[tbl], %f[tbl]}, %f[idx]\n"
+        : [ret] "=&w"(ret)
+        : [tbl] "w"(tbl), [idx] "w"(idx_masked));
+    return vreinterpretq_m128i_s8(ret);
+#else
+    // use this line if testing on aarch64
+    int8x8x2_t a_split = {vget_low_s8(tbl), vget_high_s8(tbl)};
+    return vreinterpretq_m128i_s8(
+        vcombine_s8(vtbl2_s8(a_split, vget_low_u8(idx_masked)),
+                    vtbl2_s8(a_split, vget_high_u8(idx_masked))));
+#endif
+}
+
+// Shuffle packed 8-bit integers in a according to shuffle control mask in the
+// corresponding 8-bit element of b, and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_shuffle_pi8
+FORCE_INLINE __m64 _mm_shuffle_pi8(__m64 a, __m64 b)
+{
+    const int8x8_t controlMask =
+        vand_s8(vreinterpret_s8_m64(b), vdup_n_s8((int8_t) (0x1 << 7 | 0x07)));
+    int8x8_t res = vtbl1_s8(vreinterpret_s8_m64(a), controlMask);
+    return vreinterpret_m64_s8(res);
+}
+
+// Negate packed 16-bit integers in a when the corresponding signed
+// 16-bit integer in b is negative, and store the results in dst.
+// Element in dst are zeroed out when the corresponding element
+// in b is zero.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_sign_epi16
+FORCE_INLINE __m128i _mm_sign_epi16(__m128i _a, __m128i _b)
+{
+    int16x8_t a = vreinterpretq_s16_m128i(_a);
+    int16x8_t b = vreinterpretq_s16_m128i(_b);
+
+    // signed shift right: faster than vclt
+    // (b < 0) ? 0xFFFF : 0
+    uint16x8_t ltMask = vreinterpretq_u16_s16(vshrq_n_s16(b, 15));
+    // (b == 0) ? 0xFFFF : 0
+#if defined(__aarch64__) || defined(_M_ARM64)
+    int16x8_t zeroMask = vreinterpretq_s16_u16(vceqzq_s16(b));
+#else
+    int16x8_t zeroMask = vreinterpretq_s16_u16(vceqq_s16(b, vdupq_n_s16(0)));
+#endif
+
+    // bitwise select either a or negative 'a' (vnegq_s16(a) equals to negative
+    // 'a') based on ltMask
+    int16x8_t masked = vbslq_s16(ltMask, vnegq_s16(a), a);
+    // res = masked & (~zeroMask)
+    int16x8_t res = vbicq_s16(masked, zeroMask);
+    return vreinterpretq_m128i_s16(res);
+}
+
+// Negate packed 32-bit integers in a when the corresponding signed
+// 32-bit integer in b is negative, and store the results in dst.
+// Element in dst are zeroed out when the corresponding element
+// in b is zero.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_sign_epi32
+FORCE_INLINE __m128i _mm_sign_epi32(__m128i _a, __m128i _b)
+{
+    int32x4_t a = vreinterpretq_s32_m128i(_a);
+    int32x4_t b = vreinterpretq_s32_m128i(_b);
+
+    // signed shift right: faster than vclt
+    // (b < 0) ? 0xFFFFFFFF : 0
+    uint32x4_t ltMask = vreinterpretq_u32_s32(vshrq_n_s32(b, 31));
+
+    // (b == 0) ? 0xFFFFFFFF : 0
+#if defined(__aarch64__) || defined(_M_ARM64)
+    int32x4_t zeroMask = vreinterpretq_s32_u32(vceqzq_s32(b));
+#else
+    int32x4_t zeroMask = vreinterpretq_s32_u32(vceqq_s32(b, vdupq_n_s32(0)));
+#endif
+
+    // bitwise select either a or negative 'a' (vnegq_s32(a) equals to negative
+    // 'a') based on ltMask
+    int32x4_t masked = vbslq_s32(ltMask, vnegq_s32(a), a);
+    // res = masked & (~zeroMask)
+    int32x4_t res = vbicq_s32(masked, zeroMask);
+    return vreinterpretq_m128i_s32(res);
+}
+
+// Negate packed 8-bit integers in a when the corresponding signed
+// 8-bit integer in b is negative, and store the results in dst.
+// Element in dst are zeroed out when the corresponding element
+// in b is zero.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_sign_epi8
+FORCE_INLINE __m128i _mm_sign_epi8(__m128i _a, __m128i _b)
+{
+    int8x16_t a = vreinterpretq_s8_m128i(_a);
+    int8x16_t b = vreinterpretq_s8_m128i(_b);
+
+    // signed shift right: faster than vclt
+    // (b < 0) ? 0xFF : 0
+    uint8x16_t ltMask = vreinterpretq_u8_s8(vshrq_n_s8(b, 7));
+
+    // (b == 0) ? 0xFF : 0
+#if defined(__aarch64__) || defined(_M_ARM64)
+    int8x16_t zeroMask = vreinterpretq_s8_u8(vceqzq_s8(b));
+#else
+    int8x16_t zeroMask = vreinterpretq_s8_u8(vceqq_s8(b, vdupq_n_s8(0)));
+#endif
+
+    // bitwise select either a or negative 'a' (vnegq_s8(a) return negative 'a')
+    // based on ltMask
+    int8x16_t masked = vbslq_s8(ltMask, vnegq_s8(a), a);
+    // res = masked & (~zeroMask)
+    int8x16_t res = vbicq_s8(masked, zeroMask);
+
+    return vreinterpretq_m128i_s8(res);
+}
+
+// Negate packed 16-bit integers in a when the corresponding signed 16-bit
+// integer in b is negative, and store the results in dst. Element in dst are
+// zeroed out when the corresponding element in b is zero.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_sign_pi16
+FORCE_INLINE __m64 _mm_sign_pi16(__m64 _a, __m64 _b)
+{
+    int16x4_t a = vreinterpret_s16_m64(_a);
+    int16x4_t b = vreinterpret_s16_m64(_b);
+
+    // signed shift right: faster than vclt
+    // (b < 0) ? 0xFFFF : 0
+    uint16x4_t ltMask = vreinterpret_u16_s16(vshr_n_s16(b, 15));
+
+    // (b == 0) ? 0xFFFF : 0
+#if defined(__aarch64__) || defined(_M_ARM64)
+    int16x4_t zeroMask = vreinterpret_s16_u16(vceqz_s16(b));
+#else
+    int16x4_t zeroMask = vreinterpret_s16_u16(vceq_s16(b, vdup_n_s16(0)));
+#endif
+
+    // bitwise select either a or negative 'a' (vneg_s16(a) return negative 'a')
+    // based on ltMask
+    int16x4_t masked = vbsl_s16(ltMask, vneg_s16(a), a);
+    // res = masked & (~zeroMask)
+    int16x4_t res = vbic_s16(masked, zeroMask);
+
+    return vreinterpret_m64_s16(res);
+}
+
+// Negate packed 32-bit integers in a when the corresponding signed 32-bit
+// integer in b is negative, and store the results in dst. Element in dst are
+// zeroed out when the corresponding element in b is zero.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_sign_pi32
+FORCE_INLINE __m64 _mm_sign_pi32(__m64 _a, __m64 _b)
+{
+    int32x2_t a = vreinterpret_s32_m64(_a);
+    int32x2_t b = vreinterpret_s32_m64(_b);
+
+    // signed shift right: faster than vclt
+    // (b < 0) ? 0xFFFFFFFF : 0
+    uint32x2_t ltMask = vreinterpret_u32_s32(vshr_n_s32(b, 31));
+
+    // (b == 0) ? 0xFFFFFFFF : 0
+#if defined(__aarch64__) || defined(_M_ARM64)
+    int32x2_t zeroMask = vreinterpret_s32_u32(vceqz_s32(b));
+#else
+    int32x2_t zeroMask = vreinterpret_s32_u32(vceq_s32(b, vdup_n_s32(0)));
+#endif
+
+    // bitwise select either a or negative 'a' (vneg_s32(a) return negative 'a')
+    // based on ltMask
+    int32x2_t masked = vbsl_s32(ltMask, vneg_s32(a), a);
+    // res = masked & (~zeroMask)
+    int32x2_t res = vbic_s32(masked, zeroMask);
+
+    return vreinterpret_m64_s32(res);
+}
+
+// Negate packed 8-bit integers in a when the corresponding signed 8-bit integer
+// in b is negative, and store the results in dst. Element in dst are zeroed out
+// when the corresponding element in b is zero.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_sign_pi8
+FORCE_INLINE __m64 _mm_sign_pi8(__m64 _a, __m64 _b)
+{
+    int8x8_t a = vreinterpret_s8_m64(_a);
+    int8x8_t b = vreinterpret_s8_m64(_b);
+
+    // signed shift right: faster than vclt
+    // (b < 0) ? 0xFF : 0
+    uint8x8_t ltMask = vreinterpret_u8_s8(vshr_n_s8(b, 7));
+
+    // (b == 0) ? 0xFF : 0
+#if defined(__aarch64__) || defined(_M_ARM64)
+    int8x8_t zeroMask = vreinterpret_s8_u8(vceqz_s8(b));
+#else
+    int8x8_t zeroMask = vreinterpret_s8_u8(vceq_s8(b, vdup_n_s8(0)));
+#endif
+
+    // bitwise select either a or negative 'a' (vneg_s8(a) return negative 'a')
+    // based on ltMask
+    int8x8_t masked = vbsl_s8(ltMask, vneg_s8(a), a);
+    // res = masked & (~zeroMask)
+    int8x8_t res = vbic_s8(masked, zeroMask);
+
+    return vreinterpret_m64_s8(res);
+}
+
+/* SSE4.1 */
+
+// Blend packed 16-bit integers from a and b using control mask imm8, and store
+// the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_blend_epi16
+// FORCE_INLINE __m128i _mm_blend_epi16(__m128i a, __m128i b,
+//                                      __constrange(0,255) int imm)
+#define _mm_blend_epi16(a, b, imm)                                      \
+    _sse2neon_define2(                                                  \
+        __m128i, a, b,                                                  \
+        const uint16_t _mask[8] =                                       \
+            _sse2neon_init(((imm) & (1 << 0)) ? (uint16_t) -1 : 0x0,    \
+                           ((imm) & (1 << 1)) ? (uint16_t) -1 : 0x0,    \
+                           ((imm) & (1 << 2)) ? (uint16_t) -1 : 0x0,    \
+                           ((imm) & (1 << 3)) ? (uint16_t) -1 : 0x0,    \
+                           ((imm) & (1 << 4)) ? (uint16_t) -1 : 0x0,    \
+                           ((imm) & (1 << 5)) ? (uint16_t) -1 : 0x0,    \
+                           ((imm) & (1 << 6)) ? (uint16_t) -1 : 0x0,    \
+                           ((imm) & (1 << 7)) ? (uint16_t) -1 : 0x0);   \
+        uint16x8_t _mask_vec = vld1q_u16(_mask);                        \
+        uint16x8_t __a = vreinterpretq_u16_m128i(_a);                   \
+        uint16x8_t __b = vreinterpretq_u16_m128i(_b); _sse2neon_return( \
+            vreinterpretq_m128i_u16(vbslq_u16(_mask_vec, __b, __a)));)
+
+// Blend packed double-precision (64-bit) floating-point elements from a and b
+// using control mask imm8, and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_blend_pd
+#define _mm_blend_pd(a, b, imm)                                              \
+    _sse2neon_define2(                                                       \
+        __m128d, a, b,                                                       \
+        const uint64_t _mask[2] =                                            \
+            _sse2neon_init(((imm) & (1 << 0)) ? ~UINT64_C(0) : UINT64_C(0),  \
+                           ((imm) & (1 << 1)) ? ~UINT64_C(0) : UINT64_C(0)); \
+        uint64x2_t _mask_vec = vld1q_u64(_mask);                             \
+        uint64x2_t __a = vreinterpretq_u64_m128d(_a);                        \
+        uint64x2_t __b = vreinterpretq_u64_m128d(_b); _sse2neon_return(      \
+            vreinterpretq_m128d_u64(vbslq_u64(_mask_vec, __b, __a)));)
+
+// Blend packed single-precision (32-bit) floating-point elements from a and b
+// using mask, and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_blend_ps
+FORCE_INLINE __m128 _mm_blend_ps(__m128 _a, __m128 _b, const char imm8)
+{
+    const uint32_t ALIGN_STRUCT(16)
+        data[4] = {((imm8) & (1 << 0)) ? UINT32_MAX : 0,
+                   ((imm8) & (1 << 1)) ? UINT32_MAX : 0,
+                   ((imm8) & (1 << 2)) ? UINT32_MAX : 0,
+                   ((imm8) & (1 << 3)) ? UINT32_MAX : 0};
+    uint32x4_t mask = vld1q_u32(data);
+    float32x4_t a = vreinterpretq_f32_m128(_a);
+    float32x4_t b = vreinterpretq_f32_m128(_b);
+    return vreinterpretq_m128_f32(vbslq_f32(mask, b, a));
+}
+
+// Blend packed 8-bit integers from a and b using mask, and store the results in
+// dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_blendv_epi8
+FORCE_INLINE __m128i _mm_blendv_epi8(__m128i _a, __m128i _b, __m128i _mask)
+{
+    // Use a signed shift right to create a mask with the sign bit
+    uint8x16_t mask =
+        vreinterpretq_u8_s8(vshrq_n_s8(vreinterpretq_s8_m128i(_mask), 7));
+    uint8x16_t a = vreinterpretq_u8_m128i(_a);
+    uint8x16_t b = vreinterpretq_u8_m128i(_b);
+    return vreinterpretq_m128i_u8(vbslq_u8(mask, b, a));
+}
+
+// Blend packed double-precision (64-bit) floating-point elements from a and b
+// using mask, and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_blendv_pd
+FORCE_INLINE __m128d _mm_blendv_pd(__m128d _a, __m128d _b, __m128d _mask)
+{
+    uint64x2_t mask =
+        vreinterpretq_u64_s64(vshrq_n_s64(vreinterpretq_s64_m128d(_mask), 63));
+#if defined(__aarch64__) || defined(_M_ARM64)
+    float64x2_t a = vreinterpretq_f64_m128d(_a);
+    float64x2_t b = vreinterpretq_f64_m128d(_b);
+    return vreinterpretq_m128d_f64(vbslq_f64(mask, b, a));
+#else
+    uint64x2_t a = vreinterpretq_u64_m128d(_a);
+    uint64x2_t b = vreinterpretq_u64_m128d(_b);
+    return vreinterpretq_m128d_u64(vbslq_u64(mask, b, a));
+#endif
+}
+
+// Blend packed single-precision (32-bit) floating-point elements from a and b
+// using mask, and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_blendv_ps
+FORCE_INLINE __m128 _mm_blendv_ps(__m128 _a, __m128 _b, __m128 _mask)
+{
+    // Use a signed shift right to create a mask with the sign bit
+    uint32x4_t mask =
+        vreinterpretq_u32_s32(vshrq_n_s32(vreinterpretq_s32_m128(_mask), 31));
+    float32x4_t a = vreinterpretq_f32_m128(_a);
+    float32x4_t b = vreinterpretq_f32_m128(_b);
+    return vreinterpretq_m128_f32(vbslq_f32(mask, b, a));
+}
+
+// Round the packed double-precision (64-bit) floating-point elements in a up
+// to an integer value, and store the results as packed double-precision
+// floating-point elements in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_ceil_pd
+FORCE_INLINE __m128d _mm_ceil_pd(__m128d a)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    return vreinterpretq_m128d_f64(vrndpq_f64(vreinterpretq_f64_m128d(a)));
+#else
+    double a0, a1;
+    a0 = sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(a), 0));
+    a1 = sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(a), 1));
+    return _mm_set_pd(ceil(a1), ceil(a0));
+#endif
+}
+
+// Round the packed single-precision (32-bit) floating-point elements in a up to
+// an integer value, and store the results as packed single-precision
+// floating-point elements in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_ceil_ps
+FORCE_INLINE __m128 _mm_ceil_ps(__m128 a)
+{
+#if (defined(__aarch64__) || defined(_M_ARM64)) || \
+    defined(__ARM_FEATURE_DIRECTED_ROUNDING)
+    return vreinterpretq_m128_f32(vrndpq_f32(vreinterpretq_f32_m128(a)));
+#else
+    float *f = (float *) &a;
+    return _mm_set_ps(ceilf(f[3]), ceilf(f[2]), ceilf(f[1]), ceilf(f[0]));
+#endif
+}
+
+// Round the lower double-precision (64-bit) floating-point element in b up to
+// an integer value, store the result as a double-precision floating-point
+// element in the lower element of dst, and copy the upper element from a to the
+// upper element of dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_ceil_sd
+FORCE_INLINE __m128d _mm_ceil_sd(__m128d a, __m128d b)
+{
+    return _mm_move_sd(a, _mm_ceil_pd(b));
+}
+
+// Round the lower single-precision (32-bit) floating-point element in b up to
+// an integer value, store the result as a single-precision floating-point
+// element in the lower element of dst, and copy the upper 3 packed elements
+// from a to the upper elements of dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_ceil_ss
+FORCE_INLINE __m128 _mm_ceil_ss(__m128 a, __m128 b)
+{
+    return _mm_move_ss(a, _mm_ceil_ps(b));
+}
+
+// Compare packed 64-bit integers in a and b for equality, and store the results
+// in dst
+FORCE_INLINE __m128i _mm_cmpeq_epi64(__m128i a, __m128i b)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    return vreinterpretq_m128i_u64(
+        vceqq_u64(vreinterpretq_u64_m128i(a), vreinterpretq_u64_m128i(b)));
+#else
+    // ARMv7 lacks vceqq_u64
+    // (a == b) -> (a_lo == b_lo) && (a_hi == b_hi)
+    uint32x4_t cmp =
+        vceqq_u32(vreinterpretq_u32_m128i(a), vreinterpretq_u32_m128i(b));
+    uint32x4_t swapped = vrev64q_u32(cmp);
+    return vreinterpretq_m128i_u32(vandq_u32(cmp, swapped));
+#endif
+}
+
+// Sign extend packed 16-bit integers in a to packed 32-bit integers, and store
+// the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cvtepi16_epi32
+FORCE_INLINE __m128i _mm_cvtepi16_epi32(__m128i a)
+{
+    return vreinterpretq_m128i_s32(
+        vmovl_s16(vget_low_s16(vreinterpretq_s16_m128i(a))));
+}
+
+// Sign extend packed 16-bit integers in a to packed 64-bit integers, and store
+// the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cvtepi16_epi64
+FORCE_INLINE __m128i _mm_cvtepi16_epi64(__m128i a)
+{
+    int16x8_t s16x8 = vreinterpretq_s16_m128i(a);     /* xxxx xxxx xxxx 0B0A */
+    int32x4_t s32x4 = vmovl_s16(vget_low_s16(s16x8)); /* 000x 000x 000B 000A */
+    int64x2_t s64x2 = vmovl_s32(vget_low_s32(s32x4)); /* 0000 000B 0000 000A */
+    return vreinterpretq_m128i_s64(s64x2);
+}
+
+// Sign extend packed 32-bit integers in a to packed 64-bit integers, and store
+// the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cvtepi32_epi64
+FORCE_INLINE __m128i _mm_cvtepi32_epi64(__m128i a)
+{
+    return vreinterpretq_m128i_s64(
+        vmovl_s32(vget_low_s32(vreinterpretq_s32_m128i(a))));
+}
+
+// Sign extend packed 8-bit integers in a to packed 16-bit integers, and store
+// the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cvtepi8_epi16
+FORCE_INLINE __m128i _mm_cvtepi8_epi16(__m128i a)
+{
+    int8x16_t s8x16 = vreinterpretq_s8_m128i(a);    /* xxxx xxxx xxxx DCBA */
+    int16x8_t s16x8 = vmovl_s8(vget_low_s8(s8x16)); /* 0x0x 0x0x 0D0C 0B0A */
+    return vreinterpretq_m128i_s16(s16x8);
+}
+
+// Sign extend packed 8-bit integers in a to packed 32-bit integers, and store
+// the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cvtepi8_epi32
+FORCE_INLINE __m128i _mm_cvtepi8_epi32(__m128i a)
+{
+    int8x16_t s8x16 = vreinterpretq_s8_m128i(a);      /* xxxx xxxx xxxx DCBA */
+    int16x8_t s16x8 = vmovl_s8(vget_low_s8(s8x16));   /* 0x0x 0x0x 0D0C 0B0A */
+    int32x4_t s32x4 = vmovl_s16(vget_low_s16(s16x8)); /* 000D 000C 000B 000A */
+    return vreinterpretq_m128i_s32(s32x4);
+}
+
+// Sign extend packed 8-bit integers in the low 8 bytes of a to packed 64-bit
+// integers, and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cvtepi8_epi64
+FORCE_INLINE __m128i _mm_cvtepi8_epi64(__m128i a)
+{
+    int8x16_t s8x16 = vreinterpretq_s8_m128i(a);      /* xxxx xxxx xxxx xxBA */
+    int16x8_t s16x8 = vmovl_s8(vget_low_s8(s8x16));   /* 0x0x 0x0x 0x0x 0B0A */
+    int32x4_t s32x4 = vmovl_s16(vget_low_s16(s16x8)); /* 000x 000x 000B 000A */
+    int64x2_t s64x2 = vmovl_s32(vget_low_s32(s32x4)); /* 0000 000B 0000 000A */
+    return vreinterpretq_m128i_s64(s64x2);
+}
+
+// Zero extend packed unsigned 16-bit integers in a to packed 32-bit integers,
+// and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cvtepu16_epi32
+FORCE_INLINE __m128i _mm_cvtepu16_epi32(__m128i a)
+{
+    return vreinterpretq_m128i_u32(
+        vmovl_u16(vget_low_u16(vreinterpretq_u16_m128i(a))));
+}
+
+// Zero extend packed unsigned 16-bit integers in a to packed 64-bit integers,
+// and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cvtepu16_epi64
+FORCE_INLINE __m128i _mm_cvtepu16_epi64(__m128i a)
+{
+    uint16x8_t u16x8 = vreinterpretq_u16_m128i(a);     /* xxxx xxxx xxxx 0B0A */
+    uint32x4_t u32x4 = vmovl_u16(vget_low_u16(u16x8)); /* 000x 000x 000B 000A */
+    uint64x2_t u64x2 = vmovl_u32(vget_low_u32(u32x4)); /* 0000 000B 0000 000A */
+    return vreinterpretq_m128i_u64(u64x2);
+}
+
+// Zero extend packed unsigned 32-bit integers in a to packed 64-bit integers,
+// and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cvtepu32_epi64
+FORCE_INLINE __m128i _mm_cvtepu32_epi64(__m128i a)
+{
+    return vreinterpretq_m128i_u64(
+        vmovl_u32(vget_low_u32(vreinterpretq_u32_m128i(a))));
+}
+
+// Zero extend packed unsigned 8-bit integers in a to packed 16-bit integers,
+// and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cvtepu8_epi16
+FORCE_INLINE __m128i _mm_cvtepu8_epi16(__m128i a)
+{
+    uint8x16_t u8x16 = vreinterpretq_u8_m128i(a);    /* xxxx xxxx HGFE DCBA */
+    uint16x8_t u16x8 = vmovl_u8(vget_low_u8(u8x16)); /* 0H0G 0F0E 0D0C 0B0A */
+    return vreinterpretq_m128i_u16(u16x8);
+}
+
+// Zero extend packed unsigned 8-bit integers in a to packed 32-bit integers,
+// and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cvtepu8_epi32
+FORCE_INLINE __m128i _mm_cvtepu8_epi32(__m128i a)
+{
+    uint8x16_t u8x16 = vreinterpretq_u8_m128i(a);      /* xxxx xxxx xxxx DCBA */
+    uint16x8_t u16x8 = vmovl_u8(vget_low_u8(u8x16));   /* 0x0x 0x0x 0D0C 0B0A */
+    uint32x4_t u32x4 = vmovl_u16(vget_low_u16(u16x8)); /* 000D 000C 000B 000A */
+    return vreinterpretq_m128i_u32(u32x4);
+}
+
+// Zero extend packed unsigned 8-bit integers in the low 8 bytes of a to packed
+// 64-bit integers, and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cvtepu8_epi64
+FORCE_INLINE __m128i _mm_cvtepu8_epi64(__m128i a)
+{
+    uint8x16_t u8x16 = vreinterpretq_u8_m128i(a);      /* xxxx xxxx xxxx xxBA */
+    uint16x8_t u16x8 = vmovl_u8(vget_low_u8(u8x16));   /* 0x0x 0x0x 0x0x 0B0A */
+    uint32x4_t u32x4 = vmovl_u16(vget_low_u16(u16x8)); /* 000x 000x 000B 000A */
+    uint64x2_t u64x2 = vmovl_u32(vget_low_u32(u32x4)); /* 0000 000B 0000 000A */
+    return vreinterpretq_m128i_u64(u64x2);
+}
+
+// Conditionally multiply the packed double-precision (64-bit) floating-point
+// elements in a and b using the high 4 bits in imm8, sum the four products, and
+// conditionally store the sum in dst using the low 4 bits of imm8.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_dp_pd
+FORCE_INLINE __m128d _mm_dp_pd(__m128d a, __m128d b, const int imm)
+{
+    // Generate mask value from constant immediate bit value
+    const int64_t bit0Mask = imm & 0x01 ? UINT64_MAX : 0;
+    const int64_t bit1Mask = imm & 0x02 ? UINT64_MAX : 0;
+#if !SSE2NEON_PRECISE_DP
+    const int64_t bit4Mask = imm & 0x10 ? UINT64_MAX : 0;
+    const int64_t bit5Mask = imm & 0x20 ? UINT64_MAX : 0;
+#endif
+    // Conditional multiplication
+#if !SSE2NEON_PRECISE_DP
+    __m128d mul = _mm_mul_pd(a, b);
+    const __m128d mulMask =
+        _mm_castsi128_pd(_mm_set_epi64x(bit5Mask, bit4Mask));
+    __m128d tmp = _mm_and_pd(mul, mulMask);
+#else
+#if defined(__aarch64__) || defined(_M_ARM64)
+    double d0 = (imm & 0x10) ? vgetq_lane_f64(vreinterpretq_f64_m128d(a), 0) *
+                                   vgetq_lane_f64(vreinterpretq_f64_m128d(b), 0)
+                             : 0;
+    double d1 = (imm & 0x20) ? vgetq_lane_f64(vreinterpretq_f64_m128d(a), 1) *
+                                   vgetq_lane_f64(vreinterpretq_f64_m128d(b), 1)
+                             : 0;
+#else
+    double a0 =
+        sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(a), 0));
+    double a1 =
+        sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(a), 1));
+    double b0 =
+        sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(b), 0));
+    double b1 =
+        sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(b), 1));
+    double d0 = (imm & 0x10) ? a0 * b0 : 0;
+    double d1 = (imm & 0x20) ? a1 * b1 : 0;
+#endif
+    __m128d tmp = _mm_set_pd(d1, d0);
+#endif
+    // Sum the products
+#if defined(__aarch64__) || defined(_M_ARM64)
+    double sum = vpaddd_f64(vreinterpretq_f64_m128d(tmp));
+#else
+    double _tmp0 = sse2neon_recast_u64_f64(
+        vgetq_lane_u64(vreinterpretq_u64_m128d(tmp), 0));
+    double _tmp1 = sse2neon_recast_u64_f64(
+        vgetq_lane_u64(vreinterpretq_u64_m128d(tmp), 1));
+    double sum = _tmp0 + _tmp1;
+#endif
+    // Conditionally store the sum
+    const __m128d sumMask =
+        _mm_castsi128_pd(_mm_set_epi64x(bit1Mask, bit0Mask));
+    __m128d res = _mm_and_pd(_mm_set_pd1(sum), sumMask);
+    return res;
+}
+
+// Conditionally multiply the packed single-precision (32-bit) floating-point
+// elements in a and b using the high 4 bits in imm8, sum the four products,
+// and conditionally store the sum in dst using the low 4 bits of imm.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_dp_ps
+FORCE_INLINE __m128 _mm_dp_ps(__m128 a, __m128 b, const int imm)
+{
+    float32x4_t elementwise_prod = _mm_mul_ps(a, b);
+
+#if defined(__aarch64__) || defined(_M_ARM64)
+    /* shortcuts */
+    if (imm == 0xFF) {
+        return _mm_set1_ps(vaddvq_f32(elementwise_prod));
+    }
+
+    if ((imm & 0x0F) == 0x0F) {
+        if (!(imm & (1 << 4)))
+            elementwise_prod = vsetq_lane_f32(0.0f, elementwise_prod, 0);
+        if (!(imm & (1 << 5)))
+            elementwise_prod = vsetq_lane_f32(0.0f, elementwise_prod, 1);
+        if (!(imm & (1 << 6)))
+            elementwise_prod = vsetq_lane_f32(0.0f, elementwise_prod, 2);
+        if (!(imm & (1 << 7)))
+            elementwise_prod = vsetq_lane_f32(0.0f, elementwise_prod, 3);
+
+        return _mm_set1_ps(vaddvq_f32(elementwise_prod));
+    }
+#endif
+
+    float s = 0.0f;
+
+    if (imm & (1 << 4))
+        s += vgetq_lane_f32(elementwise_prod, 0);
+    if (imm & (1 << 5))
+        s += vgetq_lane_f32(elementwise_prod, 1);
+    if (imm & (1 << 6))
+        s += vgetq_lane_f32(elementwise_prod, 2);
+    if (imm & (1 << 7))
+        s += vgetq_lane_f32(elementwise_prod, 3);
+
+    const float32_t res[4] = {
+        (imm & 0x1) ? s : 0.0f,
+        (imm & 0x2) ? s : 0.0f,
+        (imm & 0x4) ? s : 0.0f,
+        (imm & 0x8) ? s : 0.0f,
+    };
+    return vreinterpretq_m128_f32(vld1q_f32(res));
+}
+
+// Extract a 32-bit integer from a, selected with imm8, and store the result in
+// dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_extract_epi32
+// FORCE_INLINE int _mm_extract_epi32(__m128i a, __constrange(0,4) int imm)
+#define _mm_extract_epi32(a, imm) \
+    vgetq_lane_s32(vreinterpretq_s32_m128i(a), (imm))
+
+// Extract a 64-bit integer from a, selected with imm8, and store the result in
+// dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_extract_epi64
+// FORCE_INLINE __int64 _mm_extract_epi64(__m128i a, __constrange(0,2) int imm)
+#define _mm_extract_epi64(a, imm) \
+    vgetq_lane_s64(vreinterpretq_s64_m128i(a), (imm))
+
+// Extract an 8-bit integer from a, selected with imm8, and store the result in
+// the lower element of dst. FORCE_INLINE int _mm_extract_epi8(__m128i a,
+// __constrange(0,16) int imm)
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_extract_epi8
+#define _mm_extract_epi8(a, imm) vgetq_lane_u8(vreinterpretq_u8_m128i(a), (imm))
+
+// Extracts the selected single-precision (32-bit) floating-point from a.
+// FORCE_INLINE int _mm_extract_ps(__m128 a, __constrange(0,4) int imm)
+#define _mm_extract_ps(a, imm) vgetq_lane_s32(vreinterpretq_s32_m128(a), (imm))
+
+// Round the packed double-precision (64-bit) floating-point elements in a down
+// to an integer value, and store the results as packed double-precision
+// floating-point elements in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_floor_pd
+FORCE_INLINE __m128d _mm_floor_pd(__m128d a)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    return vreinterpretq_m128d_f64(vrndmq_f64(vreinterpretq_f64_m128d(a)));
+#else
+    double a0, a1;
+    a0 = sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(a), 0));
+    a1 = sse2neon_recast_u64_f64(vgetq_lane_u64(vreinterpretq_u64_m128d(a), 1));
+    return _mm_set_pd(floor(a1), floor(a0));
+#endif
+}
+
+// Round the packed single-precision (32-bit) floating-point elements in a down
+// to an integer value, and store the results as packed single-precision
+// floating-point elements in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_floor_ps
+FORCE_INLINE __m128 _mm_floor_ps(__m128 a)
+{
+#if (defined(__aarch64__) || defined(_M_ARM64)) || \
+    defined(__ARM_FEATURE_DIRECTED_ROUNDING)
+    return vreinterpretq_m128_f32(vrndmq_f32(vreinterpretq_f32_m128(a)));
+#else
+    float *f = (float *) &a;
+    return _mm_set_ps(floorf(f[3]), floorf(f[2]), floorf(f[1]), floorf(f[0]));
+#endif
+}
+
+// Round the lower double-precision (64-bit) floating-point element in b down to
+// an integer value, store the result as a double-precision floating-point
+// element in the lower element of dst, and copy the upper element from a to the
+// upper element of dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_floor_sd
+FORCE_INLINE __m128d _mm_floor_sd(__m128d a, __m128d b)
+{
+    return _mm_move_sd(a, _mm_floor_pd(b));
+}
+
+// Round the lower single-precision (32-bit) floating-point element in b down to
+// an integer value, store the result as a single-precision floating-point
+// element in the lower element of dst, and copy the upper 3 packed elements
+// from a to the upper elements of dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_floor_ss
+FORCE_INLINE __m128 _mm_floor_ss(__m128 a, __m128 b)
+{
+    return _mm_move_ss(a, _mm_floor_ps(b));
+}
+
+// Copy a to dst, and insert the 32-bit integer i into dst at the location
+// specified by imm8.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_insert_epi32
+// FORCE_INLINE __m128i _mm_insert_epi32(__m128i a, int b,
+//                                       __constrange(0,4) int imm)
+#define _mm_insert_epi32(a, b, imm) \
+    vreinterpretq_m128i_s32(        \
+        vsetq_lane_s32((b), vreinterpretq_s32_m128i(a), (imm)))
+
+// Copy a to dst, and insert the 64-bit integer i into dst at the location
+// specified by imm8.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_insert_epi64
+// FORCE_INLINE __m128i _mm_insert_epi64(__m128i a, __int64 b,
+//                                       __constrange(0,2) int imm)
+#define _mm_insert_epi64(a, b, imm) \
+    vreinterpretq_m128i_s64(        \
+        vsetq_lane_s64((b), vreinterpretq_s64_m128i(a), (imm)))
+
+// Copy a to dst, and insert the lower 8-bit integer from i into dst at the
+// location specified by imm8.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_insert_epi8
+// FORCE_INLINE __m128i _mm_insert_epi8(__m128i a, int b,
+//                                      __constrange(0,16) int imm)
+#define _mm_insert_epi8(a, b, imm) \
+    vreinterpretq_m128i_s8(vsetq_lane_s8((b), vreinterpretq_s8_m128i(a), (imm)))
+
+// Copy a to tmp, then insert a single-precision (32-bit) floating-point
+// element from b into tmp using the control in imm8. Store tmp to dst using
+// the mask in imm8 (elements are zeroed out when the corresponding bit is set).
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=insert_ps
+#define _mm_insert_ps(a, b, imm8)                                            \
+    _sse2neon_define2(                                                       \
+        __m128, a, b,                                                        \
+        float32x4_t tmp1 =                                                   \
+            vsetq_lane_f32(vgetq_lane_f32(_b, (imm8 >> 6) & 0x3),            \
+                           vreinterpretq_f32_m128(_a), 0);                   \
+        float32x4_t tmp2 =                                                   \
+            vsetq_lane_f32(vgetq_lane_f32(tmp1, 0),                          \
+                           vreinterpretq_f32_m128(_a), ((imm8 >> 4) & 0x3)); \
+        const uint32_t data[4] =                                             \
+            _sse2neon_init(((imm8) & (1 << 0)) ? UINT32_MAX : 0,             \
+                           ((imm8) & (1 << 1)) ? UINT32_MAX : 0,             \
+                           ((imm8) & (1 << 2)) ? UINT32_MAX : 0,             \
+                           ((imm8) & (1 << 3)) ? UINT32_MAX : 0);            \
+        uint32x4_t mask = vld1q_u32(data);                                   \
+        float32x4_t all_zeros = vdupq_n_f32(0);                              \
+                                                                             \
+        _sse2neon_return(vreinterpretq_m128_f32(                             \
+            vbslq_f32(mask, all_zeros, vreinterpretq_f32_m128(tmp2))));)
+
+// Compare packed signed 32-bit integers in a and b, and store packed maximum
+// values in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_max_epi32
+FORCE_INLINE __m128i _mm_max_epi32(__m128i a, __m128i b)
+{
+    return vreinterpretq_m128i_s32(
+        vmaxq_s32(vreinterpretq_s32_m128i(a), vreinterpretq_s32_m128i(b)));
+}
+
+// Compare packed signed 8-bit integers in a and b, and store packed maximum
+// values in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_max_epi8
+FORCE_INLINE __m128i _mm_max_epi8(__m128i a, __m128i b)
+{
+    return vreinterpretq_m128i_s8(
+        vmaxq_s8(vreinterpretq_s8_m128i(a), vreinterpretq_s8_m128i(b)));
+}
+
+// Compare packed unsigned 16-bit integers in a and b, and store packed maximum
+// values in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_max_epu16
+FORCE_INLINE __m128i _mm_max_epu16(__m128i a, __m128i b)
+{
+    return vreinterpretq_m128i_u16(
+        vmaxq_u16(vreinterpretq_u16_m128i(a), vreinterpretq_u16_m128i(b)));
+}
+
+// Compare packed unsigned 32-bit integers in a and b, and store packed maximum
+// values in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_max_epu32
+FORCE_INLINE __m128i _mm_max_epu32(__m128i a, __m128i b)
+{
+    return vreinterpretq_m128i_u32(
+        vmaxq_u32(vreinterpretq_u32_m128i(a), vreinterpretq_u32_m128i(b)));
+}
+
+// Compare packed signed 32-bit integers in a and b, and store packed minimum
+// values in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_min_epi32
+FORCE_INLINE __m128i _mm_min_epi32(__m128i a, __m128i b)
+{
+    return vreinterpretq_m128i_s32(
+        vminq_s32(vreinterpretq_s32_m128i(a), vreinterpretq_s32_m128i(b)));
+}
+
+// Compare packed signed 8-bit integers in a and b, and store packed minimum
+// values in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_min_epi8
+FORCE_INLINE __m128i _mm_min_epi8(__m128i a, __m128i b)
+{
+    return vreinterpretq_m128i_s8(
+        vminq_s8(vreinterpretq_s8_m128i(a), vreinterpretq_s8_m128i(b)));
+}
+
+// Compare packed unsigned 16-bit integers in a and b, and store packed minimum
+// values in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_min_epu16
+FORCE_INLINE __m128i _mm_min_epu16(__m128i a, __m128i b)
+{
+    return vreinterpretq_m128i_u16(
+        vminq_u16(vreinterpretq_u16_m128i(a), vreinterpretq_u16_m128i(b)));
+}
+
+// Compare packed unsigned 32-bit integers in a and b, and store packed minimum
+// values in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_max_epu32
+FORCE_INLINE __m128i _mm_min_epu32(__m128i a, __m128i b)
+{
+    return vreinterpretq_m128i_u32(
+        vminq_u32(vreinterpretq_u32_m128i(a), vreinterpretq_u32_m128i(b)));
+}
+
+// Horizontally compute the minimum amongst the packed unsigned 16-bit integers
+// in a, store the minimum and index in dst, and zero the remaining bits in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_minpos_epu16
+FORCE_INLINE __m128i _mm_minpos_epu16(__m128i a)
+{
+    __m128i dst;
+    uint16_t min, idx = 0;
+#if defined(__aarch64__) || defined(_M_ARM64)
+    // Find the minimum value
+    min = vminvq_u16(vreinterpretq_u16_m128i(a));
+
+    // Get the index of the minimum value
+    static const uint16_t idxv[] = {0, 1, 2, 3, 4, 5, 6, 7};
+    uint16x8_t minv = vdupq_n_u16(min);
+    uint16x8_t cmeq = vceqq_u16(minv, vreinterpretq_u16_m128i(a));
+    idx = vminvq_u16(vornq_u16(vld1q_u16(idxv), cmeq));
+#else
+    // Find the minimum value
+    __m64 tmp;
+    tmp = vreinterpret_m64_u16(
+        vmin_u16(vget_low_u16(vreinterpretq_u16_m128i(a)),
+                 vget_high_u16(vreinterpretq_u16_m128i(a))));
+    tmp = vreinterpret_m64_u16(
+        vpmin_u16(vreinterpret_u16_m64(tmp), vreinterpret_u16_m64(tmp)));
+    tmp = vreinterpret_m64_u16(
+        vpmin_u16(vreinterpret_u16_m64(tmp), vreinterpret_u16_m64(tmp)));
+    min = vget_lane_u16(vreinterpret_u16_m64(tmp), 0);
+    // Get the index of the minimum value
+    int i;
+    for (i = 0; i < 8; i++) {
+        if (min == vgetq_lane_u16(vreinterpretq_u16_m128i(a), 0)) {
+            idx = (uint16_t) i;
+            break;
+        }
+        a = _mm_srli_si128(a, 2);
+    }
+#endif
+    // Generate result
+    dst = _mm_setzero_si128();
+    dst = vreinterpretq_m128i_u16(
+        vsetq_lane_u16(min, vreinterpretq_u16_m128i(dst), 0));
+    dst = vreinterpretq_m128i_u16(
+        vsetq_lane_u16(idx, vreinterpretq_u16_m128i(dst), 1));
+    return dst;
+}
+
+// Compute the sum of absolute differences (SADs) of quadruplets of unsigned
+// 8-bit integers in a compared to those in b, and store the 16-bit results in
+// dst. Eight SADs are performed using one quadruplet from b and eight
+// quadruplets from a. One quadruplet is selected from b starting at on the
+// offset specified in imm8. Eight quadruplets are formed from sequential 8-bit
+// integers selected from a starting at the offset specified in imm8.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_mpsadbw_epu8
+FORCE_INLINE __m128i _mm_mpsadbw_epu8(__m128i a, __m128i b, const int imm)
+{
+    uint8x16_t _a, _b;
+
+    switch (imm & 0x4) {
+    case 0:
+        // do nothing
+        _a = vreinterpretq_u8_m128i(a);
+        break;
+    case 4:
+        _a = vreinterpretq_u8_u32(vextq_u32(vreinterpretq_u32_m128i(a),
+                                            vreinterpretq_u32_m128i(a), 1));
+        break;
+    default:
+#if defined(__GNUC__) || defined(__clang__)
+        __builtin_unreachable();
+#elif defined(_MSC_VER)
+        __assume(0);
+#endif
+        break;
+    }
+
+    switch (imm & 0x3) {
+    case 0:
+        _b = vreinterpretq_u8_u32(
+            vdupq_n_u32(vgetq_lane_u32(vreinterpretq_u32_m128i(b), 0)));
+        break;
+    case 1:
+        _b = vreinterpretq_u8_u32(
+            vdupq_n_u32(vgetq_lane_u32(vreinterpretq_u32_m128i(b), 1)));
+        break;
+    case 2:
+        _b = vreinterpretq_u8_u32(
+            vdupq_n_u32(vgetq_lane_u32(vreinterpretq_u32_m128i(b), 2)));
+        break;
+    case 3:
+        _b = vreinterpretq_u8_u32(
+            vdupq_n_u32(vgetq_lane_u32(vreinterpretq_u32_m128i(b), 3)));
+        break;
+    default:
+#if defined(__GNUC__) || defined(__clang__)
+        __builtin_unreachable();
+#elif defined(_MSC_VER)
+        __assume(0);
+#endif
+        break;
+    }
+
+    int16x8_t c04, c15, c26, c37;
+    uint8x8_t low_b = vget_low_u8(_b);
+    c04 = vreinterpretq_s16_u16(vabdl_u8(vget_low_u8(_a), low_b));
+    uint8x16_t _a_1 = vextq_u8(_a, _a, 1);
+    c15 = vreinterpretq_s16_u16(vabdl_u8(vget_low_u8(_a_1), low_b));
+    uint8x16_t _a_2 = vextq_u8(_a, _a, 2);
+    c26 = vreinterpretq_s16_u16(vabdl_u8(vget_low_u8(_a_2), low_b));
+    uint8x16_t _a_3 = vextq_u8(_a, _a, 3);
+    c37 = vreinterpretq_s16_u16(vabdl_u8(vget_low_u8(_a_3), low_b));
+#if defined(__aarch64__) || defined(_M_ARM64)
+    // |0|4|2|6|
+    c04 = vpaddq_s16(c04, c26);
+    // |1|5|3|7|
+    c15 = vpaddq_s16(c15, c37);
+
+    int32x4_t trn1_c =
+        vtrn1q_s32(vreinterpretq_s32_s16(c04), vreinterpretq_s32_s16(c15));
+    int32x4_t trn2_c =
+        vtrn2q_s32(vreinterpretq_s32_s16(c04), vreinterpretq_s32_s16(c15));
+    return vreinterpretq_m128i_s16(vpaddq_s16(vreinterpretq_s16_s32(trn1_c),
+                                              vreinterpretq_s16_s32(trn2_c)));
+#else
+    int16x4_t c01, c23, c45, c67;
+    c01 = vpadd_s16(vget_low_s16(c04), vget_low_s16(c15));
+    c23 = vpadd_s16(vget_low_s16(c26), vget_low_s16(c37));
+    c45 = vpadd_s16(vget_high_s16(c04), vget_high_s16(c15));
+    c67 = vpadd_s16(vget_high_s16(c26), vget_high_s16(c37));
+
+    return vreinterpretq_m128i_s16(
+        vcombine_s16(vpadd_s16(c01, c23), vpadd_s16(c45, c67)));
+#endif
+}
+
+// Multiply the low signed 32-bit integers from each packed 64-bit element in
+// a and b, and store the signed 64-bit results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_mul_epi32
+FORCE_INLINE __m128i _mm_mul_epi32(__m128i a, __m128i b)
+{
+    // vmull_s32 upcasts instead of masking, so we downcast.
+    int32x2_t a_lo = vmovn_s64(vreinterpretq_s64_m128i(a));
+    int32x2_t b_lo = vmovn_s64(vreinterpretq_s64_m128i(b));
+    return vreinterpretq_m128i_s64(vmull_s32(a_lo, b_lo));
+}
+
+// Multiply the packed 32-bit integers in a and b, producing intermediate 64-bit
+// integers, and store the low 32 bits of the intermediate integers in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_mullo_epi32
+FORCE_INLINE __m128i _mm_mullo_epi32(__m128i a, __m128i b)
+{
+    return vreinterpretq_m128i_s32(
+        vmulq_s32(vreinterpretq_s32_m128i(a), vreinterpretq_s32_m128i(b)));
+}
+
+// Convert packed signed 32-bit integers from a and b to packed 16-bit integers
+// using unsigned saturation, and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_packus_epi32
+FORCE_INLINE __m128i _mm_packus_epi32(__m128i a, __m128i b)
+{
+    return vreinterpretq_m128i_u16(
+        vcombine_u16(vqmovun_s32(vreinterpretq_s32_m128i(a)),
+                     vqmovun_s32(vreinterpretq_s32_m128i(b))));
+}
+
+// Round the packed double-precision (64-bit) floating-point elements in a using
+// the rounding parameter, and store the results as packed double-precision
+// floating-point elements in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_round_pd
+FORCE_INLINE_OPTNONE __m128d _mm_round_pd(__m128d a, int rounding)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    switch (rounding) {
+    case (_MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC):
+        return vreinterpretq_m128d_f64(vrndnq_f64(vreinterpretq_f64_m128d(a)));
+    case (_MM_FROUND_TO_NEG_INF | _MM_FROUND_NO_EXC):
+        return _mm_floor_pd(a);
+    case (_MM_FROUND_TO_POS_INF | _MM_FROUND_NO_EXC):
+        return _mm_ceil_pd(a);
+    case (_MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC):
+        return vreinterpretq_m128d_f64(vrndq_f64(vreinterpretq_f64_m128d(a)));
+    default:  //_MM_FROUND_CUR_DIRECTION
+        return vreinterpretq_m128d_f64(vrndiq_f64(vreinterpretq_f64_m128d(a)));
+    }
+#else
+    double *v_double = (double *) &a;
+
+    if (rounding == (_MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC) ||
+        (rounding == _MM_FROUND_CUR_DIRECTION &&
+         _MM_GET_ROUNDING_MODE() == _MM_ROUND_NEAREST)) {
+        double res[2], tmp;
+        for (int i = 0; i < 2; i++) {
+            tmp = (v_double[i] < 0) ? -v_double[i] : v_double[i];
+            double roundDown = floor(tmp);  // Round down value
+            double roundUp = ceil(tmp);     // Round up value
+            double diffDown = tmp - roundDown;
+            double diffUp = roundUp - tmp;
+            if (diffDown < diffUp) {
+                /* If it's closer to the round down value, then use it */
+                res[i] = roundDown;
+            } else if (diffDown > diffUp) {
+                /* If it's closer to the round up value, then use it */
+                res[i] = roundUp;
+            } else {
+                /* If it's equidistant between round up and round down value,
+                 * pick the one which is an even number */
+                double half = roundDown / 2;
+                if (half != floor(half)) {
+                    /* If the round down value is odd, return the round up value
+                     */
+                    res[i] = roundUp;
+                } else {
+                    /* If the round up value is odd, return the round down value
+                     */
+                    res[i] = roundDown;
+                }
+            }
+            res[i] = (v_double[i] < 0) ? -res[i] : res[i];
+        }
+        return _mm_set_pd(res[1], res[0]);
+    } else if (rounding == (_MM_FROUND_TO_NEG_INF | _MM_FROUND_NO_EXC) ||
+               (rounding == _MM_FROUND_CUR_DIRECTION &&
+                _MM_GET_ROUNDING_MODE() == _MM_ROUND_DOWN)) {
+        return _mm_floor_pd(a);
+    } else if (rounding == (_MM_FROUND_TO_POS_INF | _MM_FROUND_NO_EXC) ||
+               (rounding == _MM_FROUND_CUR_DIRECTION &&
+                _MM_GET_ROUNDING_MODE() == _MM_ROUND_UP)) {
+        return _mm_ceil_pd(a);
+    }
+    return _mm_set_pd(v_double[1] > 0 ? floor(v_double[1]) : ceil(v_double[1]),
+                      v_double[0] > 0 ? floor(v_double[0]) : ceil(v_double[0]));
+#endif
+}
+
+// Round the packed single-precision (32-bit) floating-point elements in a using
+// the rounding parameter, and store the results as packed single-precision
+// floating-point elements in dst.
+// software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_round_ps
+FORCE_INLINE_OPTNONE __m128 _mm_round_ps(__m128 a, int rounding)
+{
+#if (defined(__aarch64__) || defined(_M_ARM64)) || \
+    defined(__ARM_FEATURE_DIRECTED_ROUNDING)
+    switch (rounding) {
+    case (_MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC):
+        return vreinterpretq_m128_f32(vrndnq_f32(vreinterpretq_f32_m128(a)));
+    case (_MM_FROUND_TO_NEG_INF | _MM_FROUND_NO_EXC):
+        return _mm_floor_ps(a);
+    case (_MM_FROUND_TO_POS_INF | _MM_FROUND_NO_EXC):
+        return _mm_ceil_ps(a);
+    case (_MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC):
+        return vreinterpretq_m128_f32(vrndq_f32(vreinterpretq_f32_m128(a)));
+    default:  //_MM_FROUND_CUR_DIRECTION
+        return vreinterpretq_m128_f32(vrndiq_f32(vreinterpretq_f32_m128(a)));
+    }
+#else
+    float *v_float = (float *) &a;
+
+    if (rounding == (_MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC) ||
+        (rounding == _MM_FROUND_CUR_DIRECTION &&
+         _MM_GET_ROUNDING_MODE() == _MM_ROUND_NEAREST)) {
+        uint32x4_t signmask = vdupq_n_u32(0x80000000);
+        float32x4_t half = vbslq_f32(signmask, vreinterpretq_f32_m128(a),
+                                     vdupq_n_f32(0.5f)); /* +/- 0.5 */
+        int32x4_t r_normal = vcvtq_s32_f32(vaddq_f32(
+            vreinterpretq_f32_m128(a), half)); /* round to integer: [a + 0.5]*/
+        int32x4_t r_trunc = vcvtq_s32_f32(
+            vreinterpretq_f32_m128(a)); /* truncate to integer: [a] */
+        int32x4_t plusone = vreinterpretq_s32_u32(vshrq_n_u32(
+            vreinterpretq_u32_s32(vnegq_s32(r_trunc)), 31)); /* 1 or 0 */
+        int32x4_t r_even = vbicq_s32(vaddq_s32(r_trunc, plusone),
+                                     vdupq_n_s32(1)); /* ([a] + {0,1}) & ~1 */
+        float32x4_t delta = vsubq_f32(
+            vreinterpretq_f32_m128(a),
+            vcvtq_f32_s32(r_trunc)); /* compute delta: delta = (a - [a]) */
+        uint32x4_t is_delta_half =
+            vceqq_f32(delta, half); /* delta == +/- 0.5 */
+        return vreinterpretq_m128_f32(
+            vcvtq_f32_s32(vbslq_s32(is_delta_half, r_even, r_normal)));
+    } else if (rounding == (_MM_FROUND_TO_NEG_INF | _MM_FROUND_NO_EXC) ||
+               (rounding == _MM_FROUND_CUR_DIRECTION &&
+                _MM_GET_ROUNDING_MODE() == _MM_ROUND_DOWN)) {
+        return _mm_floor_ps(a);
+    } else if (rounding == (_MM_FROUND_TO_POS_INF | _MM_FROUND_NO_EXC) ||
+               (rounding == _MM_FROUND_CUR_DIRECTION &&
+                _MM_GET_ROUNDING_MODE() == _MM_ROUND_UP)) {
+        return _mm_ceil_ps(a);
+    }
+    return _mm_set_ps(v_float[3] > 0 ? floorf(v_float[3]) : ceilf(v_float[3]),
+                      v_float[2] > 0 ? floorf(v_float[2]) : ceilf(v_float[2]),
+                      v_float[1] > 0 ? floorf(v_float[1]) : ceilf(v_float[1]),
+                      v_float[0] > 0 ? floorf(v_float[0]) : ceilf(v_float[0]));
+#endif
+}
+
+// Round the lower double-precision (64-bit) floating-point element in b using
+// the rounding parameter, store the result as a double-precision floating-point
+// element in the lower element of dst, and copy the upper element from a to the
+// upper element of dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_round_sd
+FORCE_INLINE __m128d _mm_round_sd(__m128d a, __m128d b, int rounding)
+{
+    return _mm_move_sd(a, _mm_round_pd(b, rounding));
+}
+
+// Round the lower single-precision (32-bit) floating-point element in b using
+// the rounding parameter, store the result as a single-precision floating-point
+// element in the lower element of dst, and copy the upper 3 packed elements
+// from a to the upper elements of dst. Rounding is done according to the
+// rounding[3:0] parameter, which can be one of:
+//     (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and
+//     suppress exceptions
+//     (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and
+//     suppress exceptions
+//     (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress
+//     exceptions
+//     (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress
+//     exceptions _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see
+//     _MM_SET_ROUNDING_MODE
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_round_ss
+FORCE_INLINE __m128 _mm_round_ss(__m128 a, __m128 b, int rounding)
+{
+    return _mm_move_ss(a, _mm_round_ps(b, rounding));
+}
+
+// Load 128-bits of integer data from memory into dst using a non-temporal
+// memory hint. mem_addr must be aligned on a 16-byte boundary or a
+// general-protection exception may be generated.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_stream_load_si128
+FORCE_INLINE __m128i _mm_stream_load_si128(__m128i *p)
+{
+#if __has_builtin(__builtin_nontemporal_store)
+    return __builtin_nontemporal_load(p);
+#else
+    return vreinterpretq_m128i_s64(vld1q_s64((int64_t *) p));
+#endif
+}
+
+// Compute the bitwise NOT of a and then AND with a 128-bit vector containing
+// all 1's, and return 1 if the result is zero, otherwise return 0.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_test_all_ones
+FORCE_INLINE int _mm_test_all_ones(__m128i a)
+{
+    return (uint64_t) (vgetq_lane_s64(a, 0) & vgetq_lane_s64(a, 1)) ==
+           ~(uint64_t) 0;
+}
+
+// Compute the bitwise AND of 128 bits (representing integer data) in a and
+// mask, and return 1 if the result is zero, otherwise return 0.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_test_all_zeros
+FORCE_INLINE int _mm_test_all_zeros(__m128i a, __m128i mask)
+{
+    int64x2_t a_and_mask =
+        vandq_s64(vreinterpretq_s64_m128i(a), vreinterpretq_s64_m128i(mask));
+    return !(vgetq_lane_s64(a_and_mask, 0) | vgetq_lane_s64(a_and_mask, 1));
+}
+
+// Compute the bitwise AND of 128 bits (representing integer data) in a and
+// mask, and set ZF to 1 if the result is zero, otherwise set ZF to 0. Compute
+// the bitwise NOT of a and then AND with mask, and set CF to 1 if the result is
+// zero, otherwise set CF to 0. Return 1 if both the ZF and CF values are zero,
+// otherwise return 0.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=mm_test_mix_ones_zero
+// Note: Argument names may be wrong in the Intel intrinsics guide.
+FORCE_INLINE int _mm_test_mix_ones_zeros(__m128i a, __m128i mask)
+{
+    uint64x2_t v = vreinterpretq_u64_m128i(a);
+    uint64x2_t m = vreinterpretq_u64_m128i(mask);
+
+    // find ones (set-bits) and zeros (clear-bits) under clip mask
+    uint64x2_t ones = vandq_u64(m, v);
+    uint64x2_t zeros = vbicq_u64(m, v);
+
+    // If both 128-bit variables are populated (non-zero) then return 1.
+    // For comparison purposes, first compact each var down to 32-bits.
+    uint32x2_t reduced = vpmax_u32(vqmovn_u64(ones), vqmovn_u64(zeros));
+
+    // if folding minimum is non-zero then both vars must be non-zero
+    return (vget_lane_u32(vpmin_u32(reduced, reduced), 0) != 0);
+}
+
+// Compute the bitwise AND of 128 bits (representing integer data) in a and b,
+// and set ZF to 1 if the result is zero, otherwise set ZF to 0. Compute the
+// bitwise NOT of a and then AND with b, and set CF to 1 if the result is zero,
+// otherwise set CF to 0. Return the CF value.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_testc_si128
+FORCE_INLINE int _mm_testc_si128(__m128i a, __m128i b)
+{
+    int64x2_t s64 =
+        vbicq_s64(vreinterpretq_s64_m128i(b), vreinterpretq_s64_m128i(a));
+    return !(vgetq_lane_s64(s64, 0) | vgetq_lane_s64(s64, 1));
+}
+
+// Compute the bitwise AND of 128 bits (representing integer data) in a and b,
+// and set ZF to 1 if the result is zero, otherwise set ZF to 0. Compute the
+// bitwise NOT of a and then AND with b, and set CF to 1 if the result is zero,
+// otherwise set CF to 0. Return 1 if both the ZF and CF values are zero,
+// otherwise return 0.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_testnzc_si128
+#define _mm_testnzc_si128(a, b) _mm_test_mix_ones_zeros(a, b)
+
+// Compute the bitwise AND of 128 bits (representing integer data) in a and b,
+// and set ZF to 1 if the result is zero, otherwise set ZF to 0. Compute the
+// bitwise NOT of a and then AND with b, and set CF to 1 if the result is zero,
+// otherwise set CF to 0. Return the ZF value.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_testz_si128
+FORCE_INLINE int _mm_testz_si128(__m128i a, __m128i b)
+{
+    int64x2_t s64 =
+        vandq_s64(vreinterpretq_s64_m128i(a), vreinterpretq_s64_m128i(b));
+    return !(vgetq_lane_s64(s64, 0) | vgetq_lane_s64(s64, 1));
+}
+
+/* SSE4.2 */
+
+static const uint16_t ALIGN_STRUCT(16) _sse2neon_cmpestr_mask16b[8] = {
+    0x01, 0x02, 0x04, 0x08, 0x10, 0x20, 0x40, 0x80,
+};
+static const uint8_t ALIGN_STRUCT(16) _sse2neon_cmpestr_mask8b[16] = {
+    0x01, 0x02, 0x04, 0x08, 0x10, 0x20, 0x40, 0x80,
+    0x01, 0x02, 0x04, 0x08, 0x10, 0x20, 0x40, 0x80,
+};
+
+/* specify the source data format */
+#define _SIDD_UBYTE_OPS 0x00 /* unsigned 8-bit characters */
+#define _SIDD_UWORD_OPS 0x01 /* unsigned 16-bit characters */
+#define _SIDD_SBYTE_OPS 0x02 /* signed 8-bit characters */
+#define _SIDD_SWORD_OPS 0x03 /* signed 16-bit characters */
+
+/* specify the comparison operation */
+#define _SIDD_CMP_EQUAL_ANY 0x00     /* compare equal any: strchr */
+#define _SIDD_CMP_RANGES 0x04        /* compare ranges */
+#define _SIDD_CMP_EQUAL_EACH 0x08    /* compare equal each: strcmp */
+#define _SIDD_CMP_EQUAL_ORDERED 0x0C /* compare equal ordered */
+
+/* specify the polarity */
+#define _SIDD_POSITIVE_POLARITY 0x00
+#define _SIDD_MASKED_POSITIVE_POLARITY 0x20
+#define _SIDD_NEGATIVE_POLARITY 0x10 /* negate results */
+#define _SIDD_MASKED_NEGATIVE_POLARITY \
+    0x30 /* negate results only before end of string */
+
+/* specify the output selection in _mm_cmpXstri */
+#define _SIDD_LEAST_SIGNIFICANT 0x00
+#define _SIDD_MOST_SIGNIFICANT 0x40
+
+/* specify the output selection in _mm_cmpXstrm */
+#define _SIDD_BIT_MASK 0x00
+#define _SIDD_UNIT_MASK 0x40
+
+/* Pattern Matching for C macros.
+ * https://github.com/pfultz2/Cloak/wiki/C-Preprocessor-tricks,-tips,-and-idioms
+ */
+
+/* catenate */
+#define SSE2NEON_PRIMITIVE_CAT(a, ...) a##__VA_ARGS__
+#define SSE2NEON_CAT(a, b) SSE2NEON_PRIMITIVE_CAT(a, b)
+
+#define SSE2NEON_IIF(c) SSE2NEON_PRIMITIVE_CAT(SSE2NEON_IIF_, c)
+/* run the 2nd parameter */
+#define SSE2NEON_IIF_0(t, ...) __VA_ARGS__
+/* run the 1st parameter */
+#define SSE2NEON_IIF_1(t, ...) t
+
+#define SSE2NEON_COMPL(b) SSE2NEON_PRIMITIVE_CAT(SSE2NEON_COMPL_, b)
+#define SSE2NEON_COMPL_0 1
+#define SSE2NEON_COMPL_1 0
+
+#define SSE2NEON_DEC(x) SSE2NEON_PRIMITIVE_CAT(SSE2NEON_DEC_, x)
+#define SSE2NEON_DEC_1 0
+#define SSE2NEON_DEC_2 1
+#define SSE2NEON_DEC_3 2
+#define SSE2NEON_DEC_4 3
+#define SSE2NEON_DEC_5 4
+#define SSE2NEON_DEC_6 5
+#define SSE2NEON_DEC_7 6
+#define SSE2NEON_DEC_8 7
+#define SSE2NEON_DEC_9 8
+#define SSE2NEON_DEC_10 9
+#define SSE2NEON_DEC_11 10
+#define SSE2NEON_DEC_12 11
+#define SSE2NEON_DEC_13 12
+#define SSE2NEON_DEC_14 13
+#define SSE2NEON_DEC_15 14
+#define SSE2NEON_DEC_16 15
+
+/* detection */
+#define SSE2NEON_CHECK_N(x, n, ...) n
+#define SSE2NEON_CHECK(...) SSE2NEON_CHECK_N(__VA_ARGS__, 0, )
+#define SSE2NEON_PROBE(x) x, 1,
+
+#define SSE2NEON_NOT(x) SSE2NEON_CHECK(SSE2NEON_PRIMITIVE_CAT(SSE2NEON_NOT_, x))
+#define SSE2NEON_NOT_0 SSE2NEON_PROBE(~)
+
+#define SSE2NEON_BOOL(x) SSE2NEON_COMPL(SSE2NEON_NOT(x))
+#define SSE2NEON_IF(c) SSE2NEON_IIF(SSE2NEON_BOOL(c))
+
+#define SSE2NEON_EAT(...)
+#define SSE2NEON_EXPAND(...) __VA_ARGS__
+#define SSE2NEON_WHEN(c) SSE2NEON_IF(c)(SSE2NEON_EXPAND, SSE2NEON_EAT)
+
+/* recursion */
+/* deferred expression */
+#define SSE2NEON_EMPTY()
+#define SSE2NEON_DEFER(id) id SSE2NEON_EMPTY()
+#define SSE2NEON_OBSTRUCT(...) __VA_ARGS__ SSE2NEON_DEFER(SSE2NEON_EMPTY)()
+#define SSE2NEON_EXPAND(...) __VA_ARGS__
+
+#define SSE2NEON_EVAL(...) \
+    SSE2NEON_EVAL1(SSE2NEON_EVAL1(SSE2NEON_EVAL1(__VA_ARGS__)))
+#define SSE2NEON_EVAL1(...) \
+    SSE2NEON_EVAL2(SSE2NEON_EVAL2(SSE2NEON_EVAL2(__VA_ARGS__)))
+#define SSE2NEON_EVAL2(...) \
+    SSE2NEON_EVAL3(SSE2NEON_EVAL3(SSE2NEON_EVAL3(__VA_ARGS__)))
+#define SSE2NEON_EVAL3(...) __VA_ARGS__
+
+#define SSE2NEON_REPEAT(count, macro, ...)                         \
+    SSE2NEON_WHEN(count)                                           \
+    (SSE2NEON_OBSTRUCT(SSE2NEON_REPEAT_INDIRECT)()(                \
+        SSE2NEON_DEC(count), macro,                                \
+        __VA_ARGS__) SSE2NEON_OBSTRUCT(macro)(SSE2NEON_DEC(count), \
+                                              __VA_ARGS__))
+#define SSE2NEON_REPEAT_INDIRECT() SSE2NEON_REPEAT
+
+#define SSE2NEON_SIZE_OF_byte 8
+#define SSE2NEON_NUMBER_OF_LANES_byte 16
+#define SSE2NEON_SIZE_OF_word 16
+#define SSE2NEON_NUMBER_OF_LANES_word 8
+
+#define SSE2NEON_COMPARE_EQUAL_THEN_FILL_LANE(i, type)                         \
+    mtx[i] = vreinterpretq_m128i_##type(vceqq_##type(                          \
+        vdupq_n_##type(vgetq_lane_##type(vreinterpretq_##type##_m128i(b), i)), \
+        vreinterpretq_##type##_m128i(a)));
+
+#define SSE2NEON_FILL_LANE(i, type) \
+    vec_b[i] =                      \
+        vdupq_n_##type(vgetq_lane_##type(vreinterpretq_##type##_m128i(b), i));
+
+#define PCMPSTR_RANGES(a, b, mtx, data_type_prefix, type_prefix, size,        \
+                       number_of_lanes, byte_or_word)                         \
+    do {                                                                      \
+        SSE2NEON_CAT(                                                         \
+            data_type_prefix,                                                 \
+            SSE2NEON_CAT(size,                                                \
+                         SSE2NEON_CAT(x, SSE2NEON_CAT(number_of_lanes, _t)))) \
+        vec_b[number_of_lanes];                                               \
+        __m128i mask = SSE2NEON_IIF(byte_or_word)(                            \
+            vreinterpretq_m128i_u16(vdupq_n_u16(0xff)),                       \
+            vreinterpretq_m128i_u32(vdupq_n_u32(0xffff)));                    \
+        SSE2NEON_EVAL(SSE2NEON_REPEAT(number_of_lanes, SSE2NEON_FILL_LANE,    \
+                                      SSE2NEON_CAT(type_prefix, size)))       \
+        for (int i = 0; i < number_of_lanes; i++) {                           \
+            mtx[i] = SSE2NEON_CAT(vreinterpretq_m128i_u,                      \
+                                  size)(SSE2NEON_CAT(vbslq_u, size)(          \
+                SSE2NEON_CAT(vreinterpretq_u,                                 \
+                             SSE2NEON_CAT(size, _m128i))(mask),               \
+                SSE2NEON_CAT(vcgeq_, SSE2NEON_CAT(type_prefix, size))(        \
+                    vec_b[i],                                                 \
+                    SSE2NEON_CAT(                                             \
+                        vreinterpretq_,                                       \
+                        SSE2NEON_CAT(type_prefix,                             \
+                                     SSE2NEON_CAT(size, _m128i(a))))),        \
+                SSE2NEON_CAT(vcleq_, SSE2NEON_CAT(type_prefix, size))(        \
+                    vec_b[i],                                                 \
+                    SSE2NEON_CAT(                                             \
+                        vreinterpretq_,                                       \
+                        SSE2NEON_CAT(type_prefix,                             \
+                                     SSE2NEON_CAT(size, _m128i(a)))))));      \
+        }                                                                     \
+    } while (0)
+
+#define PCMPSTR_EQ(a, b, mtx, size, number_of_lanes)                         \
+    do {                                                                     \
+        SSE2NEON_EVAL(SSE2NEON_REPEAT(number_of_lanes,                       \
+                                      SSE2NEON_COMPARE_EQUAL_THEN_FILL_LANE, \
+                                      SSE2NEON_CAT(u, size)))                \
+    } while (0)
+
+#define SSE2NEON_CMP_EQUAL_ANY_IMPL(type)                                     \
+    static int _sse2neon_cmp_##type##_equal_any(__m128i a, int la, __m128i b, \
+                                                int lb)                       \
+    {                                                                         \
+        __m128i mtx[16];                                                      \
+        PCMPSTR_EQ(a, b, mtx, SSE2NEON_CAT(SSE2NEON_SIZE_OF_, type),          \
+                   SSE2NEON_CAT(SSE2NEON_NUMBER_OF_LANES_, type));            \
+        return SSE2NEON_CAT(                                                  \
+            _sse2neon_aggregate_equal_any_,                                   \
+            SSE2NEON_CAT(                                                     \
+                SSE2NEON_CAT(SSE2NEON_SIZE_OF_, type),                        \
+                SSE2NEON_CAT(x, SSE2NEON_CAT(SSE2NEON_NUMBER_OF_LANES_,       \
+                                             type))))(la, lb, mtx);           \
+    }
+
+#define SSE2NEON_CMP_RANGES_IMPL(type, data_type, us, byte_or_word)            \
+    static int _sse2neon_cmp_##us##type##_ranges(__m128i a, int la, __m128i b, \
+                                                 int lb)                       \
+    {                                                                          \
+        __m128i mtx[16];                                                       \
+        PCMPSTR_RANGES(                                                        \
+            a, b, mtx, data_type, us, SSE2NEON_CAT(SSE2NEON_SIZE_OF_, type),   \
+            SSE2NEON_CAT(SSE2NEON_NUMBER_OF_LANES_, type), byte_or_word);      \
+        return SSE2NEON_CAT(                                                   \
+            _sse2neon_aggregate_ranges_,                                       \
+            SSE2NEON_CAT(                                                      \
+                SSE2NEON_CAT(SSE2NEON_SIZE_OF_, type),                         \
+                SSE2NEON_CAT(x, SSE2NEON_CAT(SSE2NEON_NUMBER_OF_LANES_,        \
+                                             type))))(la, lb, mtx);            \
+    }
+
+#define SSE2NEON_CMP_EQUAL_ORDERED_IMPL(type)                                  \
+    static int _sse2neon_cmp_##type##_equal_ordered(__m128i a, int la,         \
+                                                    __m128i b, int lb)         \
+    {                                                                          \
+        __m128i mtx[16];                                                       \
+        PCMPSTR_EQ(a, b, mtx, SSE2NEON_CAT(SSE2NEON_SIZE_OF_, type),           \
+                   SSE2NEON_CAT(SSE2NEON_NUMBER_OF_LANES_, type));             \
+        return SSE2NEON_CAT(                                                   \
+            _sse2neon_aggregate_equal_ordered_,                                \
+            SSE2NEON_CAT(                                                      \
+                SSE2NEON_CAT(SSE2NEON_SIZE_OF_, type),                         \
+                SSE2NEON_CAT(x,                                                \
+                             SSE2NEON_CAT(SSE2NEON_NUMBER_OF_LANES_, type))))( \
+            SSE2NEON_CAT(SSE2NEON_NUMBER_OF_LANES_, type), la, lb, mtx);       \
+    }
+
+static int _sse2neon_aggregate_equal_any_8x16(int la, int lb, __m128i mtx[16])
+{
+    int res = 0;
+    int m = (1 << la) - 1;
+    uint8x8_t vec_mask = vld1_u8(_sse2neon_cmpestr_mask8b);
+    uint8x8_t t_lo = vtst_u8(vdup_n_u8(m & 0xff), vec_mask);
+    uint8x8_t t_hi = vtst_u8(vdup_n_u8(m >> 8), vec_mask);
+    uint8x16_t vec = vcombine_u8(t_lo, t_hi);
+    for (int j = 0; j < lb; j++) {
+        mtx[j] = vreinterpretq_m128i_u8(
+            vandq_u8(vec, vreinterpretq_u8_m128i(mtx[j])));
+        mtx[j] = vreinterpretq_m128i_u8(
+            vshrq_n_u8(vreinterpretq_u8_m128i(mtx[j]), 7));
+        int tmp = _sse2neon_vaddvq_u8(vreinterpretq_u8_m128i(mtx[j])) ? 1 : 0;
+        res |= (tmp << j);
+    }
+    return res;
+}
+
+static int _sse2neon_aggregate_equal_any_16x8(int la, int lb, __m128i mtx[16])
+{
+    int res = 0;
+    int m = (1 << la) - 1;
+    uint16x8_t vec =
+        vtstq_u16(vdupq_n_u16(m), vld1q_u16(_sse2neon_cmpestr_mask16b));
+    for (int j = 0; j < lb; j++) {
+        mtx[j] = vreinterpretq_m128i_u16(
+            vandq_u16(vec, vreinterpretq_u16_m128i(mtx[j])));
+        mtx[j] = vreinterpretq_m128i_u16(
+            vshrq_n_u16(vreinterpretq_u16_m128i(mtx[j]), 15));
+        int tmp = _sse2neon_vaddvq_u16(vreinterpretq_u16_m128i(mtx[j])) ? 1 : 0;
+        res |= (tmp << j);
+    }
+    return res;
+}
+
+/* clang-format off */
+#define SSE2NEON_GENERATE_CMP_EQUAL_ANY(prefix) \
+    prefix##IMPL(byte) \
+    prefix##IMPL(word)
+/* clang-format on */
+
+SSE2NEON_GENERATE_CMP_EQUAL_ANY(SSE2NEON_CMP_EQUAL_ANY_)
+
+static int _sse2neon_aggregate_ranges_16x8(int la, int lb, __m128i mtx[16])
+{
+    int res = 0;
+    int m = (1 << la) - 1;
+    uint16x8_t vec =
+        vtstq_u16(vdupq_n_u16(m), vld1q_u16(_sse2neon_cmpestr_mask16b));
+    for (int j = 0; j < lb; j++) {
+        mtx[j] = vreinterpretq_m128i_u16(
+            vandq_u16(vec, vreinterpretq_u16_m128i(mtx[j])));
+        mtx[j] = vreinterpretq_m128i_u16(
+            vshrq_n_u16(vreinterpretq_u16_m128i(mtx[j]), 15));
+        __m128i tmp = vreinterpretq_m128i_u32(
+            vshrq_n_u32(vreinterpretq_u32_m128i(mtx[j]), 16));
+        uint32x4_t vec_res = vandq_u32(vreinterpretq_u32_m128i(mtx[j]),
+                                       vreinterpretq_u32_m128i(tmp));
+#if defined(__aarch64__) || defined(_M_ARM64)
+        int t = vaddvq_u32(vec_res) ? 1 : 0;
+#else
+        uint64x2_t sumh = vpaddlq_u32(vec_res);
+        int t = vgetq_lane_u64(sumh, 0) + vgetq_lane_u64(sumh, 1);
+#endif
+        res |= (t << j);
+    }
+    return res;
+}
+
+static int _sse2neon_aggregate_ranges_8x16(int la, int lb, __m128i mtx[16])
+{
+    int res = 0;
+    int m = (1 << la) - 1;
+    uint8x8_t vec_mask = vld1_u8(_sse2neon_cmpestr_mask8b);
+    uint8x8_t t_lo = vtst_u8(vdup_n_u8(m & 0xff), vec_mask);
+    uint8x8_t t_hi = vtst_u8(vdup_n_u8(m >> 8), vec_mask);
+    uint8x16_t vec = vcombine_u8(t_lo, t_hi);
+    for (int j = 0; j < lb; j++) {
+        mtx[j] = vreinterpretq_m128i_u8(
+            vandq_u8(vec, vreinterpretq_u8_m128i(mtx[j])));
+        mtx[j] = vreinterpretq_m128i_u8(
+            vshrq_n_u8(vreinterpretq_u8_m128i(mtx[j]), 7));
+        __m128i tmp = vreinterpretq_m128i_u16(
+            vshrq_n_u16(vreinterpretq_u16_m128i(mtx[j]), 8));
+        uint16x8_t vec_res = vandq_u16(vreinterpretq_u16_m128i(mtx[j]),
+                                       vreinterpretq_u16_m128i(tmp));
+        int t = _sse2neon_vaddvq_u16(vec_res) ? 1 : 0;
+        res |= (t << j);
+    }
+    return res;
+}
+
+#define SSE2NEON_CMP_RANGES_IS_BYTE 1
+#define SSE2NEON_CMP_RANGES_IS_WORD 0
+
+/* clang-format off */
+#define SSE2NEON_GENERATE_CMP_RANGES(prefix)             \
+    prefix##IMPL(byte, uint, u, prefix##IS_BYTE)         \
+    prefix##IMPL(byte, int, s, prefix##IS_BYTE)          \
+    prefix##IMPL(word, uint, u, prefix##IS_WORD)         \
+    prefix##IMPL(word, int, s, prefix##IS_WORD)
+/* clang-format on */
+
+SSE2NEON_GENERATE_CMP_RANGES(SSE2NEON_CMP_RANGES_)
+
+#undef SSE2NEON_CMP_RANGES_IS_BYTE
+#undef SSE2NEON_CMP_RANGES_IS_WORD
+
+static int _sse2neon_cmp_byte_equal_each(__m128i a, int la, __m128i b, int lb)
+{
+    uint8x16_t mtx =
+        vceqq_u8(vreinterpretq_u8_m128i(a), vreinterpretq_u8_m128i(b));
+    int m0 = (la < lb) ? 0 : ((1 << la) - (1 << lb));
+    int m1 = 0x10000 - (1 << la);
+    int tb = 0x10000 - (1 << lb);
+    uint8x8_t vec_mask, vec0_lo, vec0_hi, vec1_lo, vec1_hi;
+    uint8x8_t tmp_lo, tmp_hi, res_lo, res_hi;
+    vec_mask = vld1_u8(_sse2neon_cmpestr_mask8b);
+    vec0_lo = vtst_u8(vdup_n_u8(m0), vec_mask);
+    vec0_hi = vtst_u8(vdup_n_u8(m0 >> 8), vec_mask);
+    vec1_lo = vtst_u8(vdup_n_u8(m1), vec_mask);
+    vec1_hi = vtst_u8(vdup_n_u8(m1 >> 8), vec_mask);
+    tmp_lo = vtst_u8(vdup_n_u8(tb), vec_mask);
+    tmp_hi = vtst_u8(vdup_n_u8(tb >> 8), vec_mask);
+
+    res_lo = vbsl_u8(vec0_lo, vdup_n_u8(0), vget_low_u8(mtx));
+    res_hi = vbsl_u8(vec0_hi, vdup_n_u8(0), vget_high_u8(mtx));
+    res_lo = vbsl_u8(vec1_lo, tmp_lo, res_lo);
+    res_hi = vbsl_u8(vec1_hi, tmp_hi, res_hi);
+    res_lo = vand_u8(res_lo, vec_mask);
+    res_hi = vand_u8(res_hi, vec_mask);
+
+    int res = _sse2neon_vaddv_u8(res_lo) + (_sse2neon_vaddv_u8(res_hi) << 8);
+    return res;
+}
+
+static int _sse2neon_cmp_word_equal_each(__m128i a, int la, __m128i b, int lb)
+{
+    uint16x8_t mtx =
+        vceqq_u16(vreinterpretq_u16_m128i(a), vreinterpretq_u16_m128i(b));
+    int m0 = (la < lb) ? 0 : ((1 << la) - (1 << lb));
+    int m1 = 0x100 - (1 << la);
+    int tb = 0x100 - (1 << lb);
+    uint16x8_t vec_mask = vld1q_u16(_sse2neon_cmpestr_mask16b);
+    uint16x8_t vec0 = vtstq_u16(vdupq_n_u16(m0), vec_mask);
+    uint16x8_t vec1 = vtstq_u16(vdupq_n_u16(m1), vec_mask);
+    uint16x8_t tmp = vtstq_u16(vdupq_n_u16(tb), vec_mask);
+    mtx = vbslq_u16(vec0, vdupq_n_u16(0), mtx);
+    mtx = vbslq_u16(vec1, tmp, mtx);
+    mtx = vandq_u16(mtx, vec_mask);
+    return _sse2neon_vaddvq_u16(mtx);
+}
+
+#define SSE2NEON_AGGREGATE_EQUAL_ORDER_IS_UBYTE 1
+#define SSE2NEON_AGGREGATE_EQUAL_ORDER_IS_UWORD 0
+
+#define SSE2NEON_AGGREGATE_EQUAL_ORDER_IMPL(size, number_of_lanes, data_type)  \
+    static int _sse2neon_aggregate_equal_ordered_##size##x##number_of_lanes(   \
+        int bound, int la, int lb, __m128i mtx[16])                            \
+    {                                                                          \
+        int res = 0;                                                           \
+        int m1 = SSE2NEON_IIF(data_type)(0x10000, 0x100) - (1 << la);          \
+        uint##size##x8_t vec_mask = SSE2NEON_IIF(data_type)(                   \
+            vld1_u##size(_sse2neon_cmpestr_mask##size##b),                     \
+            vld1q_u##size(_sse2neon_cmpestr_mask##size##b));                   \
+        uint##size##x##number_of_lanes##_t vec1 = SSE2NEON_IIF(data_type)(     \
+            vcombine_u##size(vtst_u##size(vdup_n_u##size(m1), vec_mask),       \
+                             vtst_u##size(vdup_n_u##size(m1 >> 8), vec_mask)), \
+            vtstq_u##size(vdupq_n_u##size(m1), vec_mask));                     \
+        uint##size##x##number_of_lanes##_t vec_minusone = vdupq_n_u##size(-1); \
+        uint##size##x##number_of_lanes##_t vec_zero = vdupq_n_u##size(0);      \
+        for (int j = 0; j < lb; j++) {                                         \
+            mtx[j] = vreinterpretq_m128i_u##size(vbslq_u##size(                \
+                vec1, vec_minusone, vreinterpretq_u##size##_m128i(mtx[j])));   \
+        }                                                                      \
+        for (int j = lb; j < bound; j++) {                                     \
+            mtx[j] = vreinterpretq_m128i_u##size(                              \
+                vbslq_u##size(vec1, vec_minusone, vec_zero));                  \
+        }                                                                      \
+        unsigned SSE2NEON_IIF(data_type)(char, short) *ptr =                   \
+            (unsigned SSE2NEON_IIF(data_type)(char, short) *) mtx;             \
+        for (int i = 0; i < bound; i++) {                                      \
+            int val = 1;                                                       \
+            for (int j = 0, k = i; j < bound - i && k < bound; j++, k++)       \
+                val &= ptr[k * bound + j];                                     \
+            res += val << i;                                                   \
+        }                                                                      \
+        return res;                                                            \
+    }
+
+/* clang-format off */
+#define SSE2NEON_GENERATE_AGGREGATE_EQUAL_ORDER(prefix) \
+    prefix##IMPL(8, 16, prefix##IS_UBYTE)               \
+    prefix##IMPL(16, 8, prefix##IS_UWORD)
+/* clang-format on */
+
+SSE2NEON_GENERATE_AGGREGATE_EQUAL_ORDER(SSE2NEON_AGGREGATE_EQUAL_ORDER_)
+
+#undef SSE2NEON_AGGREGATE_EQUAL_ORDER_IS_UBYTE
+#undef SSE2NEON_AGGREGATE_EQUAL_ORDER_IS_UWORD
+
+/* clang-format off */
+#define SSE2NEON_GENERATE_CMP_EQUAL_ORDERED(prefix) \
+    prefix##IMPL(byte)                              \
+    prefix##IMPL(word)
+/* clang-format on */
+
+SSE2NEON_GENERATE_CMP_EQUAL_ORDERED(SSE2NEON_CMP_EQUAL_ORDERED_)
+
+#define SSE2NEON_CMPESTR_LIST                          \
+    _(CMP_UBYTE_EQUAL_ANY, cmp_byte_equal_any)         \
+    _(CMP_UWORD_EQUAL_ANY, cmp_word_equal_any)         \
+    _(CMP_SBYTE_EQUAL_ANY, cmp_byte_equal_any)         \
+    _(CMP_SWORD_EQUAL_ANY, cmp_word_equal_any)         \
+    _(CMP_UBYTE_RANGES, cmp_ubyte_ranges)              \
+    _(CMP_UWORD_RANGES, cmp_uword_ranges)              \
+    _(CMP_SBYTE_RANGES, cmp_sbyte_ranges)              \
+    _(CMP_SWORD_RANGES, cmp_sword_ranges)              \
+    _(CMP_UBYTE_EQUAL_EACH, cmp_byte_equal_each)       \
+    _(CMP_UWORD_EQUAL_EACH, cmp_word_equal_each)       \
+    _(CMP_SBYTE_EQUAL_EACH, cmp_byte_equal_each)       \
+    _(CMP_SWORD_EQUAL_EACH, cmp_word_equal_each)       \
+    _(CMP_UBYTE_EQUAL_ORDERED, cmp_byte_equal_ordered) \
+    _(CMP_UWORD_EQUAL_ORDERED, cmp_word_equal_ordered) \
+    _(CMP_SBYTE_EQUAL_ORDERED, cmp_byte_equal_ordered) \
+    _(CMP_SWORD_EQUAL_ORDERED, cmp_word_equal_ordered)
+
+enum {
+#define _(name, func_suffix) name,
+    SSE2NEON_CMPESTR_LIST
+#undef _
+};
+typedef int (*cmpestr_func_t)(__m128i a, int la, __m128i b, int lb);
+static cmpestr_func_t _sse2neon_cmpfunc_table[] = {
+#define _(name, func_suffix) _sse2neon_##func_suffix,
+    SSE2NEON_CMPESTR_LIST
+#undef _
+};
+
+FORCE_INLINE int _sse2neon_sido_negative(int res, int lb, int imm8, int bound)
+{
+    switch (imm8 & 0x30) {
+    case _SIDD_NEGATIVE_POLARITY:
+        res ^= 0xffffffff;
+        break;
+    case _SIDD_MASKED_NEGATIVE_POLARITY:
+        res ^= (1 << lb) - 1;
+        break;
+    default:
+        break;
+    }
+
+    return res & ((bound == 8) ? 0xFF : 0xFFFF);
+}
+
+FORCE_INLINE int _sse2neon_clz(unsigned int x)
+{
+#if defined(_MSC_VER) && !defined(__clang__)
+    unsigned long cnt = 0;
+    if (_BitScanReverse(&cnt, x))
+        return 31 - cnt;
+    return 32;
+#else
+    return x != 0 ? __builtin_clz(x) : 32;
+#endif
+}
+
+FORCE_INLINE int _sse2neon_ctz(unsigned int x)
+{
+#if defined(_MSC_VER) && !defined(__clang__)
+    unsigned long cnt = 0;
+    if (_BitScanForward(&cnt, x))
+        return cnt;
+    return 32;
+#else
+    return x != 0 ? __builtin_ctz(x) : 32;
+#endif
+}
+
+FORCE_INLINE int _sse2neon_ctzll(unsigned long long x)
+{
+#ifdef _MSC_VER
+    unsigned long cnt;
+#if defined(SSE2NEON_HAS_BITSCAN64)
+    if (_BitScanForward64(&cnt, x))
+        return (int) (cnt);
+#else
+    if (_BitScanForward(&cnt, (unsigned long) (x)))
+        return (int) cnt;
+    if (_BitScanForward(&cnt, (unsigned long) (x >> 32)))
+        return (int) (cnt + 32);
+#endif /* SSE2NEON_HAS_BITSCAN64 */
+    return 64;
+#else /* assume GNU compatible compilers */
+    return x != 0 ? __builtin_ctzll(x) : 64;
+#endif
+}
+
+#define SSE2NEON_MIN(x, y) (x) < (y) ? (x) : (y)
+
+#define SSE2NEON_CMPSTR_SET_UPPER(var, imm) \
+    const int var = (imm & 0x01) ? 8 : 16
+
+#define SSE2NEON_CMPESTRX_LEN_PAIR(a, b, la, lb) \
+    int tmp1 = la ^ (la >> 31);                  \
+    la = tmp1 - (la >> 31);                      \
+    int tmp2 = lb ^ (lb >> 31);                  \
+    lb = tmp2 - (lb >> 31);                      \
+    la = SSE2NEON_MIN(la, bound);                \
+    lb = SSE2NEON_MIN(lb, bound)
+
+// Compare all pairs of character in string a and b,
+// then aggregate the result.
+// As the only difference of PCMPESTR* and PCMPISTR* is the way to calculate the
+// length of string, we use SSE2NEON_CMP{I,E}STRX_GET_LEN to get the length of
+// string a and b.
+#define SSE2NEON_COMP_AGG(a, b, la, lb, imm8, IE)                  \
+    SSE2NEON_CMPSTR_SET_UPPER(bound, imm8);                        \
+    SSE2NEON_##IE##_LEN_PAIR(a, b, la, lb);                        \
+    int r2 = (_sse2neon_cmpfunc_table[imm8 & 0x0f])(a, la, b, lb); \
+    r2 = _sse2neon_sido_negative(r2, lb, imm8, bound)
+
+#define SSE2NEON_CMPSTR_GENERATE_INDEX(r2, bound, imm8)          \
+    return (r2 == 0) ? bound                                     \
+                     : ((imm8 & 0x40) ? (31 - _sse2neon_clz(r2)) \
+                                      : _sse2neon_ctz(r2))
+
+#define SSE2NEON_CMPSTR_GENERATE_MASK(dst)                                     \
+    __m128i dst = vreinterpretq_m128i_u8(vdupq_n_u8(0));                       \
+    if (imm8 & 0x40) {                                                         \
+        if (bound == 8) {                                                      \
+            uint16x8_t tmp = vtstq_u16(vdupq_n_u16(r2),                        \
+                                       vld1q_u16(_sse2neon_cmpestr_mask16b));  \
+            dst = vreinterpretq_m128i_u16(vbslq_u16(                           \
+                tmp, vdupq_n_u16(-1), vreinterpretq_u16_m128i(dst)));          \
+        } else {                                                               \
+            uint8x16_t vec_r2 =                                                \
+                vcombine_u8(vdup_n_u8(r2), vdup_n_u8(r2 >> 8));                \
+            uint8x16_t tmp =                                                   \
+                vtstq_u8(vec_r2, vld1q_u8(_sse2neon_cmpestr_mask8b));          \
+            dst = vreinterpretq_m128i_u8(                                      \
+                vbslq_u8(tmp, vdupq_n_u8(-1), vreinterpretq_u8_m128i(dst)));   \
+        }                                                                      \
+    } else {                                                                   \
+        if (bound == 16) {                                                     \
+            dst = vreinterpretq_m128i_u16(                                     \
+                vsetq_lane_u16(r2 & 0xffff, vreinterpretq_u16_m128i(dst), 0)); \
+        } else {                                                               \
+            dst = vreinterpretq_m128i_u8(                                      \
+                vsetq_lane_u8(r2 & 0xff, vreinterpretq_u8_m128i(dst), 0));     \
+        }                                                                      \
+    }                                                                          \
+    return dst
+
+// Compare packed strings in a and b with lengths la and lb using the control
+// in imm8, and returns 1 if b did not contain a null character and the
+// resulting mask was zero, and 0 otherwise.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cmpestra
+FORCE_INLINE int _mm_cmpestra(__m128i a,
+                              int la,
+                              __m128i b,
+                              int lb,
+                              const int imm8)
+{
+    int lb_cpy = lb;
+    SSE2NEON_COMP_AGG(a, b, la, lb, imm8, CMPESTRX);
+    return !r2 & (lb_cpy > bound);
+}
+
+// Compare packed strings in a and b with lengths la and lb using the control in
+// imm8, and returns 1 if the resulting mask was non-zero, and 0 otherwise.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cmpestrc
+FORCE_INLINE int _mm_cmpestrc(__m128i a,
+                              int la,
+                              __m128i b,
+                              int lb,
+                              const int imm8)
+{
+    SSE2NEON_COMP_AGG(a, b, la, lb, imm8, CMPESTRX);
+    return r2 != 0;
+}
+
+// Compare packed strings in a and b with lengths la and lb using the control
+// in imm8, and store the generated index in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cmpestri
+FORCE_INLINE int _mm_cmpestri(__m128i a,
+                              int la,
+                              __m128i b,
+                              int lb,
+                              const int imm8)
+{
+    SSE2NEON_COMP_AGG(a, b, la, lb, imm8, CMPESTRX);
+    SSE2NEON_CMPSTR_GENERATE_INDEX(r2, bound, imm8);
+}
+
+// Compare packed strings in a and b with lengths la and lb using the control
+// in imm8, and store the generated mask in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cmpestrm
+FORCE_INLINE __m128i
+_mm_cmpestrm(__m128i a, int la, __m128i b, int lb, const int imm8)
+{
+    SSE2NEON_COMP_AGG(a, b, la, lb, imm8, CMPESTRX);
+    SSE2NEON_CMPSTR_GENERATE_MASK(dst);
+}
+
+// Compare packed strings in a and b with lengths la and lb using the control in
+// imm8, and returns bit 0 of the resulting bit mask.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cmpestro
+FORCE_INLINE int _mm_cmpestro(__m128i a,
+                              int la,
+                              __m128i b,
+                              int lb,
+                              const int imm8)
+{
+    SSE2NEON_COMP_AGG(a, b, la, lb, imm8, CMPESTRX);
+    return r2 & 1;
+}
+
+// Compare packed strings in a and b with lengths la and lb using the control in
+// imm8, and returns 1 if any character in a was null, and 0 otherwise.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cmpestrs
+FORCE_INLINE int _mm_cmpestrs(__m128i a,
+                              int la,
+                              __m128i b,
+                              int lb,
+                              const int imm8)
+{
+    (void) a;
+    (void) b;
+    (void) lb;
+    SSE2NEON_CMPSTR_SET_UPPER(bound, imm8);
+    return la <= (bound - 1);
+}
+
+// Compare packed strings in a and b with lengths la and lb using the control in
+// imm8, and returns 1 if any character in b was null, and 0 otherwise.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cmpestrz
+FORCE_INLINE int _mm_cmpestrz(__m128i a,
+                              int la,
+                              __m128i b,
+                              int lb,
+                              const int imm8)
+{
+    (void) a;
+    (void) b;
+    (void) la;
+    SSE2NEON_CMPSTR_SET_UPPER(bound, imm8);
+    return lb <= (bound - 1);
+}
+
+#define SSE2NEON_CMPISTRX_LENGTH(str, len, imm8)                         \
+    do {                                                                 \
+        if (imm8 & 0x01) {                                               \
+            uint16x8_t equal_mask_##str =                                \
+                vceqq_u16(vreinterpretq_u16_m128i(str), vdupq_n_u16(0)); \
+            uint8x8_t res_##str = vshrn_n_u16(equal_mask_##str, 4);      \
+            uint64_t matches_##str =                                     \
+                vget_lane_u64(vreinterpret_u64_u8(res_##str), 0);        \
+            len = _sse2neon_ctzll(matches_##str) >> 3;                   \
+        } else {                                                         \
+            uint16x8_t equal_mask_##str = vreinterpretq_u16_u8(          \
+                vceqq_u8(vreinterpretq_u8_m128i(str), vdupq_n_u8(0)));   \
+            uint8x8_t res_##str = vshrn_n_u16(equal_mask_##str, 4);      \
+            uint64_t matches_##str =                                     \
+                vget_lane_u64(vreinterpret_u64_u8(res_##str), 0);        \
+            len = _sse2neon_ctzll(matches_##str) >> 2;                   \
+        }                                                                \
+    } while (0)
+
+#define SSE2NEON_CMPISTRX_LEN_PAIR(a, b, la, lb) \
+    int la, lb;                                  \
+    do {                                         \
+        SSE2NEON_CMPISTRX_LENGTH(a, la, imm8);   \
+        SSE2NEON_CMPISTRX_LENGTH(b, lb, imm8);   \
+    } while (0)
+
+// Compare packed strings with implicit lengths in a and b using the control in
+// imm8, and returns 1 if b did not contain a null character and the resulting
+// mask was zero, and 0 otherwise.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cmpistra
+FORCE_INLINE int _mm_cmpistra(__m128i a, __m128i b, const int imm8)
+{
+    SSE2NEON_COMP_AGG(a, b, la, lb, imm8, CMPISTRX);
+    return !r2 & (lb >= bound);
+}
+
+// Compare packed strings with implicit lengths in a and b using the control in
+// imm8, and returns 1 if the resulting mask was non-zero, and 0 otherwise.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cmpistrc
+FORCE_INLINE int _mm_cmpistrc(__m128i a, __m128i b, const int imm8)
+{
+    SSE2NEON_COMP_AGG(a, b, la, lb, imm8, CMPISTRX);
+    return r2 != 0;
+}
+
+// Compare packed strings with implicit lengths in a and b using the control in
+// imm8, and store the generated index in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cmpistri
+FORCE_INLINE int _mm_cmpistri(__m128i a, __m128i b, const int imm8)
+{
+    SSE2NEON_COMP_AGG(a, b, la, lb, imm8, CMPISTRX);
+    SSE2NEON_CMPSTR_GENERATE_INDEX(r2, bound, imm8);
+}
+
+// Compare packed strings with implicit lengths in a and b using the control in
+// imm8, and store the generated mask in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cmpistrm
+FORCE_INLINE __m128i _mm_cmpistrm(__m128i a, __m128i b, const int imm8)
+{
+    SSE2NEON_COMP_AGG(a, b, la, lb, imm8, CMPISTRX);
+    SSE2NEON_CMPSTR_GENERATE_MASK(dst);
+}
+
+// Compare packed strings with implicit lengths in a and b using the control in
+// imm8, and returns bit 0 of the resulting bit mask.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cmpistro
+FORCE_INLINE int _mm_cmpistro(__m128i a, __m128i b, const int imm8)
+{
+    SSE2NEON_COMP_AGG(a, b, la, lb, imm8, CMPISTRX);
+    return r2 & 1;
+}
+
+// Compare packed strings with implicit lengths in a and b using the control in
+// imm8, and returns 1 if any character in a was null, and 0 otherwise.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cmpistrs
+FORCE_INLINE int _mm_cmpistrs(__m128i a, __m128i b, const int imm8)
+{
+    (void) b;
+    SSE2NEON_CMPSTR_SET_UPPER(bound, imm8);
+    int la;
+    SSE2NEON_CMPISTRX_LENGTH(a, la, imm8);
+    return la <= (bound - 1);
+}
+
+// Compare packed strings with implicit lengths in a and b using the control in
+// imm8, and returns 1 if any character in b was null, and 0 otherwise.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cmpistrz
+FORCE_INLINE int _mm_cmpistrz(__m128i a, __m128i b, const int imm8)
+{
+    (void) a;
+    SSE2NEON_CMPSTR_SET_UPPER(bound, imm8);
+    int lb;
+    SSE2NEON_CMPISTRX_LENGTH(b, lb, imm8);
+    return lb <= (bound - 1);
+}
+
+// Compares the 2 signed 64-bit integers in a and the 2 signed 64-bit integers
+// in b for greater than.
+FORCE_INLINE __m128i _mm_cmpgt_epi64(__m128i a, __m128i b)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    return vreinterpretq_m128i_u64(
+        vcgtq_s64(vreinterpretq_s64_m128i(a), vreinterpretq_s64_m128i(b)));
+#else
+    return vreinterpretq_m128i_s64(vshrq_n_s64(
+        vqsubq_s64(vreinterpretq_s64_m128i(b), vreinterpretq_s64_m128i(a)),
+        63));
+#endif
+}
+
+// Starting with the initial value in crc, accumulates a CRC32 value for
+// unsigned 16-bit integer v, and stores the result in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_crc32_u16
+FORCE_INLINE uint32_t _mm_crc32_u16(uint32_t crc, uint16_t v)
+{
+#if defined(__aarch64__) && defined(__ARM_FEATURE_CRC32)
+    __asm__ __volatile__("crc32ch %w[c], %w[c], %w[v]\n\t"
+                         : [c] "+r"(crc)
+                         : [v] "r"(v));
+#elif ((__ARM_ARCH == 8) && defined(__ARM_FEATURE_CRC32)) || \
+    (defined(_M_ARM64) && !defined(__clang__))
+    crc = __crc32ch(crc, v);
+#else
+    crc = _mm_crc32_u8(crc, v & 0xff);
+    crc = _mm_crc32_u8(crc, (v >> 8) & 0xff);
+#endif
+    return crc;
+}
+
+// Starting with the initial value in crc, accumulates a CRC32 value for
+// unsigned 32-bit integer v, and stores the result in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_crc32_u32
+FORCE_INLINE uint32_t _mm_crc32_u32(uint32_t crc, uint32_t v)
+{
+#if defined(__aarch64__) && defined(__ARM_FEATURE_CRC32)
+    __asm__ __volatile__("crc32cw %w[c], %w[c], %w[v]\n\t"
+                         : [c] "+r"(crc)
+                         : [v] "r"(v));
+#elif ((__ARM_ARCH == 8) && defined(__ARM_FEATURE_CRC32)) || \
+    (defined(_M_ARM64) && !defined(__clang__))
+    crc = __crc32cw(crc, v);
+#else
+    crc = _mm_crc32_u16(crc, v & 0xffff);
+    crc = _mm_crc32_u16(crc, (v >> 16) & 0xffff);
+#endif
+    return crc;
+}
+
+// Starting with the initial value in crc, accumulates a CRC32 value for
+// unsigned 64-bit integer v, and stores the result in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_crc32_u64
+FORCE_INLINE uint64_t _mm_crc32_u64(uint64_t crc, uint64_t v)
+{
+#if defined(__aarch64__) && defined(__ARM_FEATURE_CRC32)
+    __asm__ __volatile__("crc32cx %w[c], %w[c], %x[v]\n\t"
+                         : [c] "+r"(crc)
+                         : [v] "r"(v));
+#elif (defined(_M_ARM64) && !defined(__clang__))
+    crc = __crc32cd((uint32_t) crc, v);
+#else
+    crc = _mm_crc32_u32((uint32_t) (crc), v & 0xffffffff);
+    crc = _mm_crc32_u32((uint32_t) (crc), (v >> 32) & 0xffffffff);
+#endif
+    return crc;
+}
+
+// Starting with the initial value in crc, accumulates a CRC32 value for
+// unsigned 8-bit integer v, and stores the result in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_crc32_u8
+FORCE_INLINE uint32_t _mm_crc32_u8(uint32_t crc, uint8_t v)
+{
+#if defined(__aarch64__) && defined(__ARM_FEATURE_CRC32)
+    __asm__ __volatile__("crc32cb %w[c], %w[c], %w[v]\n\t"
+                         : [c] "+r"(crc)
+                         : [v] "r"(v));
+#elif ((__ARM_ARCH == 8) && defined(__ARM_FEATURE_CRC32)) || \
+    (defined(_M_ARM64) && !defined(__clang__))
+    crc = __crc32cb(crc, v);
+#else
+    crc ^= v;
+#if defined(__ARM_FEATURE_CRYPTO)
+    // Adapted from: https://mary.rs/lab/crc32/
+    // Barrent reduction
+    uint64x2_t orig =
+        vcombine_u64(vcreate_u64((uint64_t) (crc) << 24), vcreate_u64(0x0));
+    uint64x2_t tmp = orig;
+
+    // Polynomial P(x) of CRC32C
+    uint64_t p = 0x105EC76F1;
+    // Barrett Reduction (in bit-reflected form) constant mu_{64} = \lfloor
+    // 2^{64} / P(x) \rfloor = 0x11f91caf6
+    uint64_t mu = 0x1dea713f1;
+
+    // Multiply by mu_{64}
+    tmp = _sse2neon_vmull_p64(vget_low_u64(tmp), vcreate_u64(mu));
+    // Divide by 2^{64} (mask away the unnecessary bits)
+    tmp =
+        vandq_u64(tmp, vcombine_u64(vcreate_u64(0xFFFFFFFF), vcreate_u64(0x0)));
+    // Multiply by P(x) (shifted left by 1 for alignment reasons)
+    tmp = _sse2neon_vmull_p64(vget_low_u64(tmp), vcreate_u64(p));
+    // Subtract original from result
+    tmp = veorq_u64(tmp, orig);
+
+    // Extract the 'lower' (in bit-reflected sense) 32 bits
+    crc = vgetq_lane_u32(vreinterpretq_u32_u64(tmp), 1);
+#else  // Fall back to the generic table lookup approach
+    // Adapted from: https://create.stephan-brumme.com/crc32/
+    // Apply half-byte comparison algorithm for the best ratio between
+    // performance and lookup table.
+
+    // The lookup table just needs to store every 16th entry
+    // of the standard look-up table.
+    static const uint32_t crc32_half_byte_tbl[] = {
+        0x00000000, 0x105ec76f, 0x20bd8ede, 0x30e349b1, 0x417b1dbc, 0x5125dad3,
+        0x61c69362, 0x7198540d, 0x82f63b78, 0x92a8fc17, 0xa24bb5a6, 0xb21572c9,
+        0xc38d26c4, 0xd3d3e1ab, 0xe330a81a, 0xf36e6f75,
+    };
+
+    crc = (crc >> 4) ^ crc32_half_byte_tbl[crc & 0x0F];
+    crc = (crc >> 4) ^ crc32_half_byte_tbl[crc & 0x0F];
+#endif
+#endif
+    return crc;
+}
+
+/* AES */
+
+#if !defined(__ARM_FEATURE_CRYPTO) && (!defined(_M_ARM64) || defined(__clang__))
+/* clang-format off */
+#define SSE2NEON_AES_SBOX(w)                                           \
+    {                                                                  \
+        w(0x63), w(0x7c), w(0x77), w(0x7b), w(0xf2), w(0x6b), w(0x6f), \
+        w(0xc5), w(0x30), w(0x01), w(0x67), w(0x2b), w(0xfe), w(0xd7), \
+        w(0xab), w(0x76), w(0xca), w(0x82), w(0xc9), w(0x7d), w(0xfa), \
+        w(0x59), w(0x47), w(0xf0), w(0xad), w(0xd4), w(0xa2), w(0xaf), \
+        w(0x9c), w(0xa4), w(0x72), w(0xc0), w(0xb7), w(0xfd), w(0x93), \
+        w(0x26), w(0x36), w(0x3f), w(0xf7), w(0xcc), w(0x34), w(0xa5), \
+        w(0xe5), w(0xf1), w(0x71), w(0xd8), w(0x31), w(0x15), w(0x04), \
+        w(0xc7), w(0x23), w(0xc3), w(0x18), w(0x96), w(0x05), w(0x9a), \
+        w(0x07), w(0x12), w(0x80), w(0xe2), w(0xeb), w(0x27), w(0xb2), \
+        w(0x75), w(0x09), w(0x83), w(0x2c), w(0x1a), w(0x1b), w(0x6e), \
+        w(0x5a), w(0xa0), w(0x52), w(0x3b), w(0xd6), w(0xb3), w(0x29), \
+        w(0xe3), w(0x2f), w(0x84), w(0x53), w(0xd1), w(0x00), w(0xed), \
+        w(0x20), w(0xfc), w(0xb1), w(0x5b), w(0x6a), w(0xcb), w(0xbe), \
+        w(0x39), w(0x4a), w(0x4c), w(0x58), w(0xcf), w(0xd0), w(0xef), \
+        w(0xaa), w(0xfb), w(0x43), w(0x4d), w(0x33), w(0x85), w(0x45), \
+        w(0xf9), w(0x02), w(0x7f), w(0x50), w(0x3c), w(0x9f), w(0xa8), \
+        w(0x51), w(0xa3), w(0x40), w(0x8f), w(0x92), w(0x9d), w(0x38), \
+        w(0xf5), w(0xbc), w(0xb6), w(0xda), w(0x21), w(0x10), w(0xff), \
+        w(0xf3), w(0xd2), w(0xcd), w(0x0c), w(0x13), w(0xec), w(0x5f), \
+        w(0x97), w(0x44), w(0x17), w(0xc4), w(0xa7), w(0x7e), w(0x3d), \
+        w(0x64), w(0x5d), w(0x19), w(0x73), w(0x60), w(0x81), w(0x4f), \
+        w(0xdc), w(0x22), w(0x2a), w(0x90), w(0x88), w(0x46), w(0xee), \
+        w(0xb8), w(0x14), w(0xde), w(0x5e), w(0x0b), w(0xdb), w(0xe0), \
+        w(0x32), w(0x3a), w(0x0a), w(0x49), w(0x06), w(0x24), w(0x5c), \
+        w(0xc2), w(0xd3), w(0xac), w(0x62), w(0x91), w(0x95), w(0xe4), \
+        w(0x79), w(0xe7), w(0xc8), w(0x37), w(0x6d), w(0x8d), w(0xd5), \
+        w(0x4e), w(0xa9), w(0x6c), w(0x56), w(0xf4), w(0xea), w(0x65), \
+        w(0x7a), w(0xae), w(0x08), w(0xba), w(0x78), w(0x25), w(0x2e), \
+        w(0x1c), w(0xa6), w(0xb4), w(0xc6), w(0xe8), w(0xdd), w(0x74), \
+        w(0x1f), w(0x4b), w(0xbd), w(0x8b), w(0x8a), w(0x70), w(0x3e), \
+        w(0xb5), w(0x66), w(0x48), w(0x03), w(0xf6), w(0x0e), w(0x61), \
+        w(0x35), w(0x57), w(0xb9), w(0x86), w(0xc1), w(0x1d), w(0x9e), \
+        w(0xe1), w(0xf8), w(0x98), w(0x11), w(0x69), w(0xd9), w(0x8e), \
+        w(0x94), w(0x9b), w(0x1e), w(0x87), w(0xe9), w(0xce), w(0x55), \
+        w(0x28), w(0xdf), w(0x8c), w(0xa1), w(0x89), w(0x0d), w(0xbf), \
+        w(0xe6), w(0x42), w(0x68), w(0x41), w(0x99), w(0x2d), w(0x0f), \
+        w(0xb0), w(0x54), w(0xbb), w(0x16)                             \
+    }
+#define SSE2NEON_AES_RSBOX(w)                                          \
+    {                                                                  \
+        w(0x52), w(0x09), w(0x6a), w(0xd5), w(0x30), w(0x36), w(0xa5), \
+        w(0x38), w(0xbf), w(0x40), w(0xa3), w(0x9e), w(0x81), w(0xf3), \
+        w(0xd7), w(0xfb), w(0x7c), w(0xe3), w(0x39), w(0x82), w(0x9b), \
+        w(0x2f), w(0xff), w(0x87), w(0x34), w(0x8e), w(0x43), w(0x44), \
+        w(0xc4), w(0xde), w(0xe9), w(0xcb), w(0x54), w(0x7b), w(0x94), \
+        w(0x32), w(0xa6), w(0xc2), w(0x23), w(0x3d), w(0xee), w(0x4c), \
+        w(0x95), w(0x0b), w(0x42), w(0xfa), w(0xc3), w(0x4e), w(0x08), \
+        w(0x2e), w(0xa1), w(0x66), w(0x28), w(0xd9), w(0x24), w(0xb2), \
+        w(0x76), w(0x5b), w(0xa2), w(0x49), w(0x6d), w(0x8b), w(0xd1), \
+        w(0x25), w(0x72), w(0xf8), w(0xf6), w(0x64), w(0x86), w(0x68), \
+        w(0x98), w(0x16), w(0xd4), w(0xa4), w(0x5c), w(0xcc), w(0x5d), \
+        w(0x65), w(0xb6), w(0x92), w(0x6c), w(0x70), w(0x48), w(0x50), \
+        w(0xfd), w(0xed), w(0xb9), w(0xda), w(0x5e), w(0x15), w(0x46), \
+        w(0x57), w(0xa7), w(0x8d), w(0x9d), w(0x84), w(0x90), w(0xd8), \
+        w(0xab), w(0x00), w(0x8c), w(0xbc), w(0xd3), w(0x0a), w(0xf7), \
+        w(0xe4), w(0x58), w(0x05), w(0xb8), w(0xb3), w(0x45), w(0x06), \
+        w(0xd0), w(0x2c), w(0x1e), w(0x8f), w(0xca), w(0x3f), w(0x0f), \
+        w(0x02), w(0xc1), w(0xaf), w(0xbd), w(0x03), w(0x01), w(0x13), \
+        w(0x8a), w(0x6b), w(0x3a), w(0x91), w(0x11), w(0x41), w(0x4f), \
+        w(0x67), w(0xdc), w(0xea), w(0x97), w(0xf2), w(0xcf), w(0xce), \
+        w(0xf0), w(0xb4), w(0xe6), w(0x73), w(0x96), w(0xac), w(0x74), \
+        w(0x22), w(0xe7), w(0xad), w(0x35), w(0x85), w(0xe2), w(0xf9), \
+        w(0x37), w(0xe8), w(0x1c), w(0x75), w(0xdf), w(0x6e), w(0x47), \
+        w(0xf1), w(0x1a), w(0x71), w(0x1d), w(0x29), w(0xc5), w(0x89), \
+        w(0x6f), w(0xb7), w(0x62), w(0x0e), w(0xaa), w(0x18), w(0xbe), \
+        w(0x1b), w(0xfc), w(0x56), w(0x3e), w(0x4b), w(0xc6), w(0xd2), \
+        w(0x79), w(0x20), w(0x9a), w(0xdb), w(0xc0), w(0xfe), w(0x78), \
+        w(0xcd), w(0x5a), w(0xf4), w(0x1f), w(0xdd), w(0xa8), w(0x33), \
+        w(0x88), w(0x07), w(0xc7), w(0x31), w(0xb1), w(0x12), w(0x10), \
+        w(0x59), w(0x27), w(0x80), w(0xec), w(0x5f), w(0x60), w(0x51), \
+        w(0x7f), w(0xa9), w(0x19), w(0xb5), w(0x4a), w(0x0d), w(0x2d), \
+        w(0xe5), w(0x7a), w(0x9f), w(0x93), w(0xc9), w(0x9c), w(0xef), \
+        w(0xa0), w(0xe0), w(0x3b), w(0x4d), w(0xae), w(0x2a), w(0xf5), \
+        w(0xb0), w(0xc8), w(0xeb), w(0xbb), w(0x3c), w(0x83), w(0x53), \
+        w(0x99), w(0x61), w(0x17), w(0x2b), w(0x04), w(0x7e), w(0xba), \
+        w(0x77), w(0xd6), w(0x26), w(0xe1), w(0x69), w(0x14), w(0x63), \
+        w(0x55), w(0x21), w(0x0c), w(0x7d)                             \
+    }
+/* clang-format on */
+
+/* X Macro trick. See https://en.wikipedia.org/wiki/X_Macro */
+#define SSE2NEON_AES_H0(x) (x)
+static const uint8_t _sse2neon_sbox[256] = SSE2NEON_AES_SBOX(SSE2NEON_AES_H0);
+static const uint8_t _sse2neon_rsbox[256] = SSE2NEON_AES_RSBOX(SSE2NEON_AES_H0);
+#undef SSE2NEON_AES_H0
+
+/* x_time function and matrix multiply function */
+#if !defined(__aarch64__) && !defined(_M_ARM64)
+#define SSE2NEON_XT(x) (((x) << 1) ^ ((((x) >> 7) & 1) * 0x1b))
+#define SSE2NEON_MULTIPLY(x, y)                                  \
+    (((y & 1) * x) ^ ((y >> 1 & 1) * SSE2NEON_XT(x)) ^           \
+     ((y >> 2 & 1) * SSE2NEON_XT(SSE2NEON_XT(x))) ^              \
+     ((y >> 3 & 1) * SSE2NEON_XT(SSE2NEON_XT(SSE2NEON_XT(x)))) ^ \
+     ((y >> 4 & 1) * SSE2NEON_XT(SSE2NEON_XT(SSE2NEON_XT(SSE2NEON_XT(x))))))
+#endif
+
+// In the absence of crypto extensions, implement aesenc using regular NEON
+// intrinsics instead. See:
+// https://www.workofard.com/2017/01/accelerated-aes-for-the-arm64-linux-kernel/
+// https://www.workofard.com/2017/07/ghash-for-low-end-cores/ and
+// for more information.
+FORCE_INLINE __m128i _mm_aesenc_si128(__m128i a, __m128i RoundKey)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    static const uint8_t shift_rows[] = {
+        0x0, 0x5, 0xa, 0xf, 0x4, 0x9, 0xe, 0x3,
+        0x8, 0xd, 0x2, 0x7, 0xc, 0x1, 0x6, 0xb,
+    };
+    static const uint8_t ror32by8[] = {
+        0x1, 0x2, 0x3, 0x0, 0x5, 0x6, 0x7, 0x4,
+        0x9, 0xa, 0xb, 0x8, 0xd, 0xe, 0xf, 0xc,
+    };
+
+    uint8x16_t v;
+    uint8x16_t w = vreinterpretq_u8_m128i(a);
+
+    /* shift rows */
+    w = vqtbl1q_u8(w, vld1q_u8(shift_rows));
+
+    /* sub bytes */
+    // Here, we separate the whole 256-bytes table into 4 64-bytes tables, and
+    // look up each of the table. After each lookup, we load the next table
+    // which locates at the next 64-bytes. In the meantime, the index in the
+    // table would be smaller than it was, so the index parameters of
+    // `vqtbx4q_u8()` need to be added the same constant as the loaded tables.
+    v = vqtbl4q_u8(_sse2neon_vld1q_u8_x4(_sse2neon_sbox), w);
+    // 'w-0x40' equals to 'vsubq_u8(w, vdupq_n_u8(0x40))'
+    v = vqtbx4q_u8(v, _sse2neon_vld1q_u8_x4(_sse2neon_sbox + 0x40), w - 0x40);
+    v = vqtbx4q_u8(v, _sse2neon_vld1q_u8_x4(_sse2neon_sbox + 0x80), w - 0x80);
+    v = vqtbx4q_u8(v, _sse2neon_vld1q_u8_x4(_sse2neon_sbox + 0xc0), w - 0xc0);
+
+    /* mix columns */
+    w = (v << 1) ^ (uint8x16_t) (((int8x16_t) v >> 7) & 0x1b);
+    w ^= (uint8x16_t) vrev32q_u16((uint16x8_t) v);
+    w ^= vqtbl1q_u8(v ^ w, vld1q_u8(ror32by8));
+
+    /* add round key */
+    return vreinterpretq_m128i_u8(w) ^ RoundKey;
+
+#else /* ARMv7-A implementation for a table-based AES */
+#define SSE2NEON_AES_B2W(b0, b1, b2, b3)                 \
+    (((uint32_t) (b3) << 24) | ((uint32_t) (b2) << 16) | \
+     ((uint32_t) (b1) << 8) | (uint32_t) (b0))
+// multiplying 'x' by 2 in GF(2^8)
+#define SSE2NEON_AES_F2(x) ((x << 1) ^ (((x >> 7) & 1) * 0x011b /* WPOLY */))
+// multiplying 'x' by 3 in GF(2^8)
+#define SSE2NEON_AES_F3(x) (SSE2NEON_AES_F2(x) ^ x)
+#define SSE2NEON_AES_U0(p) \
+    SSE2NEON_AES_B2W(SSE2NEON_AES_F2(p), p, p, SSE2NEON_AES_F3(p))
+#define SSE2NEON_AES_U1(p) \
+    SSE2NEON_AES_B2W(SSE2NEON_AES_F3(p), SSE2NEON_AES_F2(p), p, p)
+#define SSE2NEON_AES_U2(p) \
+    SSE2NEON_AES_B2W(p, SSE2NEON_AES_F3(p), SSE2NEON_AES_F2(p), p)
+#define SSE2NEON_AES_U3(p) \
+    SSE2NEON_AES_B2W(p, p, SSE2NEON_AES_F3(p), SSE2NEON_AES_F2(p))
+
+    // this generates a table containing every possible permutation of
+    // shift_rows() and sub_bytes() with mix_columns().
+    static const uint32_t ALIGN_STRUCT(16) aes_table[4][256] = {
+        SSE2NEON_AES_SBOX(SSE2NEON_AES_U0),
+        SSE2NEON_AES_SBOX(SSE2NEON_AES_U1),
+        SSE2NEON_AES_SBOX(SSE2NEON_AES_U2),
+        SSE2NEON_AES_SBOX(SSE2NEON_AES_U3),
+    };
+#undef SSE2NEON_AES_B2W
+#undef SSE2NEON_AES_F2
+#undef SSE2NEON_AES_F3
+#undef SSE2NEON_AES_U0
+#undef SSE2NEON_AES_U1
+#undef SSE2NEON_AES_U2
+#undef SSE2NEON_AES_U3
+
+    uint32_t x0 = _mm_cvtsi128_si32(a);  // get a[31:0]
+    uint32_t x1 =
+        _mm_cvtsi128_si32(_mm_shuffle_epi32(a, 0x55));  // get a[63:32]
+    uint32_t x2 =
+        _mm_cvtsi128_si32(_mm_shuffle_epi32(a, 0xAA));  // get a[95:64]
+    uint32_t x3 =
+        _mm_cvtsi128_si32(_mm_shuffle_epi32(a, 0xFF));  // get a[127:96]
+
+    // finish the modulo addition step in mix_columns()
+    __m128i out = _mm_set_epi32(
+        (aes_table[0][x3 & 0xff] ^ aes_table[1][(x0 >> 8) & 0xff] ^
+         aes_table[2][(x1 >> 16) & 0xff] ^ aes_table[3][x2 >> 24]),
+        (aes_table[0][x2 & 0xff] ^ aes_table[1][(x3 >> 8) & 0xff] ^
+         aes_table[2][(x0 >> 16) & 0xff] ^ aes_table[3][x1 >> 24]),
+        (aes_table[0][x1 & 0xff] ^ aes_table[1][(x2 >> 8) & 0xff] ^
+         aes_table[2][(x3 >> 16) & 0xff] ^ aes_table[3][x0 >> 24]),
+        (aes_table[0][x0 & 0xff] ^ aes_table[1][(x1 >> 8) & 0xff] ^
+         aes_table[2][(x2 >> 16) & 0xff] ^ aes_table[3][x3 >> 24]));
+
+    return _mm_xor_si128(out, RoundKey);
+#endif
+}
+
+// Perform one round of an AES decryption flow on data (state) in a using the
+// round key in RoundKey, and store the result in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_aesdec_si128
+FORCE_INLINE __m128i _mm_aesdec_si128(__m128i a, __m128i RoundKey)
+{
+#if defined(__aarch64__)
+    static const uint8_t inv_shift_rows[] = {
+        0x0, 0xd, 0xa, 0x7, 0x4, 0x1, 0xe, 0xb,
+        0x8, 0x5, 0x2, 0xf, 0xc, 0x9, 0x6, 0x3,
+    };
+    static const uint8_t ror32by8[] = {
+        0x1, 0x2, 0x3, 0x0, 0x5, 0x6, 0x7, 0x4,
+        0x9, 0xa, 0xb, 0x8, 0xd, 0xe, 0xf, 0xc,
+    };
+
+    uint8x16_t v;
+    uint8x16_t w = vreinterpretq_u8_m128i(a);
+
+    // inverse shift rows
+    w = vqtbl1q_u8(w, vld1q_u8(inv_shift_rows));
+
+    // inverse sub bytes
+    v = vqtbl4q_u8(_sse2neon_vld1q_u8_x4(_sse2neon_rsbox), w);
+    v = vqtbx4q_u8(v, _sse2neon_vld1q_u8_x4(_sse2neon_rsbox + 0x40), w - 0x40);
+    v = vqtbx4q_u8(v, _sse2neon_vld1q_u8_x4(_sse2neon_rsbox + 0x80), w - 0x80);
+    v = vqtbx4q_u8(v, _sse2neon_vld1q_u8_x4(_sse2neon_rsbox + 0xc0), w - 0xc0);
+
+    // inverse mix columns
+    // multiplying 'v' by 4 in GF(2^8)
+    w = (v << 1) ^ (uint8x16_t) (((int8x16_t) v >> 7) & 0x1b);
+    w = (w << 1) ^ (uint8x16_t) (((int8x16_t) w >> 7) & 0x1b);
+    v ^= w;
+    v ^= (uint8x16_t) vrev32q_u16((uint16x8_t) w);
+
+    w = (v << 1) ^ (uint8x16_t) (((int8x16_t) v >> 7) &
+                                 0x1b);  // multiplying 'v' by 2 in GF(2^8)
+    w ^= (uint8x16_t) vrev32q_u16((uint16x8_t) v);
+    w ^= vqtbl1q_u8(v ^ w, vld1q_u8(ror32by8));
+
+    // add round key
+    return vreinterpretq_m128i_u8(w) ^ RoundKey;
+
+#else /* ARMv7-A NEON implementation */
+    /* FIXME: optimized for NEON */
+    uint8_t i, e, f, g, h, v[4][4];
+    uint8_t *_a = (uint8_t *) &a;
+    for (i = 0; i < 16; ++i) {
+        v[((i / 4) + (i % 4)) % 4][i % 4] = _sse2neon_rsbox[_a[i]];
+    }
+
+    // inverse mix columns
+    for (i = 0; i < 4; ++i) {
+        e = v[i][0];
+        f = v[i][1];
+        g = v[i][2];
+        h = v[i][3];
+
+        v[i][0] = SSE2NEON_MULTIPLY(e, 0x0e) ^ SSE2NEON_MULTIPLY(f, 0x0b) ^
+                  SSE2NEON_MULTIPLY(g, 0x0d) ^ SSE2NEON_MULTIPLY(h, 0x09);
+        v[i][1] = SSE2NEON_MULTIPLY(e, 0x09) ^ SSE2NEON_MULTIPLY(f, 0x0e) ^
+                  SSE2NEON_MULTIPLY(g, 0x0b) ^ SSE2NEON_MULTIPLY(h, 0x0d);
+        v[i][2] = SSE2NEON_MULTIPLY(e, 0x0d) ^ SSE2NEON_MULTIPLY(f, 0x09) ^
+                  SSE2NEON_MULTIPLY(g, 0x0e) ^ SSE2NEON_MULTIPLY(h, 0x0b);
+        v[i][3] = SSE2NEON_MULTIPLY(e, 0x0b) ^ SSE2NEON_MULTIPLY(f, 0x0d) ^
+                  SSE2NEON_MULTIPLY(g, 0x09) ^ SSE2NEON_MULTIPLY(h, 0x0e);
+    }
+
+    return vreinterpretq_m128i_u8(vld1q_u8((uint8_t *) v)) ^ RoundKey;
+#endif
+}
+
+// Perform the last round of an AES encryption flow on data (state) in a using
+// the round key in RoundKey, and store the result in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_aesenclast_si128
+FORCE_INLINE __m128i _mm_aesenclast_si128(__m128i a, __m128i RoundKey)
+{
+#if defined(__aarch64__)
+    static const uint8_t shift_rows[] = {
+        0x0, 0x5, 0xa, 0xf, 0x4, 0x9, 0xe, 0x3,
+        0x8, 0xd, 0x2, 0x7, 0xc, 0x1, 0x6, 0xb,
+    };
+
+    uint8x16_t v;
+    uint8x16_t w = vreinterpretq_u8_m128i(a);
+
+    // shift rows
+    w = vqtbl1q_u8(w, vld1q_u8(shift_rows));
+
+    // sub bytes
+    v = vqtbl4q_u8(_sse2neon_vld1q_u8_x4(_sse2neon_sbox), w);
+    v = vqtbx4q_u8(v, _sse2neon_vld1q_u8_x4(_sse2neon_sbox + 0x40), w - 0x40);
+    v = vqtbx4q_u8(v, _sse2neon_vld1q_u8_x4(_sse2neon_sbox + 0x80), w - 0x80);
+    v = vqtbx4q_u8(v, _sse2neon_vld1q_u8_x4(_sse2neon_sbox + 0xc0), w - 0xc0);
+
+    // add round key
+    return vreinterpretq_m128i_u8(v) ^ RoundKey;
+
+#else /* ARMv7-A implementation */
+    uint8_t v[16] = {
+        _sse2neon_sbox[vgetq_lane_u8(vreinterpretq_u8_m128i(a), 0)],
+        _sse2neon_sbox[vgetq_lane_u8(vreinterpretq_u8_m128i(a), 5)],
+        _sse2neon_sbox[vgetq_lane_u8(vreinterpretq_u8_m128i(a), 10)],
+        _sse2neon_sbox[vgetq_lane_u8(vreinterpretq_u8_m128i(a), 15)],
+        _sse2neon_sbox[vgetq_lane_u8(vreinterpretq_u8_m128i(a), 4)],
+        _sse2neon_sbox[vgetq_lane_u8(vreinterpretq_u8_m128i(a), 9)],
+        _sse2neon_sbox[vgetq_lane_u8(vreinterpretq_u8_m128i(a), 14)],
+        _sse2neon_sbox[vgetq_lane_u8(vreinterpretq_u8_m128i(a), 3)],
+        _sse2neon_sbox[vgetq_lane_u8(vreinterpretq_u8_m128i(a), 8)],
+        _sse2neon_sbox[vgetq_lane_u8(vreinterpretq_u8_m128i(a), 13)],
+        _sse2neon_sbox[vgetq_lane_u8(vreinterpretq_u8_m128i(a), 2)],
+        _sse2neon_sbox[vgetq_lane_u8(vreinterpretq_u8_m128i(a), 7)],
+        _sse2neon_sbox[vgetq_lane_u8(vreinterpretq_u8_m128i(a), 12)],
+        _sse2neon_sbox[vgetq_lane_u8(vreinterpretq_u8_m128i(a), 1)],
+        _sse2neon_sbox[vgetq_lane_u8(vreinterpretq_u8_m128i(a), 6)],
+        _sse2neon_sbox[vgetq_lane_u8(vreinterpretq_u8_m128i(a), 11)],
+    };
+
+    return vreinterpretq_m128i_u8(vld1q_u8(v)) ^ RoundKey;
+#endif
+}
+
+// Perform the last round of an AES decryption flow on data (state) in a using
+// the round key in RoundKey, and store the result in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_aesdeclast_si128
+FORCE_INLINE __m128i _mm_aesdeclast_si128(__m128i a, __m128i RoundKey)
+{
+#if defined(__aarch64__)
+    static const uint8_t inv_shift_rows[] = {
+        0x0, 0xd, 0xa, 0x7, 0x4, 0x1, 0xe, 0xb,
+        0x8, 0x5, 0x2, 0xf, 0xc, 0x9, 0x6, 0x3,
+    };
+
+    uint8x16_t v;
+    uint8x16_t w = vreinterpretq_u8_m128i(a);
+
+    // inverse shift rows
+    w = vqtbl1q_u8(w, vld1q_u8(inv_shift_rows));
+
+    // inverse sub bytes
+    v = vqtbl4q_u8(_sse2neon_vld1q_u8_x4(_sse2neon_rsbox), w);
+    v = vqtbx4q_u8(v, _sse2neon_vld1q_u8_x4(_sse2neon_rsbox + 0x40), w - 0x40);
+    v = vqtbx4q_u8(v, _sse2neon_vld1q_u8_x4(_sse2neon_rsbox + 0x80), w - 0x80);
+    v = vqtbx4q_u8(v, _sse2neon_vld1q_u8_x4(_sse2neon_rsbox + 0xc0), w - 0xc0);
+
+    // add round key
+    return vreinterpretq_m128i_u8(v) ^ RoundKey;
+
+#else /* ARMv7-A NEON implementation */
+    /* FIXME: optimized for NEON */
+    uint8_t v[4][4];
+    uint8_t *_a = (uint8_t *) &a;
+    for (int i = 0; i < 16; ++i) {
+        v[((i / 4) + (i % 4)) % 4][i % 4] = _sse2neon_rsbox[_a[i]];
+    }
+
+    return vreinterpretq_m128i_u8(vld1q_u8((uint8_t *) v)) ^ RoundKey;
+#endif
+}
+
+// Perform the InvMixColumns transformation on a and store the result in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_aesimc_si128
+FORCE_INLINE __m128i _mm_aesimc_si128(__m128i a)
+{
+#if defined(__aarch64__)
+    static const uint8_t ror32by8[] = {
+        0x1, 0x2, 0x3, 0x0, 0x5, 0x6, 0x7, 0x4,
+        0x9, 0xa, 0xb, 0x8, 0xd, 0xe, 0xf, 0xc,
+    };
+    uint8x16_t v = vreinterpretq_u8_m128i(a);
+    uint8x16_t w;
+
+    // multiplying 'v' by 4 in GF(2^8)
+    w = (v << 1) ^ (uint8x16_t) (((int8x16_t) v >> 7) & 0x1b);
+    w = (w << 1) ^ (uint8x16_t) (((int8x16_t) w >> 7) & 0x1b);
+    v ^= w;
+    v ^= (uint8x16_t) vrev32q_u16((uint16x8_t) w);
+
+    // multiplying 'v' by 2 in GF(2^8)
+    w = (v << 1) ^ (uint8x16_t) (((int8x16_t) v >> 7) & 0x1b);
+    w ^= (uint8x16_t) vrev32q_u16((uint16x8_t) v);
+    w ^= vqtbl1q_u8(v ^ w, vld1q_u8(ror32by8));
+    return vreinterpretq_m128i_u8(w);
+
+#else /* ARMv7-A NEON implementation */
+    uint8_t i, e, f, g, h, v[4][4];
+    vst1q_u8((uint8_t *) v, vreinterpretq_u8_m128i(a));
+    for (i = 0; i < 4; ++i) {
+        e = v[i][0];
+        f = v[i][1];
+        g = v[i][2];
+        h = v[i][3];
+
+        v[i][0] = SSE2NEON_MULTIPLY(e, 0x0e) ^ SSE2NEON_MULTIPLY(f, 0x0b) ^
+                  SSE2NEON_MULTIPLY(g, 0x0d) ^ SSE2NEON_MULTIPLY(h, 0x09);
+        v[i][1] = SSE2NEON_MULTIPLY(e, 0x09) ^ SSE2NEON_MULTIPLY(f, 0x0e) ^
+                  SSE2NEON_MULTIPLY(g, 0x0b) ^ SSE2NEON_MULTIPLY(h, 0x0d);
+        v[i][2] = SSE2NEON_MULTIPLY(e, 0x0d) ^ SSE2NEON_MULTIPLY(f, 0x09) ^
+                  SSE2NEON_MULTIPLY(g, 0x0e) ^ SSE2NEON_MULTIPLY(h, 0x0b);
+        v[i][3] = SSE2NEON_MULTIPLY(e, 0x0b) ^ SSE2NEON_MULTIPLY(f, 0x0d) ^
+                  SSE2NEON_MULTIPLY(g, 0x09) ^ SSE2NEON_MULTIPLY(h, 0x0e);
+    }
+
+    return vreinterpretq_m128i_u8(vld1q_u8((uint8_t *) v));
+#endif
+}
+
+// Assist in expanding the AES cipher key by computing steps towards generating
+// a round key for encryption cipher using data from a and an 8-bit round
+// constant specified in imm8, and store the result in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_aeskeygenassist_si128
+//
+// Emits the Advanced Encryption Standard (AES) instruction aeskeygenassist.
+// This instruction generates a round key for AES encryption. See
+// https://kazakov.life/2017/11/01/cryptocurrency-mining-on-ios-devices/
+// for details.
+FORCE_INLINE __m128i _mm_aeskeygenassist_si128(__m128i a, const int rcon)
+{
+#if defined(__aarch64__)
+    uint8x16_t _a = vreinterpretq_u8_m128i(a);
+    uint8x16_t v = vqtbl4q_u8(_sse2neon_vld1q_u8_x4(_sse2neon_sbox), _a);
+    v = vqtbx4q_u8(v, _sse2neon_vld1q_u8_x4(_sse2neon_sbox + 0x40), _a - 0x40);
+    v = vqtbx4q_u8(v, _sse2neon_vld1q_u8_x4(_sse2neon_sbox + 0x80), _a - 0x80);
+    v = vqtbx4q_u8(v, _sse2neon_vld1q_u8_x4(_sse2neon_sbox + 0xc0), _a - 0xc0);
+
+    uint32x4_t v_u32 = vreinterpretq_u32_u8(v);
+    uint32x4_t ror_v = vorrq_u32(vshrq_n_u32(v_u32, 8), vshlq_n_u32(v_u32, 24));
+    uint32x4_t ror_xor_v = veorq_u32(ror_v, vdupq_n_u32(rcon));
+
+    return vreinterpretq_m128i_u32(vtrn2q_u32(v_u32, ror_xor_v));
+
+#else /* ARMv7-A NEON implementation */
+    uint32_t X1 = _mm_cvtsi128_si32(_mm_shuffle_epi32(a, 0x55));
+    uint32_t X3 = _mm_cvtsi128_si32(_mm_shuffle_epi32(a, 0xFF));
+    for (int i = 0; i < 4; ++i) {
+        ((uint8_t *) &X1)[i] = _sse2neon_sbox[((uint8_t *) &X1)[i]];
+        ((uint8_t *) &X3)[i] = _sse2neon_sbox[((uint8_t *) &X3)[i]];
+    }
+    return _mm_set_epi32(((X3 >> 8) | (X3 << 24)) ^ rcon, X3,
+                         ((X1 >> 8) | (X1 << 24)) ^ rcon, X1);
+#endif
+}
+#undef SSE2NEON_AES_SBOX
+#undef SSE2NEON_AES_RSBOX
+
+#if defined(__aarch64__)
+#undef SSE2NEON_XT
+#undef SSE2NEON_MULTIPLY
+#endif
+
+#else /* __ARM_FEATURE_CRYPTO */
+// Implements equivalent of 'aesenc' by combining AESE (with an empty key) and
+// AESMC and then manually applying the real key as an xor operation. This
+// unfortunately means an additional xor op; the compiler should be able to
+// optimize this away for repeated calls however. See
+// https://blog.michaelbrase.com/2018/05/08/emulating-x86-aes-intrinsics-on-armv8-a
+// for more details.
+FORCE_INLINE __m128i _mm_aesenc_si128(__m128i a, __m128i b)
+{
+    return vreinterpretq_m128i_u8(veorq_u8(
+        vaesmcq_u8(vaeseq_u8(vreinterpretq_u8_m128i(a), vdupq_n_u8(0))),
+        vreinterpretq_u8_m128i(b)));
+}
+
+// Perform one round of an AES decryption flow on data (state) in a using the
+// round key in RoundKey, and store the result in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_aesdec_si128
+FORCE_INLINE __m128i _mm_aesdec_si128(__m128i a, __m128i RoundKey)
+{
+    return vreinterpretq_m128i_u8(veorq_u8(
+        vaesimcq_u8(vaesdq_u8(vreinterpretq_u8_m128i(a), vdupq_n_u8(0))),
+        vreinterpretq_u8_m128i(RoundKey)));
+}
+
+// Perform the last round of an AES encryption flow on data (state) in a using
+// the round key in RoundKey, and store the result in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_aesenclast_si128
+FORCE_INLINE __m128i _mm_aesenclast_si128(__m128i a, __m128i RoundKey)
+{
+    return _mm_xor_si128(vreinterpretq_m128i_u8(vaeseq_u8(
+                             vreinterpretq_u8_m128i(a), vdupq_n_u8(0))),
+                         RoundKey);
+}
+
+// Perform the last round of an AES decryption flow on data (state) in a using
+// the round key in RoundKey, and store the result in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_aesdeclast_si128
+FORCE_INLINE __m128i _mm_aesdeclast_si128(__m128i a, __m128i RoundKey)
+{
+    return vreinterpretq_m128i_u8(
+        veorq_u8(vaesdq_u8(vreinterpretq_u8_m128i(a), vdupq_n_u8(0)),
+                 vreinterpretq_u8_m128i(RoundKey)));
+}
+
+// Perform the InvMixColumns transformation on a and store the result in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_aesimc_si128
+FORCE_INLINE __m128i _mm_aesimc_si128(__m128i a)
+{
+    return vreinterpretq_m128i_u8(vaesimcq_u8(vreinterpretq_u8_m128i(a)));
+}
+
+// Assist in expanding the AES cipher key by computing steps towards generating
+// a round key for encryption cipher using data from a and an 8-bit round
+// constant specified in imm8, and store the result in dst."
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_aeskeygenassist_si128
+FORCE_INLINE __m128i _mm_aeskeygenassist_si128(__m128i a, const int rcon)
+{
+    // AESE does ShiftRows and SubBytes on A
+    uint8x16_t u8 = vaeseq_u8(vreinterpretq_u8_m128i(a), vdupq_n_u8(0));
+
+#if !defined(_MSC_VER) || defined(__clang__)
+    uint8x16_t dest = {
+        // Undo ShiftRows step from AESE and extract X1 and X3
+        u8[0x4], u8[0x1], u8[0xE], u8[0xB],  // SubBytes(X1)
+        u8[0x1], u8[0xE], u8[0xB], u8[0x4],  // ROT(SubBytes(X1))
+        u8[0xC], u8[0x9], u8[0x6], u8[0x3],  // SubBytes(X3)
+        u8[0x9], u8[0x6], u8[0x3], u8[0xC],  // ROT(SubBytes(X3))
+    };
+    uint32x4_t r = {0, (unsigned) rcon, 0, (unsigned) rcon};
+    return vreinterpretq_m128i_u8(dest) ^ vreinterpretq_m128i_u32(r);
+#else
+    // We have to do this hack because MSVC is strictly adhering to the CPP
+    // standard, in particular C++03 8.5.1 sub-section 15, which states that
+    // unions must be initialized by their first member type.
+
+    // As per the Windows ARM64 ABI, it is always little endian, so this works
+    __n128 dest{
+        ((uint64_t) u8.n128_u8[0x4] << 0) | ((uint64_t) u8.n128_u8[0x1] << 8) |
+            ((uint64_t) u8.n128_u8[0xE] << 16) |
+            ((uint64_t) u8.n128_u8[0xB] << 24) |
+            ((uint64_t) u8.n128_u8[0x1] << 32) |
+            ((uint64_t) u8.n128_u8[0xE] << 40) |
+            ((uint64_t) u8.n128_u8[0xB] << 48) |
+            ((uint64_t) u8.n128_u8[0x4] << 56),
+        ((uint64_t) u8.n128_u8[0xC] << 0) | ((uint64_t) u8.n128_u8[0x9] << 8) |
+            ((uint64_t) u8.n128_u8[0x6] << 16) |
+            ((uint64_t) u8.n128_u8[0x3] << 24) |
+            ((uint64_t) u8.n128_u8[0x9] << 32) |
+            ((uint64_t) u8.n128_u8[0x6] << 40) |
+            ((uint64_t) u8.n128_u8[0x3] << 48) |
+            ((uint64_t) u8.n128_u8[0xC] << 56)};
+
+    dest.n128_u32[1] = dest.n128_u32[1] ^ rcon;
+    dest.n128_u32[3] = dest.n128_u32[3] ^ rcon;
+
+    return dest;
+#endif
+}
+#endif
+
+/* Others */
+
+// Perform a carry-less multiplication of two 64-bit integers, selected from a
+// and b according to imm8, and store the results in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_clmulepi64_si128
+FORCE_INLINE __m128i _mm_clmulepi64_si128(__m128i _a, __m128i _b, const int imm)
+{
+    uint64x2_t a = vreinterpretq_u64_m128i(_a);
+    uint64x2_t b = vreinterpretq_u64_m128i(_b);
+    switch (imm & 0x11) {
+    case 0x00:
+        return vreinterpretq_m128i_u64(
+            _sse2neon_vmull_p64(vget_low_u64(a), vget_low_u64(b)));
+    case 0x01:
+        return vreinterpretq_m128i_u64(
+            _sse2neon_vmull_p64(vget_high_u64(a), vget_low_u64(b)));
+    case 0x10:
+        return vreinterpretq_m128i_u64(
+            _sse2neon_vmull_p64(vget_low_u64(a), vget_high_u64(b)));
+    case 0x11:
+        return vreinterpretq_m128i_u64(
+            _sse2neon_vmull_p64(vget_high_u64(a), vget_high_u64(b)));
+    default:
+        abort();
+    }
+}
+
+FORCE_INLINE unsigned int _sse2neon_mm_get_denormals_zero_mode(void)
+{
+    union {
+        fpcr_bitfield field;
+#if defined(__aarch64__) || defined(_M_ARM64)
+        uint64_t value;
+#else
+        uint32_t value;
+#endif
+    } r;
+
+#if defined(__aarch64__) || defined(_M_ARM64)
+    r.value = _sse2neon_get_fpcr();
+#else
+    __asm__ __volatile__("vmrs %0, FPSCR" : "=r"(r.value)); /* read */
+#endif
+
+    return r.field.bit24 ? _MM_DENORMALS_ZERO_ON : _MM_DENORMALS_ZERO_OFF;
+}
+
+// Count the number of bits set to 1 in unsigned 32-bit integer a, and
+// return that count in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_popcnt_u32
+FORCE_INLINE int _mm_popcnt_u32(unsigned int a)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+#if __has_builtin(__builtin_popcount)
+    return __builtin_popcount(a);
+#elif defined(_MSC_VER)
+    return _CountOneBits(a);
+#else
+    return (int) vaddlv_u8(vcnt_u8(vcreate_u8((uint64_t) a)));
+#endif
+#else
+    uint32_t count = 0;
+    uint8x8_t input_val, count8x8_val;
+    uint16x4_t count16x4_val;
+    uint32x2_t count32x2_val;
+
+    input_val = vld1_u8((uint8_t *) &a);
+    count8x8_val = vcnt_u8(input_val);
+    count16x4_val = vpaddl_u8(count8x8_val);
+    count32x2_val = vpaddl_u16(count16x4_val);
+
+    vst1_u32(&count, count32x2_val);
+    return count;
+#endif
+}
+
+// Count the number of bits set to 1 in unsigned 64-bit integer a, and
+// return that count in dst.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_popcnt_u64
+FORCE_INLINE int64_t _mm_popcnt_u64(uint64_t a)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+#if __has_builtin(__builtin_popcountll)
+    return __builtin_popcountll(a);
+#elif defined(_MSC_VER)
+    return _CountOneBits64(a);
+#else
+    return (int64_t) vaddlv_u8(vcnt_u8(vcreate_u8(a)));
+#endif
+#else
+    uint64_t count = 0;
+    uint8x8_t input_val, count8x8_val;
+    uint16x4_t count16x4_val;
+    uint32x2_t count32x2_val;
+    uint64x1_t count64x1_val;
+
+    input_val = vld1_u8((uint8_t *) &a);
+    count8x8_val = vcnt_u8(input_val);
+    count16x4_val = vpaddl_u8(count8x8_val);
+    count32x2_val = vpaddl_u16(count16x4_val);
+    count64x1_val = vpaddl_u32(count32x2_val);
+    vst1_u64(&count, count64x1_val);
+    return count;
+#endif
+}
+
+FORCE_INLINE_OPTNONE void _sse2neon_mm_set_denormals_zero_mode(
+    unsigned int flag)
+{
+    // AArch32 Advanced SIMD arithmetic always uses the Flush-to-zero setting,
+    // regardless of the value of the FZ bit.
+    union {
+        fpcr_bitfield field;
+#if defined(__aarch64__) || defined(_M_ARM64)
+        uint64_t value;
+#else
+        uint32_t value;
+#endif
+    } r;
+
+#if defined(__aarch64__) || defined(_M_ARM64)
+    r.value = _sse2neon_get_fpcr();
+#else
+    __asm__ __volatile__("vmrs %0, FPSCR" : "=r"(r.value)); /* read */
+#endif
+
+    r.field.bit24 = (flag & _MM_DENORMALS_ZERO_MASK) == _MM_DENORMALS_ZERO_ON;
+
+#if defined(__aarch64__) || defined(_M_ARM64)
+    _sse2neon_set_fpcr(r.value);
+#else
+    __asm__ __volatile__("vmsr FPSCR, %0" ::"r"(r));        /* write */
+#endif
+}
+
+// Return the current 64-bit value of the processor's time-stamp counter.
+// https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=rdtsc
+FORCE_INLINE uint64_t _rdtsc(void)
+{
+#if defined(__aarch64__) || defined(_M_ARM64)
+    uint64_t val;
+
+    /* According to ARM DDI 0487F.c, from Armv8.0 to Armv8.5 inclusive, the
+     * system counter is at least 56 bits wide; from Armv8.6, the counter
+     * must be 64 bits wide.  So the system counter could be less than 64
+     * bits wide and it is attributed with the flag 'cap_user_time_short'
+     * is true.
+     */
+#if defined(_MSC_VER) && !defined(__clang__)
+    val = _ReadStatusReg(ARM64_SYSREG(3, 3, 14, 0, 2));
+#else
+    __asm__ __volatile__("mrs %0, cntvct_el0" : "=r"(val));
+#endif
+
+    return val;
+#else
+    uint32_t pmccntr, pmuseren, pmcntenset;
+    // Read the user mode Performance Monitoring Unit (PMU)
+    // User Enable Register (PMUSERENR) access permissions.
+    __asm__ __volatile__("mrc p15, 0, %0, c9, c14, 0" : "=r"(pmuseren));
+    if (pmuseren & 1) {  // Allows reading PMUSERENR for user mode code.
+        __asm__ __volatile__("mrc p15, 0, %0, c9, c12, 1" : "=r"(pmcntenset));
+        if (pmcntenset & 0x80000000UL) {  // Is it counting?
+            __asm__ __volatile__("mrc p15, 0, %0, c9, c13, 0" : "=r"(pmccntr));
+            // The counter is set up to count every 64th cycle
+            return (uint64_t) (pmccntr) << 6;
+        }
+    }
+
+    // Fallback to syscall as we can't enable PMUSERENR in user mode.
+    struct timeval tv;
+    gettimeofday(&tv, NULL);
+    return (uint64_t) (tv.tv_sec) * 1000000 + tv.tv_usec;
+#endif
+}
+
+#if defined(__GNUC__) || defined(__clang__)
+#pragma pop_macro("ALIGN_STRUCT")
+#pragma pop_macro("FORCE_INLINE")
+#pragma pop_macro("FORCE_INLINE_OPTNONE")
+#endif
+
+#if defined(__GNUC__) && !defined(__clang__)
+#pragma GCC pop_options
+#endif
+
+#endif
diff --git a/EEDI3/vectorclass/vector_convert.h b/EEDI3/vectorclass/vector_convert.h
new file mode 100644
index 0000000..f942ecb
--- /dev/null
+++ b/EEDI3/vectorclass/vector_convert.h
@@ -0,0 +1,828 @@
+/**************************  vector_convert.h   *******************************
+* Author:        Agner Fog
+* Date created:  2014-07-23
+* Last modified: 2022-07-20
+* Version:       2.02.00
+* Project:       vector class library
+* Description:
+* Header file for conversion between different vector classes with different
+* sizes. Also includes verious generic template functions.
+*
+* (c) Copyright 2012-2022 Agner Fog.
+* Apache License version 2.0 or later.
+*****************************************************************************/
+
+#ifndef VECTOR_CONVERT_H
+#define VECTOR_CONVERT_H
+
+#ifndef VECTORCLASS_H
+#include "vectorclass.h"
+#endif
+
+#if VECTORCLASS_H < 20200
+#error Incompatible versions of vector class library mixed
+#endif
+
+#ifdef VCL_NAMESPACE
+namespace VCL_NAMESPACE {
+#endif
+
+#if MAX_VECTOR_SIZE >= 256
+
+/*****************************************************************************
+*
+*          Extend from 128 to 256 bit vectors
+*
+*****************************************************************************/
+
+#if INSTRSET >= 8  // AVX2. 256 bit integer vectors
+
+// sign extend
+static inline Vec16s extend (Vec16c const a) {
+    return _mm256_cvtepi8_epi16(a);
+}
+
+// zero extend
+static inline Vec16us extend (Vec16uc const a) {
+    return _mm256_cvtepu8_epi16(a);
+}
+
+// sign extend
+static inline Vec8i extend (Vec8s const a) {
+    return _mm256_cvtepi16_epi32(a);
+}
+
+// zero extend
+static inline Vec8ui extend (Vec8us const a) {
+    return _mm256_cvtepu16_epi32(a);
+}
+
+// sign extend
+static inline Vec4q extend (Vec4i const a) {
+    return _mm256_cvtepi32_epi64(a);
+}
+
+// zero extend
+static inline Vec4uq extend (Vec4ui const a) {
+    return _mm256_cvtepu32_epi64(a);
+}
+
+
+#else  // no AVX2. 256 bit integer vectors are emulated
+
+// sign extend and zero extend functions:
+static inline Vec16s extend (Vec16c const a) {
+    return Vec16s(extend_low(a), extend_high(a));
+}
+
+static inline Vec16us extend (Vec16uc const a) {
+    return Vec16us(extend_low(a), extend_high(a));
+}
+
+static inline Vec8i extend (Vec8s const a) {
+    return Vec8i(extend_low(a), extend_high(a));
+}
+
+static inline Vec8ui extend (Vec8us const a) {
+    return Vec8ui(extend_low(a), extend_high(a));
+}
+
+static inline Vec4q extend (Vec4i const a) {
+    return Vec4q(extend_low(a), extend_high(a));
+}
+
+static inline Vec4uq extend (Vec4ui const a) {
+    return Vec4uq(extend_low(a), extend_high(a));
+}
+
+#endif  // AVX2
+
+/*****************************************************************************
+*
+*          Conversions between float and double
+*
+*****************************************************************************/
+#if INSTRSET >= 7  // AVX. 256 bit float vectors
+
+// float to double
+static inline Vec4d to_double (Vec4f const a) {
+    return _mm256_cvtps_pd(a);
+}
+
+// double to float
+static inline Vec4f to_float (Vec4d const a) {
+    return _mm256_cvtpd_ps(a);
+}
+
+#else  // no AVX2. 256 bit float vectors are emulated
+
+// float to double
+static inline Vec4d to_double (Vec4f const a) {
+    Vec2d lo = _mm_cvtps_pd(a);
+    Vec2d hi = _mm_cvtps_pd(_mm_movehl_ps(a, a));
+    return Vec4d(lo,hi);
+}
+
+// double to float
+static inline Vec4f to_float (Vec4d const a) {
+    Vec4f lo = _mm_cvtpd_ps(a.get_low());
+    Vec4f hi = _mm_cvtpd_ps(a.get_high());
+    return _mm_movelh_ps(lo, hi);
+}
+
+#endif
+
+/*****************************************************************************
+*
+*          Reduce from 256 to 128 bit vectors
+*
+*****************************************************************************/
+#if INSTRSET >= 10  // AVX512VL
+
+// compress functions. overflow wraps around
+static inline Vec16c compress (Vec16s const a) {
+    return _mm256_cvtepi16_epi8(a);
+}
+
+static inline Vec16uc compress (Vec16us const a) {
+    return _mm256_cvtepi16_epi8(a);
+}
+
+static inline Vec8s compress (Vec8i const a) {
+    return _mm256_cvtepi32_epi16(a);
+}
+
+static inline Vec8us compress (Vec8ui const a) {
+    return _mm256_cvtepi32_epi16(a);
+}
+
+static inline Vec4i compress (Vec4q const a) {
+    return _mm256_cvtepi64_epi32(a);
+}
+
+static inline Vec4ui compress (Vec4uq const a) {
+    return _mm256_cvtepi64_epi32(a);
+}
+
+// compress_saturated functions. overflow saturates
+static inline Vec16c compress_saturated (Vec16s const a) {
+    return _mm256_cvtsepi16_epi8(a);
+}
+
+static inline Vec16uc compress_saturated (Vec16us const a) {
+    return _mm256_cvtusepi16_epi8(a);
+}
+
+static inline Vec8s compress_saturated (Vec8i const a) {
+    return _mm256_cvtsepi32_epi16(a);
+}
+
+static inline Vec8us compress_saturated (Vec8ui const a) {
+    return _mm256_cvtusepi32_epi16(a);
+}
+
+static inline Vec4i compress_saturated (Vec4q const a) {
+    return _mm256_cvtsepi64_epi32(a);
+}
+
+static inline Vec4ui compress_saturated (Vec4uq const a) {
+    return _mm256_cvtusepi64_epi32(a);
+}
+
+
+#else  // no AVX512
+
+// compress functions. overflow wraps around
+static inline Vec16c compress (Vec16s const a) {
+    return compress(a.get_low(), a.get_high());
+}
+
+static inline Vec16uc compress (Vec16us const a) {
+    return compress(a.get_low(), a.get_high());
+}
+
+static inline Vec8s compress (Vec8i const a) {
+    return compress(a.get_low(), a.get_high());
+}
+
+static inline Vec8us compress (Vec8ui const a) {
+    return compress(a.get_low(), a.get_high());
+}
+
+static inline Vec4i compress (Vec4q const a) {
+    return compress(a.get_low(), a.get_high());
+}
+
+static inline Vec4ui compress (Vec4uq const a) {
+    return compress(a.get_low(), a.get_high());
+}
+
+// compress_saturated functions. overflow saturates
+static inline Vec16c compress_saturated (Vec16s const a) {
+    return compress_saturated(a.get_low(), a.get_high());
+}
+
+static inline Vec16uc compress_saturated (Vec16us const a) {
+    return compress_saturated(a.get_low(), a.get_high());
+}
+
+static inline Vec8s compress_saturated (Vec8i const a) {
+    return compress_saturated(a.get_low(), a.get_high());
+}
+
+static inline Vec8us compress_saturated (Vec8ui const a) {
+    return compress_saturated(a.get_low(), a.get_high());
+}
+
+static inline Vec4i compress_saturated (Vec4q const a) {
+    return compress_saturated(a.get_low(), a.get_high());
+}
+
+static inline Vec4ui compress_saturated (Vec4uq const a) {
+    return compress_saturated(a.get_low(), a.get_high());
+}
+
+#endif  // AVX512
+
+#endif // MAX_VECTOR_SIZE >= 256
+
+
+#if MAX_VECTOR_SIZE >= 512
+
+/*****************************************************************************
+*
+*          Reduce from 512 to 256 bit vectors
+*
+*****************************************************************************/
+#if INSTRSET >= 10  // AVX512VL
+
+// compress_saturated functions. overflow saturates
+static inline Vec32c compress_saturated (Vec32s const a) {
+    return _mm512_cvtsepi16_epi8(a);
+}
+
+static inline Vec32uc compress_saturated (Vec32us const a) {
+    return _mm512_cvtusepi16_epi8(a);
+}
+
+static inline Vec16s compress_saturated (Vec16i const a) {
+    return _mm512_cvtsepi32_epi16(a);
+}
+
+static inline Vec16us compress_saturated (Vec16ui const a) {
+    return _mm512_cvtusepi32_epi16(a);
+}
+
+static inline Vec8i compress_saturated (Vec8q const a) {
+    return _mm512_cvtsepi64_epi32(a);
+}
+
+static inline Vec8ui compress_saturated (Vec8uq const a) {
+    return _mm512_cvtusepi64_epi32(a);
+}
+
+#else  // no AVX512
+
+// compress_saturated functions. overflow saturates
+static inline Vec32c compress_saturated (Vec32s const a) {
+    return compress_saturated(a.get_low(), a.get_high());
+}
+
+static inline Vec32uc compress_saturated (Vec32us const a) {
+    return compress_saturated(a.get_low(), a.get_high());
+}
+
+static inline Vec16s compress_saturated (Vec16i const a) {
+    return compress_saturated(a.get_low(), a.get_high());
+}
+
+static inline Vec16us compress_saturated (Vec16ui const a) {
+    return compress_saturated(a.get_low(), a.get_high());
+}
+
+static inline Vec8i compress_saturated (Vec8q const a) {
+    return compress_saturated(a.get_low(), a.get_high());
+}
+
+static inline Vec8ui compress_saturated (Vec8uq const a) {
+    return compress_saturated(a.get_low(), a.get_high());
+}
+
+#endif  // AVX512
+
+/*****************************************************************************
+*
+*          Extend from 256 to 512 bit vectors
+*
+*****************************************************************************/
+
+#if INSTRSET >= 9  // AVX512. 512 bit integer vectors
+
+// sign extend
+static inline Vec32s extend (Vec32c const a) {
+#if INSTRSET >= 10
+    return _mm512_cvtepi8_epi16(a);
+#else
+    return Vec32s(extend_low(a), extend_high(a));
+#endif
+}
+
+// zero extend
+static inline Vec32us extend (Vec32uc const a) {
+#if INSTRSET >= 10
+    return _mm512_cvtepu8_epi16(a);
+#else
+    return Vec32us(extend_low(a), extend_high(a));
+#endif
+}
+
+// sign extend
+static inline Vec16i extend (Vec16s const a) {
+    return _mm512_cvtepi16_epi32(a);
+}
+
+// zero extend
+static inline Vec16ui extend (Vec16us const a) {
+    return _mm512_cvtepu16_epi32(a);
+}
+
+// sign extend
+static inline Vec8q extend (Vec8i const a) {
+    return _mm512_cvtepi32_epi64(a);
+}
+
+// zero extend
+static inline Vec8uq extend (Vec8ui const a) {
+    return _mm512_cvtepu32_epi64(a);
+}
+
+#else  // no AVX512. 512 bit vectors are emulated
+
+
+
+// sign extend
+static inline Vec32s extend (Vec32c const a) {
+    return Vec32s(extend_low(a), extend_high(a));
+}
+
+// zero extend
+static inline Vec32us extend (Vec32uc const a) {
+    return Vec32us(extend_low(a), extend_high(a));
+}
+
+// sign extend
+static inline Vec16i extend (Vec16s const a) {
+    return Vec16i(extend_low(a), extend_high(a));
+}
+
+// zero extend
+static inline Vec16ui extend (Vec16us const a) {
+    return Vec16ui(extend_low(a), extend_high(a));
+}
+
+// sign extend
+static inline Vec8q extend (Vec8i const a) {
+    return Vec8q(extend_low(a), extend_high(a));
+}
+
+// zero extend
+static inline Vec8uq extend (Vec8ui const a) {
+    return Vec8uq(extend_low(a), extend_high(a));
+}
+
+#endif  // AVX512
+
+
+/*****************************************************************************
+*
+*          Reduce from 512 to 256 bit vectors
+*
+*****************************************************************************/
+#if INSTRSET >= 9  // AVX512F
+
+// compress functions. overflow wraps around
+static inline Vec32c compress (Vec32s const a) {
+#if INSTRSET >= 10  // AVVX512BW
+    return _mm512_cvtepi16_epi8(a);
+#else
+    return compress(a.get_low(), a.get_high());
+#endif
+}
+
+static inline Vec32uc compress (Vec32us const a) {
+    return Vec32uc(compress(Vec32s(a)));
+}
+
+static inline Vec16s compress (Vec16i const a) {
+    return _mm512_cvtepi32_epi16(a);
+}
+
+static inline Vec16us compress (Vec16ui const a) {
+    return _mm512_cvtepi32_epi16(a);
+}
+
+static inline Vec8i compress (Vec8q const a) {
+    return _mm512_cvtepi64_epi32(a);
+}
+
+static inline Vec8ui compress (Vec8uq const a) {
+    return _mm512_cvtepi64_epi32(a);
+}
+
+#else  // no AVX512
+
+// compress functions. overflow wraps around
+static inline Vec32c compress (Vec32s const a) {
+    return compress(a.get_low(), a.get_high());
+}
+
+static inline Vec32uc compress (Vec32us const a) {
+    return compress(a.get_low(), a.get_high());
+}
+
+static inline Vec16s compress (Vec16i const a) {
+    return compress(a.get_low(), a.get_high());
+}
+
+static inline Vec16us compress (Vec16ui const a) {
+    return compress(a.get_low(), a.get_high());
+}
+
+static inline Vec8i compress (Vec8q const a) {
+    return compress(a.get_low(), a.get_high());
+}
+
+static inline Vec8ui compress (Vec8uq const a) {
+    return compress(a.get_low(), a.get_high());
+}
+
+#endif  // AVX512
+
+/*****************************************************************************
+*
+*          Conversions between float and double
+*
+*****************************************************************************/
+
+#if INSTRSET >= 9  // AVX512. 512 bit float vectors
+
+// float to double
+static inline Vec8d to_double (Vec8f const a) {
+    return _mm512_cvtps_pd(a);
+}
+
+// double to float
+static inline Vec8f to_float (Vec8d const a) {
+    return _mm512_cvtpd_ps(a);
+}
+
+#else  // no AVX512. 512 bit float vectors are emulated
+
+// float to double
+static inline Vec8d to_double (Vec8f const a) {
+    Vec4d lo = to_double(a.get_low());
+    Vec4d hi = to_double(a.get_high());
+    return Vec8d(lo,hi);
+}
+
+// double to float
+static inline Vec8f to_float (Vec8d const a) {
+    Vec4f lo = to_float(a.get_low());
+    Vec4f hi = to_float(a.get_high());
+    return Vec8f(lo, hi);
+}
+
+#endif
+
+#endif // MAX_VECTOR_SIZE >= 512
+
+// double to float
+static inline Vec4f to_float (Vec2d const a) {
+    return _mm_cvtpd_ps(a);
+}
+
+
+/*****************************************************************************
+*
+*          Generic template functions
+*
+*  These templates define functions for multiple vector types in one template
+*
+*****************************************************************************/
+
+// concatenate two vectors into one vector of double size
+template <typename T> auto concatenate2(T const a, T const b) {
+    static_assert(sizeof(T) * 8 < MAX_VECTOR_SIZE, "Maximum vector size exceeded");
+    return decltype(extend_z(a))(a, b);   // call constructor for double size vector type
+}
+
+
+// horizontal min/max of vector elements
+// implemented with universal template, works for all vector types:
+
+template <typename T> auto horizontal_min(T const x) {
+    if constexpr (T::elementtype() >= 15) {
+        // T is a float or double vector
+        if (horizontal_or(is_nan(x))) {
+            // check for NAN because min does not guarantee NAN propagation
+            return x[horizontal_find_first(is_nan(x))];
+        }
+    }
+    return horizontal_min1(x);
+}
+
+template <typename T> auto horizontal_min1(T const x) {
+    if constexpr (T::elementtype() <= 3) {       // boolean vector type
+        return horizontal_and(x);
+    }
+    else if constexpr (sizeof(T) >= 32) {
+        // split recursively into smaller vectors
+        return horizontal_min1(min(x.get_low(), x.get_high()));
+    }
+    else if constexpr (T::size() == 2) {
+        T a = permute2 <1, V_DC>(x);             // high half
+        T b = min(a, x);
+        return b[0];
+    }
+    else if constexpr (T::size() == 4) {
+        T a = permute4<2, 3, V_DC, V_DC>(x);     // high half
+        T b = min(a, x);
+        a = permute4<1, V_DC, V_DC, V_DC>(b);
+        b = min(a, b);
+        return b[0];
+    }
+    else if constexpr (T::size() == 8) {
+        T a = permute8<4, 5, 6, 7, V_DC, V_DC, V_DC, V_DC>(x);  // high half
+        T b = min(a, x);
+        a = permute8<2, 3, V_DC, V_DC, V_DC, V_DC, V_DC, V_DC>(b);
+        b = min(a, b);
+        a = permute8<1, V_DC, V_DC, V_DC, V_DC, V_DC, V_DC, V_DC>(b);
+        b = min(a, b);
+        return b[0];
+    }
+    else {
+        static_assert(T::size() == 16);          // no other size is allowed
+        T a = permute16<8, 9, 10, 11, 12, 13, 14, 15, V_DC, V_DC, V_DC, V_DC, V_DC, V_DC, V_DC, V_DC >(x);  // high half
+        T b = min(a, x);
+        a = permute16<4, 5, 6, 7, V_DC, V_DC, V_DC, V_DC, V_DC, V_DC, V_DC, V_DC, V_DC, V_DC, V_DC, V_DC>(b);
+        b = min(a, b);
+        a = permute16<2, 3, V_DC, V_DC, V_DC, V_DC, V_DC, V_DC, V_DC, V_DC, V_DC, V_DC, V_DC, V_DC, V_DC, V_DC>(b);
+        b = min(a, b);
+        a = permute16<1, V_DC, V_DC, V_DC, V_DC, V_DC, V_DC, V_DC, V_DC, V_DC, V_DC, V_DC, V_DC, V_DC, V_DC, V_DC>(b);
+        b = min(a, b);
+        return b[0];
+    }
+}
+
+template <typename T> auto horizontal_max(T const x) {
+    if constexpr (T::elementtype() >= 15) {
+        // T is a float or double vector
+        if (horizontal_or(is_nan(x))) {
+            // check for NAN because max does not guarantee NAN propagation
+            return x[horizontal_find_first(is_nan(x))];
+        }
+    }
+    return horizontal_max1(x);
+}
+
+template <typename T> auto horizontal_max1(T const x) {
+    if constexpr (T::elementtype() <= 3) {       // boolean vector type
+        return horizontal_or(x);
+    }
+    else if constexpr (sizeof(T) >= 32) {
+        // split recursively into smaller vectors
+        return horizontal_max1(max(x.get_low(), x.get_high()));
+    }
+    else if constexpr (T::size() == 2) {
+        T a = permute2 <1, V_DC>(x);             // high half
+        T b = max(a, x);
+        return b[0];
+    }
+    else if constexpr (T::size() == 4) {
+        T a = permute4<2, 3, V_DC, V_DC>(x);     // high half
+        T b = max(a, x);
+        a = permute4<1, V_DC, V_DC, V_DC>(b);
+        b = max(a, b);
+        return b[0];
+    }
+    else if constexpr (T::size() == 8) {
+        T a = permute8<4, 5, 6, 7, V_DC, V_DC, V_DC, V_DC>(x);  // high half
+        T b = max(a, x);
+        a = permute8<2, 3, V_DC, V_DC, V_DC, V_DC, V_DC, V_DC>(b);
+        b = max(a, b);
+        a = permute8<1, V_DC, V_DC, V_DC, V_DC, V_DC, V_DC, V_DC>(b);
+        b = max(a, b);
+        return b[0];
+    }
+    else {
+        static_assert(T::size() == 16);          // no other size is allowed
+        T a = permute16<8, 9, 10, 11, 12, 13, 14, 15, V_DC, V_DC, V_DC, V_DC, V_DC, V_DC, V_DC, V_DC >(x);  // high half
+        T b = max(a, x);
+        a = permute16<4, 5, 6, 7, V_DC, V_DC, V_DC, V_DC, V_DC, V_DC, V_DC, V_DC, V_DC, V_DC, V_DC, V_DC>(b);
+        b = max(a, b);
+        a = permute16<2, 3, V_DC, V_DC, V_DC, V_DC, V_DC, V_DC, V_DC, V_DC, V_DC, V_DC, V_DC, V_DC, V_DC, V_DC>(b);
+        b = max(a, b);
+        a = permute16<1, V_DC, V_DC, V_DC, V_DC, V_DC, V_DC, V_DC, V_DC, V_DC, V_DC, V_DC, V_DC, V_DC, V_DC, V_DC>(b);
+        b = max(a, b);
+        return b[0];
+    }
+}
+
+// Find first element that is true in a boolean vector
+template <typename V>
+static inline int horizontal_find_first(V const x) {
+    static_assert(V::elementtype() == 2 || V::elementtype() == 3, "Boolean vector expected");
+    auto bits = to_bits(x);                      // convert to bits
+    if (bits == 0) return -1;
+    if constexpr (V::size() < 32) {
+        return bit_scan_forward((uint32_t)bits);
+    }
+    else {
+        return bit_scan_forward(bits);
+    }
+}
+
+// Count the number of elements that are true in a boolean vector
+template <typename V>
+static inline int horizontal_count(V const x) {
+    static_assert(V::elementtype() == 2 || V::elementtype() == 3, "Boolean vector expected");
+    auto bits = to_bits(x);                      // convert to bits
+    if constexpr (V::size() < 32) {
+        return vml_popcnt((uint32_t)bits);
+    }
+    else {
+        return (int)vml_popcnt(bits);
+    }
+}
+
+// maximum and minimum functions. This version is sure to propagate NANs,
+// conforming to the new IEEE-754 2019 standard
+template <typename V>
+static inline V maximum(V const a, V const b) {
+    if constexpr (V::elementtype() < 15) {
+        return max(a, b);              // integer type
+    }
+    else {                             // float or double vector
+        V y = select(is_nan(a), a, max(a, b));
+#ifdef SIGNED_ZERO                     // pedantic about signed zero
+        y = select(a == b, a & b, y);  // maximum(+0, -0) = +0
+#endif
+        return y;
+    }
+}
+
+template <typename V>
+static inline V minimum(V const a, V const b) {
+    if constexpr (V::elementtype() < 15) {
+        return min(a, b);              // integer type
+    }
+    else {                             // float or double vector
+        V y = select(is_nan(a), a, min(a, b));
+#ifdef SIGNED_ZERO                     // pedantic about signed zero
+        y = select(a == b, a | b, y);  // minimum(+0, -0) = -0
+#endif
+        return y;
+    }
+}
+
+// floating point remainder
+// -denominator/2 <= result < denominator/2
+template <typename V>
+static inline V fremainder(V const numerator, double const denominator) {
+    // (Optimization notice: Calculation of 1/denominator and constants for extended precision reduction
+    // may be optimized by a compiler moving loop-invariant code. This is intended)
+    static_assert(V::elementtype() == 16 || V::elementtype() == 17, "wrong vector type"); // supports only float and double
+    if (denominator > 0.) {                                // denominator must be positive
+        if constexpr (V::elementtype() == 16) {            // float
+#ifdef __FMA__
+            float recipd = float(1.0 / denominator);       // reciprocal denominator
+            float fd = float(denominator);                 // denominator rounded to single precision
+            float d2 = float(denominator - fd);            // remaining bits for double precision
+            V q = round(numerator * recipd);               // divide and round
+            V m = nmul_add(q, d2, nmul_add(q, fd, numerator));// double precision reduction
+#else  // no FMA. Use extended precision reduction
+            union {
+                float f;
+                uint32_t i;
+            } u;
+            u.f = float(denominator);
+            u.i &= 0xFFFFF000;                             // remove 12 least significant bits for extended precision reduction
+            float d2 = denominator - u.f;                  // remaining bits
+            float recipd = float(1.0 / denominator);       // reciprocal
+            V q = round(numerator * recipd);               // divide and round
+            V m = nmul_add(q, d2, nmul_add(q, u.f, numerator));// extended precision reduction
+#endif  // FMA
+            if (true) { // Check that result is within desired interval. This may be omitted if not essential:
+                // This check may be needed in extreme cases of numerator > 1.E5 * denominator
+                auto too_high = m >= float( denominator * 0.5);
+                auto too_low  = m <  float(-denominator * 0.5);
+                m = if_sub(too_high, m, float(denominator));
+                m = if_add(too_low,  m, float(denominator));
+            }
+            return m;
+        }
+        else if constexpr (V::elementtype() == 17) {       // double precision
+#ifdef __FMA__
+            double recipd = 1.0 / denominator;             // reciprocal
+            V q = round(numerator * recipd);               // divide and round
+            V m = nmul_add(q, denominator, numerator);     // nmul_add has extended precision
+#else  // no FMA. Use extended precision reduction
+            union {
+                double f;
+                uint64_t i;
+            } u;
+            u.f = denominator;
+            u.i &= 0xFFFFFFFFFF000000;                     // remove 24 least significant bits for extended precision reduction
+            double d2 = denominator - u.f;                 // remaining bits
+            double recipd = 1.0 / denominator;             // reciprocal
+            V q = round(numerator * recipd);               // divide and round
+            V m = nmul_add(q, d2, nmul_add(q, u.f, numerator));// extended precision reduction
+#endif  // FMA
+            if (true) { // Check that result is within desired interval. This may be omitted if not essential:
+                // This check is rarely needed except in extreme cases of numerator > 1.E14 * denominator
+                auto too_high = m >=  denominator * 0.5;
+                auto too_low  = m <  -denominator * 0.5;
+                m = if_sub(too_high, m, denominator);
+                m = if_add(too_low,  m, denominator);
+            }
+            return m;
+        }
+    }
+    else {
+        return nan_vec<V>(1);                              //  denominator is not positive
+    }
+}
+
+// floating point modulo
+// 0 <= result < denominator
+template <typename V>
+static inline V fmodulo(V const numerator, double const denominator) {
+    // (Optimization notice: Calculation of 1/denominator and constants for extended precision reduction
+    // may be optimized by a compiler moving loop-invariant code. This is intended)
+    static_assert(V::elementtype() == 16 || V::elementtype() == 17, "wrong vector type"); // supports only float and double
+    if (denominator > 0.) {                                // denominator must be positive
+        if constexpr (V::elementtype() == 16) {            // float
+#ifdef __FMA__
+            float recipd = float(1.0 / denominator);       // reciprocal denominator
+            float fd = float(denominator);                 // denominator rounded to single precision
+            float d2 = float(denominator - fd);            // remaining bits for double precision
+            V q = floor(numerator * recipd);               // divide and floor
+            V m = nmul_add(q, d2, nmul_add(q, fd, numerator));// double precision reduction
+#else  // no FMA. Use extended precision reduction
+            union {
+                float f;
+                uint32_t i;
+            } u;
+            u.f = float(denominator);
+            u.i &= 0xFFFFF000;                             // remove 12 least significant bits for extended precision reduction
+            float d2 = denominator - u.f;                  // remaining bits
+            float recipd = float(1.0 / denominator);       // reciprocal
+            V q = floor(numerator * recipd);               // divide and floor
+            V m = nmul_add(q, d2, nmul_add(q, u.f, numerator));// extended precision reduction
+#endif  // FMA
+            if (true) { // Check that result is within desired interval. This may be omitted if not essential:
+                // This check may be needed in extreme cases of numerator > 1.E5 * denominator
+                auto too_high = m >= float(denominator);
+                auto too_low  = m <  0.f;
+                m = if_sub(too_high, m, float(denominator));
+                m = if_add(too_low,  m, float(denominator));
+            }
+            return m;
+        }
+        else if constexpr (V::elementtype() == 17) {       // double precision
+#ifdef __FMA__
+            double recipd = 1.0 / denominator;             // reciprocal
+            V q = floor(numerator * recipd);               // divide and floor
+            V m = nmul_add(q, denominator, numerator);     // nmul_add has extended precision
+#else  // no FMA. Use extended precision reduction
+            union {
+                double f;
+                uint64_t i;
+            } u;
+            u.f = denominator;
+            u.i &= 0xFFFFFFFFFF000000;                     // remove 24 least significant bits for extended precision reduction
+            double d2 = denominator - u.f;                 // remaining bits
+            double recipd = 1.0 / denominator;             // reciprocal
+            V q = floor(numerator * recipd);               // divide and floor
+            V m = nmul_add(q, d2, nmul_add(q, u.f, numerator));// extended precision reduction
+#endif  // FMA
+            if (true) { // Check that result is within desired interval. This may be omitted if not essential:
+                // This check is rarely needed except in extreme cases of numerator > 1.E14 * denominator
+                auto too_high = m >= denominator;
+                auto too_low  = m <  0.;
+                m = if_sub(too_high, m, denominator);
+                m = if_add(too_low,  m, denominator);
+            }
+            return m;
+        }
+    }
+    else {
+        return nan_vec<V>(1);                              //  denominator is not positive
+    }
+}
+
+#ifdef VCL_NAMESPACE
+}
+#endif
+
+#endif // VECTOR_CONVERT_H
diff --git a/EEDI3/vectorclass/vectorf256.h b/EEDI3/vectorclass/vectorf256.h
index feeeda5..2b222c7 100644
--- a/EEDI3/vectorclass/vectorf256.h
+++ b/EEDI3/vectorclass/vectorf256.h
@@ -1,16 +1,13 @@
 /****************************  vectorf256.h   *******************************
 * Author:        Agner Fog
 * Date created:  2012-05-30
-* Last modified: 2017-07-27
-* Version:       1.30
-* Project:       vector classes
+* Last modified: 2023-07-04
+* Version:       2.02.02
+* Project:       vector class library
 * Description:
-* Header file defining 256-bit floating point vector classes as interface
-* to intrinsic functions in x86 microprocessors with AVX instruction set.
+* Header file defining 256-bit floating point vector classes
 *
-* Instructions:
-* Use Gnu, Intel or Microsoft C++ compiler. Compile for the desired 
-* instruction set, which must be at least AVX.
+* Instructions: see vcl_manual.pdf
 *
 * The following vector classes are defined here:
 * Vec8f     Vector of 8 single precision floating point numbers
@@ -21,81 +18,54 @@
 * Each vector object is represented internally in the CPU as a 256-bit register.
 * This header file defines operators and functions for these vectors.
 *
-* For example:
-* Vec4d a(1., 2., 3., 4.), b(5., 6., 7., 8.), c;
-* c = a + b;     // now c contains (6., 8., 10., 12.)
-*
-* For detailed instructions, see VectorClass.pdf
-*
-* (c) Copyright 2012-2017 GNU General Public License http://www.gnu.org/licenses
+* (c) Copyright 2012-2023 Agner Fog.
+* Apache License version 2.0 or later.
 *****************************************************************************/
 
-// check combination of header files
-#if defined (VECTORF256_H)
-#if    VECTORF256_H != 2
-#error Two different versions of vectorf256.h included
+#ifndef VECTORF256_H
+#define VECTORF256_H  1
+
+#ifndef VECTORCLASS_H
+#include "vectorclass.h"
+#endif
+
+#if VECTORCLASS_H < 20200
+#error Incompatible versions of vector class library mixed
 #endif
-#else
-#define VECTORF256_H  2
 
-#if INSTRSET < 7   // AVX required
-#error Please compile for the AVX instruction set or higher
+#ifdef VECTORF256E_H
+#error Two different versions of vectorf256.h included
 #endif
 
-#include "vectorf128.h"  // Define 128-bit vectors
 
 #ifdef VCL_NAMESPACE
 namespace VCL_NAMESPACE {
 #endif
 
-/*****************************************************************************
-*
-*          select functions
-*
-*****************************************************************************/
-// Select between two __m256 sources, element by element. Used in various functions 
-// and operators. Corresponds to this pseudocode:
-// for (int i = 0; i < 8; i++) result[i] = s[i] ? a[i] : b[i];
-// Each element in s must be either 0 (false) or 0xFFFFFFFF (true).
-static inline __m256 selectf (__m256 const & s, __m256 const & a, __m256 const & b) {
-    return _mm256_blendv_ps (b, a, s);
-}
-
-// Same, with two __m256d sources.
-// and operators. Corresponds to this pseudocode:
-// for (int i = 0; i < 4; i++) result[i] = s[i] ? a[i] : b[i];
-// Each element in s must be either 0 (false) or 0xFFFFFFFFFFFFFFFF (true). No other 
-// values are allowed.
-static inline __m256d selectd (__m256d const & s, __m256d const & a, __m256d const & b) {
-    return _mm256_blendv_pd (b, a, s);
-}
-
-
 
 /*****************************************************************************
 *
 *          Generate compile-time constant vector
 *
 *****************************************************************************/
-// Generate a constant vector of 8 integers stored in memory,
-// load as __m256
-template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7>
-static inline __m256 constant8f() {
-    static const union {
-        int     i[8];
-        __m256  ymm;
+
+// Generate a constant vector of 8 integers stored in memory
+template <uint32_t i0, uint32_t i1, uint32_t i2, uint32_t i3, uint32_t i4, uint32_t i5, uint32_t i6, uint32_t i7>
+inline __m256 constant8f() {
+    /*
+    const union {
+        uint32_t i[8];
+        __m256   ymm;
     } u = {{i0,i1,i2,i3,i4,i5,i6,i7}};
     return u.ymm;
+    */
+    return _mm256_castsi256_ps(_mm256_setr_epi32(i0,i1,i2,i3,i4,i5,i6,i7));
 }
 
 
-/*****************************************************************************
-*
-*         Join two 128-bit vectors
-*
-*****************************************************************************/
+//    Join two 128-bit vectors. Used below
 #define set_m128r(lo,hi) _mm256_insertf128_ps(_mm256_castps128_ps256(lo),(hi),1)
-    // _mm256_set_m128(hi,lo); // not defined in all versions of immintrin.h
+// _mm256_set_m128(hi,lo); // not defined in all versions of immintrin.h
 
 
 /*****************************************************************************
@@ -104,17 +74,18 @@ static inline __m256 constant8f() {
 *
 *****************************************************************************/
 
+#if INSTRSET < 10  // broad boolean vectors
+
 class Vec8fb {
 protected:
     __m256 ymm; // Float vector
 public:
     // Default constructor:
-    Vec8fb() {
-    }
+    Vec8fb() = default;
     // Constructor to build from all elements:
     Vec8fb(bool b0, bool b1, bool b2, bool b3, bool b4, bool b5, bool b6, bool b7) {
 #if INSTRSET >= 8  // AVX2
-        ymm = _mm256_castsi256_ps(_mm256_setr_epi32(-(int)b0, -(int)b1, -(int)b2, -(int)b3, -(int)b4, -(int)b5, -(int)b6, -(int)b7)); 
+        ymm = _mm256_castsi256_ps(_mm256_setr_epi32(-(int)b0, -(int)b1, -(int)b2, -(int)b3, -(int)b4, -(int)b5, -(int)b6, -(int)b7));
 #else
         __m128 blo = _mm_castsi128_ps(_mm_setr_epi32(-(int)b0, -(int)b1, -(int)b2, -(int)b3));
         __m128 bhi = _mm_castsi128_ps(_mm_setr_epi32(-(int)b4, -(int)b5, -(int)b6, -(int)b7));
@@ -122,15 +93,15 @@ public:
 #endif
     }
     // Constructor to build from two Vec4fb:
-    Vec8fb(Vec4fb const & a0, Vec4fb const & a1) {
+    Vec8fb(Vec4fb const a0, Vec4fb const a1) {
         ymm = set_m128r(a0, a1);
     }
     // Constructor to convert from type __m256 used in intrinsics:
-    Vec8fb(__m256 const & x) {
+    Vec8fb(__m256 const x) {
         ymm = x;
     }
     // Assignment operator to convert from type __m256 used in intrinsics:
-    Vec8fb & operator = (__m256 const & x) {
+    Vec8fb & operator = (__m256 const x) {
         ymm = x;
         return *this;
     }
@@ -149,51 +120,58 @@ public:
         *this = Vec8fb(b);
         return *this;
     }
-private: // Prevent constructing from int, etc.
-    Vec8fb(int b);
-    Vec8fb & operator = (int x);
-public:
     // Type cast operator to convert to __m256 used in intrinsics
     operator __m256() const {
         return ymm;
     }
-#if defined (VECTORI256_H)
-#if VECTORI256_H >= 2  // AVX2 version
+#if INSTRSET >= 8  // AVX2
     // Constructor to convert from type Vec8ib used as Boolean for integer vectors
-    Vec8fb(Vec8ib const & x) {
+    Vec8fb(Vec8ib const x) {
         ymm = _mm256_castsi256_ps(x);
     }
     // Assignment operator to convert from type Vec8ib used as Boolean for integer vectors
-    Vec8fb & operator = (Vec8ib const & x) {
+    Vec8fb & operator = (Vec8ib const x) {
         ymm = _mm256_castsi256_ps(x);
         return *this;
     }
+    // Member function to change a bitfield to a boolean vector
+    Vec8fb & load_bits(uint8_t a) {
+        Vec8ib b;  b.load_bits(a);
+        ymm = _mm256_castsi256_ps(b);
+        return *this;
+    }
 #ifndef FIX_CLANG_VECTOR_ALIAS_AMBIGUITY
     // Type cast operator to convert to type Vec8ib used as Boolean for integer vectors
     operator Vec8ib() const {
         return _mm256_castps_si256(ymm);
     }
 #endif
-#else
+#else  // AVX version
     // Constructor to convert from type Vec8ib used as Boolean for integer vectors
-    Vec8fb(Vec8ib const & x) {
+    Vec8fb(Vec8ib const x) {
         ymm = set_m128r(_mm_castsi128_ps(x.get_low()), _mm_castsi128_ps(x.get_high()));
     }
     // Assignment operator to convert from type Vec8ib used as Boolean for integer vectors
-    Vec8fb & operator = (Vec8ib const & x) {
+    Vec8fb & operator = (Vec8ib const x) {
         ymm = set_m128r(_mm_castsi128_ps(x.get_low()), _mm_castsi128_ps(x.get_high()));
         return *this;
     }
+    // Member function to change a bitfield to a boolean vector
+    // AVX version. Cannot use float instructions if subnormals are disabled
+    Vec8fb & load_bits(uint8_t a) {
+        Vec4fb y0 = Vec4fb().load_bits(a);
+        Vec4fb y1 = Vec4fb().load_bits(uint8_t(a >> 4u));
+        *this = Vec8fb(y0, y1);
+        return *this;
+    }
     // Type cast operator to convert to type Vec8ib used as Boolean for integer vectors
     operator Vec8ib() const {
         return Vec8i(_mm_castps_si128(get_low()), _mm_castps_si128(get_high()));
     }
-#endif
-#endif // VECTORI256_H
+#endif // AVX2
     // Member function to change a single element in vector
-    // Note: This function is inefficient. Use load function if changing more than one element
-    Vec8fb const & insert(uint32_t index, bool value) {
-        static const int32_t maskl[16] = {0,0,0,0,0,0,0,0,-1,0,0,0,0,0,0,0};
+    Vec8fb const insert(int index, bool value) {
+        const int32_t maskl[16] = {0,0,0,0,0,0,0,0,-1,0,0,0,0,0,0,0};
         __m256 mask  = _mm256_loadu_ps((float const*)(maskl+8-(index & 7))); // mask with FFFFFFFF at index position
         if (value) {
             ymm = _mm256_or_ps(ymm,mask);
@@ -204,7 +182,7 @@ public:
         return *this;
     }
     // Member function extract a single element from vector
-    bool extract(uint32_t index) const {
+    bool extract(int index) const {
         union {
             float   f[8];
             int32_t i[8];
@@ -213,7 +191,7 @@ public:
         return u.i[index & 7] != 0;
     }
     // Extract a single element. Operator [] can only read an element, not write.
-    bool operator [] (uint32_t index) const {
+    bool operator [] (int index) const {
         return extract(index);
     }
     // Member functions to split into two Vec4fb:
@@ -223,94 +201,116 @@ public:
     Vec4fb get_high() const {
         return _mm256_extractf128_ps(ymm,1);
     }
-    static int size () {
+    static constexpr int size() {
         return 8;
     }
-};
+    static constexpr int elementtype() {
+        return 3;
+    }
+    // Prevent constructing from int, etc.
+    Vec8fb(int b) = delete;
+    Vec8fb & operator = (int x) = delete;
+    };
+
+#else
+
+typedef Vec8b Vec8fb;  // compact boolean vector
+
+#endif
 
 
 /*****************************************************************************
 *
-*          Operators for Vec8fb
+*          Operators and functions for Vec8fb
 *
 *****************************************************************************/
 
+#if INSTRSET < 10  // broad boolean vectors
+
 // vector operator & : bitwise and
-static inline Vec8fb operator & (Vec8fb const & a, Vec8fb const & b) {
+static inline Vec8fb operator & (Vec8fb const a, Vec8fb const b) {
     return _mm256_and_ps(a, b);
 }
-static inline Vec8fb operator && (Vec8fb const & a, Vec8fb const & b) {
+static inline Vec8fb operator && (Vec8fb const a, Vec8fb const b) {
     return a & b;
 }
 
 // vector operator &= : bitwise and
-static inline Vec8fb & operator &= (Vec8fb & a, Vec8fb const & b) {
+static inline Vec8fb & operator &= (Vec8fb & a, Vec8fb const b) {
     a = a & b;
     return a;
 }
 
 // vector operator | : bitwise or
-static inline Vec8fb operator | (Vec8fb const & a, Vec8fb const & b) {
+static inline Vec8fb operator | (Vec8fb const a, Vec8fb const b) {
     return _mm256_or_ps(a, b);
 }
-static inline Vec8fb operator || (Vec8fb const & a, Vec8fb const & b) {
+static inline Vec8fb operator || (Vec8fb const a, Vec8fb const b) {
     return a | b;
 }
 
 // vector operator |= : bitwise or
-static inline Vec8fb & operator |= (Vec8fb & a, Vec8fb const & b) {
+static inline Vec8fb & operator |= (Vec8fb & a, Vec8fb const b) {
     a = a | b;
     return a;
 }
 
+// vector operator ~ : bitwise not
+static inline Vec8fb operator ~ (Vec8fb const a) {
+    return _mm256_xor_ps(a, constant8f<0xFFFFFFFFu,0xFFFFFFFFu,0xFFFFFFFFu,0xFFFFFFFFu,0xFFFFFFFFu,0xFFFFFFFFu,0xFFFFFFFFu,0xFFFFFFFFu>());
+}
+
 // vector operator ^ : bitwise xor
-static inline Vec8fb operator ^ (Vec8fb const & a, Vec8fb const & b) {
+static inline Vec8fb operator ^ (Vec8fb const a, Vec8fb const b) {
+    return _mm256_xor_ps(a, b);
+}
+
+// vector operator == : xnor
+static inline Vec8fb operator == (Vec8fb const a, Vec8fb const b) {
+    return Vec8fb(a ^ Vec8fb(~b));
+}
+
+// vector operator != : xor
+static inline Vec8fb operator != (Vec8fb const a, Vec8fb const b) {
     return _mm256_xor_ps(a, b);
 }
 
 // vector operator ^= : bitwise xor
-static inline Vec8fb & operator ^= (Vec8fb & a, Vec8fb const & b) {
+static inline Vec8fb & operator ^= (Vec8fb & a, Vec8fb const b) {
     a = a ^ b;
     return a;
 }
 
-// vector operator ~ : bitwise not
-static inline Vec8fb operator ~ (Vec8fb const & a) {
-    return _mm256_xor_ps(a, constant8f<-1,-1,-1,-1,-1,-1,-1,-1>());
-}
-
 // vector operator ! : logical not
-// (operator ! is less efficient than operator ~. Use only where not
-// all bits in an element are the same)
-static inline Vec8fb operator ! (Vec8fb const & a) {
+// (operator ! is less efficient than operator ~. Use only where not all bits in an element are the same)
+static inline Vec8fb operator ! (Vec8fb const a) {
 return Vec8fb( !Vec8ib(a));
 }
 
 // Functions for Vec8fb
 
 // andnot: a & ~ b
-static inline Vec8fb andnot(Vec8fb const & a, Vec8fb const & b) {
+static inline Vec8fb andnot(Vec8fb const a, Vec8fb const b) {
     return _mm256_andnot_ps(b, a);
 }
 
-
-
-/*****************************************************************************
-*
-*          Horizontal Boolean functions
-*
-*****************************************************************************/
-
 // horizontal_and. Returns true if all bits are 1
-static inline bool horizontal_and (Vec8fb const & a) {
-    return _mm256_testc_ps(a,constant8f<-1,-1,-1,-1,-1,-1,-1,-1>()) != 0;
+static inline bool horizontal_and (Vec8fb const a) {
+    return _mm256_testc_ps(a,constant8f<0xFFFFFFFFu,0xFFFFFFFFu,0xFFFFFFFFu,0xFFFFFFFFu,0xFFFFFFFFu,0xFFFFFFFFu,0xFFFFFFFFu,0xFFFFFFFFu>()) != 0;
 }
 
 // horizontal_or. Returns true if at least one bit is 1
-static inline bool horizontal_or (Vec8fb const & a) {
+static inline bool horizontal_or (Vec8fb const a) {
     return ! _mm256_testz_ps(a,a);
 }
 
+// to_bits: convert boolean vector to integer bitfield
+static inline uint8_t to_bits(Vec8fb const x) {
+    return to_bits(Vec8ib(x));
+}
+
+#endif
+
 
 /*****************************************************************************
 *
@@ -318,17 +318,18 @@ static inline bool horizontal_or (Vec8fb const & a) {
 *
 *****************************************************************************/
 
+#if INSTRSET < 10  // broad boolean vectors
+
 class Vec4db {
 protected:
     __m256d ymm; // double vector
 public:
     // Default constructor:
-    Vec4db() {
-    }
+    Vec4db() = default;
     // Constructor to build from all elements:
     Vec4db(bool b0, bool b1, bool b2, bool b3) {
 #if INSTRSET >= 8  // AVX2
-        ymm = _mm256_castsi256_pd(_mm256_setr_epi64x(-(int64_t)b0, -(int64_t)b1, -(int64_t)b2, -(int64_t)b3)); 
+        ymm = _mm256_castsi256_pd(_mm256_setr_epi64x(-(int64_t)b0, -(int64_t)b1, -(int64_t)b2, -(int64_t)b3));
 #else
         __m128 blo = _mm_castsi128_ps(_mm_setr_epi32(-(int)b0, -(int)b0, -(int)b1, -(int)b1));
         __m128 bhi = _mm_castsi128_ps(_mm_setr_epi32(-(int)b2, -(int)b2, -(int)b3, -(int)b3));
@@ -336,16 +337,16 @@ public:
 #endif
     }
     // Constructor to build from two Vec2db:
-    Vec4db(Vec2db const & a0, Vec2db const & a1) {
+    Vec4db(Vec2db const a0, Vec2db const a1) {
         ymm = _mm256_castps_pd(set_m128r(_mm_castpd_ps(a0),_mm_castpd_ps(a1)));
         //ymm = _mm256_set_m128d(a1, a0);
     }
     // Constructor to convert from type __m256d used in intrinsics:
-    Vec4db(__m256d const & x) {
+    Vec4db(__m256d const x) {
         ymm = x;
     }
     // Assignment operator to convert from type __m256d used in intrinsics:
-    Vec4db & operator = (__m256d const & x) {
+    Vec4db & operator = (__m256d const x) {
         ymm = x;
         return *this;
     }
@@ -363,25 +364,26 @@ public:
         ymm = _mm256_castsi256_pd(_mm256_set1_epi32(-int32_t(b)));
         return *this;
     }
-private: // Prevent constructing from int, etc.
-    Vec4db(int b);
-    Vec4db & operator = (int x);
-public:
     // Type cast operator to convert to __m256d used in intrinsics
     operator __m256d() const {
         return ymm;
     }
-#ifdef VECTORI256_H  
-#if VECTORI256_H == 2  // 256 bit integer vectors are available, AVX2
+#if INSTRSET >= 8  // 256 bit integer vectors are available, AVX2
     // Constructor to convert from type Vec4qb used as Boolean for integer vectors
-    Vec4db(Vec4qb const & x) {
+    Vec4db(Vec4qb const x) {
         ymm = _mm256_castsi256_pd(x);
     }
     // Assignment operator to convert from type Vec4qb used as Boolean for integer vectors
-    Vec4db & operator = (Vec4qb const & x) {
+    Vec4db & operator = (Vec4qb const x) {
         ymm = _mm256_castsi256_pd(x);
         return *this;
     }
+    // Member function to change a bitfield to a boolean vector
+    Vec4db & load_bits(uint8_t a) {
+        Vec4qb b; b.load_bits(a);
+        ymm = _mm256_castsi256_pd(b);
+        return *this;
+    }
 #ifndef FIX_CLANG_VECTOR_ALIAS_AMBIGUITY
     // Type cast operator to convert to type Vec4qb used as Boolean for integer vectors
     operator Vec4qb() const {
@@ -390,11 +392,11 @@ public:
 #endif
 #else   // 256 bit integer vectors emulated without AVX2
     // Constructor to convert from type Vec4qb used as Boolean for integer vectors
-    Vec4db(Vec4qb const & x) {
+    Vec4db(Vec4qb const x) {
         *this = Vec4db(_mm_castsi128_pd(x.get_low()), _mm_castsi128_pd(x.get_high()));
     }
     // Assignment operator to convert from type Vec4qb used as Boolean for integer vectors
-    Vec4db & operator = (Vec4qb const & x) {
+    Vec4db & operator = (Vec4qb const x) {
         *this = Vec4db(_mm_castsi128_pd(x.get_low()), _mm_castsi128_pd(x.get_high()));
         return *this;
     }
@@ -402,13 +404,20 @@ public:
     operator Vec4qb() const {
         return Vec4q(_mm_castpd_si128(get_low()), _mm_castpd_si128(get_high()));
     }
-#endif
-#endif // VECTORI256_H
+    // Member function to change a bitfield to a boolean vector
+    // AVX version. Cannot use float instructions if subnormals are disabled
+    Vec4db & load_bits(uint8_t a) {
+        Vec2db a0 = Vec2db().load_bits(a);
+        Vec2db a1 = Vec2db().load_bits(uint8_t(a>>2u));
+        *this = Vec4db(a0, a1);
+        return *this;
+    }
+#endif // AVX2
     // Member function to change a single element in vector
-    // Note: This function is inefficient. Use load function if changing more than one element
-    Vec4db const & insert(uint32_t index, bool value) {
-        static const int32_t maskl[16] = {0,0,0,0,0,0,0,0,-1,-1,0,0,0,0,0,0};
-        __m256d mask = _mm256_loadu_pd((double const*)(maskl+8-(index&3)*2)); // mask with FFFFFFFFFFFFFFFF at index position
+    Vec4db const insert(int index, bool value) {
+        const int32_t maskl[16] = {0,0,0,0,0,0,0,0,-1,-1,0,0,0,0,0,0};
+        const size_t two = 2;  // avoid silly warning from MS compiler
+        __m256d mask = _mm256_loadu_pd((double const*)(maskl+8-(index&3)*two)); // mask with FFFFFFFFFFFFFFFF at index position
         if (value) {
             ymm = _mm256_or_pd(ymm,mask);
         }
@@ -418,7 +427,7 @@ public:
         return *this;
     }
     // Member function extract a single element from vector
-    bool extract(uint32_t index) const {
+    bool extract(int index) const {
         union {
             double  f[8];
             int32_t i[16];
@@ -427,7 +436,7 @@ public:
         return u.i[(index & 3) * 2 + 1] != 0;
     }
     // Extract a single element. Operator [] can only read an element, not write.
-    bool operator [] (uint32_t index) const {
+    bool operator [] (int index) const {
         return extract(index);
     }
     // Member functions to split into two Vec4fb:
@@ -437,86 +446,101 @@ public:
     Vec2db get_high() const {
         return _mm256_extractf128_pd(ymm,1);
     }
-    static int size () {
+    static constexpr int size() {
         return 4;
     }
+    static constexpr int elementtype() {
+        return 3;
+    }
+    // Prevent constructing from int, etc.
+    Vec4db(int b) = delete;
+    Vec4db & operator = (int x) = delete;
 };
 
+#else
+
+typedef Vec4b Vec4db;  // compact boolean vector
+
+#endif
 
 /*****************************************************************************
 *
-*          Operators for Vec4db
+*          Operators and functions for Vec4db
 *
 *****************************************************************************/
 
+#if INSTRSET < 10  // broad boolean vectors
+
 // vector operator & : bitwise and
-static inline Vec4db operator & (Vec4db const & a, Vec4db const & b) {
+static inline Vec4db operator & (Vec4db const a, Vec4db const b) {
     return _mm256_and_pd(a, b);
 }
-static inline Vec4db operator && (Vec4db const & a, Vec4db const & b) {
+static inline Vec4db operator && (Vec4db const a, Vec4db const b) {
     return a & b;
 }
 
 // vector operator &= : bitwise and
-static inline Vec4db & operator &= (Vec4db & a, Vec4db const & b) {
+static inline Vec4db & operator &= (Vec4db & a, Vec4db const b) {
     a = a & b;
     return a;
 }
 
 // vector operator | : bitwise or
-static inline Vec4db operator | (Vec4db const & a, Vec4db const & b) {
+static inline Vec4db operator | (Vec4db const a, Vec4db const b) {
     return _mm256_or_pd(a, b);
 }
-static inline Vec4db operator || (Vec4db const & a, Vec4db const & b) {
+static inline Vec4db operator || (Vec4db const a, Vec4db const b) {
     return a | b;
 }
 
 // vector operator |= : bitwise or
-static inline Vec4db & operator |= (Vec4db & a, Vec4db const & b) {
+static inline Vec4db & operator |= (Vec4db & a, Vec4db const b) {
     a = a | b;
     return a;
 }
 
+// vector operator ~ : bitwise not
+static inline Vec4db operator ~ (Vec4db const a) {
+    return _mm256_xor_pd(a, _mm256_castps_pd (constant8f<0xFFFFFFFFu,0xFFFFFFFFu,0xFFFFFFFFu,0xFFFFFFFFu,0xFFFFFFFFu,0xFFFFFFFFu,0xFFFFFFFFu,0xFFFFFFFFu>()));
+}
+
 // vector operator ^ : bitwise xor
-static inline Vec4db operator ^ (Vec4db const & a, Vec4db const & b) {
+static inline Vec4db operator ^ (Vec4db const a, Vec4db const b) {
+    return _mm256_xor_pd(a, b);
+}
+
+// vector operator == : xnor
+static inline Vec4db operator == (Vec4db const a, Vec4db const b) {
+    return Vec4db(a ^ Vec4db(~b));
+}
+
+// vector operator != : xor
+static inline Vec4db operator != (Vec4db const a, Vec4db const b) {
     return _mm256_xor_pd(a, b);
 }
 
 // vector operator ^= : bitwise xor
-static inline Vec4db & operator ^= (Vec4db & a, Vec4db const & b) {
+static inline Vec4db & operator ^= (Vec4db & a, Vec4db const b) {
     a = a ^ b;
     return a;
 }
 
-// vector operator ~ : bitwise not
-static inline Vec4db operator ~ (Vec4db const & a) {
-    return _mm256_xor_pd(a, _mm256_castps_pd (constant8f<-1,-1,-1,-1,-1,-1,-1,-1>()));
-}
-
 // vector operator ! : logical not
-// (operator ! is less efficient than operator ~. Use only where not
-// all bits in an element are the same)
-static inline Vec4db operator ! (Vec4db const & a) {
+// (operator ! is less efficient than operator ~. Use only where not all bits in an element are the same)
+static inline Vec4db operator ! (Vec4db const a) {
 return Vec4db( ! Vec4qb(a));
 }
 
 // Functions for Vec8fb
 
 // andnot: a & ~ b
-static inline Vec4db andnot(Vec4db const & a, Vec4db const & b) {
+static inline Vec4db andnot(Vec4db const a, Vec4db const b) {
     return _mm256_andnot_pd(b, a);
 }
 
-
-/*****************************************************************************
-*
-*          Horizontal Boolean functions
-*
-*****************************************************************************/
-
 // horizontal_and. Returns true if all bits are 1
-static inline bool horizontal_and (Vec4db const & a) {
-#if defined (VECTORI256_H) && VECTORI256_H > 1  // 256 bit integer vectors are available, AVX2
+static inline bool horizontal_and (Vec4db const a) {
+#if INSTRSET >= 8  // 256 bit integer vectors are available, AVX2
     return horizontal_and(Vec256b(_mm256_castpd_si256(a)));
 #else  // split into 128 bit vectors
     return horizontal_and(a.get_low() & a.get_high());
@@ -524,14 +548,21 @@ static inline bool horizontal_and (Vec4db const & a) {
 }
 
 // horizontal_or. Returns true if at least one bit is 1
-static inline bool horizontal_or (Vec4db const & a) {
-#if defined (VECTORI256_H) && VECTORI256_H > 1  // 256 bit integer vectors are available, AVX2
+static inline bool horizontal_or (Vec4db const a) {
+#if INSTRSET >= 8  // 256 bit integer vectors are available, AVX2
     return horizontal_or(Vec256b(_mm256_castpd_si256(a)));
 #else  // split into 128 bit vectors
     return horizontal_or(a.get_low() | a.get_high());
 #endif
 }
 
+// to_bits: convert boolean vector to integer bitfield
+static inline uint8_t to_bits(Vec4db const x) {
+    return to_bits(Vec4qb(x));
+}
+
+#endif
+
 
  /*****************************************************************************
 *
@@ -544,27 +575,26 @@ protected:
     __m256 ymm; // Float vector
 public:
     // Default constructor:
-    Vec8f() {
-    }
+    Vec8f() = default;
     // Constructor to broadcast the same value into all elements:
     Vec8f(float f) {
         ymm = _mm256_set1_ps(f);
     }
     // Constructor to build from all elements:
     Vec8f(float f0, float f1, float f2, float f3, float f4, float f5, float f6, float f7) {
-        ymm = _mm256_setr_ps(f0, f1, f2, f3, f4, f5, f6, f7); 
+        ymm = _mm256_setr_ps(f0, f1, f2, f3, f4, f5, f6, f7);
     }
     // Constructor to build from two Vec4f:
-    Vec8f(Vec4f const & a0, Vec4f const & a1) {
+    Vec8f(Vec4f const a0, Vec4f const a1) {
         ymm = set_m128r(a0, a1);
         //ymm = _mm256_set_m128(a1, a0);
     }
     // Constructor to convert from type __m256 used in intrinsics:
-    Vec8f(__m256 const & x) {
+    Vec8f(__m256 const x) {
         ymm = x;
     }
     // Assignment operator to convert from type __m256 used in intrinsics:
-    Vec8f & operator = (__m256 const & x) {
+    Vec8f & operator = (__m256 const x) {
         ymm = x;
         return *this;
     }
@@ -573,36 +603,39 @@ public:
         return ymm;
     }
     // Member function to load from array (unaligned)
-    Vec8f & load(void const * p) {
-        ymm = _mm256_loadu_ps((float const*)p);
+    Vec8f & load(float const * p) {
+        ymm = _mm256_loadu_ps(p);
         return *this;
     }
     // Member function to load from array, aligned by 32
-    // You may use load_a instead of load if you are certain that p points to an address
-    // divisible by 32.
-    Vec8f & load_a(void const * p) {
-        ymm = _mm256_load_ps((float const*)p);
+    // You may use load_a instead of load if you are certain that p points to an address divisible by 32
+    Vec8f & load_a(float const * p) {
+        ymm = _mm256_load_ps(p);
         return *this;
     }
     // Member function to store into array (unaligned)
     void store(float * p) const {
         _mm256_storeu_ps(p, ymm);
     }
-    // Member function to store into array, aligned by 32
-    // You may use store_a instead of store if you are certain that p points to an address
-    // divisible by 32.
+    // Member function storing into array, aligned by 32
+    // You may use store_a instead of store if you are certain that p points to an address divisible by 32
     void store_a(float * p) const {
         _mm256_store_ps(p, ymm);
     }
-    // Member function to store into array using a non-temporal memory hint, aligned by 32
-    void stream(float * p) const {
+    // Member function storing to aligned uncached memory (non-temporal store).
+    // This may be more efficient than store_a when storing large blocks of memory if it 
+    // is unlikely that the data will stay in the cache until it is read again.
+    // Note: Will generate runtime error if p is not aligned by 32
+    void store_nt(float * p) const {
         _mm256_stream_ps(p, ymm);
     }
     // Partial load. Load n elements and set the rest to 0
     Vec8f & load_partial(int n, float const * p) {
+#if INSTRSET >= 10  // AVX512VL
+        ymm = _mm256_maskz_loadu_ps(__mmask8((1u << n) - 1), p);
+#else
         if (n > 0 && n <= 4) {
             *this = Vec8f(Vec4f().load_partial(n, p), _mm_setzero_ps());
-            // ymm = _mm256_castps128_ps256(Vec4f().load_partial<n>(p)); (this doesn't work on MS compiler due to sloppy definition of the cast)
         }
         else if (n > 4 && n <= 8) {
             *this = Vec8f(Vec4f().load(p), Vec4f().load_partial(n - 4, p + 4));
@@ -610,10 +643,14 @@ public:
         else {
             ymm = _mm256_setzero_ps();
         }
+#endif
         return *this;
     }
     // Partial store. Store n elements
     void store_partial(int n, float * p) const {
+#if INSTRSET >= 10  // AVX512VL
+        _mm256_mask_storeu_ps(p, __mmask8((1u << n) - 1), ymm);
+#else
         if (n <= 4) {
             get_low().store_partial(n, p);
         }
@@ -621,20 +658,27 @@ public:
             get_low().store(p);
             get_high().store_partial(n - 4, p + 4);
         }
+#endif
     }
     // cut off vector to n elements. The last 8-n elements are set to zero
     Vec8f & cutoff(int n) {
+#if INSTRSET >= 10
+        ymm = _mm256_maskz_mov_ps(__mmask8((1u << n) - 1), ymm);
+#else
         if (uint32_t(n) >= 8) return *this;
-        static const union {        
+        const union {
             int32_t i[16];
             float   f[16];
         } mask = {{-1,-1,-1,-1,-1,-1,-1,-1,0,0,0,0,0,0,0,0}};
         *this = Vec8fb(*this) & Vec8fb(Vec8f().load(mask.f + 8 - n));
+#endif
         return *this;
     }
     // Member function to change a single element in vector
-    // Note: This function is inefficient. Use load function if changing more than one element
-    Vec8f const & insert(uint32_t index, float value) {
+    Vec8f const insert(int index, float value) {
+#if INSTRSET >= 10   // AVX512VL
+        ymm = _mm256_mask_broadcastss_ps (ymm, __mmask8(1u << index), _mm_set_ss(value));
+#else
         __m256 v0 = _mm256_broadcast_ss(&value);
         switch (index) {
         case 0:
@@ -654,17 +698,23 @@ public:
         default:
             ymm = _mm256_blend_ps (ymm, v0, 0x80);  break;
         }
+#endif
         return *this;
     }
     // Member function extract a single element from vector
-    float extract(uint32_t index) const {
+    float extract(int index) const {
+#if INSTRSET >= 10
+        __m256 x = _mm256_maskz_compress_ps(__mmask8(1u << index), ymm);
+        return _mm256_cvtss_f32(x);
+#else
         float x[8];
         store(x);
         return x[index & 7];
+#endif
     }
     // Extract a single element. Use store function if extracting more than one element.
     // Operator [] can only read an element, not write.
-    float operator [] (uint32_t index) const {
+    float operator [] (int index) const {
         return extract(index);
     }
     // Member functions to split into two Vec4f:
@@ -674,9 +724,13 @@ public:
     Vec4f get_high() const {
         return _mm256_extractf128_ps(ymm,1);
     }
-    static int size () {
+    static constexpr int size() {
         return 8;
     }
+    static constexpr int elementtype() {
+        return 16;
+    }
+    typedef __m256 registertype;
 };
 
 
@@ -687,20 +741,20 @@ public:
 *****************************************************************************/
 
 // vector operator + : add element by element
-static inline Vec8f operator + (Vec8f const & a, Vec8f const & b) {
+static inline Vec8f operator + (Vec8f const a, Vec8f const b) {
     return _mm256_add_ps(a, b);
 }
 
 // vector operator + : add vector and scalar
-static inline Vec8f operator + (Vec8f const & a, float b) {
+static inline Vec8f operator + (Vec8f const a, float b) {
     return a + Vec8f(b);
 }
-static inline Vec8f operator + (float a, Vec8f const & b) {
+static inline Vec8f operator + (float a, Vec8f const b) {
     return Vec8f(a) + b;
 }
 
 // vector operator += : add
-static inline Vec8f & operator += (Vec8f & a, Vec8f const & b) {
+static inline Vec8f & operator += (Vec8f & a, Vec8f const b) {
     a = a + b;
     return a;
 }
@@ -719,26 +773,26 @@ static inline Vec8f & operator ++ (Vec8f & a) {
 }
 
 // vector operator - : subtract element by element
-static inline Vec8f operator - (Vec8f const & a, Vec8f const & b) {
+static inline Vec8f operator - (Vec8f const a, Vec8f const b) {
     return _mm256_sub_ps(a, b);
 }
 
 // vector operator - : subtract vector and scalar
-static inline Vec8f operator - (Vec8f const & a, float b) {
+static inline Vec8f operator - (Vec8f const a, float b) {
     return a - Vec8f(b);
 }
-static inline Vec8f operator - (float a, Vec8f const & b) {
+static inline Vec8f operator - (float a, Vec8f const b) {
     return Vec8f(a) - b;
 }
 
 // vector operator - : unary minus
 // Change sign bit, even for 0, INF and NAN
-static inline Vec8f operator - (Vec8f const & a) {
-    return _mm256_xor_ps(a, constant8f<(int)0x80000000,(int)0x80000000,(int)0x80000000,(int)0x80000000,(int)0x80000000,(int)0x80000000,(int)0x80000000,(int)0x80000000> ());
+static inline Vec8f operator - (Vec8f const a) {
+    return _mm256_xor_ps(a, Vec8f(-0.0f));
 }
 
 // vector operator -= : subtract
-static inline Vec8f & operator -= (Vec8f & a, Vec8f const & b) {
+static inline Vec8f & operator -= (Vec8f & a, Vec8f const b) {
     a = a - b;
     return a;
 }
@@ -757,118 +811,146 @@ static inline Vec8f & operator -- (Vec8f & a) {
 }
 
 // vector operator * : multiply element by element
-static inline Vec8f operator * (Vec8f const & a, Vec8f const & b) {
+static inline Vec8f operator * (Vec8f const a, Vec8f const b) {
     return _mm256_mul_ps(a, b);
 }
 
 // vector operator * : multiply vector and scalar
-static inline Vec8f operator * (Vec8f const & a, float b) {
+static inline Vec8f operator * (Vec8f const a, float b) {
     return a * Vec8f(b);
 }
-static inline Vec8f operator * (float a, Vec8f const & b) {
+static inline Vec8f operator * (float a, Vec8f const b) {
     return Vec8f(a) * b;
 }
 
 // vector operator *= : multiply
-static inline Vec8f & operator *= (Vec8f & a, Vec8f const & b) {
+static inline Vec8f & operator *= (Vec8f & a, Vec8f const b) {
     a = a * b;
     return a;
 }
 
 // vector operator / : divide all elements by same integer
-static inline Vec8f operator / (Vec8f const & a, Vec8f const & b) {
+static inline Vec8f operator / (Vec8f const a, Vec8f const b) {
     return _mm256_div_ps(a, b);
 }
 
 // vector operator / : divide vector and scalar
-static inline Vec8f operator / (Vec8f const & a, float b) {
+static inline Vec8f operator / (Vec8f const a, float b) {
     return a / Vec8f(b);
 }
-static inline Vec8f operator / (float a, Vec8f const & b) {
+static inline Vec8f operator / (float a, Vec8f const b) {
     return Vec8f(a) / b;
 }
 
 // vector operator /= : divide
-static inline Vec8f & operator /= (Vec8f & a, Vec8f const & b) {
+static inline Vec8f & operator /= (Vec8f & a, Vec8f const b) {
     a = a / b;
     return a;
 }
 
 // vector operator == : returns true for elements for which a == b
-static inline Vec8fb operator == (Vec8f const & a, Vec8f const & b) {
+static inline Vec8fb operator == (Vec8f const a, Vec8f const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_cmp_ps_mask(a, b, 0);
+#else
     return _mm256_cmp_ps(a, b, 0);
+#endif
 }
 
 // vector operator != : returns true for elements for which a != b
-static inline Vec8fb operator != (Vec8f const & a, Vec8f const & b) {
+static inline Vec8fb operator != (Vec8f const a, Vec8f const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_cmp_ps_mask(a, b, 4);
+#else
     return _mm256_cmp_ps(a, b, 4);
+#endif
 }
 
 // vector operator < : returns true for elements for which a < b
-static inline Vec8fb operator < (Vec8f const & a, Vec8f const & b) {
+static inline Vec8fb operator < (Vec8f const a, Vec8f const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_cmp_ps_mask(a, b, 1);
+#else
     return _mm256_cmp_ps(a, b, 1);
+#endif
 }
 
 // vector operator <= : returns true for elements for which a <= b
-static inline Vec8fb operator <= (Vec8f const & a, Vec8f const & b) {
+static inline Vec8fb operator <= (Vec8f const a, Vec8f const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_cmp_ps_mask(a, b, 2);
+#else
     return _mm256_cmp_ps(a, b, 2);
+#endif
 }
 
 // vector operator > : returns true for elements for which a > b
-static inline Vec8fb operator > (Vec8f const & a, Vec8f const & b) {
+static inline Vec8fb operator > (Vec8f const a, Vec8f const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_cmp_ps_mask(a, b, 6+8);
+#else
     return b < a;
+#endif
 }
 
 // vector operator >= : returns true for elements for which a >= b
-static inline Vec8fb operator >= (Vec8f const & a, Vec8f const & b) {
+static inline Vec8fb operator >= (Vec8f const a, Vec8f const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_cmp_ps_mask(a, b, 5+8);
+#else
     return b <= a;
+#endif
 }
 
 // Bitwise logical operators
 
 // vector operator & : bitwise and
-static inline Vec8f operator & (Vec8f const & a, Vec8f const & b) {
+static inline Vec8f operator & (Vec8f const a, Vec8f const b) {
     return _mm256_and_ps(a, b);
 }
 
 // vector operator &= : bitwise and
-static inline Vec8f & operator &= (Vec8f & a, Vec8f const & b) {
+static inline Vec8f & operator &= (Vec8f & a, Vec8f const b) {
     a = a & b;
     return a;
 }
 
 // vector operator & : bitwise and of Vec8f and Vec8fb
-static inline Vec8f operator & (Vec8f const & a, Vec8fb const & b) {
+static inline Vec8f operator & (Vec8f const a, Vec8fb const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_maskz_mov_ps(b, a);
+#else
     return _mm256_and_ps(a, b);
+#endif
 }
-static inline Vec8f operator & (Vec8fb const & a, Vec8f const & b) {
-    return _mm256_and_ps(a, b);
+static inline Vec8f operator & (Vec8fb const a, Vec8f const b) {
+    return b & a;
 }
 
 // vector operator | : bitwise or
-static inline Vec8f operator | (Vec8f const & a, Vec8f const & b) {
+static inline Vec8f operator | (Vec8f const a, Vec8f const b) {
     return _mm256_or_ps(a, b);
 }
 
 // vector operator |= : bitwise or
-static inline Vec8f & operator |= (Vec8f & a, Vec8f const & b) {
+static inline Vec8f & operator |= (Vec8f & a, Vec8f const b) {
     a = a | b;
     return a;
 }
 
 // vector operator ^ : bitwise xor
-static inline Vec8f operator ^ (Vec8f const & a, Vec8f const & b) {
+static inline Vec8f operator ^ (Vec8f const a, Vec8f const b) {
     return _mm256_xor_ps(a, b);
 }
 
 // vector operator ^= : bitwise xor
-static inline Vec8f & operator ^= (Vec8f & a, Vec8f const & b) {
+static inline Vec8f & operator ^= (Vec8f & a, Vec8f const b) {
     a = a ^ b;
     return a;
 }
 
 // vector operator ! : logical not. Returns Boolean vector
-static inline Vec8fb operator ! (Vec8f const & a) {
+static inline Vec8fb operator ! (Vec8f const a) {
     return a == Vec8f(0.0f);
 }
 
@@ -879,216 +961,328 @@ static inline Vec8fb operator ! (Vec8f const & a) {
 *
 *****************************************************************************/
 
-static inline Vec8f zero_8f() {
-    return _mm256_setzero_ps();
-}
-
 // Select between two operands. Corresponds to this pseudocode:
 // for (int i = 0; i < 8; i++) result[i] = s[i] ? a[i] : b[i];
-// Each byte in s must be either 0 (false) or 0xFFFFFFFF (true). No other values are allowed.
-static inline Vec8f select (Vec8fb const & s, Vec8f const & a, Vec8f const & b) {
+static inline Vec8f select (Vec8fb const s, Vec8f const a, Vec8f const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_mask_mov_ps(b, s, a);
+#else
     return _mm256_blendv_ps (b, a, s);
+#endif
 }
 
 // Conditional add: For all vector elements i: result[i] = f[i] ? (a[i] + b[i]) : a[i]
-static inline Vec8f if_add (Vec8fb const & f, Vec8f const & a, Vec8f const & b) {
+static inline Vec8f if_add (Vec8fb const f, Vec8f const a, Vec8f const b) {
+#if INSTRSET >= 10
+    return _mm256_mask_add_ps (a, f, a, b);
+#else
     return a + (Vec8f(f) & b);
+#endif
+}
+
+// Conditional subtract
+static inline Vec8f if_sub (Vec8fb const f, Vec8f const a, Vec8f const b) {
+#if INSTRSET >= 10
+    return _mm256_mask_sub_ps (a, f, a, b);
+#else
+    return a - (Vec8f(f) & b);
+#endif
 }
 
-// Conditional multiply: For all vector elements i: result[i] = f[i] ? (a[i] * b[i]) : a[i]
-static inline Vec8f if_mul (Vec8fb const & f, Vec8f const & a, Vec8f const & b) {
+// Conditional multiply
+static inline Vec8f if_mul (Vec8fb const f, Vec8f const a, Vec8f const b) {
+#if INSTRSET >= 10
+    return _mm256_mask_mul_ps (a, f, a, b);
+#else
     return a * select(f, b, 1.f);
+#endif
+}
+
+// Conditional divide
+static inline Vec8f if_div (Vec8fb const f, Vec8f const a, Vec8f const b) {
+#if INSTRSET >= 10
+    return _mm256_mask_div_ps (a, f, a, b);
+#else
+    return a / select(f, b, 1.f);
+#endif
+}
+
+// Sign functions
+
+// Function sign_bit: gives true for elements that have the sign bit set
+// even for -0.0f, -INF and -NAN
+// (the underscore in the name avoids a conflict with a macro in Intel's mathimf.h)
+static inline Vec8fb sign_bit(Vec8f const a) {
+#if INSTRSET >= 8  // 256 bit integer vectors are available, AVX2
+    Vec8i t1 = _mm256_castps_si256(a);    // reinterpret as 32-bit integer
+    Vec8i t2 = t1 >> 31;                  // extend sign bit
+#if INSTRSET >= 10
+    return t2 != 0;
+#else
+    return _mm256_castsi256_ps(t2);       // reinterpret as 32-bit Boolean
+#endif
+#else
+    return Vec8fb(sign_bit(a.get_low()), sign_bit(a.get_high()));
+#endif
+}
+
+// Function sign_combine: changes the sign of a when b has the sign bit set
+// same as select(sign_bit(b), -a, a)
+static inline Vec8f sign_combine(Vec8f const a, Vec8f const b) {
+#if INSTRSET < 10
+    return a ^ (b & Vec8f(-0.0f));
+#else
+    return _mm256_castsi256_ps (_mm256_ternarylogic_epi32(
+        _mm256_castps_si256(a), _mm256_castps_si256(b), Vec8i(0x80000000), 0x78));
+#endif
+}
+
+// Categorization functions
+
+// Function is_finite: gives true for elements that are normal, subnormal or zero,
+// false for INF and NAN
+// (the underscore in the name avoids a conflict with a macro in Intel's mathimf.h)
+static inline Vec8fb is_finite(Vec8f const a) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return __mmask8(~ _mm256_fpclass_ps_mask (a, 0x99));
+#elif INSTRSET >= 8  // 256 bit integer vectors are available, AVX2
+    Vec8i t1 = _mm256_castps_si256(a);    // reinterpret as 32-bit integer
+    Vec8i t2 = t1 << 1;                   // shift out sign bit
+    Vec8ib t3 = Vec8i(t2 & 0xFF000000) != 0xFF000000; // exponent field is not all 1s
+    return t3;
+#else
+    return Vec8fb(is_finite(a.get_low()), is_finite(a.get_high()));
+#endif
+}
+
+// Function is_inf: gives true for elements that are +INF or -INF
+// false for finite numbers and NAN
+// (the underscore in the name avoids a conflict with a macro in Intel's mathimf.h)
+static inline Vec8fb is_inf(Vec8f const a) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_fpclass_ps_mask (a, 0x18);
+#elif INSTRSET >= 8  //  256 bit integer vectors are available, AVX2
+    Vec8i t1 = _mm256_castps_si256(a); // reinterpret as 32-bit integer
+    Vec8i t2 = t1 << 1;                // shift out sign bit
+    return t2 == 0xFF000000;           // exponent is all 1s, fraction is 0
+#else
+    return Vec8fb(is_inf(a.get_low()), is_inf(a.get_high()));
+#endif
+}
+
+// Function is_nan: gives true for elements that are +NAN or -NAN
+// false for finite numbers and +/-INF
+// (the underscore in the name avoids a conflict with a macro in Intel's mathimf.h)
+#if INSTRSET >= 10
+static inline Vec8fb is_nan(Vec8f const a) {
+    // assume that compiler does not optimize this away with -ffinite-math-only:
+    return _mm256_fpclass_ps_mask (a, 0x81);
+}
+//#elif defined(__GNUC__) && !defined(__INTEL_COMPILER) && !defined(__clang__)
+//__attribute__((optimize("-fno-unsafe-math-optimizations")))
+//static inline Vec8fb is_nan(Vec8f const a) {
+//    return a != a; // not safe with -ffinite-math-only compiler option
+//}
+#elif (defined(__GNUC__) || defined(__clang__)) && !defined(__INTEL_COMPILER)
+static inline Vec8fb is_nan(Vec8f const a) {
+    __m256 aa = a;
+    __m256 unordered;
+    __asm volatile("vcmpps $3, %1, %1, %0" : "=v" (unordered) :  "v" (aa) );
+    return Vec8fb(unordered);
+}
+#else
+static inline Vec8fb is_nan(Vec8f const a) {
+    // assume that compiler does not optimize this away with -ffinite-math-only:
+    return _mm256_cmp_ps(a, a, 3); // compare unordered
+    // return a != a; // This is not safe with -ffinite-math-only, -ffast-math, or /fp:fast compiler option
+}
+#endif
+
+
+// Function is_subnormal: gives true for elements that are subnormal
+// false for finite numbers, zero, NAN and INF
+static inline Vec8fb is_subnormal(Vec8f const a) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_fpclass_ps_mask (a, 0x20);
+#elif INSTRSET >= 8  // 256 bit integer vectors are available, AVX2
+    Vec8i t1 = _mm256_castps_si256(a);      // reinterpret as 32-bit integer
+    Vec8i t2 = t1 << 1;                     // shift out sign bit
+    Vec8i t3 = 0xFF000000;                  // exponent mask
+    Vec8i t4 = t2 & t3;                     // exponent
+    Vec8i t5 = _mm256_andnot_si256(t3,t2);  // fraction
+    return Vec8ib(t4 == 0 && t5 != 0);      // exponent = 0 and fraction != 0
+#else
+    return Vec8fb(is_subnormal(a.get_low()), is_subnormal(a.get_high()));
+#endif
+}
+
+// Function is_zero_or_subnormal: gives true for elements that are zero or subnormal
+// false for finite numbers, NAN and INF
+static inline Vec8fb is_zero_or_subnormal(Vec8f const a) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_fpclass_ps_mask (a, 0x26);
+#elif INSTRSET >= 8  // 256 bit integer vectors are available, AVX2    Vec8i t = _mm256_castps_si256(a);            // reinterpret as 32-bit integer
+    Vec8i t = _mm256_castps_si256(a);       // reinterpret as 32-bit integer
+    t &= 0x7F800000;                        // isolate exponent
+    return t == 0;                          // exponent = 0
+#else
+    return Vec8fb(is_zero_or_subnormal(a.get_low()), is_zero_or_subnormal(a.get_high()));
+#endif
 }
 
+// change signs on vectors Vec8f
+// Each index i0 - i7 is 1 for changing sign on the corresponding element, 0 for no change
+template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7>
+inline Vec8f change_sign(Vec8f const a) {
+    if ((i0 | i1 | i2 | i3 | i4 | i5 | i6 | i7) == 0) return a;
+    __m256 mask = constant8f<
+        (i0 ? 0x80000000u : 0u), (i1 ? 0x80000000u : 0u), (i2 ? 0x80000000u : 0u), (i3 ? 0x80000000u : 0u),
+        (i4 ? 0x80000000u : 0u), (i5 ? 0x80000000u : 0u), (i6 ? 0x80000000u : 0u), (i7 ? 0x80000000u : 0u)> ();
+    return _mm256_xor_ps(a, mask);
+}
 
 // General arithmetic functions, etc.
 
 // Horizontal add: Calculates the sum of all vector elements.
-static inline float horizontal_add (Vec8f const & a) {
-    __m256 t1 = _mm256_hadd_ps(a,a);
-    __m256 t2 = _mm256_hadd_ps(t1,t1);
-    __m128 t3 = _mm256_extractf128_ps(t2,1);
-    __m128 t4 = _mm_add_ss(_mm256_castps256_ps128(t2),t3);
-    return _mm_cvtss_f32(t4);        
+static inline float horizontal_add (Vec8f const a) {
+    return horizontal_add(a.get_low()+a.get_high());
 }
 
 // function max: a > b ? a : b
-static inline Vec8f max(Vec8f const & a, Vec8f const & b) {
+static inline Vec8f max(Vec8f const a, Vec8f const b) {
     return _mm256_max_ps(a,b);
 }
 
 // function min: a < b ? a : b
-static inline Vec8f min(Vec8f const & a, Vec8f const & b) {
+static inline Vec8f min(Vec8f const a, Vec8f const b) {
     return _mm256_min_ps(a,b);
 }
+// NAN-safe versions of maximum and minimum are in vector_convert.h
 
 // function abs: absolute value
-// Removes sign bit, even for -0.0f, -INF and -NAN
-static inline Vec8f abs(Vec8f const & a) {
-    __m256 mask = constant8f<0x7FFFFFFF,0x7FFFFFFF,0x7FFFFFFF,0x7FFFFFFF,0x7FFFFFFF,0x7FFFFFFF,0x7FFFFFFF,0x7FFFFFFF> ();
+static inline Vec8f abs(Vec8f const a) {
+#if INSTRSET >= 10  // AVX512VL
+    return _mm256_range_ps(a, a, 8);
+#else
+    __m256 mask = constant8f<0x7FFFFFFFu,0x7FFFFFFFu,0x7FFFFFFFu,0x7FFFFFFFu,0x7FFFFFFFu,0x7FFFFFFFu,0x7FFFFFFFu,0x7FFFFFFFu> ();
     return _mm256_and_ps(a,mask);
+#endif
 }
 
 // function sqrt: square root
-static inline Vec8f sqrt(Vec8f const & a) {
+static inline Vec8f sqrt(Vec8f const a) {
     return _mm256_sqrt_ps(a);
 }
 
 // function square: a * a
-static inline Vec8f square(Vec8f const & a) {
+static inline Vec8f square(Vec8f const a) {
     return a * a;
 }
 
-// pow(Vec8f, int):
-template <typename TT> static Vec8f pow(Vec8f const & a, TT const & n);
+// The purpose of this template is to prevent implicit conversion of a float
+// exponent to int when calling pow(vector, float) and vectormath_exp.h is not included
+template <typename TT> static Vec8f pow(Vec8f const a, TT const n);
 
 // Raise floating point numbers to integer power n
 template <>
-inline Vec8f pow<int>(Vec8f const & x0, int const & n) {
+inline Vec8f pow<int>(Vec8f const x0, int const n) {
     return pow_template_i<Vec8f>(x0, n);
 }
 
 // allow conversion from unsigned int
 template <>
-inline Vec8f pow<uint32_t>(Vec8f const & x0, uint32_t const & n) {
+inline Vec8f pow<uint32_t>(Vec8f const x0, uint32_t const n) {
     return pow_template_i<Vec8f>(x0, (int)n);
 }
 
-
 // Raise floating point numbers to integer power n, where n is a compile-time constant
 template <int n>
-static inline Vec8f pow_n(Vec8f const & a) {
-    if (n < 0)    return Vec8f(1.0f) / pow_n<-n>(a);
-    if (n == 0)   return Vec8f(1.0f);
-    if (n >= 256) return pow(a, n);
-    Vec8f x = a;                       // a^(2^i)
-    Vec8f y;                           // accumulator
-    const int lowest = n - (n & (n-1));// lowest set bit in n
-    if (n & 1) y = x;
-    if (n < 2) return y;
-    x = x*x;                           // x^2
-    if (n & 2) {
-        if (lowest == 2) y = x; else y *= x;
-    }
-    if (n < 4) return y;
-    x = x*x;                           // x^4
-    if (n & 4) {
-        if (lowest == 4) y = x; else y *= x;
-    }
-    if (n < 8) return y;
-    x = x*x;                           // x^8
-    if (n & 8) {
-        if (lowest == 8) y = x; else y *= x;
-    }
-    if (n < 16) return y;
-    x = x*x;                           // x^16
-    if (n & 16) {
-        if (lowest == 16) y = x; else y *= x;
-    }
-    if (n < 32) return y;
-    x = x*x;                           // x^32
-    if (n & 32) {
-        if (lowest == 32) y = x; else y *= x;
-    }
-    if (n < 64) return y;
-    x = x*x;                           // x^64
-    if (n & 64) {
-        if (lowest == 64) y = x; else y *= x;
-    }
-    if (n < 128) return y;
-    x = x*x;                           // x^128
-    if (n & 128) {
-        if (lowest == 128) y = x; else y *= x;
-    }
-    return y;
-}
-
-template <int n>
-static inline Vec8f pow(Vec8f const & a, Const_int_t<n>) {
-    return pow_n<n>(a);
+static inline Vec8f pow(Vec8f const a, Const_int_t<n>) {
+    return pow_n<Vec8f, n>(a);
 }
 
-
 // function round: round to nearest integer (even). (result as float vector)
-static inline Vec8f round(Vec8f const & a) {
+static inline Vec8f round(Vec8f const a) {
     return _mm256_round_ps(a, 0+8);
 }
 
 // function truncate: round towards zero. (result as float vector)
-static inline Vec8f truncate(Vec8f const & a) {
+static inline Vec8f truncate(Vec8f const a) {
     return _mm256_round_ps(a, 3+8);
 }
 
 // function floor: round towards minus infinity. (result as float vector)
-static inline Vec8f floor(Vec8f const & a) {
+static inline Vec8f floor(Vec8f const a) {
     return _mm256_round_ps(a, 1+8);
 }
 
 // function ceil: round towards plus infinity. (result as float vector)
-static inline Vec8f ceil(Vec8f const & a) {
+static inline Vec8f ceil(Vec8f const a) {
     return _mm256_round_ps(a, 2+8);
 }
 
-#ifdef VECTORI256_H  // 256 bit integer vectors are available
-#if VECTORI256_H > 1  // AVX2
-// function round_to_int: round to nearest integer (even). (result as integer vector)
-static inline Vec8i round_to_int(Vec8f const & a) {
+#if INSTRSET >= 8  // 256 bit integer vectors are available
+
+// function roundi: round to nearest integer (even). (result as integer vector)
+static inline Vec8i roundi(Vec8f const a) {
     // Note: assume MXCSR control register is set to rounding
+    // Note: +INF gives 0x80000000
     return _mm256_cvtps_epi32(a);
 }
 
-// function truncate_to_int: round towards zero. (result as integer vector)
-static inline Vec8i truncate_to_int(Vec8f const & a) {
+// function truncatei: round towards zero. (result as integer vector)
+static inline Vec8i truncatei(Vec8f const a) {
     return _mm256_cvttps_epi32(a);
 }
 
 // function to_float: convert integer vector to float vector
-static inline Vec8f to_float(Vec8i const & a) {
+static inline Vec8f to_float(Vec8i const a) {
     return _mm256_cvtepi32_ps(a);
 }
 
 // function to_float: convert unsigned integer vector to float vector
-static inline Vec8f to_float(Vec8ui const & a) {
-#ifdef __AVX512VL__
+static inline Vec8f to_float(Vec8ui const a) {
+#if INSTRSET >= 10 && !defined (_MSC_VER)  // _mm256_cvtepu32_ps missing in VS2019
     return _mm256_cvtepu32_ps(a);
+#elif INSTRSET >= 9  // __AVX512F__
+    return _mm512_castps512_ps256(_mm512_cvtepu32_ps(_mm512_castsi256_si512(a)));
 #else
-    Vec8f b = to_float(Vec8i(a & 0x7FFFFFFF));             // 31 bits
-    Vec8i c = Vec8i(a) >> 31;                              // generate mask from highest bit
-    Vec8f d = Vec8f(2147483648.f) & Vec8f(_mm256_castsi256_ps(c));// mask floating point constant 2^31
-    return b + d;
+    Vec8f b = to_float(Vec8i(a & 0xFFFFF));             // 20 bits
+    Vec8f c = to_float(Vec8i(a >> 20));                 // remaining bits
+    Vec8f d = b + c * 1048576.f;  // 2^20
+    return d;
 #endif
 }
 
 #else // no AVX2
 
-// function round_to_int: round to nearest integer (even). (result as integer vector)
-static inline Vec8i round_to_int(Vec8f const & a) {
+// function roundi: round to nearest integer (even). (result as integer vector)
+static inline Vec8i roundi(Vec8f const a) {
     // Note: assume MXCSR control register is set to rounding
     return Vec8i(_mm_cvtps_epi32(a.get_low()), _mm_cvtps_epi32(a.get_high()));
 }
 
-// function truncate_to_int: round towards zero. (result as integer vector)
-static inline Vec8i truncate_to_int(Vec8f const & a) {
+// function truncatei: round towards zero. (result as integer vector)
+static inline Vec8i truncatei(Vec8f const a) {
     return Vec8i(_mm_cvttps_epi32(a.get_low()), _mm_cvttps_epi32(a.get_high()));
 }
 
 // function to_float: convert integer vector to float vector
-static inline Vec8f to_float(Vec8i const & a) {
+static inline Vec8f to_float(Vec8i const a) {
     return Vec8f(_mm_cvtepi32_ps(a.get_low()), _mm_cvtepi32_ps(a.get_high()));
 }
 
 // function to_float: convert unsigned integer vector to float vector
-static inline Vec8f to_float(Vec8ui const & a) {
+static inline Vec8f to_float(Vec8ui const a) {
     return Vec8f(to_float(a.get_low()), to_float(a.get_high()));
 }
-#endif
-#endif // VECTORI256_H
+#endif // AVX2
 
 
 // Fused multiply and add functions
 
 // Multiply and add
-static inline Vec8f mul_add(Vec8f const & a, Vec8f const & b, Vec8f const & c) {
+static inline Vec8f mul_add(Vec8f const a, Vec8f const b, Vec8f const c) {
 #ifdef __FMA__
     return _mm256_fmadd_ps(a, b, c);
 #elif defined (__FMA4__)
@@ -1096,22 +1290,21 @@ static inline Vec8f mul_add(Vec8f const & a, Vec8f const & b, Vec8f const & c) {
 #else
     return a * b + c;
 #endif
-    
 }
 
 // Multiply and subtract
-static inline Vec8f mul_sub(Vec8f const & a, Vec8f const & b, Vec8f const & c) {
+static inline Vec8f mul_sub(Vec8f const a, Vec8f const b, Vec8f const c) {
 #ifdef __FMA__
     return _mm256_fmsub_ps(a, b, c);
 #elif defined (__FMA4__)
     return _mm256_msub_ps(a, b, c);
 #else
     return a * b - c;
-#endif    
+#endif
 }
 
 // Multiply and inverse subtract
-static inline Vec8f nmul_add(Vec8f const & a, Vec8f const & b, Vec8f const & c) {
+static inline Vec8f nmul_add(Vec8f const a, Vec8f const b, Vec8f const c) {
 #ifdef __FMA__
     return _mm256_fnmadd_ps(a, b, c);
 #elif defined (__FMA4__)
@@ -1122,16 +1315,18 @@ static inline Vec8f nmul_add(Vec8f const & a, Vec8f const & b, Vec8f const & c)
 }
 
 
-// Multiply and subtract with extra precision on the intermediate calculations, 
+// Multiply and subtract with extra precision on the intermediate calculations,
 // even if FMA instructions not supported, using Veltkamp-Dekker split
-static inline Vec8f mul_sub_x(Vec8f const & a, Vec8f const & b, Vec8f const & c) {
+// This is used in mathematical functions. Do not use it in general code
+// because it is inaccurate in certain cases
+static inline Vec8f mul_sub_x(Vec8f const a, Vec8f const b, Vec8f const c) {
 #ifdef __FMA__
     return _mm256_fmsub_ps(a, b, c);
 #elif defined (__FMA4__)
     return _mm256_msub_ps(a, b, c);
 #else
     // calculate a * b - c with extra precision
-    const int b12 = -(1 << 12);                  // mask to remove lower 12 bits
+    const uint32_t b12 = uint32_t(-(1 << 12));   // mask to remove lower 12 bits
     Vec8f upper_mask = constant8f<b12,b12,b12,b12,b12,b12,b12,b12>();
     Vec8f a_high = a & upper_mask;               // split into high and low parts
     Vec8f b_high = b & upper_mask;
@@ -1148,32 +1343,29 @@ static inline Vec8f mul_sub_x(Vec8f const & a, Vec8f const & b, Vec8f const & c)
 // Approximate math functions
 
 // approximate reciprocal (Faster than 1.f / a. relative accuracy better than 2^-11)
-static inline Vec8f approx_recipr(Vec8f const & a) {
-#if INSTRSET >= 9  // use more accurate version if available. (none of these will raise exceptions on zero)
+static inline Vec8f approx_recipr(Vec8f const a) {
 #ifdef __AVX512ER__  // AVX512ER: full precision
     // todo: if future processors have both AVX512ER and AVX512VL: _mm256_rcp28_round_ps(a, _MM_FROUND_NO_EXC);
     return _mm512_castps512_ps256(_mm512_rcp28_round_ps(_mm512_castps256_ps512(a), _MM_FROUND_NO_EXC));
-#elif defined __AVX512VL__  // AVX512VL: 14 bit precision
+#elif INSTRSET >= 10  // AVX512VL: 14 bit precision
     return _mm256_rcp14_ps(a);
-#else  // AVX512F: 14 bit precision
+#elif INSTRSET >= 9   // AVX512F: 14 bit precision
     return _mm512_castps512_ps256(_mm512_rcp14_ps(_mm512_castps256_ps512(a)));
-#endif
 #else  // AVX: 11 bit precision
     return _mm256_rcp_ps(a);
 #endif
 }
 
 // approximate reciprocal squareroot (Faster than 1.f / sqrt(a). Relative accuracy better than 2^-11)
-static inline Vec8f approx_rsqrt(Vec8f const & a) {
-#if INSTRSET >= 9  // use more accurate version if available. (none of these will raise exceptions on zero)
+static inline Vec8f approx_rsqrt(Vec8f const a) {
+// use more accurate version if available. (none of these will raise exceptions on zero)
 #ifdef __AVX512ER__  // AVX512ER: full precision
     // todo: if future processors have both AVX512ER and AVX521VL: _mm256_rsqrt28_round_ps(a, _MM_FROUND_NO_EXC);
     return _mm512_castps512_ps256(_mm512_rsqrt28_round_ps(_mm512_castps256_ps512(a), _MM_FROUND_NO_EXC));
-#elif defined __AVX512VL__  // AVX512VL: 14 bit precision
+#elif INSTRSET >= 10 && defined(_mm256_rsqrt14_ps)  // missing in VS2019
     return _mm256_rsqrt14_ps(a);
-#else  // AVX512F: 14 bit precision
+#elif INSTRSET >= 9  // AVX512F: 14 bit precision
     return _mm512_castps512_ps256(_mm512_rsqrt14_ps(_mm512_castps256_ps512(a)));
-#endif
 #else  // AVX: 11 bit precision
     return _mm256_rsqrt_ps(a);
 #endif
@@ -1182,12 +1374,11 @@ static inline Vec8f approx_rsqrt(Vec8f const & a) {
 
 // Math functions using fast bit manipulation
 
-#ifdef VECTORI256_H  // 256 bit integer vectors are available, AVX2
 // Extract the exponent as an integer
 // exponent(a) = floor(log2(abs(a)));
 // exponent(1.0f) = 0, exponent(0.0f) = -127, exponent(INF) = +128, exponent(NAN) = +128
-static inline Vec8i exponent(Vec8f const & a) {
-#if  VECTORI256_H > 1  // AVX2
+static inline Vec8i exponent(Vec8f const a) {
+#if INSTRSET >= 8  // 256 bit integer vectors are available, AVX2
     Vec8ui t1 = _mm256_castps_si256(a);// reinterpret as 32-bit integer
     Vec8ui t2 = t1 << 1;               // shift out sign bit
     Vec8ui t3 = t2 >> 24;              // shift down logical to position 0
@@ -1197,13 +1388,14 @@ static inline Vec8i exponent(Vec8f const & a) {
     return Vec8i(exponent(a.get_low()), exponent(a.get_high()));
 #endif
 }
-#endif
 
 // Extract the fraction part of a floating point number
 // a = 2^exponent(a) * fraction(a), except for a = 0
-// fraction(1.0f) = 1.0f, fraction(5.0f) = 1.25f 
-static inline Vec8f fraction(Vec8f const & a) {
-#if defined (VECTORI256_H) && VECTORI256_H > 2  // 256 bit integer vectors are available, AVX2
+// fraction(1.0f) = 1.0f, fraction(5.0f) = 1.25f
+static inline Vec8f fraction(Vec8f const a) {
+#if INSTRSET >= 10
+    return _mm256_getmant_ps(a, _MM_MANT_NORM_1_2, _MM_MANT_SIGN_zero);
+#elif INSTRSET >= 8 // AVX2. 256 bit integer vectors are available
     Vec8ui t1 = _mm256_castps_si256(a);   // reinterpret as 32-bit integer
     Vec8ui t2 = (t1 & 0x007FFFFF) | 0x3F800000; // set exponent to 0 + bias
     return _mm256_castsi256_ps(t2);
@@ -1212,14 +1404,13 @@ static inline Vec8f fraction(Vec8f const & a) {
 #endif
 }
 
-#ifdef VECTORI256_H  // 256 bit integer vectors are available, AVX2
 // Fast calculation of pow(2,n) with n integer
 // n  =    0 gives 1.0f
 // n >=  128 gives +INF
 // n <= -127 gives 0.0f
-// This function will never produce denormals, and never raise exceptions
-static inline Vec8f exp2(Vec8i const & n) {
-#if  VECTORI256_H > 1  // AVX2
+// This function will never produce subnormals, and never raise exceptions
+static inline Vec8f exp2(Vec8i const n) {
+#if INSTRSET >= 8  // 256 bit integer vectors are available, AVX2
     Vec8i t1 = max(n,  -0x7F);         // limit to allowed range
     Vec8i t2 = min(t1,  0x80);
     Vec8i t3 = t2 + 0x7F;              // add bias
@@ -1227,159 +1418,43 @@ static inline Vec8f exp2(Vec8i const & n) {
     return _mm256_castsi256_ps(t4);    // reinterpret as float
 #else
     return Vec8f(exp2(n.get_low()), exp2(n.get_high()));
-#endif
+#endif // AVX2
 }
-//static inline Vec8f exp2(Vec8f const & x); // defined in vectormath_exp.h
-
-#endif // VECTORI256_H
+//static inline Vec8f exp2(Vec8f const x); // defined in vectormath_exp.h
 
 
-// Categorization functions
 
-// Function sign_bit: gives true for elements that have the sign bit set
-// even for -0.0f, -INF and -NAN
-// Note that sign_bit(Vec8f(-0.0f)) gives true, while Vec8f(-0.0f) < Vec8f(0.0f) gives false
-// (the underscore in the name avoids a conflict with a macro in Intel's mathimf.h)
-static inline Vec8fb sign_bit(Vec8f const & a) {
-#if defined (VECTORI256_H) && VECTORI256_H > 1  // 256 bit integer vectors are available, AVX2
-    Vec8i t1 = _mm256_castps_si256(a);    // reinterpret as 32-bit integer
-    Vec8i t2 = t1 >> 31;                  // extend sign bit
-    return _mm256_castsi256_ps(t2);       // reinterpret as 32-bit Boolean
-#else
-    return Vec8fb(sign_bit(a.get_low()), sign_bit(a.get_high()));
-#endif
-}
-
-// Function sign_combine: changes the sign of a when b has the sign bit set
-// same as select(sign_bit(b), -a, a)
-static inline Vec8f sign_combine(Vec8f const & a, Vec8f const & b) {
-    Vec8f signmask = constant8f<(int)0x80000000,(int)0x80000000,(int)0x80000000,(int)0x80000000,(int)0x80000000,(int)0x80000000,(int)0x80000000,(int)0x80000000>();  // -0.0
-    return a ^ (b & signmask);
-}
-
-// Function is_finite: gives true for elements that are normal, denormal or zero, 
-// false for INF and NAN
-// (the underscore in the name avoids a conflict with a macro in Intel's mathimf.h)
-static inline Vec8fb is_finite(Vec8f const & a) {
-#if defined (VECTORI256_H) && VECTORI256_H > 1  // 256 bit integer vectors are available, AVX2
-    Vec8i t1 = _mm256_castps_si256(a);    // reinterpret as 32-bit integer
-    Vec8i t2 = t1 << 1;                // shift out sign bit
-    Vec8ib t3 = Vec8i(t2 & 0xFF000000) != 0xFF000000; // exponent field is not all 1s
-    return t3;
-#else
-    return Vec8fb(is_finite(a.get_low()), is_finite(a.get_high()));
-#endif
-}
-
-// Function is_inf: gives true for elements that are +INF or -INF
-// false for finite numbers and NAN
-// (the underscore in the name avoids a conflict with a macro in Intel's mathimf.h)
-static inline Vec8fb is_inf(Vec8f const & a) {
-#if defined (VECTORI256_H) && VECTORI256_H > 1  // 256 bit integer vectors are available, AVX2
-    Vec8i t1 = _mm256_castps_si256(a); // reinterpret as 32-bit integer
-    Vec8i t2 = t1 << 1;                // shift out sign bit
-    return t2 == 0xFF000000;           // exponent is all 1s, fraction is 0
-#else
-    return Vec8fb(is_inf(a.get_low()), is_inf(a.get_high()));
-#endif
-}
-
-// Function is_nan: gives true for elements that are +NAN or -NAN
-// false for finite numbers and +/-INF
-// (the underscore in the name avoids a conflict with a macro in Intel's mathimf.h)
-static inline Vec8fb is_nan(Vec8f const & a) {
-#if defined (VECTORI256_H) && VECTORI256_H > 1  // 256 bit integer vectors are available, AVX2
-    Vec8i t1 = _mm256_castps_si256(a); // reinterpret as 32-bit integer
-    Vec8i t2 = t1 << 1;                // shift out sign bit
-    Vec8i t3 = 0xFF000000;             // exponent mask
-    Vec8i t4 = t2 & t3;                // exponent
-    Vec8i t5 = _mm256_andnot_si256(t3,t2);// fraction
-    return Vec8ib(t4 == t3 && t5 != 0);// exponent = all 1s and fraction != 0
-#else
-    return Vec8fb(is_nan(a.get_low()), is_nan(a.get_high()));
-#endif
-}
-
-// Function is_subnormal: gives true for elements that are denormal (subnormal)
-// false for finite numbers, zero, NAN and INF
-static inline Vec8fb is_subnormal(Vec8f const & a) {
-#if defined (VECTORI256_H) && VECTORI256_H > 1  // 256 bit integer vectors are available, AVX2
-    Vec8i t1 = _mm256_castps_si256(a);    // reinterpret as 32-bit integer
-    Vec8i t2 = t1 << 1;                   // shift out sign bit
-    Vec8i t3 = 0xFF000000;                // exponent mask
-    Vec8i t4 = t2 & t3;                   // exponent
-    Vec8i t5 = _mm256_andnot_si256(t3,t2);// fraction
-    return Vec8ib(t4 == 0 && t5 != 0);    // exponent = 0 and fraction != 0
-#else
-    return Vec8fb(is_subnormal(a.get_low()), is_subnormal(a.get_high()));
-#endif
-}
-
-// Function is_zero_or_subnormal: gives true for elements that are zero or subnormal (denormal)
-// false for finite numbers, NAN and INF
-static inline Vec8fb is_zero_or_subnormal(Vec8f const & a) {
-#if defined (VECTORI256_H) && VECTORI256_H > 1   // 256 bit integer vectors are available, AVX2
-    Vec8i t = _mm256_castps_si256(a);            // reinterpret as 32-bit integer
-          t &= 0x7F800000;                       // isolate exponent
-    return t == 0;                               // exponent = 0
-#else
-    return Vec8fb(is_zero_or_subnormal(a.get_low()), is_zero_or_subnormal(a.get_high()));
-#endif
-}
-
-// Function infinite4f: returns a vector where all elements are +INF
-static inline Vec8f infinite8f() {
-    return constant8f<0x7F800000,0x7F800000,0x7F800000,0x7F800000,0x7F800000,0x7F800000,0x7F800000,0x7F800000>();
-}
-
-// Function nan4f: returns a vector where all elements are +NAN (quiet)
-static inline Vec8f nan8f(int n = 0x10) {
-    return _mm256_castsi256_ps(_mm256_set1_epi32(0x7FC00000 + n));
-}
-
-// change signs on vectors Vec8f
-// Each index i0 - i7 is 1 for changing sign on the corresponding element, 0 for no change
-template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7>
-static inline Vec8f change_sign(Vec8f const & a) {
-    if ((i0 | i1 | i2 | i3 | i4 | i5 | i6 | i7) == 0) return a;
-    __m256 mask = constant8f<i0 ? (int)0x80000000 : 0, i1 ? (int)0x80000000 : 0, i2 ? (int)0x80000000 : 0, i3 ? (int)0x80000000 : 0,
-        i4 ? (int)0x80000000 : 0, i5 ? (int)0x80000000 : 0, i6 ? (int)0x80000000 : 0, i7 ? (int)0x80000000 : 0> ();
-    return _mm256_xor_ps(a, mask);
-}
-
-
-/*****************************************************************************
-*
-*          Vec4d: Vector of 4 double precision floating point values
-*
-*****************************************************************************/
+/*****************************************************************************
+*
+*          Vec4d: Vector of 4 double precision floating point values
+*
+*****************************************************************************/
 
 class Vec4d {
 protected:
     __m256d ymm; // double vector
 public:
     // Default constructor:
-    Vec4d() {
-    }
+    Vec4d() = default;
     // Constructor to broadcast the same value into all elements:
     Vec4d(double d) {
         ymm = _mm256_set1_pd(d);
     }
     // Constructor to build from all elements:
     Vec4d(double d0, double d1, double d2, double d3) {
-        ymm = _mm256_setr_pd(d0, d1, d2, d3); 
+        ymm = _mm256_setr_pd(d0, d1, d2, d3);
     }
     // Constructor to build from two Vec2d:
-    Vec4d(Vec2d const & a0, Vec2d const & a1) {
+    Vec4d(Vec2d const a0, Vec2d const a1) {
         ymm = _mm256_castps_pd(set_m128r(_mm_castpd_ps(a0), _mm_castpd_ps(a1)));
         //ymm = _mm256_set_m128d(a1, a0);
     }
     // Constructor to convert from type __m256d used in intrinsics:
-    Vec4d(__m256d const & x) {
+    Vec4d(__m256d const x) {
         ymm = x;
     }
     // Assignment operator to convert from type __m256d used in intrinsics:
-    Vec4d & operator = (__m256d const & x) {
+    Vec4d & operator = (__m256d const x) {
         ymm = x;
         return *this;
     }
@@ -1403,14 +1478,24 @@ public:
     void store(double * p) const {
         _mm256_storeu_pd(p, ymm);
     }
-    // Member function to store into array, aligned by 32
+    // Member function storing into array, aligned by 32
     // You may use store_a instead of store if you are certain that p points to an address
     // divisible by 32
     void store_a(double * p) const {
         _mm256_store_pd(p, ymm);
     }
+    // Member function storing to aligned uncached memory (non-temporal store).
+    // This may be more efficient than store_a when storing large blocks of memory if it 
+    // is unlikely that the data will stay in the cache until it is read again.
+    // Note: Will generate runtime error if p is not aligned by 32
+    void store_nt(double * p) const {
+        _mm256_stream_pd(p, ymm);
+    }
     // Partial load. Load n elements and set the rest to 0
     Vec4d & load_partial(int n, double const * p) {
+#if INSTRSET >= 10  // AVX512VL
+        ymm = _mm256_maskz_loadu_pd(__mmask8((1u << n) - 1), p);
+#else
         if (n > 0 && n <= 2) {
             *this = Vec4d(Vec2d().load_partial(n, p), _mm_setzero_pd());
         }
@@ -1420,10 +1505,14 @@ public:
         else {
             ymm = _mm256_setzero_pd();
         }
+#endif
         return *this;
     }
     // Partial store. Store n elements
     void store_partial(int n, double * p) const {
+#if INSTRSET >= 10  // AVX512VL
+        _mm256_mask_storeu_pd(p, __mmask8((1u << n) - 1), ymm);
+#else
         if (n <= 2) {
             get_low().store_partial(n, p);
         }
@@ -1431,15 +1520,23 @@ public:
             get_low().store(p);
             get_high().store_partial(n - 2, p + 2);
         }
+#endif
     }
     // cut off vector to n elements. The last 4-n elements are set to zero
     Vec4d & cutoff(int n) {
+#if INSTRSET >= 10
+        ymm = _mm256_maskz_mov_pd(__mmask8((1u << n) - 1), ymm);
+#else
         ymm = _mm256_castps_pd(Vec8f(_mm256_castpd_ps(ymm)).cutoff(n*2));
+#endif
         return *this;
     }
     // Member function to change a single element in vector
     // Note: This function is inefficient. Use load function if changing more than one element
-    Vec4d const & insert(uint32_t index, double value) {
+    Vec4d const insert(int index, double value) {
+#if INSTRSET >= 10   // AVX512VL
+        ymm = _mm256_mask_broadcastsd_pd (ymm, __mmask8(1u << index), _mm_set_sd(value));
+#else
         __m256d v0 = _mm256_broadcast_sd(&value);
         switch (index) {
         case 0:
@@ -1451,17 +1548,23 @@ public:
         default:
             ymm = _mm256_blend_pd (ymm, v0, 8);  break;
         }
+#endif
         return *this;
     }
     // Member function extract a single element from vector
-    double extract(uint32_t index) const {
+    double extract(int index) const {
+#if INSTRSET >= 10
+        __m256d x = _mm256_maskz_compress_pd(__mmask8(1u << index), ymm);
+        return _mm256_cvtsd_f64(x);
+#else
         double x[4];
         store(x);
         return x[index & 3];
+#endif
     }
     // Extract a single element. Use store function if extracting more than one element.
     // Operator [] can only read an element, not write.
-    double operator [] (uint32_t index) const {
+    double operator [] (int index) const {
         return extract(index);
     }
     // Member functions to split into two Vec2d:
@@ -1471,13 +1574,16 @@ public:
     Vec2d get_high() const {
         return _mm256_extractf128_pd(ymm,1);
     }
-    static int size () {
+    static constexpr int size() {
         return 4;
     }
+    static constexpr int elementtype() {
+        return 17;
+    }
+    typedef __m256d registertype;
 };
 
 
-
 /*****************************************************************************
 *
 *          Operators for Vec4d
@@ -1485,20 +1591,20 @@ public:
 *****************************************************************************/
 
 // vector operator + : add element by element
-static inline Vec4d operator + (Vec4d const & a, Vec4d const & b) {
+static inline Vec4d operator + (Vec4d const a, Vec4d const b) {
     return _mm256_add_pd(a, b);
 }
 
 // vector operator + : add vector and scalar
-static inline Vec4d operator + (Vec4d const & a, double b) {
+static inline Vec4d operator + (Vec4d const a, double b) {
     return a + Vec4d(b);
 }
-static inline Vec4d operator + (double a, Vec4d const & b) {
+static inline Vec4d operator + (double a, Vec4d const b) {
     return Vec4d(a) + b;
 }
 
 // vector operator += : add
-static inline Vec4d & operator += (Vec4d & a, Vec4d const & b) {
+static inline Vec4d & operator += (Vec4d & a, Vec4d const b) {
     a = a + b;
     return a;
 }
@@ -1517,26 +1623,26 @@ static inline Vec4d & operator ++ (Vec4d & a) {
 }
 
 // vector operator - : subtract element by element
-static inline Vec4d operator - (Vec4d const & a, Vec4d const & b) {
+static inline Vec4d operator - (Vec4d const a, Vec4d const b) {
     return _mm256_sub_pd(a, b);
 }
 
 // vector operator - : subtract vector and scalar
-static inline Vec4d operator - (Vec4d const & a, double b) {
+static inline Vec4d operator - (Vec4d const a, double b) {
     return a - Vec4d(b);
 }
-static inline Vec4d operator - (double a, Vec4d const & b) {
+static inline Vec4d operator - (double a, Vec4d const b) {
     return Vec4d(a) - b;
 }
 
 // vector operator - : unary minus
 // Change sign bit, even for 0, INF and NAN
-static inline Vec4d operator - (Vec4d const & a) {
-    return _mm256_xor_pd(a, _mm256_castps_pd(constant8f<0,(int)0x80000000,0,(int)0x80000000,0,(int)0x80000000,0,(int)0x80000000> ()));
+static inline Vec4d operator - (Vec4d const a) {
+    return _mm256_xor_pd(a, _mm256_castps_pd(constant8f<0u,0x80000000u,0u,0x80000000u,0u,0x80000000u,0u,0x80000000u> ()));
 }
 
 // vector operator -= : subtract
-static inline Vec4d & operator -= (Vec4d & a, Vec4d const & b) {
+static inline Vec4d & operator -= (Vec4d & a, Vec4d const b) {
     a = a - b;
     return a;
 }
@@ -1555,118 +1661,146 @@ static inline Vec4d & operator -- (Vec4d & a) {
 }
 
 // vector operator * : multiply element by element
-static inline Vec4d operator * (Vec4d const & a, Vec4d const & b) {
+static inline Vec4d operator * (Vec4d const a, Vec4d const b) {
     return _mm256_mul_pd(a, b);
 }
 
 // vector operator * : multiply vector and scalar
-static inline Vec4d operator * (Vec4d const & a, double b) {
+static inline Vec4d operator * (Vec4d const a, double b) {
     return a * Vec4d(b);
 }
-static inline Vec4d operator * (double a, Vec4d const & b) {
+static inline Vec4d operator * (double a, Vec4d const b) {
     return Vec4d(a) * b;
 }
 
 // vector operator *= : multiply
-static inline Vec4d & operator *= (Vec4d & a, Vec4d const & b) {
+static inline Vec4d & operator *= (Vec4d & a, Vec4d const b) {
     a = a * b;
     return a;
 }
 
 // vector operator / : divide all elements by same integer
-static inline Vec4d operator / (Vec4d const & a, Vec4d const & b) {
+static inline Vec4d operator / (Vec4d const a, Vec4d const b) {
     return _mm256_div_pd(a, b);
 }
 
 // vector operator / : divide vector and scalar
-static inline Vec4d operator / (Vec4d const & a, double b) {
+static inline Vec4d operator / (Vec4d const a, double b) {
     return a / Vec4d(b);
 }
-static inline Vec4d operator / (double a, Vec4d const & b) {
+static inline Vec4d operator / (double a, Vec4d const b) {
     return Vec4d(a) / b;
 }
 
 // vector operator /= : divide
-static inline Vec4d & operator /= (Vec4d & a, Vec4d const & b) {
+static inline Vec4d & operator /= (Vec4d & a, Vec4d const b) {
     a = a / b;
     return a;
 }
 
 // vector operator == : returns true for elements for which a == b
-static inline Vec4db operator == (Vec4d const & a, Vec4d const & b) {
+static inline Vec4db operator == (Vec4d const a, Vec4d const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_cmp_pd_mask(a, b, 0);
+#else
     return _mm256_cmp_pd(a, b, 0);
+#endif
 }
 
 // vector operator != : returns true for elements for which a != b
-static inline Vec4db operator != (Vec4d const & a, Vec4d const & b) {
+static inline Vec4db operator != (Vec4d const a, Vec4d const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_cmp_pd_mask(a, b, 4);
+#else
     return _mm256_cmp_pd(a, b, 4);
+#endif
 }
 
 // vector operator < : returns true for elements for which a < b
-static inline Vec4db operator < (Vec4d const & a, Vec4d const & b) {
+static inline Vec4db operator < (Vec4d const a, Vec4d const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_cmp_pd_mask(a, b, 1);
+#else
     return _mm256_cmp_pd(a, b, 1);
+#endif
 }
 
 // vector operator <= : returns true for elements for which a <= b
-static inline Vec4db operator <= (Vec4d const & a, Vec4d const & b) {
+static inline Vec4db operator <= (Vec4d const a, Vec4d const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_cmp_pd_mask(a, b, 2);
+#else
     return _mm256_cmp_pd(a, b, 2);
+#endif
 }
 
 // vector operator > : returns true for elements for which a > b
-static inline Vec4db operator > (Vec4d const & a, Vec4d const & b) {
+static inline Vec4db operator > (Vec4d const a, Vec4d const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_cmp_pd_mask(a, b, 6+8);
+#else
     return b < a;
+#endif
 }
 
 // vector operator >= : returns true for elements for which a >= b
-static inline Vec4db operator >= (Vec4d const & a, Vec4d const & b) {
+static inline Vec4db operator >= (Vec4d const a, Vec4d const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_cmp_pd_mask(a, b, 5+8);
+#else
     return b <= a;
+#endif
 }
 
 // Bitwise logical operators
 
 // vector operator & : bitwise and
-static inline Vec4d operator & (Vec4d const & a, Vec4d const & b) {
+static inline Vec4d operator & (Vec4d const a, Vec4d const b) {
     return _mm256_and_pd(a, b);
 }
 
 // vector operator &= : bitwise and
-static inline Vec4d & operator &= (Vec4d & a, Vec4d const & b) {
+static inline Vec4d & operator &= (Vec4d & a, Vec4d const b) {
     a = a & b;
     return a;
 }
 
 // vector operator & : bitwise and of Vec4d and Vec4db
-static inline Vec4d operator & (Vec4d const & a, Vec4db const & b) {
+static inline Vec4d operator & (Vec4d const a, Vec4db const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_maskz_mov_pd(b, a);
+#else
     return _mm256_and_pd(a, b);
+#endif
 }
-static inline Vec4d operator & (Vec4db const & a, Vec4d const & b) {
-    return _mm256_and_pd(a, b);
+static inline Vec4d operator & (Vec4db const a, Vec4d const b) {
+    return b & a;
 }
 
 // vector operator | : bitwise or
-static inline Vec4d operator | (Vec4d const & a, Vec4d const & b) {
+static inline Vec4d operator | (Vec4d const a, Vec4d const b) {
     return _mm256_or_pd(a, b);
 }
 
 // vector operator |= : bitwise or
-static inline Vec4d & operator |= (Vec4d & a, Vec4d const & b) {
+static inline Vec4d & operator |= (Vec4d & a, Vec4d const b) {
     a = a | b;
     return a;
 }
 
 // vector operator ^ : bitwise xor
-static inline Vec4d operator ^ (Vec4d const & a, Vec4d const & b) {
+static inline Vec4d operator ^ (Vec4d const a, Vec4d const b) {
     return _mm256_xor_pd(a, b);
 }
 
 // vector operator ^= : bitwise xor
-static inline Vec4d & operator ^= (Vec4d & a, Vec4d const & b) {
+static inline Vec4d & operator ^= (Vec4d & a, Vec4d const b) {
     a = a ^ b;
     return a;
 }
 
 // vector operator ! : logical not. Returns Boolean vector
-static inline Vec4db operator ! (Vec4d const & a) {
+static inline Vec4db operator ! (Vec4d const a) {
     return a == Vec4d(0.0);
 }
 
@@ -1679,266 +1813,340 @@ static inline Vec4db operator ! (Vec4d const & a) {
 
 // Select between two operands. Corresponds to this pseudocode:
 // for (int i = 0; i < 2; i++) result[i] = s[i] ? a[i] : b[i];
-// Each byte in s must be either 0 (false) or 0xFFFFFFFFFFFFFFFF (true). 
-// No other values are allowed.
-static inline Vec4d select (Vec4db const & s, Vec4d const & a, Vec4d const & b) {
+static inline Vec4d select (Vec4db const s, Vec4d const a, Vec4d const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_mask_mov_pd(b, s, a);
+#else
     return _mm256_blendv_pd(b, a, s);
+#endif
 }
 
 // Conditional add: For all vector elements i: result[i] = f[i] ? (a[i] + b[i]) : a[i]
-static inline Vec4d if_add (Vec4db const & f, Vec4d const & a, Vec4d const & b) {
+static inline Vec4d if_add (Vec4db const f, Vec4d const a, Vec4d const b) {
+#if INSTRSET >= 10
+    return _mm256_mask_add_pd (a, f, a, b);
+#else
     return a + (Vec4d(f) & b);
+#endif
+}
+
+// Conditional subtract
+static inline Vec4d if_sub (Vec4db const f, Vec4d const a, Vec4d const b) {
+#if INSTRSET >= 10
+    return _mm256_mask_sub_pd (a, f, a, b);
+#else
+    return a - (Vec4d(f) & b);
+#endif
 }
 
-// Conditional multiply: For all vector elements i: result[i] = f[i] ? (a[i] * b[i]) : a[i]
-static inline Vec4d if_mul (Vec4db const & f, Vec4d const & a, Vec4d const & b) {
+// Conditional multiply
+static inline Vec4d if_mul (Vec4db const f, Vec4d const a, Vec4d const b) {
+#if INSTRSET >= 10
+    return _mm256_mask_mul_pd (a, f, a, b);
+#else
     return a * select(f, b, 1.);
+#endif
+}
+
+// Conditional divide
+static inline Vec4d if_div (Vec4db const f, Vec4d const a, Vec4d const b) {
+#if INSTRSET >= 10
+    return _mm256_mask_div_pd (a, f, a, b);
+#else
+    return a / select(f, b, 1.);
+#endif
+}
+
+// sign functions
+
+// Function sign_combine: changes the sign of a when b has the sign bit set
+// same as select(sign_bit(b), -a, a)
+static inline Vec4d sign_combine(Vec4d const a, Vec4d const b) {
+#if INSTRSET < 10
+    return a ^ (b & Vec4d(-0.0));
+#else
+    return _mm256_castsi256_pd (_mm256_ternarylogic_epi64(
+        _mm256_castpd_si256(a), _mm256_castpd_si256(b), Vec4q(0x8000000000000000), 0x78));
+#endif
+}
+
+// Function is_finite: gives true for elements that are normal, subnormal or zero,
+// false for INF and NAN
+static inline Vec4db is_finite(Vec4d const a) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return __mmask8(~ _mm256_fpclass_pd_mask (a, 0x99));
+#elif INSTRSET >= 8  // 256 bit integer vectors are available, AVX2
+    Vec4q t1 = _mm256_castpd_si256(a); // reinterpret as 64-bit integer
+    Vec4q t2 = t1 << 1;                // shift out sign bit
+    Vec4q t3 = 0xFFE0000000000000;     // exponent mask
+    Vec4qb t4 = Vec4q(t2 & t3) != t3;  // exponent field is not all 1s
+    return t4;
+#else
+    return Vec4db(is_finite(a.get_low()),is_finite(a.get_high()));
+#endif
+}
+
+// categorization functions
+
+// Function is_inf: gives true for elements that are +INF or -INF
+// false for finite numbers and NAN
+static inline Vec4db is_inf(Vec4d const a) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_fpclass_pd_mask (a, 0x18);
+#elif INSTRSET >= 8  // 256 bit integer vectors are available, AVX2
+    Vec4q t1 = _mm256_castpd_si256(a); // reinterpret as 64-bit integer
+    Vec4q t2 = t1 << 1;                // shift out sign bit
+    return t2 == 0xFFE0000000000000;   // exponent is all 1s, fraction is 0
+#else
+    return Vec4db(is_inf(a.get_low()),is_inf(a.get_high()));
+#endif
+}
+
+// Function is_nan: gives true for elements that are +NAN or -NAN
+// false for finite numbers and +/-INF
+// (the underscore in the name avoids a conflict with a macro in Intel's mathimf.h)
+#if INSTRSET >= 10
+static inline Vec4db is_nan(Vec4d const a) {
+    // assume that compiler does not optimize this away with -ffinite-math-only:
+    return _mm256_fpclass_pd_mask (a, 0x81);
+}
+//#elif defined(__GNUC__) && !defined(__INTEL_COMPILER) && !defined(__clang__)
+//__attribute__((optimize("-fno-unsafe-math-optimizations")))
+//static inline Vec4db is_nan(Vec4d const a) {
+//    return a != a; // not safe with -ffinite-math-only compiler option
+//}
+#elif (defined(__GNUC__) || defined(__clang__)) && !defined(__INTEL_COMPILER)
+static inline Vec4db is_nan(Vec4d const a) {
+    __m256d aa = a;
+    __m256d unordered;
+    __asm volatile("vcmppd $3, %1, %1, %0" : "=v" (unordered) :  "v" (aa) );
+    return Vec4db(unordered);
+}
+#else
+static inline Vec4db is_nan(Vec4d const a) {
+    // assume that compiler does not optimize this away with -ffinite-math-only:
+    return _mm256_cmp_pd(a, a, 3); // compare unordered
+    // return a != a; // This is not safe with -ffinite-math-only, -ffast-math, or /fp:fast compiler option
+}
+#endif
+
+
+// Function is_subnormal: gives true for elements that are subnormal
+// false for finite numbers, zero, NAN and INF
+static inline Vec4db is_subnormal(Vec4d const a) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_fpclass_pd_mask (a, 0x20);
+#elif INSTRSET >= 8  // 256 bit integer vectors are available, AVX2
+    Vec4q t1 = _mm256_castpd_si256(a); // reinterpret as 64-bit integer
+    Vec4q t2 = t1 << 1;                // shift out sign bit
+    Vec4q t3 = 0xFFE0000000000000;     // exponent mask
+    Vec4q t4 = t2 & t3;                // exponent
+    Vec4q t5 = _mm256_andnot_si256(t3,t2);// fraction
+    return Vec4qb(t4 == 0 && t5 != 0); // exponent = 0 and fraction != 0
+#else
+    return Vec4db(is_subnormal(a.get_low()),is_subnormal(a.get_high()));
+#endif
 }
 
+// Function is_zero_or_subnormal: gives true for elements that are zero or subnormal
+// false for finite numbers, NAN and INF
+static inline Vec4db is_zero_or_subnormal(Vec4d const a) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_fpclass_pd_mask (a, 0x26);
+#elif INSTRSET >= 8  // 256 bit integer vectors are available, AVX2    Vec8i t = _mm256_castps_si256(a);            // reinterpret as 32-bit integer
+    Vec4q t = _mm256_castpd_si256(a);     // reinterpret as 32-bit integer
+    t &= 0x7FF0000000000000ll;   // isolate exponent
+    return t == 0;                     // exponent = 0
+#else
+    return Vec4db(is_zero_or_subnormal(a.get_low()),is_zero_or_subnormal(a.get_high()));
+#endif
+}
 
 // General arithmetic functions, etc.
 
 // Horizontal add: Calculates the sum of all vector elements.
-static inline double horizontal_add (Vec4d const & a) {
-    __m256d t1 = _mm256_hadd_pd(a,a);
-    __m128d t2 = _mm256_extractf128_pd(t1,1);
-    __m128d t3 = _mm_add_sd(_mm256_castpd256_pd128(t1),t2);
-    return _mm_cvtsd_f64(t3);        
+static inline double horizontal_add (Vec4d const a) {
+    return horizontal_add(a.get_low() + a.get_high());
 }
 
 // function max: a > b ? a : b
-static inline Vec4d max(Vec4d const & a, Vec4d const & b) {
+static inline Vec4d max(Vec4d const a, Vec4d const b) {
     return _mm256_max_pd(a,b);
 }
 
 // function min: a < b ? a : b
-static inline Vec4d min(Vec4d const & a, Vec4d const & b) {
+static inline Vec4d min(Vec4d const a, Vec4d const b) {
     return _mm256_min_pd(a,b);
 }
+// NAN-safe versions of maximum and minimum are in vector_convert.h
 
 // function abs: absolute value
-// Removes sign bit, even for -0.0f, -INF and -NAN
-static inline Vec4d abs(Vec4d const & a) {
-    __m256d mask = _mm256_castps_pd(constant8f<-1,0x7FFFFFFF,-1,0x7FFFFFFF,-1,0x7FFFFFFF,-1,0x7FFFFFFF> ());
+static inline Vec4d abs(Vec4d const a) {
+#if INSTRSET >= 10  // AVX512VL
+    return _mm256_range_pd(a, a, 8);
+#else
+    __m256d mask = _mm256_castps_pd(constant8f<0xFFFFFFFFu,0x7FFFFFFFu,0xFFFFFFFFu,0x7FFFFFFFu,0xFFFFFFFFu,0x7FFFFFFFu,0xFFFFFFFFu,0x7FFFFFFFu> ());
     return _mm256_and_pd(a,mask);
+#endif
 }
 
 // function sqrt: square root
-static inline Vec4d sqrt(Vec4d const & a) {
+static inline Vec4d sqrt(Vec4d const a) {
     return _mm256_sqrt_pd(a);
 }
 
 // function square: a * a
-static inline Vec4d square(Vec4d const & a) {
+static inline Vec4d square(Vec4d const a) {
     return a * a;
 }
 
-// pow(Vec4d, int):
-template <typename TT> static Vec4d pow(Vec4d const & a, TT const & n);
+// The purpose of this template is to prevent implicit conversion of a float
+// exponent to int when calling pow(vector, float) and vectormath_exp.h is not included
+template <typename TT> static Vec4d pow(Vec4d const a, TT const n);
 
 // Raise floating point numbers to integer power n
 template <>
-inline Vec4d pow<int>(Vec4d const & x0, int const & n) {
+inline Vec4d pow<int>(Vec4d const x0, int const n) {
     return pow_template_i<Vec4d>(x0, n);
 }
 
 // allow conversion from unsigned int
 template <>
-inline Vec4d pow<uint32_t>(Vec4d const & x0, uint32_t const & n) {
+inline Vec4d pow<uint32_t>(Vec4d const x0, uint32_t const n) {
     return pow_template_i<Vec4d>(x0, (int)n);
 }
 
-
 // Raise floating point numbers to integer power n, where n is a compile-time constant
 template <int n>
-static inline Vec4d pow_n(Vec4d const & a) {
-    if (n < 0)    return Vec4d(1.0) / pow_n<-n>(a);
-    if (n == 0)   return Vec4d(1.0);
-    if (n >= 256) return pow(a, n);
-    Vec4d x = a;                       // a^(2^i)
-    Vec4d y;                           // accumulator
-    const int lowest = n - (n & (n-1));// lowest set bit in n
-    if (n & 1) y = x;
-    if (n < 2) return y;
-    x = x*x;                           // x^2
-    if (n & 2) {
-        if (lowest == 2) y = x; else y *= x;
-    }
-    if (n < 4) return y;
-    x = x*x;                           // x^4
-    if (n & 4) {
-        if (lowest == 4) y = x; else y *= x;
-    }
-    if (n < 8) return y;
-    x = x*x;                           // x^8
-    if (n & 8) {
-        if (lowest == 8) y = x; else y *= x;
-    }
-    if (n < 16) return y;
-    x = x*x;                           // x^16
-    if (n & 16) {
-        if (lowest == 16) y = x; else y *= x;
-    }
-    if (n < 32) return y;
-    x = x*x;                           // x^32
-    if (n & 32) {
-        if (lowest == 32) y = x; else y *= x;
-    }
-    if (n < 64) return y;
-    x = x*x;                           // x^64
-    if (n & 64) {
-        if (lowest == 64) y = x; else y *= x;
-    }
-    if (n < 128) return y;
-    x = x*x;                           // x^128
-    if (n & 128) {
-        if (lowest == 128) y = x; else y *= x;
-    }
-    return y;
-}
-
-template <int n>
-static inline Vec4d pow(Vec4d const & a, Const_int_t<n>) {
-    return pow_n<n>(a);
+static inline Vec4d pow(Vec4d const a, Const_int_t<n>) {
+    return pow_n<Vec4d, n>(a);
 }
 
 
 // function round: round to nearest integer (even). (result as double vector)
-static inline Vec4d round(Vec4d const & a) {
+static inline Vec4d round(Vec4d const a) {
     return _mm256_round_pd(a, 0+8);
 }
 
 // function truncate: round towards zero. (result as double vector)
-static inline Vec4d truncate(Vec4d const & a) {
+static inline Vec4d truncate(Vec4d const a) {
     return _mm256_round_pd(a, 3+8);
 }
 
 // function floor: round towards minus infinity. (result as double vector)
-static inline Vec4d floor(Vec4d const & a) {
+static inline Vec4d floor(Vec4d const a) {
     return _mm256_round_pd(a, 1+8);
 }
 
 // function ceil: round towards plus infinity. (result as double vector)
-static inline Vec4d ceil(Vec4d const & a) {
+static inline Vec4d ceil(Vec4d const a) {
     return _mm256_round_pd(a, 2+8);
 }
 
-// function round_to_int: round to nearest integer (even). (result as integer vector)
-static inline Vec4i round_to_int(Vec4d const & a) {
+// function round_to_int32: round to nearest integer (even). (result as integer vector)
+static inline Vec4i round_to_int32(Vec4d const a) {
     // Note: assume MXCSR control register is set to rounding
     return _mm256_cvtpd_epi32(a);
 }
 
-// function truncate_to_int: round towards zero. (result as integer vector)
-static inline Vec4i truncate_to_int(Vec4d const & a) {
+// function truncate_to_int32: round towards zero. (result as integer vector)
+static inline Vec4i truncate_to_int32(Vec4d const a) {
     return _mm256_cvttpd_epi32(a);
 }
 
-#ifdef VECTORI256_H  // 256 bit integer vectors are available
+#if INSTRSET >= 8  // 256 bit integer vectors are available. AVX2
 
-// function truncate_to_int64: round towards zero. (inefficient)
-static inline Vec4q truncate_to_int64(Vec4d const & a) {
-#if defined (__AVX512DQ__) && defined (__AVX512VL__)
-    //return _mm256_maskz_cvttpd_epi64( __mmask8(0xFF), a);
+// function truncatei: round towards zero
+static inline Vec4q truncatei(Vec4d const a) {
+#if INSTRSET >= 10 // __AVX512DQ__ __AVX512VL__
     return _mm256_cvttpd_epi64(a);
 #else
-    double aa[4];
+    double aa[4];    // inefficient
     a.store(aa);
     return Vec4q(int64_t(aa[0]), int64_t(aa[1]), int64_t(aa[2]), int64_t(aa[3]));
 #endif
 }
 
-// function truncate_to_int64_limited: round towards zero.
-// result as 64-bit integer vector, but with limited range. Deprecated!
-static inline Vec4q truncate_to_int64_limited(Vec4d const & a) {
-#if defined (__AVX512DQ__) && defined (__AVX512VL__)
-    return truncate_to_int64(a);
-#elif VECTORI256_H > 1
-    // Note: assume MXCSR control register is set to rounding
-    Vec2q   b = _mm256_cvttpd_epi32(a);                    // round to 32-bit integers
-    __m256i c = permute4q<0,-256,1,-256>(Vec4q(b,b));      // get bits 64-127 to position 128-191
-    __m256i s = _mm256_srai_epi32(c, 31);                  // sign extension bits
-    return      _mm256_unpacklo_epi32(c, s);               // interleave with sign extensions
-#else
-    return Vec4q(truncate_to_int64_limited(a.get_low()), truncate_to_int64_limited(a.get_high()));
-#endif
-} 
-
-// function round_to_int64: round to nearest or even. (inefficient)
-static inline Vec4q round_to_int64(Vec4d const & a) {
-#if defined (__AVX512DQ__) && defined (__AVX512VL__)
+// function roundi: round to nearest or even
+static inline Vec4q roundi(Vec4d const a) {
+#if INSTRSET >= 10 // __AVX512DQ__ __AVX512VL__
     return _mm256_cvtpd_epi64(a);
 #else
-    return truncate_to_int64(round(a));
-#endif
-}
-
-// function round_to_int64_limited: round to nearest integer (even)
-// result as 64-bit integer vector, but with limited range. Deprecated!
-static inline Vec4q round_to_int64_limited(Vec4d const & a) {
-#if defined (__AVX512DQ__) && defined (__AVX512VL__)
-    return round_to_int64(a);
-#elif VECTORI256_H > 1
-    // Note: assume MXCSR control register is set to rounding
-    Vec2q   b = _mm256_cvtpd_epi32(a);                     // round to 32-bit integers
-    __m256i c = permute4q<0,-256,1,-256>(Vec4q(b,b));      // get bits 64-127 to position 128-191
-    __m256i s = _mm256_srai_epi32(c, 31);                  // sign extension bits
-    return      _mm256_unpacklo_epi32(c, s);               // interleave with sign extensions
-#else
-    return Vec4q(round_to_int64_limited(a.get_low()), round_to_int64_limited(a.get_high()));
+    return truncatei(round(a));  // inefficient
 #endif
 }
 
-// function to_double: convert integer vector elements to double vector (inefficient)
-static inline Vec4d to_double(Vec4q const & a) {
-#if defined (__AVX512DQ__) && defined (__AVX512VL__)
+// function to_double: convert integer vector elements to double vector
+static inline Vec4d to_double(Vec4q const a) {
+#if INSTRSET >= 10 // __AVX512DQ__ __AVX512VL__
         return _mm256_maskz_cvtepi64_pd( __mmask16(0xFF), a);
 #else
-        int64_t aa[4];
+        int64_t aa[4];      // inefficient
         a.store(aa);
         return Vec4d(double(aa[0]), double(aa[1]), double(aa[2]), double(aa[3]));
 #endif
 }
 
-// function to_double_limited: convert integer vector elements to double vector
-// limited to abs(x) < 2^31. Deprecated!
-static inline Vec4d to_double_limited(Vec4q const & x) {
-#if defined (__AVX512DQ__) && defined (__AVX512VL__)
-    return to_double(x);
+static inline Vec4d to_double(Vec4uq const a) {
+#if INSTRSET >= 10 // __AVX512DQ__ __AVX512VL__
+    return _mm256_cvtepu64_pd(a);
 #else
-    Vec8i compressed = permute8i<0,2,4,6,-256,-256,-256,-256>(Vec8i(x));
-    return _mm256_cvtepi32_pd(compressed.get_low());  // AVX
+    uint64_t aa[4];      // inefficient
+    a.store(aa);
+    return Vec4d(double(aa[0]), double(aa[1]), double(aa[2]), double(aa[3]));
 #endif
 }
 
-#endif // VECTORI256_H
+#else  // no 256 bit integer vectors
+
+// function truncatei: round towards zero. (inefficient)
+static inline Vec4q truncatei(Vec4d const a) {
+    return Vec4q(truncatei(a.get_low()), truncatei(a.get_high()));
+}
+
+// function roundi: round to nearest or even. (inefficient)
+static inline Vec4q roundi(Vec4d const a) {
+    return Vec4q(roundi(a.get_low()), roundi(a.get_high()));
+}
+
+// function to_double: convert integer vector elements to double vector
+static inline Vec4d to_double(Vec4q const a) {
+    return Vec4d(to_double(a.get_low()), to_double(a.get_high()));
+}
+
+static inline Vec4d to_double(Vec4uq const a) {
+    return Vec4d(to_double(a.get_low()), to_double(a.get_high()));
+}
+
+#endif // AVX2
+
 
 // function to_double: convert integer vector to double vector
-static inline Vec4d to_double(Vec4i const & a) {
+static inline Vec4d to_double(Vec4i const a) {
     return _mm256_cvtepi32_pd(a);
 }
 
 // function compress: convert two Vec4d to one Vec8f
-static inline Vec8f compress (Vec4d const & low, Vec4d const & high) {
+static inline Vec8f compress (Vec4d const low, Vec4d const high) {
     __m128 t1 = _mm256_cvtpd_ps(low);
     __m128 t2 = _mm256_cvtpd_ps(high);
     return Vec8f(t1, t2);
 }
 
 // Function extend_low : convert Vec8f vector elements 0 - 3 to Vec4d
-static inline Vec4d extend_low(Vec8f const & a) {
+static inline Vec4d extend_low(Vec8f const a) {
     return _mm256_cvtps_pd(_mm256_castps256_ps128(a));
 }
 
 // Function extend_high : convert Vec8f vector elements 4 - 7 to Vec4d
-static inline Vec4d extend_high (Vec8f const & a) {
+static inline Vec4d extend_high (Vec8f const a) {
     return _mm256_cvtps_pd(_mm256_extractf128_ps(a,1));
 }
 
 // Fused multiply and add functions
 
 // Multiply and add
-static inline Vec4d mul_add(Vec4d const & a, Vec4d const & b, Vec4d const & c) {
+static inline Vec4d mul_add(Vec4d const a, Vec4d const b, Vec4d const c) {
 #ifdef __FMA__
     return _mm256_fmadd_pd(a, b, c);
 #elif defined (__FMA4__)
@@ -1946,12 +2154,11 @@ static inline Vec4d mul_add(Vec4d const & a, Vec4d const & b, Vec4d const & c) {
 #else
     return a * b + c;
 #endif
-    
-}
 
+}
 
 // Multiply and subtract
-static inline Vec4d mul_sub(Vec4d const & a, Vec4d const & b, Vec4d const & c) {
+static inline Vec4d mul_sub(Vec4d const a, Vec4d const b, Vec4d const c) {
 #ifdef __FMA__
     return _mm256_fmsub_pd(a, b, c);
 #elif defined (__FMA4__)
@@ -1959,11 +2166,10 @@ static inline Vec4d mul_sub(Vec4d const & a, Vec4d const & b, Vec4d const & c) {
 #else
     return a * b - c;
 #endif
-   
 }
 
 // Multiply and inverse subtract
-static inline Vec4d nmul_add(Vec4d const & a, Vec4d const & b, Vec4d const & c) {
+static inline Vec4d nmul_add(Vec4d const a, Vec4d const b, Vec4d const c) {
 #ifdef __FMA__
     return _mm256_fnmadd_pd(a, b, c);
 #elif defined (__FMA4__)
@@ -1973,9 +2179,11 @@ static inline Vec4d nmul_add(Vec4d const & a, Vec4d const & b, Vec4d const & c)
 #endif
 }
 
-// Multiply and subtract with extra precision on the intermediate calculations, 
-// even if FMA instructions not supported, using Veltkamp-Dekker split
-static inline Vec4d mul_sub_x(Vec4d const & a, Vec4d const & b, Vec4d const & c) {
+// Multiply and subtract with extra precision on the intermediate calculations,
+// even if FMA instructions not supported, using Veltkamp-Dekker split.
+// This is used in mathematical functions. Do not use it in general code
+// because it is inaccurate in certain cases
+static inline Vec4d mul_sub_x(Vec4d const a, Vec4d const b, Vec4d const c) {
 #ifdef __FMA__
     return _mm256_fmsub_pd(a, b, c);
 #elif defined (__FMA4__)
@@ -1983,7 +2191,7 @@ static inline Vec4d mul_sub_x(Vec4d const & a, Vec4d const & b, Vec4d const & c)
 #else
     // calculate a * b - c with extra precision
     // mask to remove lower 27 bits
-    Vec4d upper_mask = _mm256_castps_pd(constant8f<(int)0xF8000000,-1,(int)0xF8000000,-1,(int)0xF8000000,-1,(int)0xF8000000,-1>());
+    Vec4d upper_mask = _mm256_castps_pd(constant8f<0xF8000000u,0xFFFFFFFFu,0xF8000000u,0xFFFFFFFFu,0xF8000000u,0xFFFFFFFFu,0xF8000000u,0xFFFFFFFFu>());
     Vec4d a_high = a & upper_mask;               // split into high and low parts
     Vec4d b_high = b & upper_mask;
     Vec4d a_low  = a - a_high;
@@ -1998,12 +2206,11 @@ static inline Vec4d mul_sub_x(Vec4d const & a, Vec4d const & b, Vec4d const & c)
 
 // Math functions using fast bit manipulation
 
-#ifdef VECTORI256_H  // 256 bit integer vectors are available
 // Extract the exponent as an integer
 // exponent(a) = floor(log2(abs(a)));
 // exponent(1.0) = 0, exponent(0.0) = -1023, exponent(INF) = +1024, exponent(NAN) = +1024
-static inline Vec4q exponent(Vec4d const & a) {
-#if VECTORI256_H > 1  // AVX2
+static inline Vec4q exponent(Vec4d const a) {
+#if INSTRSET >= 8  // 256 bit integer vectors are available
     Vec4uq t1 = _mm256_castpd_si256(a);// reinterpret as 64-bit integer
     Vec4uq t2 = t1 << 1;               // shift out sign bit
     Vec4uq t3 = t2 >> 53;              // shift down logical to position 0
@@ -2016,9 +2223,11 @@ static inline Vec4q exponent(Vec4d const & a) {
 
 // Extract the fraction part of a floating point number
 // a = 2^exponent(a) * fraction(a), except for a = 0
-// fraction(1.0) = 1.0, fraction(5.0) = 1.25 
-static inline Vec4d fraction(Vec4d const & a) {
-#if VECTORI256_H > 1  // AVX2
+// fraction(1.0) = 1.0, fraction(5.0) = 1.25
+static inline Vec4d fraction(Vec4d const a) {
+#if INSTRSET >= 10
+    return _mm256_getmant_pd(a, _MM_MANT_NORM_1_2, _MM_MANT_SIGN_zero);
+#elif INSTRSET >= 8 // AVX2. 256 bit integer vectors are available
     Vec4uq t1 = _mm256_castpd_si256(a);   // reinterpret as 64-bit integer
     Vec4uq t2 = Vec4uq((t1 & 0x000FFFFFFFFFFFFF) | 0x3FF0000000000000); // set exponent to 0 + bias
     return _mm256_castsi256_pd(t2);
@@ -2031,20 +2240,19 @@ static inline Vec4d fraction(Vec4d const & a) {
 // n  =     0 gives 1.0
 // n >=  1024 gives +INF
 // n <= -1023 gives 0.0
-// This function will never produce denormals, and never raise exceptions
-static inline Vec4d exp2(Vec4q const & n) {
-#if VECTORI256_H > 1  // AVX2
+// This function will never produce subnormals, and never raise exceptions
+static inline Vec4d exp2(Vec4q const n) {
+#if INSTRSET >= 8  // 256 bit integer vectors are available
     Vec4q t1 = max(n,  -0x3FF);        // limit to allowed range
     Vec4q t2 = min(t1,  0x400);
     Vec4q t3 = t2 + 0x3FF;             // add bias
     Vec4q t4 = t3 << 52;               // put exponent into position 52
-    return _mm256_castsi256_pd(t4);       // reinterpret as double
+    return _mm256_castsi256_pd(t4);    // reinterpret as double
 #else
     return Vec4d(exp2(n.get_low()), exp2(n.get_high()));
 #endif
 }
-//static inline Vec4d exp2(Vec4d const & x); // defined in vectormath_exp.h
-#endif
+//static inline Vec4d exp2(Vec4d const x); // defined in vectormath_exp.h
 
 
 // Categorization functions
@@ -2052,317 +2260,188 @@ static inline Vec4d exp2(Vec4q const & n) {
 // Function sign_bit: gives true for elements that have the sign bit set
 // even for -0.0, -INF and -NAN
 // Note that sign_bit(Vec4d(-0.0)) gives true, while Vec4d(-0.0) < Vec4d(0.0) gives false
-static inline Vec4db sign_bit(Vec4d const & a) {
-#if defined (VECTORI256_H) && VECTORI256_H > 1  // 256 bit integer vectors are available, AVX2
+static inline Vec4db sign_bit(Vec4d const a) {
+#if INSTRSET >= 8  // 256 bit integer vectors are available, AVX2
     Vec4q t1 = _mm256_castpd_si256(a);    // reinterpret as 64-bit integer
-    Vec4q t2 = t1 >> 63;               // extend sign bit
+    Vec4q t2 = t1 >> 63;                  // extend sign bit
+#if INSTRSET >= 10
+    return t2 != 0;
+#else
     return _mm256_castsi256_pd(t2);       // reinterpret as 64-bit Boolean
+#endif
 #else
     return Vec4db(sign_bit(a.get_low()),sign_bit(a.get_high()));
 #endif
 }
 
-// Function sign_combine: changes the sign of a when b has the sign bit set
-// same as select(sign_bit(b), -a, a)
-static inline Vec4d sign_combine(Vec4d const & a, Vec4d const & b) {
-    Vec4d signmask = _mm256_castps_pd(constant8f<0,(int)0x80000000,0,(int)0x80000000,0,(int)0x80000000,0,(int)0x80000000>());  // -0.0
-    return a ^ (b & signmask);
+// change signs on vectors Vec4d
+// Each index i0 - i3 is 1 for changing sign on the corresponding element, 0 for no change
+template <int i0, int i1, int i2, int i3>
+inline Vec4d change_sign(Vec4d const a) {
+    if ((i0 | i1 | i2 | i3) == 0) return a;
+    __m256d mask = _mm256_castps_pd(constant8f <
+        0u, (i0 ? 0x80000000u : 0u), 0u, (i1 ? 0x80000000u : 0u), 0u, (i2 ? 0x80000000u : 0u), 0u, (i3 ? 0x80000000u : 0u)> ());
+    return _mm256_xor_pd(a, mask);
 }
 
-// Function is_finite: gives true for elements that are normal, denormal or zero, 
-// false for INF and NAN
-static inline Vec4db is_finite(Vec4d const & a) {
-#if defined (VECTORI256_H) && VECTORI256_H > 1  // 256 bit integer vectors are available, AVX2
-    Vec4q t1 = _mm256_castpd_si256(a); // reinterpret as 64-bit integer
-    Vec4q t2 = t1 << 1;                // shift out sign bit
-    Vec4q t3 = 0xFFE0000000000000;     // exponent mask
-    Vec4qb t4 = Vec4q(t2 & t3) != t3;  // exponent field is not all 1s
-    return t4;
-#else
-    return Vec4db(is_finite(a.get_low()),is_finite(a.get_high()));
+
+/*****************************************************************************
+*
+*          Functions for reinterpretation between vector types
+*
+*****************************************************************************/
+
+#if INSTRSET >= 8  // AVX2
+
+#if defined (__GXX_ABI_VERSION) && __GXX_ABI_VERSION < 1004 && !defined(__clang__)
+#error Compiler ABI version must be at least 4
 #endif
+
+// ABI version 4 or later needed on Gcc for correct mangling of 256-bit intrinsic vectors.
+// If necessary, compile with -fabi-version=0 to get the latest abi version
+//#if !defined (GCC_VERSION) || (defined (__GXX_ABI_VERSION) && __GXX_ABI_VERSION >= 1004)
+static inline __m256i reinterpret_i (__m256i const x) {
+    return x;
 }
 
-// Function is_inf: gives true for elements that are +INF or -INF
-// false for finite numbers and NAN
-static inline Vec4db is_inf(Vec4d const & a) {
-#if defined (VECTORI256_H) && VECTORI256_H > 1  // 256 bit integer vectors are available, AVX2
-    Vec4q t1 = _mm256_castpd_si256(a); // reinterpret as 64-bit integer
-    Vec4q t2 = t1 << 1;                // shift out sign bit
-    return t2 == 0xFFE0000000000000;   // exponent is all 1s, fraction is 0
-#else
-    return Vec4db(is_inf(a.get_low()),is_inf(a.get_high()));
-#endif
+static inline __m256i reinterpret_i (__m256  const x) {
+    return _mm256_castps_si256(x);
 }
 
-// Function is_nan: gives true for elements that are +NAN or -NAN
-// false for finite numbers and +/-INF
-static inline Vec4db is_nan(Vec4d const & a) {
-#if defined (VECTORI256_H) && VECTORI256_H > 1  // 256 bit integer vectors are available, AVX2
-    Vec4q t1 = _mm256_castpd_si256(a); // reinterpret as 64-bit integer
-    Vec4q t2 = t1 << 1;                // shift out sign bit
-    Vec4q t3 = 0xFFE0000000000000;     // exponent mask
-    Vec4q t4 = t2 & t3;                // exponent
-    Vec4q t5 = _mm256_andnot_si256(t3,t2);// fraction
-    return Vec4qb(t4 == t3 && t5 != 0);// exponent = all 1s and fraction != 0
-#else
-    return Vec4db(is_nan(a.get_low()),is_nan(a.get_high()));
-#endif
+static inline __m256i reinterpret_i (__m256d const x) {
+    return _mm256_castpd_si256(x);
 }
 
-// Function is_subnormal: gives true for elements that are denormal (subnormal)
-// false for finite numbers, zero, NAN and INF
-static inline Vec4db is_subnormal(Vec4d const & a) {
-#if defined (VECTORI256_H) && VECTORI256_H > 1  // 256 bit integer vectors are available, AVX2
-    Vec4q t1 = _mm256_castpd_si256(a); // reinterpret as 64-bit integer
-    Vec4q t2 = t1 << 1;                // shift out sign bit
-    Vec4q t3 = 0xFFE0000000000000;     // exponent mask
-    Vec4q t4 = t2 & t3;                // exponent
-    Vec4q t5 = _mm256_andnot_si256(t3,t2);// fraction
-    return Vec4qb(t4 == 0 && t5 != 0); // exponent = 0 and fraction != 0
-#else
-    return Vec4db(is_subnormal(a.get_low()),is_subnormal(a.get_high()));
-#endif
-}
-
-// Function is_zero_or_subnormal: gives true for elements that are zero or subnormal (denormal)
-// false for finite numbers, NAN and INF
-static inline Vec4db is_zero_or_subnormal(Vec4d const & a) {
-#if defined (VECTORI256_H) && VECTORI256_H > 1  // 256 bit integer vectors are available, AVX2
-    Vec4q t = _mm256_castpd_si256(a);     // reinterpret as 32-bit integer
-          t &= 0x7FF0000000000000ll;   // isolate exponent
-    return t == 0;                     // exponent = 0
-#else
-    return Vec4db(is_zero_or_subnormal(a.get_low()),is_zero_or_subnormal(a.get_high()));
-#endif
-}
-
-// Function infinite2d: returns a vector where all elements are +INF
-static inline Vec4d infinite4d() {
-    return _mm256_castps_pd(constant8f<0,0x7FF00000,0,0x7FF00000,0,0x7FF00000,0,0x7FF00000>());
-}
-
-// Function nan4d: returns a vector where all elements are +NAN (quiet)
-static inline Vec4d nan4d(int n = 0x10) {
-#if defined (VECTORI256_H) && VECTORI256_H > 1  // 256 bit integer vectors are available, AVX2
-    return _mm256_castsi256_pd(Vec4q(0x7FF8000000000000 + n));
-#else
-    return Vec4d(nan2d(n),nan2d(n));
-#endif
-}
-
-// change signs on vectors Vec4d
-// Each index i0 - i3 is 1 for changing sign on the corresponding element, 0 for no change
-template <int i0, int i1, int i2, int i3>
-static inline Vec4d change_sign(Vec4d const & a) {
-    if ((i0 | i1 | i2 | i3) == 0) return a;
-    __m256 mask = constant8f<0, i0 ? (int)0x80000000 : 0, 0, i1 ? (int)0x80000000 : 0, 0, i2 ? (int)0x80000000 : 0, 0, i3 ? (int)0x80000000 : 0> ();
-    return _mm256_xor_pd(a, _mm256_castps_pd(mask));
-}
-
-
-/*****************************************************************************
-*
-*          Functions for reinterpretation between vector types
-*
-*****************************************************************************/
-
-#if defined (VECTORI256_H) && VECTORI256_H >= 2
-// AVX2 vectors defined
-
-
-// ABI version 4 or later needed on Gcc for correct mangling of 256-bit intrinsic vectors.
-// It is recommended to compile with -fabi-version=0 to get the latest abi version
-#if !defined (GCC_VERSION) || (defined (__GXX_ABI_VERSION) && __GXX_ABI_VERSION >= 1004)  
-static inline __m256i reinterpret_i (__m256i const & x) {
-    return x;
-}
-
-static inline __m256i reinterpret_i (__m256  const & x) {
-    return _mm256_castps_si256(x);
-}
-
-static inline __m256i reinterpret_i (__m256d const & x) {
-    return _mm256_castpd_si256(x);
-}
-
-static inline __m256  reinterpret_f (__m256i const & x) {
+static inline __m256  reinterpret_f (__m256i const x) {
     return _mm256_castsi256_ps(x);
 }
 
-static inline __m256  reinterpret_f (__m256  const & x) {
+static inline __m256  reinterpret_f (__m256  const x) {
     return x;
 }
 
-static inline __m256  reinterpret_f (__m256d const & x) {
+static inline __m256  reinterpret_f (__m256d const x) {
     return _mm256_castpd_ps(x);
 }
 
-static inline __m256d reinterpret_d (__m256i const & x) {
+static inline __m256d reinterpret_d (__m256i const x) {
     return _mm256_castsi256_pd(x);
 }
 
-static inline __m256d reinterpret_d (__m256  const & x) {
+static inline __m256d reinterpret_d (__m256  const x) {
     return _mm256_castps_pd(x);
 }
 
-static inline __m256d reinterpret_d (__m256d const & x) {
+static inline __m256d reinterpret_d (__m256d const x) {
     return x;
 }
 
-#else  // __GXX_ABI_VERSION < 1004
+#else  // AVX2 emulated in vectori256e.h, AVX supported
 
-static inline __m256i reinterpret_i (Vec32c const & x) {
-    return x;
-}
-
-static inline __m256i reinterpret_i (Vec16s const & x) {
-    return x;
-}
-
-static inline __m256i reinterpret_i (Vec8i const & x) {
-    return x;
-}
-
-static inline __m256i reinterpret_i (Vec4q const & x) {
-    return x;
-}
-
-static inline __m256i reinterpret_i (Vec8f  const & x) {
-    return _mm256_castps_si256(x);
-}
-
-static inline __m256i reinterpret_i (Vec4d const & x) {
-    return _mm256_castpd_si256(x);
-}
-
-static inline __m256  reinterpret_f (Vec32c const & x) {
-    return _mm256_castsi256_ps(x);
-}
-
-static inline __m256  reinterpret_f (Vec16s const & x) {
-    return _mm256_castsi256_ps(x);
-}
+// ABI version 4 or later needed on Gcc for correct mangling of 256-bit intrinsic vectors.
+// If necessary, compile with -fabi-version=0 to get the latest abi version
 
-static inline __m256  reinterpret_f (Vec8i const & x) {
-    return _mm256_castsi256_ps(x);
+static inline Vec256b reinterpret_i (__m256  const x) {
+    Vec8f xx(x);
+    return Vec256b(reinterpret_i(xx.get_low()), reinterpret_i(xx.get_high()));
 }
 
-static inline __m256  reinterpret_f (Vec4q const & x) {
-    return _mm256_castsi256_ps(x);
+static inline Vec256b reinterpret_i (__m256d const x) {
+    Vec4d xx(x);
+    return Vec256b(reinterpret_i(xx.get_low()), reinterpret_i(xx.get_high()));
 }
 
-static inline __m256  reinterpret_f (Vec8f  const & x) {
+static inline __m256  reinterpret_f (__m256  const x) {
     return x;
 }
 
-static inline __m256  reinterpret_f (Vec4d const & x) {
+static inline __m256  reinterpret_f (__m256d const x) {
     return _mm256_castpd_ps(x);
 }
 
-static inline __m256d reinterpret_d (Vec32c const & x) {
-    return _mm256_castsi256_pd(x);
-}
-
-static inline __m256d reinterpret_d (Vec16s const & x) {
-    return _mm256_castsi256_pd(x);
+static inline __m256d reinterpret_d (__m256  const x) {
+    return _mm256_castps_pd(x);
 }
 
-static inline __m256d reinterpret_d (Vec8i const & x) {
-    return _mm256_castsi256_pd(x);
+static inline __m256d reinterpret_d (__m256d const x) {
+    return x;
 }
 
-static inline __m256d reinterpret_d (Vec4q const & x) {
-    return _mm256_castsi256_pd(x);
+static inline Vec256b reinterpret_i (Vec256b const x) {
+    return x;
 }
 
-static inline __m256d reinterpret_d (Vec8f  const & x) {
-    return _mm256_castps_pd(x);
+static inline __m256  reinterpret_f (Vec256b const x) {
+    return Vec8f(Vec4f(reinterpret_f(x.get_low())), Vec4f(reinterpret_f(x.get_high())));
 }
 
-static inline __m256d reinterpret_d (Vec4d const & x) {
-    return x;
+static inline __m256d reinterpret_d (Vec256b const x) {
+    return Vec4d(Vec2d(reinterpret_d(x.get_low())), Vec2d(reinterpret_d(x.get_high())));
 }
 
-#endif  // __GXX_ABI_VERSION
+#endif  // AVX2
 
-#else
-// AVX2 emulated in vectori256e.h, AVX supported
 
-// ABI version 4 or later needed on Gcc for correct mangling of 256-bit intrinsic vectors.
-// It is recommended to compile with -fabi-version=0 to get the latest abi version
-#if !defined (GCC_VERSION) || (defined (__GXX_ABI_VERSION) && __GXX_ABI_VERSION >= 1004)  
+// extend vectors to double size by adding zeroes
 
-static inline Vec256ie reinterpret_i (__m256  const & x) {
-    Vec8f xx(x);
-    return Vec256ie(reinterpret_i(xx.get_low()), reinterpret_i(xx.get_high()));
-}
+#if defined(__GNUC__) && __GNUC__ <= 9
+// GCC v. 9 is missing the _mm256_zextps128_ps256 intrinsic
 
-static inline Vec256ie reinterpret_i (__m256d const & x) {
-    Vec4d xx(x);
-    return Vec256ie(reinterpret_i(xx.get_low()), reinterpret_i(xx.get_high()));
+static inline Vec8f extend_z(Vec4f a) {
+    return Vec8f(a, Vec4f(0));
 }
-
-static inline __m256  reinterpret_f (__m256  const & x) {
-    return x;
+static inline Vec4d extend_z(Vec2d a) {
+    return Vec4d(a, Vec2d(0));
 }
-
-static inline __m256  reinterpret_f (__m256d const & x) {
-    return _mm256_castpd_ps(x);
+#if INSTRSET < 10 
+static inline Vec8fb extend_z(Vec4fb a) {
+    return Vec8fb(a, Vec4fb(false));
 }
-
-static inline __m256d reinterpret_d (__m256  const & x) {
-    return _mm256_castps_pd(x);
+static inline Vec4db extend_z(Vec2db a) {
+    return Vec4db(a, Vec2db(false));
 }
+#endif // INSTRSET < 10 
+#else
 
-static inline __m256d reinterpret_d (__m256d const & x) {
-    return x;
+static inline Vec8f extend_z(Vec4f a) {
+    return _mm256_zextps128_ps256(a);
 }
-
-#else  // __GXX_ABI_VERSION < 1004
-
-static inline Vec256ie reinterpret_i (Vec8f const & x) {
-    Vec8f xx(x);
-    return Vec256ie(reinterpret_i(xx.get_low()), reinterpret_i(xx.get_high()));
+static inline Vec4d extend_z(Vec2d a) {
+    return _mm256_zextpd128_pd256(a);
 }
 
-static inline Vec256ie reinterpret_i (Vec4d const & x) {
-    Vec4d xx(x);
-    return Vec256ie(reinterpret_i(xx.get_low()), reinterpret_i(xx.get_high()));
-}
+#if INSTRSET < 10  // broad boolean vectors
 
-static inline __m256  reinterpret_f (Vec8f const & x) {
-    return x;
+static inline Vec8fb extend_z(Vec4fb a) {
+    return _mm256_zextps128_ps256(a);
 }
-
-static inline __m256  reinterpret_f (Vec4d const & x) {
-    return _mm256_castpd_ps(x);
+static inline Vec4db extend_z(Vec2db a) {
+    return _mm256_zextpd128_pd256(a);
 }
 
-static inline __m256d reinterpret_d (Vec8f const & x) {
-    return _mm256_castps_pd(x);
-}
+#endif // INSTRSET
+#endif // __GNUC__
 
-static inline __m256d reinterpret_d (Vec4d const & x) {
-    return x;
+// Function infinite4f: returns a vector where all elements are +INF
+static inline Vec8f infinite8f() {
+    return reinterpret_f(Vec8i(0x7F800000));
 }
 
-#endif  // __GXX_ABI_VERSION
-
-static inline Vec256ie reinterpret_i (Vec256ie const & x) {
-    return x;
+// Function nan8f: returns a vector where all elements are +NAN (quiet)
+static inline Vec8f nan8f(uint32_t n = 0x10) {
+    return nan_vec<Vec8f>(n);
 }
 
-static inline __m256  reinterpret_f (Vec256ie const & x) {
-    return Vec8f(Vec4f(reinterpret_f(x.get_low())), Vec4f(reinterpret_f(x.get_high())));
+// Function infinite2d: returns a vector where all elements are +INF
+static inline Vec4d infinite4d() {
+    return reinterpret_d(Vec4q(0x7FF0000000000000));
 }
 
-static inline __m256d reinterpret_d (Vec256ie const & x) {
-    return Vec4d(Vec2d(reinterpret_d(x.get_low())), Vec2d(reinterpret_d(x.get_high())));
+// Function nan4d: returns a vector where all elements are +NAN (quiet)
+static inline Vec4d nan4d(uint32_t n = 0x10) {
+    return nan_vec<Vec4d>(n);
 }
 
-#endif  // VECTORI256_H
-
 
 /*****************************************************************************
 *
@@ -2370,637 +2449,350 @@ static inline __m256d reinterpret_d (Vec256ie const & x) {
 *
 ******************************************************************************
 *
-* The permute function can reorder the elements of a vector and optionally
-* set some elements to zero. 
-*
-* The indexes are inserted as template parameters in <>. These indexes must be
-* constants. Each template parameter is an index to the element you want to 
-* select. An index of -1 will generate zero. An index of -256 means don't care.
-*
-* Example:
-* Vec4d a(10., 11., 12., 13.);    // a is (10, 11, 12, 13)
-* Vec4d b;
-* b = permute4d<1,0,-1,3>(a);     // b is (11, 10,  0, 13)
+* These permute functions can reorder the elements of a vector and optionally
+* set some elements to zero. See Vectori128.h for description
 *
-*
-* The blend function can mix elements from two different vectors and
-* optionally set some elements to zero. 
-*
-* The indexes are inserted as template parameters in <>. These indexes must be
-* constants. Each template parameter is an index to the element you want to 
-* select, where indexes 0 - 3 indicate an element from the first source
-* vector and indexes 4 - 7 indicate an element from the second source vector.
-* A negative index will generate zero.
-*
-*
-* Example:
-* Vec4d a(10., 11., 12., 13.);    // a is (10, 11, 12, 13)
-* Vec4d b(20., 21., 22., 23.);    // a is (20, 21, 22, 23)
-* Vec4d c;
-* c = blend4d<4,3,7,-1> (a,b);    // c is (20, 13, 23,  0)
-*
-* A lot of the code here is metaprogramming aiming to find the instructions
-* that best fit the template parameters and instruction set. The metacode
-* will be reduced out to leave only a few vector instructions in release
-* mode with optimization on.
 *****************************************************************************/
 
 // permute vector Vec4d
 template <int i0, int i1, int i2, int i3>
-static inline Vec4d permute4d(Vec4d const & a) {
-
-    const int ior = i0 | i1 | i2 | i3;  // OR indexes
-
-    // is zeroing needed
-    const bool do_zero    = ior < 0 && (ior & 0x80); // at least one index is negative, and not -0x100
-
-    // is shuffling needed
-    const bool do_shuffle = (i0>0) || (i1!=1 && i1>=0) || (i2!=2 && i2>=0) || (i3!=3 && i3>=0);
-
-    if (!do_shuffle) {       // no shuffling needed
-        if (do_zero) {       // zeroing
-            if ((i0 & i1 & i2 & i3) < 0) {
-                return _mm256_setzero_pd(); // zero everything
-            }
-            // zero some elements
-            __m256d const mask = _mm256_castps_pd (
-                constant8f< -int(i0>=0), -int(i0>=0), -int(i1>=0), -int(i1>=0), -int(i2>=0), -int(i2>=0), -int(i3>=0), -int(i3>=0) > ());
-            return _mm256_and_pd(a, mask);     // zero with AND mask
+static inline Vec4d permute4(Vec4d const a) {
+    int constexpr indexs[4] = { i0, i1, i2, i3 };          // indexes as array
+    __m256d y = a;                                         // result
+    constexpr uint64_t flags = perm_flags<Vec4d>(indexs);
+
+    static_assert((flags & perm_outofrange) == 0, "Index out of range in permute function");
+
+    if constexpr ((flags & perm_allzero) != 0) return _mm256_setzero_pd(); // just return zero
+
+    if constexpr ((flags & perm_largeblock) != 0) {        // permute 128-bit blocks
+        constexpr EList<int, 2> L = largeblock_perm<4>(indexs); // permutation pattern
+        constexpr int j0 = L.a[0];
+        constexpr int j1 = L.a[1];
+#ifndef ZEXT_MISSING
+        if constexpr (j0 == 0 && j1 == -1 && !(flags & perm_addz)) { // zero extend
+            return _mm256_zextpd128_pd256(_mm256_castpd256_pd128(y));
         }
-        else {
-            return a;  // do nothing
+        if constexpr (j0 == 1 && j1 < 0 && !(flags & perm_addz)) {   // extract upper part, zero extend
+            return _mm256_zextpd128_pd256(_mm256_extractf128_pd(y, 1));
         }
-    }
-#if INSTRSET >= 8  // AVX2: use VPERMPD
-    __m256d x = _mm256_permute4x64_pd(a, (i0&3) | (i1&3)<<2 | (i2&3)<<4 | (i3&3)<<6);
-    if (do_zero) {       // zeroing
-        // zero some elements
-        __m256d const mask2 = _mm256_castps_pd (
-            constant8f< -int(i0>=0), -int(i0>=0), -int(i1>=0), -int(i1>=0), -int(i2>=0), -int(i2>=0), -int(i3>=0), -int(i3>=0) > ());
-        x = _mm256_and_pd(x, mask2);     // zero with AND mask
-    }
-    return x;
-#else   // AVX
-
-    // Needed contents of low/high part of each source register in VSHUFPD
-    // 0: a.low, 1: a.high, 3: zero
-    const int s1 = (i0 < 0 ? 3 : (i0 & 2) >> 1) | (i2 < 0 ? 0x30 : (i2 & 2) << 3);
-    const int s2 = (i1 < 0 ? 3 : (i1 & 2) >> 1) | (i3 < 0 ? 0x30 : (i3 & 2) << 3);
-    // permute mask
-    const int sm = (i0 < 0 ? 0 : (i0 & 1)) | (i1 < 0 ? 1 : (i1 & 1)) << 1 | (i2 < 0 ? 0 : (i2 & 1)) << 2 | (i3 < 0 ? 1 : (i3 & 1)) << 3;
-
-    if (s1 == 0x01 || s1 == 0x11 || s2 == 0x01 || s2 == 0x11) {
-        // too expensive to use 256 bit permute, split into two 128 bit permutes
-        Vec2d alo = a.get_low();
-        Vec2d ahi = a.get_high();
-        Vec2d rlo = blend2d<i0, i1> (alo, ahi);
-        Vec2d rhi = blend2d<i2, i3> (alo, ahi);
-        return Vec4d(rlo, rhi);
-    }
-
-    // make operands for VSHUFPD
-    __m256d r1, r2;
-
-    switch (s1) {
-    case 0x00:  // LL
-        r1 = _mm256_insertf128_pd(a,_mm256_castpd256_pd128(a),1);  break;
-    case 0x03:  // LZ
-        r1 = _mm256_insertf128_pd(do_zero ? _mm256_setzero_pd() : __m256d(a), _mm256_castpd256_pd128(a), 1);
-        break;
-    case 0x10:  // LH
-        r1 = a;  break;
-    case 0x13:  // ZH
-        r1 = do_zero ? _mm256_and_pd(a, _mm256_castps_pd(constant8f<0,0,0,0,-1,-1,-1,-1>())) : __m256d(a);  break;
-    case 0x30:  // LZ
-        if (do_zero) {
-            __m128d t  = _mm256_castpd256_pd128(a);
-            t  = _mm_and_pd(t,t);
-            r1 = _mm256_castpd128_pd256(t);  
+#endif
+        if constexpr ((flags & perm_perm) != 0  && !(flags & perm_zeroing)) {
+            return _mm256_permute2f128_pd(y, y, (j0 & 1) | (j1 & 1) << 4);
         }
-        else r1 = a;
-        break;
-    case 0x31:  // HZ
-        r1 = _mm256_castpd128_pd256(_mm256_extractf128_pd(a,1));  break;
-    case 0x33:  // ZZ
-        r1 = do_zero ? _mm256_setzero_pd() : __m256d(a);  break;
-    default:;   // Not needed. Avoid warning in Clang
-    }
-
-    if (s2 == s1) {
-        if (sm == 0x0A) return r1;
-        r2 = r1;
     }
-    else {
-        switch (s2) {
-        case 0x00:  // LL
-            r2 = _mm256_insertf128_pd(a,_mm256_castpd256_pd128(a),1);  break;
-        case 0x03:  // ZL
-            r2 = _mm256_insertf128_pd(do_zero ? _mm256_setzero_pd() : __m256d(a), _mm256_castpd256_pd128(a), 1);
-            break;
-        case 0x10:  // LH
-            r2 = a;  break;
-        case 0x13:  // ZH
-            r2 = do_zero ? _mm256_and_pd(a,_mm256_castps_pd(constant8f<0,0,0,0,-1,-1,-1,-1>())) : __m256d(a);  break;
-        case 0x30:  // LZ
-            if (do_zero) {
-                __m128d t  = _mm256_castpd256_pd128(a);
-                t  = _mm_and_pd(t,t);
-                r2 = _mm256_castpd128_pd256(t);  
+    if constexpr ((flags & perm_perm) != 0) {              // permutation needed
+        if constexpr ((flags & perm_same_pattern) != 0) {  // same pattern in both lanes
+            if constexpr ((flags & perm_punpckh) != 0) {   // fits punpckhi
+                y = _mm256_unpackhi_pd(y, y);
+            }
+            else if constexpr ((flags & perm_punpckl)!=0){ // fits punpcklo
+                y = _mm256_unpacklo_pd(y, y);
+            }
+            else { // general permute
+                constexpr uint8_t mm0 = (i0 & 1) | (i1 & 1) << 1 | (i2 & 1) << 2 | (i3 & 1) << 3;
+                y = _mm256_permute_pd(a, mm0);             // select within same lane
             }
-            else r2 = a;
-            break;
-        case 0x31:  // HZ
-            r2 = _mm256_castpd128_pd256(_mm256_extractf128_pd(a,1));  break;
-        case 0x33:  // ZZ
-            r2 = do_zero ? _mm256_setzero_pd() : __m256d(a);  break;
-        default:;   // Not needed. Avoid warning in Clang
-        }
-    }
-    return  _mm256_shuffle_pd(r1, r2, sm);
-
-#endif  // INSTRSET >= 8
-}
-
-
-// blend vectors Vec4d
-template <int i0, int i1, int i2, int i3>
-static inline Vec4d blend4d(Vec4d const & a, Vec4d const & b) {
-
-    // Combine all the indexes into a single bitfield, with 8 bits for each
-    const int m1 = (i0 & 7) | (i1 & 7) << 8 | (i2 & 7) << 16 | (i3 & 7) << 24; 
-
-    // Mask to zero out negative indexes
-    const uint32_t mz = (i0 < 0 ? 0 : 0xFF) | (i1 < 0 ? 0 : 0xFF) << 8 | (i2 < 0 ? 0 : 0xFF) << 16 | (i3 < 0 ? 0 : 0xFF) << 24;
-
-    if (mz == 0) return _mm256_setzero_pd();  // all zero
-    
-    __m256d t1;
-    if ((((m1 & 0xFEFEFEFE) ^ 0x06020400) & mz) == 0) {
-        // fits VSHUFPD(a,b)
-        t1 = _mm256_shuffle_pd(a, b, (i0 & 1) | (i1 & 1) << 1 | (i2 & 1) << 2 | (i3 & 1) << 3);
-        if (mz == 0xFFFFFFFF) return t1;
-        return permute4d<i0 < 0 ? -1 : 0, i1 < 0 ? -1 : 1, i2 < 0 ? -1 : 2, i3 < 0 ? -1 : 3> (t1);
-    }
-    if ((((m1 & 0xFEFEFEFE) ^0x02060004) & mz) == 0) {
-        // fits VSHUFPD(b,a)
-        t1 = _mm256_shuffle_pd(b, a, (i0 & 1) | (i1 & 1) << 1 | (i2 & 1) << 2 | (i3 & 1) << 3);
-        if (mz == 0xFFFFFFFF) return t1;
-        return permute4d<i0 < 0 ? -1 : 0, i1 < 0 ? -1 : 1, i2 < 0 ? -1 : 2, i3 < 0 ? -1 : 3> (t1);
-    }
-    if ((((m1 & 0x03030303) ^ 0x03020100) & mz) == 0) {
-        // blend and zero, no permute
-        if ((m1 & 0x04040404 & mz) == 0) {
-            t1 = a;
-        }
-        else if (((m1 ^ 0x04040404) & 0x04040404 & mz) == 0) {
-            t1 = b;
         }
-        else {
-            t1 = _mm256_blend_pd(a, b, (i0&4)>>2 | (i1&4)>>1 | (i2&4) | (i3&4) << 1);
+#if INSTRSET >= 8  // AVX2
+        else if constexpr ((flags & perm_broadcast) != 0 && (flags >> perm_rot_count) == 0) {
+            y = _mm256_broadcastsd_pd(_mm256_castpd256_pd128(y)); // broadcast first element
         }
-        if (mz == 0xFFFFFFFF) return t1;
-        return permute4d<i0 < 0 ? -1 : 0, i1 < 0 ? -1 : 1, i2 < 0 ? -1 : 2, i3 < 0 ? -1 : 3> (t1);
-    }
-    if ((m1 & 0x04040404 & mz) == 0) {
-        // all from a
-        return permute4d<i0, i1, i2, i3> (a);
-    }
-    if (((m1 ^ 0x04040404) & 0x04040404 & mz) == 0) {
-        // all from b
-        return permute4d<i0 ^ 4, i1 ^ 4, i2 ^ 4, i3 ^ 4> (b);
-    }
-    // check if we can do 128-bit blend/permute
-    if (((m1 ^ 0x01000100) & 0x01010101 & mz) == 0) {
-        const uint32_t j0 = uint32_t((i0 >= 0 ? i0 : i1 >= 0 ? i1 : -1) >> 1);
-        const uint32_t j1 = uint32_t((i2 >= 0 ? i2 : i3 >= 0 ? i3 : -1) >> 1);
-        if (((m1 ^ ((j0 & 3) * 0x00000202 | (j1 & 3) * 0x02020000)) & 0x06060606 & mz) == 0) {
-            t1 = _mm256_permute2f128_pd(a, b, (j0 & 0x0F) | (j1 & 0x0F) << 4);
-            const bool partialzero = (((i0 | i1) ^ j0) & 0x80) != 0 || (((i2 | i3) ^ j1) & 0x80) != 0;
-            if (partialzero) {
-                // zero some elements
-                __m256d mask = _mm256_castps_pd (constant8f < 
-                    i0 < 0 ? 0 : -1, i0 < 0 ? 0 : -1, i1 < 0 ? 0 : -1, i1 < 0 ? 0 : -1, 
-                    i2 < 0 ? 0 : -1, i2 < 0 ? 0 : -1, i3 < 0 ? 0 : -1, i3 < 0 ? 0 : -1 > ());
-                return _mm256_and_pd(t1, mask);
+#endif
+        else {     // different patterns in two lanes
+#if INSTRSET >= 10 // AVX512VL
+            if constexpr ((flags & perm_rotate_big) != 0) { // fits big rotate
+                constexpr uint8_t rot = uint8_t(flags >> perm_rot_count); // rotation count
+                constexpr uint8_t zm = zero_mask<4>(indexs);
+                return _mm256_castsi256_pd(_mm256_maskz_alignr_epi64 (zm, _mm256_castpd_si256(y), _mm256_castpd_si256(y), rot));
+            }
+#endif
+            if constexpr ((flags & perm_cross_lane) == 0){ // no lane crossing
+                constexpr uint8_t mm0 = (i0 & 1) | (i1 & 1) << 1 | (i2 & 1) << 2 | (i3 & 1) << 3;
+                y = _mm256_permute_pd(a, mm0);             // select within same lane
+            }
+            else {
+#if INSTRSET >= 8  // AVX2
+                // full permute
+                constexpr uint8_t mms = (i0 & 3) | (i1 & 3) << 2 | (i2 & 3) << 4 | (i3 & 3) << 6;
+                y = _mm256_permute4x64_pd(a, mms);
+#else
+                // permute lanes separately
+                __m256d sw = _mm256_permute2f128_pd(a,a,1);// swap the two 128-bit lanes
+                constexpr uint8_t mml = (i0 & 1) | (i1 & 1) << 1 | (i2 & 1) << 2 | (i3 & 1) << 3;
+                __m256d y1 = _mm256_permute_pd(a, mml);    // select from same lane
+                __m256d y2 = _mm256_permute_pd(sw, mml);   // select from opposite lane
+                constexpr uint64_t blendm = make_bit_mask<4, 0x101>(indexs);  // blend mask
+                y = _mm256_blend_pd(y1, y2, uint8_t(blendm));
+#endif
             }
-            else return t1;
         }
     }
-    // general case. combine two permutes
-    Vec4d a1 = permute4d <
-        (uint32_t)i0 < 4 ? i0 : -0x100,
-        (uint32_t)i1 < 4 ? i1 : -0x100,
-        (uint32_t)i2 < 4 ? i2 : -0x100,
-        (uint32_t)i3 < 4 ? i3 : -0x100 > (a);
-    Vec4d b1 = permute4d <
-        (uint32_t)(i0^4) < 4 ? (i0^4) : -0x100,
-        (uint32_t)(i1^4) < 4 ? (i1^4) : -0x100,
-        (uint32_t)(i2^4) < 4 ? (i2^4) : -0x100,
-        (uint32_t)(i3^4) < 4 ? (i3^4) : -0x100 > (b);   
-    t1 = _mm256_blend_pd(a1, b1, (i0&4)>>2 | (i1&4)>>1 | (i2&4) | (i3&4) << 1);
-    if (mz == 0xFFFFFFFF) return t1;
-    return permute4d<i0 < 0 ? -1 : 0, i1 < 0 ? -1 : 1, i2 < 0 ? -1 : 2, i3 < 0 ? -1 : 3> (t1);
+    if constexpr ((flags & perm_zeroing) != 0) {           // additional zeroing needed
+#if INSTRSET >= 10  // use compact mask
+        y = _mm256_maskz_mov_pd(zero_mask<4>(indexs), y);
+#else               // use broad mask
+        constexpr EList <int64_t, 4> bm = zero_mask_broad<Vec4q>(indexs);
+        //y = _mm256_and_pd(_mm256_castsi256_pd( Vec4q().load(bm.a) ), y);  // does not work with INSTRSET = 7
+        __m256i bm1 = _mm256_loadu_si256((const __m256i*)(bm.a));
+        y = _mm256_and_pd(_mm256_castsi256_pd(bm1), y);
+
+#endif
+    }
+    return y;
 }
 
-/*****************************************************************************
-*
-*          Vector Vec8f permute and blend functions
-*
-*****************************************************************************/
 
 // permute vector Vec8f
 template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7>
-static inline Vec8f permute8f(Vec8f const & a) {
-
-    __m256 t1, mask;
+static inline Vec8f permute8(Vec8f const a) {
+    int constexpr indexs[8] = { i0, i1, i2, i3, i4, i5, i6, i7 }; // indexes as array
+    __m256 y = a;                                         // result
+    // get flags for possibilities that fit the permutation pattern
+    constexpr uint64_t flags = perm_flags<Vec8f>(indexs);
 
-    const int ior = i0 | i1 | i2 | i3 | i4 | i5 | i6 | i7;  // OR indexes
+    static_assert((flags & perm_outofrange) == 0, "Index out of range in permute function");
 
-    // is zeroing needed
-    const bool do_zero    = ior < 0 && (ior & 0x80); // at least one index is negative, and not -0x100
+    if constexpr ((flags & perm_allzero) != 0) return _mm256_setzero_ps();  // just return zero
 
-    // is shuffling needed
-    const bool do_shuffle = (i0>0) || (i1!=1 && i1>=0) || (i2!=2 && i2>=0) || (i3!=3 && i3>=0) ||
-        (i4!=4 && i4>=0) || (i5!=5 && i5>=0) || (i6!=6 && i6>=0) || (i7!=7 && i7>=0);
+    if constexpr ((flags & perm_perm) != 0) {              // permutation needed
 
-    if (!do_shuffle) {       // no shuffling needed
-        if (do_zero) {       // zeroing
-            if ((i0 & i1 & i2 & i3 & i4 & i5 & i6 & i7) < 0) {
-                return _mm256_setzero_ps(); // zero everything
+        if constexpr ((flags & perm_largeblock) != 0) {    // use larger permutation
+            constexpr EList<int, 4> L = largeblock_perm<8>(indexs); // permutation pattern
+            y = _mm256_castpd_ps(permute4 <L.a[0], L.a[1], L.a[2], L.a[3]>
+                (Vec4d(_mm256_castps_pd(a))));
+            if (!(flags & perm_addz)) return y;            // no remaining zeroing
+        }
+        else if constexpr ((flags & perm_same_pattern) != 0) {  // same pattern in both lanes
+            if constexpr ((flags & perm_punpckh) != 0) {   // fits punpckhi
+                y = _mm256_unpackhi_ps(y, y);
+            }
+            else if constexpr ((flags & perm_punpckl)!=0){ // fits punpcklo
+                y = _mm256_unpacklo_ps(y, y);
+            }
+            else { // general permute, same pattern in both lanes
+                y = _mm256_shuffle_ps(a, a, uint8_t(flags >> perm_ipattern));
             }
-            // zero some elements
-            mask = constant8f< -int(i0>=0), -int(i1>=0), -int(i2>=0), -int(i3>=0), -int(i4>=0), -int(i5>=0), -int(i6>=0), -int(i7>=0) > ();
-            return _mm256_and_ps(a, mask);     // zero with AND mask
         }
-        else {
-            return a;  // do nothing
+#if INSTRSET >= 10
+        else if constexpr ((flags & perm_broadcast) != 0) {
+            constexpr uint8_t e = flags >> perm_rot_count & 0xF; // broadcast one element
+            if constexpr (e > 0) {
+                y =  _mm256_castsi256_ps(_mm256_alignr_epi32( _mm256_castps_si256(y),  _mm256_castps_si256(y), e));
+            }
+            y = _mm256_broadcastss_ps(_mm256_castps256_ps128(y));
+        }
+#elif INSTRSET >= 8 // AVX2
+        else if constexpr ((flags & perm_broadcast) != 0 && (flags >> perm_rot_count == 0)) {
+            y = _mm256_broadcastss_ps(_mm256_castps256_ps128(y)); // broadcast first element
         }
-    }
-
-#if INSTRSET >= 8  // AVX2: use VPERMPS
-    if (do_shuffle) {    // shuffling
-        mask = constant8f< i0 & 7, i1 & 7, i2 & 7, i3 & 7, i4 & 7, i5 & 7, i6 & 7, i7 & 7 > ();
-#if defined (_MSC_VER) && _MSC_VER < 1700 && ! defined(__INTEL_COMPILER)
-        // bug in MS VS 11 beta: operands in wrong order. fixed in 11.0
-        t1 = _mm256_permutevar8x32_ps(mask, _mm256_castps_si256(a));      //  problem in immintrin.h
-#elif defined (GCC_VERSION) && GCC_VERSION <= 40700 && !defined(__INTEL_COMPILER) && !defined(__clang__)
-        // Gcc 4.7.0 has wrong parameter type and operands in wrong order. fixed in version 4.7.1
-        t1 = _mm256_permutevar8x32_ps(mask, a);
-#else   // no bug version
-        t1 = _mm256_permutevar8x32_ps(a, _mm256_castps_si256(mask));
 #endif
-    }
-    else {
-        t1 = a;          // no shuffling
-    }
-    if (do_zero) {       // zeroing
-        if ((i0 & i1 & i2 & i3 & i4 & i5 & i6 & i7) < 0) {
-            return _mm256_setzero_ps(); // zero everything
+#if INSTRSET >= 8  // avx2
+        else if constexpr ((flags & perm_zext) != 0) {  // zero extension
+            y = _mm256_castsi256_ps(_mm256_cvtepu32_epi64(_mm256_castsi256_si128(_mm256_castps_si256(y))));  // zero extension
+            if constexpr ((flags & perm_addz2) == 0) return y;
+        }
+#endif
+#if INSTRSET >= 10  // AVX512VL
+        else if constexpr ((flags & perm_compress) != 0) {
+            y = _mm256_maskz_compress_ps(__mmask8(compress_mask(indexs)), y); // compress
+            if constexpr ((flags & perm_addz2) == 0) return y;
         }
-        // zero some elements
-        mask = constant8f< -int(i0>=0), -int(i1>=0), -int(i2>=0), -int(i3>=0), -int(i4>=0), -int(i5>=0), -int(i6>=0), -int(i7>=0) > ();
-        t1 = _mm256_and_ps(t1, mask);     // zero with AND mask
-    }
-    return t1;
-#else   // AVX
-
-    // Combine all the indexes into a single bitfield, with 4 bits for each
-    const int m1 = (i0&7) | (i1&7)<<4 | (i2&7)<<8 | (i3&7)<<12 | (i4&7)<<16 | (i5&7)<<20 | (i6&7)<<24 | (i7&7)<<28;
-
-    // Mask to zero out negative indexes
-    const int m2 = (i0<0?0:0xF) | (i1<0?0:0xF)<<4 | (i2<0?0:0xF)<<8 | (i3<0?0:0xF)<<12 | (i4<0?0:0xF)<<16 | (i5<0?0:0xF)<<20 | (i6<0?0:0xF)<<24 | (i7<0?0:0xF)<<28;
-
-    // Check if it is possible to use VSHUFPS. Index n must match index n+4 on bit 0-1, and even index n must match odd index n+1 on bit 2
-    const bool sps = ((m1 ^ (m1 >> 16)) & 0x3333 & m2 & (m2 >> 16)) == 0  &&  ((m1 ^ (m1 >> 4)) & 0x04040404 & m2 & m2 >> 4) == 0;
-
-    if (sps) {   // can use VSHUFPS
-
-        // Index of each pair (i[n],i[n+1])
-        const int j0 = i0 >= 0 ? i0 : i1;
-        const int j1 = i2 >= 0 ? i2 : i3;
-        const int j2 = i4 >= 0 ? i4 : i5;
-        const int j3 = i6 >= 0 ? i6 : i7;
-
-        // Index of each pair (i[n],i[n+4])
-        const int k0 = i0 >= 0 ? i0 : i4;
-        const int k1 = i1 >= 0 ? i1 : i5;
-        const int k2 = i2 >= 0 ? i2 : i6;
-        const int k3 = i3 >= 0 ? i3 : i7;
-
-        // Needed contents of low/high part of each source register in VSHUFPS
-        // 0: a.low, 1: a.high, 3: zero or don't care
-        const int s1 = (j0 < 0 ? 3 : (j0 & 4) >> 2) | (j2 < 0 ? 0x30 : (j2 & 4) << 2);
-        const int s2 = (j1 < 0 ? 3 : (j1 & 4) >> 2) | (j3 < 0 ? 0x30 : (j3 & 4) << 2);
-
-        // calculate cost of using VSHUFPS
-        const int cost1 = (s1 == 0x01 || s1 == 0x11) ? 2 : (s1 == 0x00 || s1 == 0x03 || s1 == 0x31) ? 1 : 0;
-        const int cost2 = (s2 == s1) ? 0 : (s2 == 0x01 || s2 == 0x11) ? 2 : (s2 == 0x00 || (s2 == 0x03 && (s1 & 0xF0) != 0x00) || (s2 == 0x31 && (s1 & 0x0F) != 0x01)) ? 1 : 0;
-
-        if (cost1 + cost2 <= 3) {
-
-            // permute mask
-            const int sm = (k0 < 0 ? 0 : (k0 & 3)) | (k1 < 0 ? 1 : (k1 & 3)) << 2 | (k2 < 0 ? 2 : (k2 & 3)) << 4 | (k3 < 0 ? 3 : (k3 & 3)) << 6;
-
-            // make operands for VSHUFPS
-            __m256 r1, r2;
-
-            switch (s1) {
-            case 0x00:  // LL
-            case 0x03:  // ZL
-                r1 = _mm256_insertf128_ps(a,_mm256_castps256_ps128(a),1);  break;
-            case 0x01:  // HL
-                r1 = _mm256_castps128_ps256(_mm256_extractf128_ps(a,1));
-                r1 = _mm256_insertf128_ps(r1,_mm256_castps256_ps128(a),1);  break;
-            case 0x10:  // LH
-            case 0x13:  // ZH
-            case 0x30:  // LZ
-            case 0x33:  // ZZ
-                r1 = a;  break;
-            case 0x11:  // HH
-                r1 = _mm256_castps128_ps256(_mm256_extractf128_ps(a,1));
-                r1 = _mm256_insertf128_ps(r1,_mm256_castps256_ps128(r1),1);  break;
-            case 0x31:  // HZ
-                r1 = _mm256_castps128_ps256(_mm256_extractf128_ps(a,1));  break;
+        else if constexpr ((flags & perm_expand) != 0) {
+            y = _mm256_maskz_expand_ps(__mmask8(expand_mask(indexs)), y); // expand
+            if constexpr ((flags & perm_addz2) == 0) return y;
+        }
+#endif
+        else {  // different patterns in two lanes
+#if INSTRSET >= 10  // AVX512VL
+            if constexpr ((flags & perm_rotate_big) != 0) { // fits big rotate
+                constexpr uint8_t rot = uint8_t(flags >> perm_rot_count); // rotation count
+                y = _mm256_castsi256_ps(_mm256_alignr_epi32(_mm256_castps_si256(y), _mm256_castps_si256(y), rot));
             }
-
-            if (s2 == s1) {
-                if (sm == 0xE4) return r1;
-                r2 = r1;
+            else
+#endif
+            if constexpr ((flags & perm_cross_lane) == 0) {  // no lane crossing. Use vpermilps
+                __m256 m = constant8f<i0 & 3, i1 & 3, i2 & 3, i3 & 3, i4 & 3, i5 & 3, i6 & 3, i7 & 3>();
+                y = _mm256_permutevar_ps(a, _mm256_castps_si256(m));
             }
             else {
-                switch (s2) {
-                case 0x00:  // LL
-                    r2 = _mm256_insertf128_ps(a,_mm256_castps256_ps128(a),1);  break;
-                case 0x03:  // ZL
-                    if ((s1 & 0xF0) == 0x00) r2 = r1;
-                    else {
-                        r2 = _mm256_insertf128_ps(a,_mm256_castps256_ps128(a),1);
-                    }
-                    break;
-                case 0x01:  // HL
-                    r2 = _mm256_castps128_ps256(_mm256_extractf128_ps(a,1));
-                    r2 = _mm256_insertf128_ps(r2,_mm256_castps256_ps128(a),1);  break;
-                case 0x10:  // LH
-                case 0x13:  // ZH
-                case 0x30:  // LZ
-                case 0x33:  // ZZ
-                    r2 = a;  break;
-                case 0x11:  // HH
-                    r2 = _mm256_castps128_ps256(_mm256_extractf128_ps(a,1));
-                    r2 = _mm256_insertf128_ps(r2,_mm256_castps256_ps128(r2),1);  break;
-                case 0x31:  // HZ
-                    if ((s1 & 0x0F) == 0x01) r2 = r1;
-                    else {
-                        r2 = _mm256_castps128_ps256(_mm256_extractf128_ps(a,1));
-                    }
-                    break;
-                }
-            }
-
-            // now the permute instruction
-            t1 = _mm256_shuffle_ps(r1, r2, sm);
-
-            if (do_zero) {
-                // zero some elements
-                mask = constant8f< -int(i0>=0), -int(i1>=0), -int(i2>=0), -int(i3>=0), -int(i4>=0), -int(i5>=0), -int(i6>=0), -int(i7>=0) > ();
-                t1 = _mm256_and_ps(t1, mask);     // zero with AND mask
+                // full permute needed
+                __m256i permmask = _mm256_castps_si256(
+                    constant8f <i0 & 7, i1 & 7, i2 & 7, i3 & 7, i4 & 7, i5 & 7, i6 & 7, i7 & 7 >());
+#if INSTRSET >= 8  // AVX2
+                y = _mm256_permutevar8x32_ps(a, permmask);
+#else
+                // permute lanes separately
+                __m256 sw = _mm256_permute2f128_ps(a, a, 1);  // swap the two 128-bit lanes
+                __m256 y1 = _mm256_permutevar_ps(a,  permmask);   // select from same lane
+                __m256 y2 = _mm256_permutevar_ps(sw, permmask);   // select from opposite lane
+                constexpr uint64_t blendm = make_bit_mask<8, 0x102>(indexs);  // blend mask
+                y = _mm256_blend_ps(y1, y2, uint8_t(blendm));
+#endif
             }
-            return t1;
         }
     }
-    // not using VSHUFPS. Split into low and high part
-    Vec4f alo = a.get_low();
-    Vec4f ahi = a.get_high();
-    Vec4f rlo = blend4f<i0, i1, i2, i3> (alo, ahi);
-    Vec4f rhi = blend4f<i4, i5, i6, i7> (alo, ahi);
-    return Vec8f(rlo, rhi);
+    if constexpr ((flags & perm_zeroing) != 0) {
+        // additional zeroing needed
+#if INSTRSET >= 10  // use compact mask
+        y = _mm256_maskz_mov_ps(zero_mask<8>(indexs), y);
+#else  // use broad mask
+        constexpr EList <int32_t, 8> bm = zero_mask_broad<Vec8i>(indexs);
+        __m256i bm1 = _mm256_loadu_si256((const __m256i*)(bm.a));
+        y = _mm256_and_ps(_mm256_castsi256_ps(bm1), y);
 #endif
+    }
+    return y;
 }
 
 
-// blend vectors Vec8f
-template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7>
-static inline Vec8f blend8f(Vec8f const & a, Vec8f const & b) {
+// blend vectors Vec4d
+template <int i0, int i1, int i2, int i3>
+static inline Vec4d blend4(Vec4d const a, Vec4d const b) {
+    int constexpr indexs[4] = { i0, i1, i2, i3 };          // indexes as array
+    __m256d y = a;                                         // result
+    constexpr uint64_t flags = blend_flags<Vec4d>(indexs); // get flags for possibilities that fit the index pattern
 
-    const int ior = i0 | i1 | i2 | i3 | i4 | i5 | i6 | i7;  // OR indexes
+    static_assert((flags & blend_outofrange) == 0, "Index out of range in blend function");
 
-    // is zeroing needed
-    const bool do_zero  = ior < 0 && (ior & 0x80); // at least one index is negative, and not -0x100
+    if constexpr ((flags & blend_allzero) != 0) return _mm256_setzero_pd();  // just return zero
 
-    // Combine all the indexes into a single bitfield, with 4 bits for each
-    const int m1 = (i0&0xF) | (i1&0xF)<<4 | (i2&0xF)<<8 | (i3&0xF)<<12 | (i4&0xF)<<16 | (i5&0xF)<<20 | (i6&0xF)<<24 | (i7&0xF)<<28;
+    if constexpr ((flags & blend_b) == 0) {                // nothing from b. just permute a
+        return permute4 <i0, i1, i2, i3> (a);
+    }
+    if constexpr ((flags & blend_a) == 0) {                // nothing from a. just permute b
+        return permute4 <i0<0?i0:i0&3, i1<0?i1:i1&3, i2<0?i2:i2&3, i3<0?i3:i3&3> (b);
+    }
+    if constexpr ((flags & (blend_perma | blend_permb)) == 0) { // no permutation, only blending
+        constexpr uint8_t mb = (uint8_t)make_bit_mask<4, 0x302>(indexs);  // blend mask
+#if INSTRSET >= 10 // AVX512VL
+        y = _mm256_mask_mov_pd (a, mb, b);
+#else  // AVX
+        y = _mm256_blend_pd(a, b, mb); // duplicate each bit
+#endif
+    }
+    else if constexpr ((flags & blend_largeblock) != 0) {  // blend and permute 128-bit blocks
+        constexpr EList<int, 2> L = largeblock_perm<4>(indexs); // get 128-bit blend pattern
+        constexpr uint8_t pp = (L.a[0] & 0xF) | uint8_t(L.a[1] & 0xF) << 4;
+        y = _mm256_permute2f128_pd(a, b, pp);
+    }
+    // check if pattern fits special cases
+    else if constexpr ((flags & blend_punpcklab) != 0) {
+        y = _mm256_unpacklo_pd (a, b);
+    }
+    else if constexpr ((flags & blend_punpcklba) != 0) {
+        y = _mm256_unpacklo_pd (b, a);
+    }
+    else if constexpr ((flags & blend_punpckhab) != 0) {
+        y = _mm256_unpackhi_pd(a, b);
+    }
+    else if constexpr ((flags & blend_punpckhba) != 0) {
+        y = _mm256_unpackhi_pd(b, a);
+    }
+    else if constexpr ((flags & blend_shufab) != 0) {
+        y = _mm256_shuffle_pd(a, b, (flags >> blend_shufpattern) & 0xF);
+    }
+    else if constexpr ((flags & blend_shufba) != 0) {
+        y = _mm256_shuffle_pd(b, a, (flags >> blend_shufpattern) & 0xF);
+    }
+    else { // No special cases
+#if INSTRSET >= 10  // AVX512VL. use vpermi2pd
+        __m256i const maskp = constant8ui<i0 & 7, 0, i1 & 7, 0, i2 & 7, 0, i3 & 7, 0>();
+        return _mm256_maskz_permutex2var_pd (zero_mask<4>(indexs), a, maskp, b);
+#else   // permute a and b separately, then blend.
+        constexpr EList<int, 8> L = blend_perm_indexes<4, 0>(indexs); // get permutation indexes
+        __m256d ya = permute4<L.a[0], L.a[1], L.a[2], L.a[3]>(a);
+        __m256d yb = permute4<L.a[4], L.a[5], L.a[6], L.a[7]>(b);
+        constexpr uint8_t mb = (uint8_t)make_bit_mask<4, 0x302>(indexs);  // blend mask
+        y = _mm256_blend_pd(ya, yb, mb);
+#endif
+    }
+    if constexpr ((flags & blend_zeroing) != 0) {          // additional zeroing needed
+#if INSTRSET >= 10  // use compact mask
+        y = _mm256_maskz_mov_pd(zero_mask<4>(indexs), y);
+#else  // use broad mask
+        constexpr EList <int64_t, 4> bm = zero_mask_broad<Vec4q>(indexs);
+        __m256i bm1 = _mm256_loadu_si256((const __m256i*)(bm.a));
+        y = _mm256_and_pd(_mm256_castsi256_pd(bm1), y);
+#endif
+    }
+    return y;
+}
 
-    // Mask to zero out negative indexes
-    const int mz = (i0<0?0:0xF) | (i1<0?0:0xF)<<4 | (i2<0?0:0xF)<<8 | (i3<0?0:0xF)<<12 | (i4<0?0:0xF)<<16 | (i5<0?0:0xF)<<20 | (i6<0?0:0xF)<<24 | (i7<0?0:0xF)<<28;
 
-    __m256 t1, mask;
+// blend vectors Vec8f
+template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7>
+static inline Vec8f blend8(Vec8f const a, Vec8f const b) {
+    int constexpr indexs[8] = { i0, i1, i2, i3, i4, i5, i6, i7 }; // indexes as array
+    __m256 y = a;                                          // result
+    constexpr uint64_t flags = blend_flags<Vec8f>(indexs); // get flags for possibilities that fit the index pattern
 
-    if (mz == 0) return _mm256_setzero_ps();  // all zero
+    static_assert((flags & blend_outofrange) == 0, "Index out of range in blend function");
 
-    if ((m1 & 0x88888888 & mz) == 0) {
-        // all from a
-        return permute8f<i0, i1, i2, i3, i4, i5, i6, i7> (a);
-    }
+    if constexpr ((flags & blend_allzero) != 0) return _mm256_setzero_ps();  // just return zero
 
-    if (((m1 ^ 0x88888888) & 0x88888888 & mz) == 0) {
-        // all from b
-        return permute8f<i0&~8, i1&~8, i2&~8, i3&~8, i4&~8, i5&~8, i6&~8, i7&~8> (b);
+    if constexpr ((flags & blend_largeblock) != 0) {       // blend and permute 32-bit blocks
+        constexpr EList<int, 4> L = largeblock_perm<8>(indexs); // get 32-bit blend pattern
+        y = _mm256_castpd_ps(blend4 <L.a[0], L.a[1], L.a[2], L.a[3]>
+            (Vec4d(_mm256_castps_pd(a)), Vec4d(_mm256_castps_pd(b))));
+        if (!(flags & blend_addz)) return y;               // no remaining zeroing
     }
-
-    if ((((m1 & 0x77777777) ^ 0x76543210) & mz) == 0) {
-        // blend and zero, no permute
-        mask = constant8f<(i0&8)?0:-1, (i1&8)?0:-1, (i2&8)?0:-1, (i3&8)?0:-1, (i4&8)?0:-1, (i5&8)?0:-1, (i6&8)?0:-1, (i7&8)?0:-1> ();
-        t1   = select(mask, a, b);
-        if (!do_zero) return t1;
-        // zero some elements
-        mask = constant8f< (i0<0&&(i0&8)) ? 0 : -1, (i1<0&&(i1&8)) ? 0 : -1, (i2<0&&(i2&8)) ? 0 : -1, (i3<0&&(i3&8)) ? 0 : -1, 
-            (i4<0&&(i4&8)) ? 0 : -1, (i5<0&&(i5&8)) ? 0 : -1, (i6<0&&(i6&8)) ? 0 : -1, (i7<0&&(i7&8)) ? 0 : -1 > ();
-        return _mm256_and_ps(t1, mask);
+    else if constexpr ((flags & blend_b) == 0) {           // nothing from b. just permute a
+        return permute8 <i0, i1, i2, i3, i4, i5, i6, i7> (a);
     }
-
-    // check if we can do 128-bit blend/permute
-    if (((m1 ^ 0x32103210) & 0x33333333 & mz) == 0) {
-        const uint32_t j0 = (i0 >= 0 ? i0 : i1 >= 0 ? i1 : i2 >= 0 ? i2 : i3 >= 0 ? i3 : -1) >> 2;
-        const uint32_t j1 = (i4 >= 0 ? i4 : i5 >= 0 ? i5 : i6 >= 0 ? i6 : i7 >= 0 ? i7 : -1) >> 2;
-        if (((m1 ^ ((j0 & 3) * 0x00004444 | (j1 & 3) * 0x44440000)) & 0xCCCCCCCC & mz) == 0) {
-            t1 = _mm256_permute2f128_ps(a, b, (j0 & 0x0F) | (j1 & 0x0F) << 4);
-            const bool partialzero = (((i0 | i1 | i2 | i3) ^ j0) & 0x80) != 0 || (((i4 | i5 | i6 | i7) ^ j1) & 0x80) != 0;
-            if (partialzero) {
-                // zero some elements
-                mask = constant8f< i0 < 0 ? 0 : -1, i1 < 0 ? 0 : -1, i2 < 0 ? 0 : -1, i3 < 0 ? 0 : -1, 
-                    i4 < 0 ? 0 : -1, i5 < 0 ? 0 : -1, i6 < 0 ? 0 : -1, i7 < 0 ? 0 : -1 > ();
-                return _mm256_and_ps(t1, mask);
-            }
-            else return t1;
-        }
+    else if constexpr ((flags & blend_a) == 0) {           // nothing from a. just permute b
+        constexpr EList<int, 16> L = blend_perm_indexes<8, 2>(indexs); // get permutation indexes
+        return permute8 < L.a[8], L.a[9], L.a[10], L.a[11], L.a[12], L.a[13], L.a[14], L.a[15] > (b);
     }
-    // Not checking special cases for vunpckhps, vunpcklps: they are too rare
-
-    // Check if it is possible to use VSHUFPS. 
-    // Index n must match index n+4 on bit 0-1, and even index n must match odd index n+1 on bit 2-3
-    const bool sps = ((m1 ^ (m1 >> 16)) & 0x3333 & mz & (mz >> 16)) == 0  &&  ((m1 ^ (m1 >> 4)) & 0x0C0C0C0C & mz & mz >> 4) == 0;
-
-    if (sps) {   // can use VSHUFPS
-
-        // Index of each pair (i[n],i[n+1])
-        const int j0 = i0 >= 0 ? i0 : i1;
-        const int j1 = i2 >= 0 ? i2 : i3;
-        const int j2 = i4 >= 0 ? i4 : i5;
-        const int j3 = i6 >= 0 ? i6 : i7;
-
-        // Index of each pair (i[n],i[n+4])
-        const int k0 = i0 >= 0 ? i0 : i4;
-        const int k1 = i1 >= 0 ? i1 : i5;
-        const int k2 = i2 >= 0 ? i2 : i6;
-        const int k3 = i3 >= 0 ? i3 : i7;
-
-        // Needed contents of low/high part of each source register in VSHUFPS
-        // 0: a.low, 1: a.high, 2: b.low, 3: b.high, 4: zero or don't care
-        const int s1 = (j0 < 0 ? 4 : (j0 & 0xC) >> 2) | (j2 < 0 ? 0x30 : (j2 & 0xC) << 2);
-        const int s2 = (j1 < 0 ? 3 : (j1 & 0xC) >> 2) | (j3 < 0 ? 0x30 : (j3 & 0xC) << 2);
-
-        // permute mask
-        const int sm = (k0 < 0 ? 0 : (k0 & 3)) | (k1 < 0 ? 1 : (k1 & 3)) << 2 | (k2 < 0 ? 2 : (k2 & 3)) << 4 | (k3 < 0 ? 3 : (k3 & 3)) << 6;
-
-        __m256 r1, r2;
-        __m128 ahi = _mm256_extractf128_ps(a,1);    // 1
-        __m128 bhi = _mm256_extractf128_ps(b,1);    // 3
-
-        switch (s1) {
-        case 0x00:  case 0x04:
-            r1 = _mm256_insertf128_ps(a,_mm256_castps256_ps128(a),1);  break;
-        case 0x01:  case 0x41:
-            r1 = _mm256_insertf128_ps(_mm256_castps128_ps256(ahi),_mm256_castps256_ps128(a),1);  break;
-        case 0x02:
-            r1 = _mm256_insertf128_ps(b,_mm256_castps256_ps128(a),1);  break;
-        case 0x03:
-            r1 = _mm256_insertf128_ps(_mm256_castps128_ps256(bhi),_mm256_castps256_ps128(a),1);  break;
-        case 0x10:  case 0x14:  case 0x40:  case 0x44:
-            r1 = a;  break;
-        case 0x11:
-            r1 = _mm256_insertf128_ps(_mm256_castps128_ps256(ahi),ahi,1);  break;
-        case 0x12:
-            r1 = _mm256_insertf128_ps(b,ahi,1);  break;
-        case 0x13:
-            r1 = _mm256_insertf128_ps(_mm256_castps128_ps256(bhi),ahi,1);  break;
-        case 0x20:
-            r1 = _mm256_insertf128_ps(a,_mm256_castps256_ps128(b),1);  break;
-        case 0x21:
-            r1 = _mm256_insertf128_ps(_mm256_castps128_ps256(ahi),_mm256_castps256_ps128(b),1);  break;
-        case 0x22:  case 0x24:  case 0x42:
-            r1 = _mm256_insertf128_ps(b,_mm256_castps256_ps128(b),1);  break;
-        case 0x23:  case 0x43:
-            r1 = _mm256_insertf128_ps(_mm256_castps128_ps256(bhi),_mm256_castps256_ps128(b),1);  break;
-        case 0x30:
-            r1 = _mm256_insertf128_ps(a,bhi,1);  break;
-        case 0x31:
-            r1 = _mm256_insertf128_ps(_mm256_castps128_ps256(ahi),bhi,1);  break;
-        case 0x32:  case 0x34:
-            r1 = b;  break;
-        case 0x33:
-            r1 = _mm256_insertf128_ps(_mm256_castps128_ps256(bhi),bhi,1);  break;
-        }
-        if (s2 == s1 || ((s2 & 0x04) && ((s1 ^ s2) & 0xF0) == 0) || ((s2 & 0x40) && ((s1 ^ s2) & 0x0F) == 0)) {
-            // can use r2 = r1
-            if (sm == 0xE4) return r1;  // no shuffling needed
-            r2 = r1;
-        }
-        else {
-            switch (s2) {
-            case 0x00:  case 0x04:
-                r2 = _mm256_insertf128_ps(a,_mm256_castps256_ps128(a),1);  break;
-            case 0x01:  case 0x41:
-                r2 = _mm256_insertf128_ps(_mm256_castps128_ps256(ahi),_mm256_castps256_ps128(a),1);  break;
-            case 0x02:
-                r2 = _mm256_insertf128_ps(b,_mm256_castps256_ps128(a),1);  break;
-            case 0x03:
-                r2 = _mm256_insertf128_ps(_mm256_castps128_ps256(bhi),_mm256_castps256_ps128(a),1);  break;
-            case 0x10:  case 0x14:  case 0x40:  case 0x44:
-                r2 = a;  break;
-            case 0x11:
-                r2 = _mm256_insertf128_ps(_mm256_castps128_ps256(ahi),ahi,1);  break;
-            case 0x12:
-                r2 = _mm256_insertf128_ps(b,ahi,1);  break;
-            case 0x13:
-                r2 = _mm256_insertf128_ps(_mm256_castps128_ps256(bhi),ahi,1);  break;
-            case 0x20:
-                r2 = _mm256_insertf128_ps(a,_mm256_castps256_ps128(b),1);  break;
-            case 0x21:
-                r2 = _mm256_insertf128_ps(_mm256_castps128_ps256(ahi),_mm256_castps256_ps128(b),1);  break;
-            case 0x22:  case 0x24:  case 0x42:
-                r2 = _mm256_insertf128_ps(b,_mm256_castps256_ps128(b),1);  break;
-            case 0x23:  case 0x43:
-                r2 = _mm256_insertf128_ps(_mm256_castps128_ps256(bhi),_mm256_castps256_ps128(b),1);  break;
-            case 0x30:
-                r2 = _mm256_insertf128_ps(a,bhi,1);  break;
-            case 0x31:
-                r2 = _mm256_insertf128_ps(_mm256_castps128_ps256(ahi),bhi,1);  break;
-            case 0x32:  case 0x34:
-                r2 = b;  break;
-            case 0x33:
-                r2 = _mm256_insertf128_ps(_mm256_castps128_ps256(bhi),bhi,1);  break;
-            }
-        }
-
-        // now the shuffle instruction
-        t1 = _mm256_shuffle_ps(r1, r2, sm);
-
-        if (do_zero) {
-            // zero some elements
-            mask = constant8f< -int(i0>=0), -int(i1>=0), -int(i2>=0), -int(i3>=0), -int(i4>=0), -int(i5>=0), -int(i6>=0), -int(i7>=0) > ();
-            t1 = _mm256_and_ps(t1, mask);     // zero with AND mask
-        }
-        return t1;
-    }
-
-    // Check if we can use 64-bit blend. Even numbered indexes must be even and odd numbered
-    // indexes must be equal to the preceding index + 1, except for negative indexes.
-    if (((m1 ^ 0x10101010) & 0x11111111 & mz) == 0 && ((m1 ^ m1 >> 4) & 0x0E0E0E0E & mz & mz >> 4) == 0) {
-
-        const bool partialzero = int((i0 ^ i1) | (i2 ^ i3) | (i4 ^ i5) | (i6 ^ i7)) < 0; // part of a 64-bit block is zeroed
-        const int blank1 = partialzero ? -0x100 : -1;  // ignore or zero
-        const int n0 = i0 > 0 ? i0/2 : i1 > 0 ? i1/2 : blank1;  // indexes for 64 bit blend
-        const int n1 = i2 > 0 ? i2/2 : i3 > 0 ? i3/2 : blank1;
-        const int n2 = i4 > 0 ? i4/2 : i5 > 0 ? i5/2 : blank1;
-        const int n3 = i6 > 0 ? i6/2 : i7 > 0 ? i7/2 : blank1;
-        t1 = _mm256_castpd_ps (blend4d<n0,n1,n2,n3> (_mm256_castps_pd(a), _mm256_castps_pd(b)));
-        if (blank1 == -1 || !do_zero) {    
-            return  t1;
-        }
-        // need more zeroing
-        mask = constant8f< -int(i0>=0), -int(i1>=0), -int(i2>=0), -int(i3>=0), -int(i4>=0), -int(i5>=0), -int(i6>=0), -int(i7>=0) > ();
-        return _mm256_and_ps(t1, mask);     // zero with AND mask
-    }
-
-    // general case: permute and blend and possible zero
-    const int blank2 = do_zero ? -1 : -0x100;  // ignore or zero
-
-    Vec8f ta = permute8f <
-        (uint32_t)i0 < 8 ? i0 : blank2,
-        (uint32_t)i1 < 8 ? i1 : blank2,
-        (uint32_t)i2 < 8 ? i2 : blank2,
-        (uint32_t)i3 < 8 ? i3 : blank2,
-        (uint32_t)i4 < 8 ? i4 : blank2,
-        (uint32_t)i5 < 8 ? i5 : blank2,
-        (uint32_t)i6 < 8 ? i6 : blank2,
-        (uint32_t)i7 < 8 ? i7 : blank2 > (a);
-    Vec8f tb = permute8f <
-        (uint32_t)(i0^8) < 8 ? (i0^8) : blank2,
-        (uint32_t)(i1^8) < 8 ? (i1^8) : blank2,
-        (uint32_t)(i2^8) < 8 ? (i2^8) : blank2,
-        (uint32_t)(i3^8) < 8 ? (i3^8) : blank2,
-        (uint32_t)(i4^8) < 8 ? (i4^8) : blank2,
-        (uint32_t)(i5^8) < 8 ? (i5^8) : blank2,
-        (uint32_t)(i6^8) < 8 ? (i6^8) : blank2,
-        (uint32_t)(i7^8) < 8 ? (i7^8) : blank2 > (b);
-
-    if (blank2 == -1) {    
-        return  _mm256_or_ps(ta, tb); 
-    }
-    // no zeroing, need to blend
-    const int maskb = ((i0 >> 3) & 1) | ((i1 >> 2) & 2) | ((i2 >> 1) & 4) | (i3 & 8) | 
-        ((i4 << 1) & 0x10) | ((i5 << 2) & 0x20) | ((i6 << 3) & 0x40) | ((i7 << 4) & 0x80);
-    return _mm256_blend_ps(ta, tb, maskb);  // blend
+    else if constexpr ((flags & (blend_perma | blend_permb)) == 0) { // no permutation, only blending
+        constexpr uint8_t mb = (uint8_t)make_bit_mask<8, 0x303>(indexs);  // blend mask
+#if INSTRSET >= 10 // AVX512VL
+        y = _mm256_mask_mov_ps(a, mb, b);
+#else  // AVX2
+        y = _mm256_blend_ps(a, b, mb);
+#endif
+    }
+    // check if pattern fits special cases
+    else if constexpr ((flags & blend_punpcklab) != 0) {
+        y = _mm256_unpacklo_ps(a, b);
+    }
+    else if constexpr ((flags & blend_punpcklba) != 0) {
+        y = _mm256_unpacklo_ps(b, a);
+    }
+    else if constexpr ((flags & blend_punpckhab) != 0) {
+        y = _mm256_unpackhi_ps(a, b);
+    }
+    else if constexpr ((flags & blend_punpckhba) != 0) {
+        y = _mm256_unpackhi_ps(b, a);
+    }
+    else if constexpr ((flags & blend_shufab) != 0) {      // use floating point instruction shufpd
+        y = _mm256_shuffle_ps(a, b, uint8_t(flags >> blend_shufpattern));
+    }
+    else if constexpr ((flags & blend_shufba) != 0) {      // use floating point instruction shufpd
+        y = _mm256_shuffle_ps(b, a, uint8_t(flags >> blend_shufpattern));
+    }
+    else { // No special cases
+#if INSTRSET >= 10  // AVX512VL. use vpermi2d
+        __m256i const maskp = constant8ui<i0 & 15, i1 & 15, i2 & 15, i3 & 15, i4 & 15, i5 & 15, i6 & 15, i7 & 15> ();
+        return _mm256_maskz_permutex2var_ps(zero_mask<8>(indexs), a, maskp, b);
+#else   // permute a and b separately, then blend.
+        constexpr EList<int, 16> L = blend_perm_indexes<8, 0>(indexs); // get permutation indexes
+        __m256 ya = permute8<L.a[0], L.a[1], L.a[2],  L.a[3],  L.a[4],  L.a[5],  L.a[6],  L.a[7] >(a);
+        __m256 yb = permute8<L.a[8], L.a[9], L.a[10], L.a[11], L.a[12], L.a[13], L.a[14], L.a[15]>(b);
+        constexpr uint8_t mb = (uint8_t)make_bit_mask<8, 0x303>(indexs);  // blend mask
+        y = _mm256_blend_ps(ya, yb, mb);
+#endif
+    }
+    if constexpr ((flags & blend_zeroing) != 0) {          // additional zeroing needed
+#if INSTRSET >= 10  // use compact mask
+        y = _mm256_maskz_mov_ps(zero_mask<8>(indexs), y);
+#else  // use broad mask
+        constexpr EList <int32_t, 8> bm = zero_mask_broad<Vec8i>(indexs);
+        __m256i bm1 = _mm256_loadu_si256((const __m256i*)(bm.a));
+        y = _mm256_and_ps(_mm256_castsi256_ps(bm1), y);
+#endif
+    }
+    return y;
 }
 
 
@@ -3013,48 +2805,20 @@ static inline Vec8f blend8f(Vec8f const & a, Vec8f const & b) {
 * These functions use vector elements as indexes into a table.
 * The table is given as one or more vectors or as an array.
 *
-* This can be used for several purposes:
-*  - table lookup
-*  - permute or blend with variable indexes
-*  - blend from more than two sources
-*  - gather non-contiguous data
-*
-* An index out of range may produce any value - the actual value produced is
-* implementation dependent and may be different for different instruction
-* sets. An index out of range does not produce an error message or exception.
-*
-* Example:
-* Vec4i a(2,0,0,3);               // index  a is (  2,   0,   0,   3)
-* Vec4f b(1.0f,1.1f,1.2f,1.3f);   // table  b is (1.0, 1.1, 1.2, 1.3)
-* Vec4f c;
-* c = lookup4 (a,b);              // result c is (1.2, 1.0, 1.0, 1.3)
-*
 *****************************************************************************/
 
-#ifdef VECTORI256_H  // Vec8i and Vec4q must be defined
-
-static inline Vec8f lookup8(Vec8i const & index, Vec8f const & table) {
-#if INSTRSET >= 8 && VECTORI256_H > 1 // AVX2
-#if defined (_MSC_VER) && _MSC_VER < 1700 && ! defined(__INTEL_COMPILER)        
-    // bug in MS VS 11 beta: operands in wrong order. fixed in 11.0
-    return _mm256_permutevar8x32_ps(_mm256_castsi256_ps(index), _mm256_castps_si256(table)); 
-#elif defined (GCC_VERSION) && GCC_VERSION <= 40700 && !defined(__INTEL_COMPILER) && !defined(__clang__)
-        // Gcc 4.7.0 has wrong parameter type and operands in wrong order. fixed in version 4.7.1
-    return _mm256_permutevar8x32_ps(_mm256_castsi256_ps(index), table);
-#else
-    // no bug version
+static inline Vec8f lookup8(Vec8i const index, Vec8f const table) {
+#if INSTRSET >= 8  // AVX2
     return _mm256_permutevar8x32_ps(table, index);
-#endif
 
 #else // AVX
     // swap low and high part of table
-    __m256  t1 = _mm256_castps128_ps256(_mm256_extractf128_ps(table, 1));
-    __m256  t2 = _mm256_insertf128_ps(t1, _mm256_castps256_ps128(table), 1);
+    __m256  sw = _mm256_permute2f128_ps(table, table, 1);  // swap the two 128-bit lanes
     // join index parts
     __m256i index2 = _mm256_insertf128_si256(_mm256_castsi128_si256(index.get_low()), index.get_high(), 1);
     // permute within each 128-bit part
     __m256  r0 = _mm256_permutevar_ps(table, index2);
-    __m256  r1 = _mm256_permutevar_ps(t2,    index2);
+    __m256  r1 = _mm256_permutevar_ps(sw,    index2);
     // high index bit for blend
     __m128i k1 = _mm_slli_epi32(index.get_high() ^ 4, 29);
     __m128i k0 = _mm_slli_epi32(index.get_low(),      29);
@@ -3065,22 +2829,25 @@ static inline Vec8f lookup8(Vec8i const & index, Vec8f const & table) {
 }
 
 template <int n>
-static inline Vec8f lookup(Vec8i const & index, float const * table) {
-    if (n <= 0) return 0;
-    if (n <= 4) {
-        Vec4f table1 = Vec4f().load(table);        
-        return Vec8f(       
+static inline Vec8f lookup(Vec8i const index, float const * table) {
+    if constexpr (n <= 0) return 0;
+    if constexpr (n <= 4) {
+        Vec4f table1 = Vec4f().load(table);
+        return Vec8f(
             lookup4 (index.get_low(),  table1),
             lookup4 (index.get_high(), table1));
     }
 #if INSTRSET < 8  // not AVX2
-    if (n <= 8) {
+    if constexpr (n <= 8) {
         return lookup8(index, Vec8f().load(table));
     }
 #endif
     // Limit index
     Vec8ui index1;
-    if ((n & (n-1)) == 0) {
+    if constexpr (n == INT_MAX) {
+        index1 = index;
+    }
+    else if constexpr ((n & (n-1)) == 0) {
         // n is a power of 2, make index modulo n
         index1 = Vec8ui(index) & (n-1);
     }
@@ -3088,7 +2855,7 @@ static inline Vec8f lookup(Vec8i const & index, float const * table) {
         // n is not a power of 2, limit to n-1
         index1 = min(Vec8ui(index), n-1);
     }
-#if INSTRSET >= 8 && VECTORI256_H > 1 // AVX2
+#if INSTRSET >= 8  // AVX2
     return _mm256_i32gather_ps(table, index1, 4);
 #else // AVX
     return Vec8f(table[index1[0]],table[index1[1]],table[index1[2]],table[index1[3]],
@@ -3096,35 +2863,28 @@ static inline Vec8f lookup(Vec8i const & index, float const * table) {
 #endif
 }
 
-static inline Vec4d lookup4(Vec4q const & index, Vec4d const & table) {
-#if INSTRSET >= 8 && VECTORI256_H > 1 // AVX2
-    // We can't use VPERMPD because it has constant indexes.
+static inline Vec4d lookup4(Vec4q const index, Vec4d const table) {
+#if INSTRSET >= 10  // AVX512VL
+    return _mm256_permutexvar_pd(index, table);
+
+#elif INSTRSET >= 8  // AVX2
+    // We can't use VPERMPD because it has constant indexes, vpermilpd can permute only within 128-bit lanes
     // Convert the index to fit VPERMPS
-    Vec8i index1 = permute8i<0,0,2,2,4,4,6,6> (Vec8i(index+index));
-    Vec8i index2 = index1 + Vec8i(constant8i<0,1,0,1,0,1,0,1>());
-#if defined (_MSC_VER) && _MSC_VER < 1700 && ! defined(__INTEL_COMPILER)        
-    // bug in MS VS 11 beta: operands in wrong order. fixed in 11.0
-    return _mm256_castps_pd(_mm256_permutevar8x32_ps(_mm256_castsi256_ps(index2), _mm256_castpd_si256(table))); 
-#elif defined (GCC_VERSION) && GCC_VERSION <= 40700 && !defined(__INTEL_COMPILER) && !defined(__clang__)
-        // Gcc 4.7.0 has wrong parameter type and operands in wrong order
-    return _mm256_castps_pd(_mm256_permutevar8x32_ps(_mm256_castsi256_ps(index2), _mm256_castpd_ps(table)));
-#else
-    // no bug version
+    Vec8i index1 = permute8<0,0,2,2,4,4,6,6> (Vec8i(index+index));
+    Vec8i index2 = index1 + Vec8i(constant8ui<0,1,0,1,0,1,0,1>());
     return _mm256_castps_pd(_mm256_permutevar8x32_ps(_mm256_castpd_ps(table), index2));
-#endif
 
 #else // AVX
     // swap low and high part of table
-    __m256d t1 = _mm256_castpd128_pd256(_mm256_extractf128_pd(table, 1));
-    __m256d t2 = _mm256_insertf128_pd(t1, _mm256_castpd256_pd128(table), 1);
+    __m256d sw = _mm256_permute2f128_pd(table, table, 1);// swap the two 128-bit lanes
     // index << 1
     __m128i index2lo = index.get_low()  + index.get_low();
     __m128i index2hi = index.get_high() + index.get_high();
     // join index parts
     __m256i index3 = _mm256_insertf128_si256(_mm256_castsi128_si256(index2lo), index2hi, 1);
     // permute within each 128-bit part
-    __m256d r0 = _mm256_permutevar_pd(table, index3);
-    __m256d r1 = _mm256_permutevar_pd(t2,    index3);
+    __m256d r0 = _mm256_permutevar_pd(table, index3);  // permutevar_pd selects by bit 1 !
+    __m256d r1 = _mm256_permutevar_pd(sw,    index3);
     // high index bit for blend
     __m128i k1 = _mm_slli_epi64(index.get_high() ^ 2, 62);
     __m128i k0 = _mm_slli_epi64(index.get_low(),      62);
@@ -3134,38 +2894,42 @@ static inline Vec4d lookup4(Vec4q const & index, Vec4d const & table) {
 #endif
 }
 
+
 template <int n>
-static inline Vec4d lookup(Vec4q const & index, double const * table) {
-    if (n <= 0) return 0;
-    if (n <= 2) {
-        Vec2d table1 = Vec2d().load(table);        
-        return Vec4d(       
+static inline Vec4d lookup(Vec4q const index, double const * table) {
+    if constexpr (n <= 0) return 0;
+    if constexpr (n <= 2) {
+        Vec2d table1 = Vec2d().load(table);
+        return Vec4d(
             lookup2 (index.get_low(),  table1),
             lookup2 (index.get_high(), table1));
     }
 #if INSTRSET < 8  // not AVX2
-    if (n <= 4) {
+    if constexpr (n <= 4) {
         return lookup4(index, Vec4d().load(table));
     }
 #endif
     // Limit index
-    Vec8ui index1;
-    if ((n & (n-1)) == 0) {
+    Vec4uq index1;
+    if constexpr (n == INT_MAX) {
+        index1 = index;
+    }
+    else if constexpr ((n & (n-1)) == 0) {
         // n is a power of 2, make index modulo n
-        index1 = Vec8ui(index) & constant8i<n-1, 0, n-1, 0, n-1, 0, n-1, 0>();
+        index1 = Vec4uq(index) & Vec4uq(n-1);
     }
     else {
         // n is not a power of 2, limit to n-1
-        index1 = min(Vec8ui(index), constant8i<n-1, 0, n-1, 0, n-1, 0, n-1, 0>() );
+        index1 = min(Vec4uq(index), n-1);
     }
-#if INSTRSET >= 8 && VECTORI256_H > 1 // AVX2
+#if INSTRSET >= 8  // AVX2
     return _mm256_i64gather_pd(table, index1, 8);
 #else // AVX
     Vec4q index2 = Vec4q(index1);
     return Vec4d(table[index2[0]],table[index2[1]],table[index2[2]],table[index2[3]]);
 #endif
 }
-#endif  // VECTORI256_H
+
 
 /*****************************************************************************
 *
@@ -3191,34 +2955,25 @@ static inline Vec4d gather4d(void const * a) {
 ******************************************************************************
 *
 * These functions write the elements of a vector to arbitrary positions in an
-* array in memory. Each vector element is written to an array position 
+* array in memory. Each vector element is written to an array position
 * determined by an index. An element is not written if the corresponding
 * index is out of range.
 * The indexes can be specified as constant template parameters or as an
 * integer vector.
-* 
-* The scatter functions are useful if the data are distributed in a sparce
-* manner into the array. If the array is dense then it is more efficient
-* to permute the data into the right positions and then write the whole
-* permuted vector into the array.
-*
-* Example:
-* Vec8d a(10,11,12,13,14,15,16,17);
-* double b[16] = {0};
-* scatter<0,2,14,10,1,-1,5,9>(a,b); 
-* // Now, b = {10,14,11,0,0,16,0,0,0,17,13,0,0,0,12,0}
 *
 *****************************************************************************/
 
 template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7>
-static inline void scatter(Vec8f const & data, float * array) {
-#if defined (__AVX512VL__)
-    __m256i indx = constant8i<i0,i1,i2,i3,i4,i5,i6,i7>();
-    __mmask16 mask = uint16_t(i0>=0 | (i1>=0)<<1 | (i2>=0)<<2 | (i3>=0)<<3| (i4>=0)<<4| (i5>=0)<<5| (i6>=0)<<6| (i7>=0)<<7);
+static inline void scatter(Vec8f const data, float * array) {
+#if INSTRSET >= 10 //  __AVX512VL__
+    __m256i indx = constant8ui<i0,i1,i2,i3,i4,i5,i6,i7>();
+    __mmask8 mask = uint16_t((i0>=0) | ((i1>=0)<<1) | ((i2>=0)<<2) | ((i3>=0)<<3) |
+        ((i4>=0)<<4) | ((i5>=0)<<5) | ((i6>=0)<<6) | ((i7>=0)<<7));
     _mm256_mask_i32scatter_ps(array, mask, indx, data, 4);
-#elif defined (__AVX512F__)
-    __m512i indx = _mm512_castsi256_si512(constant8i<i0,i1,i2,i3,i4,i5,i6,i7>());
-    __mmask16 mask = uint16_t(i0>=0 | (i1>=0)<<1 | (i2>=0)<<2 | (i3>=0)<<3| (i4>=0)<<4| (i5>=0)<<5| (i6>=0)<<6| (i7>=0)<<7);
+#elif INSTRSET >= 9  //  __AVX512F__
+    __m512i indx = _mm512_castsi256_si512(constant8ui<i0,i1,i2,i3,i4,i5,i6,i7>());
+    __mmask16 mask = uint16_t((i0>=0) | ((i1>=0)<<1) | ((i2>=0)<<2) | ((i3>=0)<<3) |
+        ((i4>=0)<<4) | ((i5>=0)<<5) | ((i6>=0)<<6) | ((i7>=0)<<7));
     _mm512_mask_i32scatter_ps(array, mask, indx, _mm512_castps256_ps512(data), 4);
 #else
     const int index[8] = {i0,i1,i2,i3,i4,i5,i6,i7};
@@ -3229,15 +2984,15 @@ static inline void scatter(Vec8f const & data, float * array) {
 }
 
 template <int i0, int i1, int i2, int i3>
-static inline void scatter(Vec4d const & data, double * array) {
-#if defined (__AVX512VL__)
-    __m128i indx = constant4i<i0,i1,i2,i3>();
-    __mmask16 mask = uint16_t(i0>=0 | (i1>=0)<<1 | (i2>=0)<<2 | (i3>=0)<<3);
+static inline void scatter(Vec4d const data, double * array) {
+#if INSTRSET >= 10 //  __AVX512VL__
+    __m128i indx = constant4ui<i0,i1,i2,i3>();
+    __mmask8 mask = uint8_t((i0>=0) | ((i1>=0)<<1) | ((i2>=0)<<2) | ((i3>=0)<<3));
     _mm256_mask_i32scatter_pd(array, mask, indx, data, 8);
-#elif defined (__AVX512F__)
-    __m256i indx = _mm256_castsi128_si256(constant4i<i0,i1,i2,i3>());
-    __mmask16 mask = uint16_t(i0>=0 | (i1>=0)<<1 | (i2>=0)<<2 | (i3>=0)<<3);
-    _mm512_mask_i32scatter_pd(array, mask, indx, _mm512_castpd256_pd512(data), 8);
+#elif INSTRSET >= 9  //  __AVX512F__
+    __m256i indx = _mm256_castsi128_si256(constant4ui<i0,i1,i2,i3>());
+    __mmask16 mask = uint16_t((i0>=0) | ((i1>=0)<<1) | ((i2>=0)<<2) | ((i3>=0)<<3));
+    _mm512_mask_i32scatter_pd(array, (__mmask8)mask, indx, _mm512_castpd256_pd512(data), 8);
 #else
     const int index[4] = {i0,i1,i2,i3};
     for (int i = 0; i < 4; i++) {
@@ -3246,101 +3001,55 @@ static inline void scatter(Vec4d const & data, double * array) {
 #endif
 }
 
-static inline void scatter(Vec8i const & index, uint32_t limit, Vec8f const & data, float * array) {
-#if defined (__AVX512VL__)
-    __mmask16 mask = _mm256_cmplt_epu32_mask(index, Vec8ui(limit));
-    _mm256_mask_i32scatter_ps(array, mask, index, data, 4);
-#elif defined (__AVX512F__)
-    // 16 bit mask. upper 8 bits are (0<0) = false
-    __mmask16 mask = _mm512_cmplt_epu32_mask(_mm512_castsi256_si512(index), _mm512_castsi256_si512(Vec8ui(limit)));
-    _mm512_mask_i32scatter_ps(array, mask, _mm512_castsi256_si512(index), _mm512_castps256_ps512(data), 4);
+
+/*****************************************************************************
+*
+*          Scatter functions with variable indexes
+*
+*****************************************************************************/
+
+static inline void scatter(Vec8i const index, uint32_t limit, Vec8f const data, float * destination) {
+#if INSTRSET >= 10 //  __AVX512VL__
+    __mmask8 mask = _mm256_cmplt_epu32_mask(index, Vec8ui(limit));
+    _mm256_mask_i32scatter_ps(destination, mask, index, data, 4);
+#elif INSTRSET >= 9  //  __AVX512F__
+    __mmask16 mask = _mm512_mask_cmplt_epu32_mask(0xFFu, _mm512_castsi256_si512(index), _mm512_castsi256_si512(Vec8ui(limit)));
+    _mm512_mask_i32scatter_ps(destination, mask, _mm512_castsi256_si512(index), _mm512_castps256_ps512(data), 4);
 #else
     for (int i = 0; i < 8; i++) {
-        if (uint32_t(index[i]) < limit) array[index[i]] = data[i];
+        if (uint32_t(index[i]) < limit) destination[index[i]] = data[i];
     }
 #endif
 }
 
-static inline void scatter(Vec4q const & index, uint32_t limit, Vec4d const & data, double * array) {
-#if defined (__AVX512VL__)
-    __mmask16 mask = _mm256_cmplt_epu64_mask(index, Vec4uq(uint64_t(limit)));
-    _mm256_mask_i64scatter_pd(array, mask, index, data, 8);
-#elif defined (__AVX512F__)
-    // 16 bit mask. upper 8 bits are (0<0) = false
-    __mmask16 mask = _mm512_cmplt_epu64_mask(_mm512_castsi256_si512(index), _mm512_castsi256_si512(Vec4uq(uint64_t(limit))));
-    _mm512_mask_i64scatter_pd(array, mask, _mm512_castsi256_si512(index), _mm512_castpd256_pd512(data), 8);
+static inline void scatter(Vec4q const index, uint32_t limit, Vec4d const data, double * destination) {
+#if INSTRSET >= 10 //  __AVX512VL__
+    __mmask8 mask = _mm256_cmplt_epu64_mask(index, Vec4uq(uint64_t(limit)));
+    _mm256_mask_i64scatter_pd(destination, mask, index, data, 8);
+#elif INSTRSET >= 9  //  __AVX512F__
+    __mmask16 mask = _mm512_mask_cmplt_epu64_mask(0xF, _mm512_castsi256_si512(index), _mm512_castsi256_si512(Vec4uq(uint64_t(limit))));
+    _mm512_mask_i64scatter_pd(destination, (__mmask8)mask, _mm512_castsi256_si512(index), _mm512_castpd256_pd512(data), 8);
 #else
     for (int i = 0; i < 4; i++) {
-        if (uint64_t(index[i]) < uint64_t(limit)) array[index[i]] = data[i];
+        if (uint64_t(index[i]) < uint64_t(limit)) destination[index[i]] = data[i];
     }
 #endif
-} 
+}
 
-static inline void scatter(Vec4i const & index, uint32_t limit, Vec4d const & data, double * array) {
-#if defined (__AVX512VL__)
-    __mmask16 mask = _mm_cmplt_epu32_mask(index, Vec4ui(limit));
-    _mm256_mask_i32scatter_pd(array, mask, index, data, 8);
-#elif defined (__AVX512F__)
-    // 16 bit mask. upper 12 bits are (0<0) = false
-    __mmask16 mask = _mm512_cmplt_epu32_mask(_mm512_castsi128_si512(index), _mm512_castsi128_si512(Vec4ui(limit)));
-    _mm512_mask_i32scatter_pd(array, mask, _mm256_castsi128_si256(index), _mm512_castpd256_pd512(data), 8);
+static inline void scatter(Vec4i const index, uint32_t limit, Vec4d const data, double * destination) {
+#if INSTRSET >= 10   //  __AVX512VL__
+    __mmask8 mask = _mm_cmplt_epu32_mask(index, Vec4ui(limit));
+    _mm256_mask_i32scatter_pd(destination, mask, index, data, 8);
+#elif INSTRSET >= 9  //  __AVX512F__
+    __mmask16 mask = _mm512_mask_cmplt_epu32_mask(0xF, _mm512_castsi128_si512(index), _mm512_castsi128_si512(Vec4ui(limit)));
+    _mm512_mask_i32scatter_pd(destination, (__mmask8)mask, _mm256_castsi128_si256(index), _mm512_castpd256_pd512(data), 8);
 #else
     for (int i = 0; i < 4; i++) {
-        if (uint32_t(index[i]) < limit) array[index[i]] = data[i];
+        if (uint32_t(index[i]) < limit) destination[index[i]] = data[i];
     }
 #endif
-} 
-
-
-/*****************************************************************************
-*
-*          Horizontal scan functions
-*
-*****************************************************************************/
-
-// Get index to the first element that is true. Return -1 if all are false
-static inline int horizontal_find_first(Vec8fb const & x) {
-    return horizontal_find_first(Vec8ib(x));
-}
-
-static inline int horizontal_find_first(Vec4db const & x) {
-    return horizontal_find_first(Vec4qb(x));
-}
-
-// Count the number of elements that are true
-static inline uint32_t horizontal_count(Vec8fb const & x) {
-    return horizontal_count(Vec8ib(x));
 }
 
-static inline uint32_t horizontal_count(Vec4db const & x) {
-    return horizontal_count(Vec4qb(x));
-}
-
-/*****************************************************************************
-*
-*          Boolean <-> bitfield conversion functions
-*
-*****************************************************************************/
-
-// to_bits: convert boolean vector to integer bitfield
-static inline uint8_t to_bits(Vec8fb const & x) {
-    return to_bits(Vec8ib(x));
-}
-
-// to_Vec8fb: convert integer bitfield to boolean vector
-static inline Vec8fb to_Vec8fb(uint8_t x) {
-    return Vec8fb(to_Vec8ib(x));
-}
-
-// to_bits: convert boolean vector to integer bitfield
-static inline uint8_t to_bits(Vec4db const & x) {
-    return to_bits(Vec4qb(x));
-}
-
-// to_Vec4db: convert integer bitfield to boolean vector
-static inline Vec4db to_Vec4db(uint8_t x) {
-    return Vec4db(to_Vec4qb(x));
-}
 
 #ifdef VCL_NAMESPACE
 }
diff --git a/EEDI3/vectorclass/vectorf256e.h b/EEDI3/vectorclass/vectorf256e.h
index edd3a3f..709c6d0 100644
--- a/EEDI3/vectorclass/vectorf256e.h
+++ b/EEDI3/vectorclass/vectorf256e.h
@@ -1,12 +1,14 @@
 /****************************  vectorf256e.h   *******************************
 * Author:        Agner Fog
 * Date created:  2012-05-30
-* Last modified: 2017-02-19
-* Version:       1.27
-* Project:       vector classes
+* Last modified: 2023-07-04
+* Version:       2.02.02
+* Project:       vector class library
 * Description:
-* Header file defining 256-bit floating point vector classes as interface
-* to intrinsic functions. Emulated for processors without AVX instruction set.
+* Header file defining 256-bit floating point vector classes
+* Emulated for processors without AVX instruction set.
+*
+* Instructions: see vcl_manual.pdf
 *
 * The following vector classes are defined here:
 * Vec8f     Vector of 8 single precision floating point numbers
@@ -14,25 +16,28 @@
 * Vec4d     Vector of 4 double precision floating point numbers
 * Vec4db    Vector of 4 Booleans for use with Vec4d
 *
-* For detailed instructions, see VectorClass.pdf
+* Each vector object is represented internally in the CPU as two 128-bit registers.
+* This header file defines operators and functions for these vectors.
 *
-* (c) Copyright 2012-2017 GNU General Public License http://www.gnu.org/licenses
+* (c) Copyright 2012-2023 Agner Fog.
+* Apache License version 2.0 or later.
 *****************************************************************************/
 
-// check combination of header files
-#ifdef VECTORF256_H
-#if    VECTORF256_H != 1
-#error Two different versions of vectorf256.h included
+#ifndef VECTORF256E_H
+#define VECTORF256E_H  1
+
+#ifndef VECTORCLASS_H
+#include "vectorclass.h"
 #endif
-#else
-#define VECTORF256_H  1
 
-#if defined (VECTORI256_H) &&  VECTORI256_H >= 2
-#error wrong combination of header files. Use vectorf256.h instead of vectorf256e.h if you have AVX2
+#if VECTORCLASS_H < 20200
+#error Incompatible versions of vector class library mixed
 #endif
 
+#ifdef VECTORF256_H
+#error Two different versions of vectorf256.h included
+#endif
 
-#include "vectorf128.h"  // Define 128-bit vectors
 
 #ifdef VCL_NAMESPACE
 namespace VCL_NAMESPACE {
@@ -43,13 +48,14 @@ namespace VCL_NAMESPACE {
 *          base class Vec256fe and Vec256de
 *
 *****************************************************************************/
+
 // base class to replace __m256 when AVX is not supported
 class Vec256fe {
 protected:
     __m128 y0;                         // low half
     __m128 y1;                         // high half
 public:
-    Vec256fe(void) {};                 // default constructor
+    Vec256fe() = default;              // default constructor
     Vec256fe(__m128 x0, __m128 x1) {   // constructor to build from two __m128
         y0 = x0;  y1 = x1;
     }
@@ -64,7 +70,7 @@ public:
 // base class to replace __m256d when AVX is not supported
 class Vec256de {
 public:
-    Vec256de() {};                     // default constructor
+    Vec256de() = default;              // default constructor
     Vec256de(__m128d x0, __m128d x1) { // constructor to build from two __m128d
         y0 = x0;  y1 = x1;
     }
@@ -85,42 +91,24 @@ protected:
 *          select functions
 *
 *****************************************************************************/
-// Select between two Vec256fe sources, element by element. Used in various functions 
-// and operators. Corresponds to this pseudocode:
+// Select between two Vec256fe sources, element by element using broad boolean vector.
+// Used in various functions and operators. Corresponds to this pseudocode:
 // for (int i = 0; i < 8; i++) result[i] = s[i] ? a[i] : b[i];
 // Each element in s must be either 0 (false) or 0xFFFFFFFF (true).
-static inline Vec256fe selectf (Vec256fe const & s, Vec256fe const & a, Vec256fe const & b) {
+static inline Vec256fe selectf (Vec256fe const s, Vec256fe const a, Vec256fe const b) {
     return Vec256fe(selectf(b.get_low(), a.get_low(), s.get_low()), selectf(b.get_high(), a.get_high(), s.get_high()));
 }
 
 // Same, with two Vec256de sources.
 // and operators. Corresponds to this pseudocode:
 // for (int i = 0; i < 4; i++) result[i] = s[i] ? a[i] : b[i];
-// Each element in s must be either 0 (false) or 0xFFFFFFFFFFFFFFFF (true). No other 
+// Each element in s must be either 0 (false) or 0xFFFFFFFFFFFFFFFF (true). No other
 // values are allowed.
-static inline Vec256de selectd (Vec256de const & s, Vec256de const & a, Vec256de const & b) {
+static inline Vec256de selectd (Vec256de const s, Vec256de const a, Vec256de const b) {
     return Vec256de(selectd(b.get_low(), a.get_low(), s.get_low()), selectd(b.get_high(), a.get_high(), s.get_high()));
 }
 
 
-
-/*****************************************************************************
-*
-*          Generate compile-time constant vector
-*
-*****************************************************************************/
-// Generate a constant vector of 8 integers stored in memory,
-// load as __m256
-template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7>
-static inline Vec256fe constant8f() {
-    static const union {
-        int      i[8];
-        __m128   y[2];
-    } u = {{i0,i1,i2,i3,i4,i5,i6,i7}};
-    return Vec256fe(u.y[0], u.y[1]);
-}
-
-
 /*****************************************************************************
 *
 *          Vec8fb: Vector of 8 Booleans for use with Vec8f
@@ -130,60 +118,53 @@ static inline Vec256fe constant8f() {
 class Vec8fb : public Vec256fe {
 public:
     // Default constructor:
-    Vec8fb() {
-    }
+    Vec8fb() = default;
     // Constructor to build from all elements:
     Vec8fb(bool b0, bool b1, bool b2, bool b3, bool b4, bool b5, bool b6, bool b7) {
         y0 = Vec4fb(b0, b1, b2, b3);
         y1 = Vec4fb(b4, b5, b6, b7);
     }
     // Constructor to build from two Vec4fb:
-    Vec8fb(Vec4fb const & a0, Vec4fb const & a1) {
+    Vec8fb(Vec4fb const a0, Vec4fb const a1) {
         y0 = a0;  y1 = a1;
     }
     // Constructor to convert from type Vec256fe
-    Vec8fb(Vec256fe const & x) {
+    Vec8fb(Vec256fe const x) {
         y0 = x.get_low();  y1 = x.get_high();
     }
+    // Constructor to broadcast scalar value:
+    Vec8fb(bool b) {
+        y0 = y1 = Vec4fb(b);
+    }
     // Assignment operator to convert from type Vec256fe
-    Vec8fb & operator = (Vec256fe const & x) {
+    Vec8fb & operator = (Vec256fe const x) {
         y0 = x.get_low();  y1 = x.get_high();
         return *this;
     }
-#ifdef VECTORI256_H  // 256 bit integer vectors are available
     // Constructor to convert from type Vec8ib used as Boolean for integer vectors
-    Vec8fb(Vec8ib const & x) {
+    Vec8fb(Vec8ib const x) {
         y0 = _mm_castsi128_ps(Vec8i(x).get_low());
         y1 = _mm_castsi128_ps(Vec8i(x).get_high());
     }
     // Assignment operator to convert from type Vec8ib used as Boolean for integer vectors
-    Vec8fb & operator = (Vec8ib const & x) {
+    Vec8fb & operator = (Vec8ib const x) {
         y0 = _mm_castsi128_ps(Vec8i(x).get_low());
         y1 = _mm_castsi128_ps(Vec8i(x).get_high());
         return *this;
     }
-    // Constructor to broadcast the same value into all elements:
-    Vec8fb(bool b) {
-        y1 = y0 = Vec4fb(b);
-    }
     // Assignment operator to broadcast scalar value:
     Vec8fb & operator = (bool b) {
         y0 = y1 = Vec4fb(b);
         return *this;
     }
-private: // Prevent constructing from int, etc.
-    Vec8fb(int b);
-    Vec8fb & operator = (int x);
-public:
     // Type cast operator to convert to type Vec8ib used as Boolean for integer vectors
     operator Vec8ib() const {
         return Vec8i(_mm_castps_si128(y0), _mm_castps_si128(y1));
     }
-#endif // VECTORI256_H
+
     // Member function to change a single element in vector
-    // Note: This function is inefficient. Use load function if changing more than one element
-    Vec8fb const & insert(uint32_t index, bool value) {
-        if (index < 4) {
+    Vec8fb const insert(int index, bool value) {
+        if ((uint32_t)index < 4) {
             y0 = Vec4fb(y0).insert(index, value);
         }
         else {
@@ -192,9 +173,8 @@ public:
         return *this;
     }
     // Member function extract a single element from vector
-    // Note: This function is inefficient. Use store function if extracting more than one element
-    bool extract(uint32_t index) const {
-        if (index < 4) {
+    bool extract(int index) const {
+        if ((uint32_t)index < 4) {
             return Vec4fb(y0).extract(index);
         }
         else {
@@ -202,7 +182,7 @@ public:
         }
     }
     // Extract a single element. Operator [] can only read an element, not write.
-    bool operator [] (uint32_t index) const {
+    bool operator [] (int index) const {
         return extract(index);
     }
     // Member functions to split into two Vec4fb:
@@ -212,9 +192,21 @@ public:
     Vec4fb get_high() const {
         return y1;
     }
-    static int size () {
+    // Member function to change a bitfield to a boolean vector
+    Vec8fb & load_bits(uint8_t a) {
+        y0 = Vec4fb().load_bits(a);
+        y1 = Vec4fb().load_bits(uint8_t(a>>4u));
+        return *this;
+    }
+    static constexpr int size() {
         return 8;
     }
+    static constexpr int elementtype() {
+        return 3;
+    }
+    // Prevent constructing from int, etc.
+    Vec8fb(int b) = delete;
+    Vec8fb & operator = (int x) = delete;
 };
 
 
@@ -225,84 +217,85 @@ public:
 *****************************************************************************/
 
 // vector operator & : bitwise and
-static inline Vec8fb operator & (Vec8fb const & a, Vec8fb const & b) {
+static inline Vec8fb operator & (Vec8fb const a, Vec8fb const b) {
     return Vec8fb(a.get_low() & b.get_low(), a.get_high() & b.get_high());
 }
 
-static inline Vec8fb operator && (Vec8fb const & a, Vec8fb const & b) {
+static inline Vec8fb operator && (Vec8fb const a, Vec8fb const b) {
     return a & b;
 }
 
 // vector operator &= : bitwise and
-static inline Vec8fb & operator &= (Vec8fb & a, Vec8fb const & b) {
+static inline Vec8fb & operator &= (Vec8fb & a, Vec8fb const b) {
     a = a & b;
     return a;
 }
 
 // vector operator | : bitwise or
-static inline Vec8fb operator | (Vec8fb const & a, Vec8fb const & b) {
+static inline Vec8fb operator | (Vec8fb const a, Vec8fb const b) {
     return Vec8fb(a.get_low() | b.get_low(), a.get_high() | b.get_high());
 }
-static inline Vec8fb operator || (Vec8fb const & a, Vec8fb const & b) {
+static inline Vec8fb operator || (Vec8fb const a, Vec8fb const b) {
     return a | b;
 }
 
 // vector operator |= : bitwise or
-static inline Vec8fb & operator |= (Vec8fb & a, Vec8fb const & b) {
+static inline Vec8fb & operator |= (Vec8fb & a, Vec8fb const b) {
     a = a | b;
     return a;
 }
 
 // vector operator ^ : bitwise xor
-static inline Vec8fb operator ^ (Vec8fb const & a, Vec8fb const & b) {
+static inline Vec8fb operator ^ (Vec8fb const a, Vec8fb const b) {
     return Vec8fb(a.get_low() ^ b.get_low(), a.get_high() ^ b.get_high());
 }
 
 // vector operator ^= : bitwise xor
-static inline Vec8fb & operator ^= (Vec8fb & a, Vec8fb const & b) {
+static inline Vec8fb & operator ^= (Vec8fb & a, Vec8fb const b) {
     a = a ^ b;
     return a;
 }
 
 // vector operator ~ : bitwise not
-static inline Vec8fb operator ~ (Vec8fb const & a) {
+static inline Vec8fb operator ~ (Vec8fb const a) {
     return Vec8fb(~a.get_low(), ~a.get_high());
 }
 
+// vector operator == : xnor
+static inline Vec8fb operator == (Vec8fb const a, Vec8fb const b) {
+    return Vec8fb(Vec8fb(a) ^ Vec8fb(~b));
+}
+
+// vector operator != : xor
+static inline Vec8fb operator != (Vec8fb const a, Vec8fb const b) {
+    return Vec8fb(a ^ b);
+}
+
 // vector operator ! : logical not
 // (operator ! is less efficient than operator ~. Use only where not
 // all bits in an element are the same)
-static inline Vec8fb operator ! (Vec8fb const & a) {
+static inline Vec8fb operator ! (Vec8fb const a) {
     return Vec8fb(!a.get_low(), !a.get_high());
 }
 
 // Functions for Vec8fb
 
 // andnot: a & ~ b
-static inline Vec8fb andnot(Vec8fb const & a, Vec8fb const & b) {
+static inline Vec8fb andnot(Vec8fb const a, Vec8fb const b) {
     return Vec8fb(andnot(a.get_low(), b.get_low()), andnot(a.get_high(), b.get_high()));
 }
 
-
-
-/*****************************************************************************
-*
-*          Horizontal Boolean functions
-*
-*****************************************************************************/
-
 // horizontal_and. Returns true if all bits are 1
-static inline bool horizontal_and (Vec8fb const & a) {
+static inline bool horizontal_and (Vec8fb const a) {
     return horizontal_and(a.get_low() & a.get_high());
 }
 
 // horizontal_or. Returns true if at least one bit is 1
-static inline bool horizontal_or (Vec8fb const & a) {
+static inline bool horizontal_or (Vec8fb const a) {
     return horizontal_or(a.get_low() | a.get_high());
 }
 
 
-
 /*****************************************************************************
 *
 *          Vec4db: Vector of 4 Booleans for use with Vec4d
@@ -312,60 +305,54 @@ static inline bool horizontal_or (Vec8fb const & a) {
 class Vec4db : public Vec256de {
 public:
     // Default constructor:
-    Vec4db() {
-    }
+    Vec4db() = default;
     // Constructor to build from all elements:
     Vec4db(bool b0, bool b1, bool b2, bool b3) {
         y0 = Vec2db(b0, b1);
         y1 = Vec2db(b2, b3);
     }
     // Constructor to build from two Vec2db:
-    Vec4db(Vec2db const & a0, Vec2db const & a1) {
+    Vec4db(Vec2db const a0, Vec2db const a1) {
         y0 = a0;  y1 = a1;
     }
     // Constructor to convert from type Vec256de
-    Vec4db(Vec256de const & x) {
+    Vec4db(Vec256de const x) {
         y0 = x.get_low();  y1 = x.get_high();
     }
+    // Constructor to broadcast scalar value:
+    Vec4db(bool b) {
+        y0 = y1 = Vec2db(b);
+    }
     // Assignment operator to convert from type Vec256de
-    Vec4db & operator = (Vec256de const & x) {
+    Vec4db & operator = (Vec256de const x) {
         y0 = x.get_low();  y1 = x.get_high();
         return *this;
     }
-#ifdef VECTORI256_H  // 256 bit integer vectors are available
+
     // Constructor to convert from type Vec4qb used as Boolean for integer vectors
-    Vec4db(Vec4qb const & x) {
+    Vec4db(Vec4qb const x) {
         y0 = _mm_castsi128_pd(Vec4q(x).get_low());
         y1 = _mm_castsi128_pd(Vec4q(x).get_high());
     }
     // Assignment operator to convert from type Vec4qb used as Boolean for integer vectors
-    Vec4db & operator = (Vec4qb const & x) {
+    Vec4db & operator = (Vec4qb const x) {
         y0 = _mm_castsi128_pd(Vec4q(x).get_low());
         y1 = _mm_castsi128_pd(Vec4q(x).get_high());
         return *this;
     }
-    // Constructor to broadcast the same value into all elements:
-    Vec4db(bool b) {
-        y1 = y0 = Vec2db(b);
-    }
     // Assignment operator to broadcast scalar value:
     Vec4db & operator = (bool b) {
         y0 = y1 = Vec2db(b);
         return *this;
     }
-private: // Prevent constructing from int, etc.
-    Vec4db(int b);
-    Vec4db & operator = (int x);
-public:
     // Type cast operator to convert to type Vec4qb used as Boolean for integer vectors
     operator Vec4qb() const {
         return Vec4q(_mm_castpd_si128(y0), _mm_castpd_si128(y1));
     }
-#endif // VECTORI256_H
+
     // Member function to change a single element in vector
-    // Note: This function is inefficient. Use load function if changing more than one element
-    Vec4db const & insert(uint32_t index, bool value) {
-        if (index < 2) {
+    Vec4db const insert(int index, bool value) {
+        if ((uint32_t)index < 2) {
             y0 = Vec2db(y0).insert(index, value);
         }
         else {
@@ -374,9 +361,8 @@ public:
         return *this;
     }
     // Member function extract a single element from vector
-    // Note: This function is inefficient. Use store function if extracting more than one element
-    bool extract(uint32_t index) const {
-        if (index < 2) {
+    bool extract(int index) const {
+        if ((uint32_t)index < 2) {
             return Vec2db(y0).extract(index);
         }
         else {
@@ -384,7 +370,7 @@ public:
         }
     }
     // Extract a single element. Operator [] can only read an element, not write.
-    bool operator [] (uint32_t index) const {
+    bool operator [] (int index) const {
         return extract(index);
     }
     // Member functions to split into two Vec4fb:
@@ -394,9 +380,21 @@ public:
     Vec2db get_high() const {
         return y1;
     }
-    static int size () {
+    // Member function to change a bitfield to a boolean vector
+    Vec4db & load_bits(uint8_t a) {
+        y0 = Vec2db().load_bits(a);
+        y1 = Vec2db().load_bits(uint8_t(a>>2u));
+        return *this;
+    }
+    static constexpr int size() {
         return 4;
     }
+    static constexpr int elementtype() {
+        return 3;
+    }
+    // Prevent constructing from int, etc.
+    Vec4db(int b) = delete;
+    Vec4db & operator = (int x) = delete;
 };
 
 
@@ -407,83 +405,84 @@ public:
 *****************************************************************************/
 
 // vector operator & : bitwise and
-static inline Vec4db operator & (Vec4db const & a, Vec4db const & b) {
+static inline Vec4db operator & (Vec4db const a, Vec4db const b) {
     return Vec4db(a.get_low() & b.get_low(), a.get_high() & b.get_high());
 }
-static inline Vec4db operator && (Vec4db const & a, Vec4db const & b) {
+static inline Vec4db operator && (Vec4db const a, Vec4db const b) {
     return a & b;
 }
 
 // vector operator &= : bitwise and
-static inline Vec4db & operator &= (Vec4db & a, Vec4db const & b) {
+static inline Vec4db & operator &= (Vec4db & a, Vec4db const b) {
     a = a & b;
     return a;
 }
 
 // vector operator | : bitwise or
-static inline Vec4db operator | (Vec4db const & a, Vec4db const & b) {
+static inline Vec4db operator | (Vec4db const a, Vec4db const b) {
     return Vec4db(a.get_low() | b.get_low(), a.get_high() | b.get_high());
 }
-static inline Vec4db operator || (Vec4db const & a, Vec4db const & b) {
+static inline Vec4db operator || (Vec4db const a, Vec4db const b) {
     return a | b;
 }
 
 // vector operator |= : bitwise or
-static inline Vec4db & operator |= (Vec4db & a, Vec4db const & b) {
+static inline Vec4db & operator |= (Vec4db & a, Vec4db const b) {
     a = a | b;
     return a;
 }
 
 // vector operator ^ : bitwise xor
-static inline Vec4db operator ^ (Vec4db const & a, Vec4db const & b) {
+static inline Vec4db operator ^ (Vec4db const a, Vec4db const b) {
     return Vec4db(a.get_low() ^ b.get_low(), a.get_high() ^ b.get_high());
-
 }
 
 // vector operator ^= : bitwise xor
-static inline Vec4db & operator ^= (Vec4db & a, Vec4db const & b) {
+static inline Vec4db & operator ^= (Vec4db & a, Vec4db const b) {
     a = a ^ b;
     return a;
 }
 
 // vector operator ~ : bitwise not
-static inline Vec4db operator ~ (Vec4db const & a) {
+static inline Vec4db operator ~ (Vec4db const a) {
     return Vec4db(~a.get_low(), ~a.get_high());
 }
 
+// vector operator == : xnor
+static inline Vec4db operator == (Vec4db const a, Vec4db const b) {
+    return Vec4db(Vec4db(a) ^ Vec4db(~b));
+}
+
+// vector operator != : xor
+static inline Vec4db operator != (Vec4db const a, Vec4db const b) {
+    return Vec4db(a ^ b);
+}
+
 // vector operator ! : logical not
 // (operator ! is less efficient than operator ~. Use only where not
 // all bits in an element are the same)
-static inline Vec4db operator ! (Vec4db const & a) {
+static inline Vec4db operator ! (Vec4db const a) {
     return Vec4db(!a.get_low(), !a.get_high());
 }
 
 // Functions for Vec4db
 
 // andnot: a & ~ b
-static inline Vec4db andnot(Vec4db const & a, Vec4db const & b) {
+static inline Vec4db andnot(Vec4db const a, Vec4db const b) {
     return Vec4db(andnot(a.get_low(), b.get_low()), andnot(a.get_high(), b.get_high()));
 }
 
-
-/*****************************************************************************
-*
-*          Horizontal Boolean functions
-*
-*****************************************************************************/
-
 // horizontal_and. Returns true if all bits are 1
-static inline bool horizontal_and (Vec4db const & a) {
+static inline bool horizontal_and (Vec4db const a) {
     return horizontal_and(a.get_low() & a.get_high());
 }
 
 // horizontal_or. Returns true if at least one bit is 1
-static inline bool horizontal_or (Vec4db const & a) {
+static inline bool horizontal_or (Vec4db const a) {
     return horizontal_or(a.get_low() | a.get_high());
 }
 
 
-
 /*****************************************************************************
 *
 *          Vec8f: Vector of 8 single precision floating point values
@@ -493,8 +492,7 @@ static inline bool horizontal_or (Vec4db const & a) {
 class Vec8f : public Vec256fe {
 public:
     // Default constructor:
-    Vec8f() {
-    }
+    Vec8f() = default;
     // Constructor to broadcast the same value into all elements:
     Vec8f(float f) {
         y1 = y0 = _mm_set1_ps(f);
@@ -502,18 +500,18 @@ public:
     // Constructor to build from all elements:
     Vec8f(float f0, float f1, float f2, float f3, float f4, float f5, float f6, float f7) {
         y0 = _mm_setr_ps(f0, f1, f2, f3);
-        y1 = _mm_setr_ps(f4, f5, f6, f7); 
+        y1 = _mm_setr_ps(f4, f5, f6, f7);
     }
     // Constructor to build from two Vec4f:
-    Vec8f(Vec4f const & a0, Vec4f const & a1) {
+    Vec8f(Vec4f const a0, Vec4f const a1) {
         y0 = a0;  y1 = a1;
     }
     // Constructor to convert from type Vec256fe
-    Vec8f(Vec256fe const & x) {
+    Vec8f(Vec256fe const x) {
         y0 = x.get_low();  y1 = x.get_high();
     }
     // Assignment operator to convert from type Vec256fe
-    Vec8f & operator = (Vec256fe const & x) {
+    Vec8f & operator = (Vec256fe const x) {
         y0 = x.get_low();  y1 = x.get_high();
         return *this;
     }
@@ -524,8 +522,7 @@ public:
         return *this;
     }
     // Member function to load from array, aligned by 32
-    // You may use load_a instead of load if you are certain that p points to an address
-    // divisible by 32.
+    // You may use load_a instead of load if you are certain that p points to an address divisible by 32.
     Vec8f & load_a(float const * p) {
         y0 = _mm_load_ps(p);
         y1 = _mm_load_ps(p+4);
@@ -536,13 +533,18 @@ public:
         _mm_storeu_ps(p,   y0);
         _mm_storeu_ps(p+4, y1);
     }
-    // Member function to store into array, aligned by 32
-    // You may use store_a instead of store if you are certain that p points to an address
-    // divisible by 32.
+    // Member function storing into array, aligned by 32
+    // You may use store_a instead of store if you are certain that p points to an address divisible by 32.
     void store_a(float * p) const {
         _mm_store_ps(p,   y0);
         _mm_store_ps(p+4, y1);
     }
+    // Member function storing to aligned uncached memory (non-temporal store).
+    // Note: Will generate runtime error if p is not aligned by 32
+    void store_nt(float * p) const {
+        _mm_stream_ps(p,   y0);
+        _mm_stream_ps(p+4, y1);
+    }
     // Partial load. Load n elements and set the rest to 0
     Vec8f & load_partial(int n, float const * p) {
         if (n > 0 && n <= 4) {
@@ -579,9 +581,8 @@ public:
         return *this;
     }
     // Member function to change a single element in vector
-    // Note: This function is inefficient. Use load function if changing more than one element
-    Vec8f const & insert(uint32_t index, float value) {
-        if (index < 4) {
+    Vec8f const insert(int index, float value) {
+        if ((uint32_t)index < 4) {
             y0 = Vec4f(y0).insert(index, value);
         }
         else {
@@ -590,9 +591,8 @@ public:
         return *this;
     }
     // Member function extract a single element from vector
-    // Note: This function is inefficient. Use store function if extracting more than one element
-    float extract(uint32_t index) const {
-        if (index < 4) {
+    float extract(int index) const {
+        if ((uint32_t)index < 4) {
             return Vec4f(y0).extract(index);
         }
         else {
@@ -601,7 +601,7 @@ public:
     }
     // Extract a single element. Use store function if extracting more than one element.
     // Operator [] can only read an element, not write.
-    float operator [] (uint32_t index) const {
+    float operator [] (int index) const {
         return extract(index);
     }
     // Member functions to split into two Vec4f:
@@ -611,9 +611,12 @@ public:
     Vec4f get_high() const {
         return y1;
     }
-    static int size () {
+    static constexpr int size() {
         return 8;
     }
+    static constexpr int elementtype() {
+        return 16;
+    }
 };
 
 
@@ -624,20 +627,20 @@ public:
 *****************************************************************************/
 
 // vector operator + : add element by element
-static inline Vec8f operator + (Vec8f const & a, Vec8f const & b) {
+static inline Vec8f operator + (Vec8f const a, Vec8f const b) {
     return Vec8f(a.get_low() + b.get_low(), a.get_high() + b.get_high());
 }
 
 // vector operator + : add vector and scalar
-static inline Vec8f operator + (Vec8f const & a, float b) {
+static inline Vec8f operator + (Vec8f const a, float b) {
     return a + Vec8f(b);
 }
-static inline Vec8f operator + (float a, Vec8f const & b) {
+static inline Vec8f operator + (float a, Vec8f const b) {
     return Vec8f(a) + b;
 }
 
 // vector operator += : add
-static inline Vec8f & operator += (Vec8f & a, Vec8f const & b) {
+static inline Vec8f & operator += (Vec8f & a, Vec8f const b) {
     a = a + b;
     return a;
 }
@@ -656,26 +659,26 @@ static inline Vec8f & operator ++ (Vec8f & a) {
 }
 
 // vector operator - : subtract element by element
-static inline Vec8f operator - (Vec8f const & a, Vec8f const & b) {
+static inline Vec8f operator - (Vec8f const a, Vec8f const b) {
     return Vec8f(a.get_low() - b.get_low(), a.get_high() - b.get_high());
 }
 
 // vector operator - : subtract vector and scalar
-static inline Vec8f operator - (Vec8f const & a, float b) {
+static inline Vec8f operator - (Vec8f const a, float b) {
     return a - Vec8f(b);
 }
-static inline Vec8f operator - (float a, Vec8f const & b) {
+static inline Vec8f operator - (float a, Vec8f const b) {
     return Vec8f(a) - b;
 }
 
 // vector operator - : unary minus
 // Change sign bit, even for 0, INF and NAN
-static inline Vec8f operator - (Vec8f const & a) {
+static inline Vec8f operator - (Vec8f const a) {
     return Vec8f(-a.get_low(), -a.get_high());
 }
 
 // vector operator -= : subtract
-static inline Vec8f & operator -= (Vec8f & a, Vec8f const & b) {
+static inline Vec8f & operator -= (Vec8f & a, Vec8f const b) {
     a = a - b;
     return a;
 }
@@ -694,118 +697,118 @@ static inline Vec8f & operator -- (Vec8f & a) {
 }
 
 // vector operator * : multiply element by element
-static inline Vec8f operator * (Vec8f const & a, Vec8f const & b) {
+static inline Vec8f operator * (Vec8f const a, Vec8f const b) {
     return Vec8f(a.get_low() * b.get_low(), a.get_high() * b.get_high());
 }
 
 // vector operator * : multiply vector and scalar
-static inline Vec8f operator * (Vec8f const & a, float b) {
+static inline Vec8f operator * (Vec8f const a, float b) {
     return a * Vec8f(b);
 }
-static inline Vec8f operator * (float a, Vec8f const & b) {
+static inline Vec8f operator * (float a, Vec8f const b) {
     return Vec8f(a) * b;
 }
 
 // vector operator *= : multiply
-static inline Vec8f & operator *= (Vec8f & a, Vec8f const & b) {
+static inline Vec8f & operator *= (Vec8f & a, Vec8f const b) {
     a = a * b;
     return a;
 }
 
 // vector operator / : divide all elements by same integer
-static inline Vec8f operator / (Vec8f const & a, Vec8f const & b) {
+static inline Vec8f operator / (Vec8f const a, Vec8f const b) {
     return Vec8f(a.get_low() / b.get_low(), a.get_high() / b.get_high());
 }
 
 // vector operator / : divide vector and scalar
-static inline Vec8f operator / (Vec8f const & a, float b) {
+static inline Vec8f operator / (Vec8f const a, float b) {
     return a / Vec8f(b);
 }
-static inline Vec8f operator / (float a, Vec8f const & b) {
+static inline Vec8f operator / (float a, Vec8f const b) {
     return Vec8f(a) / b;
 }
 
 // vector operator /= : divide
-static inline Vec8f & operator /= (Vec8f & a, Vec8f const & b) {
+static inline Vec8f & operator /= (Vec8f & a, Vec8f const b) {
     a = a / b;
     return a;
 }
 
 // vector operator == : returns true for elements for which a == b
-static inline Vec8fb operator == (Vec8f const & a, Vec8f const & b) {
+static inline Vec8fb operator == (Vec8f const a, Vec8f const b) {
     return Vec8fb(a.get_low() == b.get_low(), a.get_high() == b.get_high());
 }
 
 // vector operator != : returns true for elements for which a != b
-static inline Vec8fb operator != (Vec8f const & a, Vec8f const & b) {
+static inline Vec8fb operator != (Vec8f const a, Vec8f const b) {
     return Vec8fb(a.get_low() != b.get_low(), a.get_high() != b.get_high());
 }
 
 // vector operator < : returns true for elements for which a < b
-static inline Vec8fb operator < (Vec8f const & a, Vec8f const & b) {
+static inline Vec8fb operator < (Vec8f const a, Vec8f const b) {
     return Vec8fb(a.get_low() < b.get_low(), a.get_high() < b.get_high());
 }
 
 // vector operator <= : returns true for elements for which a <= b
-static inline Vec8fb operator <= (Vec8f const & a, Vec8f const & b) {
+static inline Vec8fb operator <= (Vec8f const a, Vec8f const b) {
     return Vec8fb(a.get_low() <= b.get_low(), a.get_high() <= b.get_high());
 }
 
 // vector operator > : returns true for elements for which a > b
-static inline Vec8fb operator > (Vec8f const & a, Vec8f const & b) {
+static inline Vec8fb operator > (Vec8f const a, Vec8f const b) {
     return Vec8fb(a.get_low() > b.get_low(), a.get_high() > b.get_high());
 }
 
 // vector operator >= : returns true for elements for which a >= b
-static inline Vec8fb operator >= (Vec8f const & a, Vec8f const & b) {
+static inline Vec8fb operator >= (Vec8f const a, Vec8f const b) {
     return Vec8fb(a.get_low() >= b.get_low(), a.get_high() >= b.get_high());
 }
 
 // Bitwise logical operators
 
 // vector operator & : bitwise and
-static inline Vec8f operator & (Vec8f const & a, Vec8f const & b) {
+static inline Vec8f operator & (Vec8f const a, Vec8f const b) {
     return Vec8f(a.get_low() & b.get_low(), a.get_high() & b.get_high());
 }
 
 // vector operator &= : bitwise and
-static inline Vec8f & operator &= (Vec8f & a, Vec8f const & b) {
+static inline Vec8f & operator &= (Vec8f & a, Vec8f const b) {
     a = a & b;
     return a;
 }
 
 // vector operator & : bitwise and of Vec8f and Vec8fb
-static inline Vec8f operator & (Vec8f const & a, Vec8fb const & b) {
+static inline Vec8f operator & (Vec8f const a, Vec8fb const b) {
     return Vec8f(a.get_low() & b.get_low(), a.get_high() & b.get_high());
 }
-static inline Vec8f operator & (Vec8fb const & a, Vec8f const & b) {
+static inline Vec8f operator & (Vec8fb const a, Vec8f const b) {
     return Vec8f(a.get_low() & b.get_low(), a.get_high() & b.get_high());
 }
 
 // vector operator | : bitwise or
-static inline Vec8f operator | (Vec8f const & a, Vec8f const & b) {
+static inline Vec8f operator | (Vec8f const a, Vec8f const b) {
     return Vec8f(a.get_low() | b.get_low(), a.get_high() | b.get_high());
 }
 
 // vector operator |= : bitwise or
-static inline Vec8f & operator |= (Vec8f & a, Vec8f const & b) {
+static inline Vec8f & operator |= (Vec8f & a, Vec8f const b) {
     a = a | b;
     return a;
 }
 
 // vector operator ^ : bitwise xor
-static inline Vec8f operator ^ (Vec8f const & a, Vec8f const & b) {
+static inline Vec8f operator ^ (Vec8f const a, Vec8f const b) {
     return Vec8f(a.get_low() ^ b.get_low(), a.get_high() ^ b.get_high());
 }
 
 // vector operator ^= : bitwise xor
-static inline Vec8f & operator ^= (Vec8f & a, Vec8f const & b) {
+static inline Vec8f & operator ^= (Vec8f & a, Vec8f const b) {
     a = a ^ b;
     return a;
 }
 
 // vector operator ! : logical not. Returns Boolean vector
-static inline Vec8fb operator ! (Vec8f const & a) {
+static inline Vec8fb operator ! (Vec8f const a) {
     return Vec8fb(!a.get_low(), !a.get_high());
 }
 
@@ -819,194 +822,188 @@ static inline Vec8fb operator ! (Vec8f const & a) {
 // Select between two operands. Corresponds to this pseudocode:
 // for (int i = 0; i < 8; i++) result[i] = s[i] ? a[i] : b[i];
 // Each byte in s must be either 0 (false) or 0xFFFFFFFF (true). No other values are allowed.
-static inline Vec8f select (Vec8fb const & s, Vec8f const & a, Vec8f const & b) {
+static inline Vec8f select (Vec8fb const s, Vec8f const a, Vec8f const b) {
     return Vec8f(select(s.get_low(),a.get_low(),b.get_low()), select(s.get_high(),a.get_high(),b.get_high()));
 }
 
 // Conditional add: For all vector elements i: result[i] = f[i] ? (a[i] + b[i]) : a[i]
-static inline Vec8f if_add (Vec8fb const & f, Vec8f const & a, Vec8f const & b) {
+static inline Vec8f if_add (Vec8fb const f, Vec8f const a, Vec8f const b) {
     return a + (Vec8f(f) & b);
 }
 
-// Conditional multiply: For all vector elements i: result[i] = f[i] ? (a[i] * b[i]) : a[i]
-static inline Vec8f if_mul (Vec8fb const & f, Vec8f const & a, Vec8f const & b) {
+// Conditional subtract
+static inline Vec8f if_sub (Vec8fb const f, Vec8f const a, Vec8f const b) {
+    return a - (Vec8f(f) & b);
+}
+
+// Conditional multiply
+static inline Vec8f if_mul (Vec8fb const f, Vec8f const a, Vec8f const b) {
     return a * select(f, b, 1.f);
 }
 
+// Conditional divide
+static inline Vec8f if_div (Vec8fb const f, Vec8f const a, Vec8f const b) {
+    return a / select(f, b, 1.f);
+}
 
 // General arithmetic functions, etc.
 
 // Horizontal add: Calculates the sum of all vector elements.
-static inline float horizontal_add (Vec8f const & a) {
+static inline float horizontal_add (Vec8f const a) {
     return horizontal_add(a.get_low() + a.get_high());
 }
 
 // function max: a > b ? a : b
-static inline Vec8f max(Vec8f const & a, Vec8f const & b) {
+static inline Vec8f max(Vec8f const a, Vec8f const b) {
     return Vec8f(max(a.get_low(),b.get_low()), max(a.get_high(),b.get_high()));
 }
 
 // function min: a < b ? a : b
-static inline Vec8f min(Vec8f const & a, Vec8f const & b) {
+static inline Vec8f min(Vec8f const a, Vec8f const b) {
     return Vec8f(min(a.get_low(),b.get_low()), min(a.get_high(),b.get_high()));
 }
+// NAN-safe versions of maximum and minimum are in vector_convert.h
 
 // function abs: absolute value
 // Removes sign bit, even for -0.0f, -INF and -NAN
-static inline Vec8f abs(Vec8f const & a) {
+static inline Vec8f abs(Vec8f const a) {
     return Vec8f(abs(a.get_low()), abs(a.get_high()));
 }
 
 // function sqrt: square root
-static inline Vec8f sqrt(Vec8f const & a) {
+static inline Vec8f sqrt(Vec8f const a) {
     return Vec8f(sqrt(a.get_low()), sqrt(a.get_high()));
 }
 
 // function square: a * a
-static inline Vec8f square(Vec8f const & a) {
+static inline Vec8f square(Vec8f const a) {
     return Vec8f(square(a.get_low()), square(a.get_high()));
 }
 
 // pow(Vec8f, int):
-template <typename TT> static Vec8f pow(Vec8f const & a, TT const & n);
+template <typename TT> static Vec8f pow(Vec8f const a, TT const n);
 
 // Raise floating point numbers to integer power n
 template <>
-inline Vec8f pow<int>(Vec8f const & x0, int const & n) {
+inline Vec8f pow<int>(Vec8f const x0, int const n) {
     return pow_template_i<Vec8f>(x0, n);
 }
 
 // allow conversion from unsigned int
 template <>
-inline Vec8f pow<uint32_t>(Vec8f const & x0, uint32_t const & n) {
+inline Vec8f pow<uint32_t>(Vec8f const x0, uint32_t const n) {
     return pow_template_i<Vec8f>(x0, (int)n);
 }
 
-
 // Raise floating point numbers to integer power n, where n is a compile-time constant
+// implement as function pow(vector, const_int)
 template <int n>
-static inline Vec8f pow_n(Vec8f const & a) {
-    return Vec8f(pow_n<n>(a.get_low()), pow_n<n>(a.get_high()));
-}
-
-template <int n>
-static inline Vec8f pow(Vec8f const & a, Const_int_t<n>) {
-    return pow_n<n>(a);
+static inline Vec8f pow(Vec8f const a, Const_int_t<n>) {
+    return pow_n<Vec8f, n>(a);
 }
 
 
 // function round: round to nearest integer (even). (result as float vector)
-static inline Vec8f round(Vec8f const & a) {
+static inline Vec8f round(Vec8f const a) {
     return Vec8f(round(a.get_low()), round(a.get_high()));
 }
 
 // function truncate: round towards zero. (result as float vector)
-static inline Vec8f truncate(Vec8f const & a) {
+static inline Vec8f truncate(Vec8f const a) {
     return Vec8f(truncate(a.get_low()), truncate(a.get_high()));
 }
 
 // function floor: round towards minus infinity. (result as float vector)
-static inline Vec8f floor(Vec8f const & a) {
+static inline Vec8f floor(Vec8f const a) {
     return Vec8f(floor(a.get_low()), floor(a.get_high()));
 }
 
 // function ceil: round towards plus infinity. (result as float vector)
-static inline Vec8f ceil(Vec8f const & a) {
+static inline Vec8f ceil(Vec8f const a) {
     return Vec8f(ceil(a.get_low()), ceil(a.get_high()));
 }
 
-#ifdef VECTORI256_H  // 256 bit integer vectors are available
-// function round_to_int: round to nearest integer (even). (result as integer vector)
-static inline Vec8i round_to_int(Vec8f const & a) {
-    // Note: assume MXCSR control register is set to rounding
-    return Vec8i(round_to_int(a.get_low()), round_to_int(a.get_high()));
+// function roundi: round to nearest integer (even). (result as integer vector)
+static inline Vec8i roundi(Vec8f const a) {
+    return Vec8i(roundi(a.get_low()), roundi(a.get_high()));
 }
 
-// function truncate_to_int: round towards zero. (result as integer vector)
-static inline Vec8i truncate_to_int(Vec8f const & a) {
-    return Vec8i(truncate_to_int(a.get_low()), truncate_to_int(a.get_high()));
+// function truncatei: round towards zero. (result as integer vector)
+static inline Vec8i truncatei(Vec8f const a) {
+    return Vec8i(truncatei(a.get_low()), truncatei(a.get_high()));
 }
 
 // function to_float: convert integer vector to float vector
-static inline Vec8f to_float(Vec8i const & a) {
+static inline Vec8f to_float(Vec8i const a) {
     return Vec8f(to_float(a.get_low()), to_float(a.get_high()));
 }
 
 // function to_float: convert unsigned integer vector to float vector
-static inline Vec8f to_float(Vec8ui const & a) {
+static inline Vec8f to_float(Vec8ui const a) {
     return Vec8f(to_float(a.get_low()), to_float(a.get_high()));
 }
 
-#endif // VECTORI256_H 
-
 
 // Approximate math functions
 
 // approximate reciprocal (Faster than 1.f / a. relative accuracy better than 2^-11)
-static inline Vec8f approx_recipr(Vec8f const & a) {
+static inline Vec8f approx_recipr(Vec8f const a) {
     return Vec8f(approx_recipr(a.get_low()), approx_recipr(a.get_high()));
 }
 
 // approximate reciprocal squareroot (Faster than 1.f / sqrt(a). Relative accuracy better than 2^-11)
-static inline Vec8f approx_rsqrt(Vec8f const & a) {
+static inline Vec8f approx_rsqrt(Vec8f const a) {
     return Vec8f(approx_rsqrt(a.get_low()), approx_rsqrt(a.get_high()));
 }
 
 // Fused multiply and add functions
 
 // Multiply and add
-static inline Vec8f mul_add(Vec8f const & a, Vec8f const & b, Vec8f const & c) {
+static inline Vec8f mul_add(Vec8f const a, Vec8f const b, Vec8f const c) {
     return Vec8f(mul_add(a.get_low(),b.get_low(),c.get_low()), mul_add(a.get_high(),b.get_high(),c.get_high()));
 }
 
 // Multiply and subtract
-static inline Vec8f mul_sub(Vec8f const & a, Vec8f const & b, Vec8f const & c) {
+static inline Vec8f mul_sub(Vec8f const a, Vec8f const b, Vec8f const c) {
     return Vec8f(mul_sub(a.get_low(),b.get_low(),c.get_low()), mul_sub(a.get_high(),b.get_high(),c.get_high()));
 }
 
 // Multiply and inverse subtract
-static inline Vec8f nmul_add(Vec8f const & a, Vec8f const & b, Vec8f const & c) {
+static inline Vec8f nmul_add(Vec8f const a, Vec8f const b, Vec8f const c) {
     return Vec8f(nmul_add(a.get_low(),b.get_low(),c.get_low()), nmul_add(a.get_high(),b.get_high(),c.get_high()));
 }
 
 
-// Multiply and subtract with extra precision on the intermediate calculations, 
-// even if FMA instructions not supported, using Veltkamp-Dekker split
-static inline Vec8f mul_sub_x(Vec8f const & a, Vec8f const & b, Vec8f const & c) {
+// Multiply and subtract with extra precision on the intermediate calculations, used internally
+static inline Vec8f mul_sub_x(Vec8f const a, Vec8f const b, Vec8f const c) {
     return Vec8f(mul_sub_x(a.get_low(),b.get_low(),c.get_low()), mul_sub_x(a.get_high(),b.get_high(),c.get_high()));
 }
 
-
 // Math functions using fast bit manipulation
 
-#ifdef VECTORI256_H  // 256 bit integer vectors are available
 // Extract the exponent as an integer
 // exponent(a) = floor(log2(abs(a)));
 // exponent(1.0f) = 0, exponent(0.0f) = -127, exponent(INF) = +128, exponent(NAN) = +128
-static inline Vec8i exponent(Vec8f const & a) {
+static inline Vec8i exponent(Vec8f const a) {
     return Vec8i(exponent(a.get_low()), exponent(a.get_high()));
 }
-#endif
-
-// Extract the fraction part of a floating point number
-// a = 2^exponent(a) * fraction(a), except for a = 0
-// fraction(1.0f) = 1.0f, fraction(5.0f) = 1.25f 
-static inline Vec8f fraction(Vec8f const & a) {
-    return Vec8f(fraction(a.get_low()), fraction(a.get_high()));
-}
 
-#ifdef VECTORI256_H  // 256 bit integer vectors are available
 // Fast calculation of pow(2,n) with n integer
 // n  =    0 gives 1.0f
 // n >=  128 gives +INF
 // n <= -127 gives 0.0f
 // This function will never produce denormals, and never raise exceptions
-static inline Vec8f exp2(Vec8i const & a) {
+static inline Vec8f exp2(Vec8i const a) {
     return Vec8f(exp2(a.get_low()), exp2(a.get_high()));
 }
-//static Vec8f exp2(Vec8f const & x); // defined in vectormath_exp.h
-#endif // VECTORI256_H
+//static Vec8f exp2(Vec8f const x); // defined in vectormath_exp.h
 
+// Extract the fraction part of a floating point number
+// a = 2^exponent(a) * fraction(a), except for a = 0
+// fraction(1.0f) = 1.0f, fraction(5.0f) = 1.25f
+static inline Vec8f fraction(Vec8f const a) {
+    return Vec8f(fraction(a.get_low()), fraction(a.get_high()));
+}
 
 
 // Categorization functions
@@ -1015,63 +1012,63 @@ static inline Vec8f exp2(Vec8i const & a) {
 // even for -0.0f, -INF and -NAN
 // Note that sign_bit(Vec8f(-0.0f)) gives true, while Vec8f(-0.0f) < Vec8f(0.0f) gives false
 // (the underscore in the name avoids a conflict with a macro in Intel's mathimf.h)
-static inline Vec8fb sign_bit(Vec8f const & a) {
+static inline Vec8fb sign_bit(Vec8f const a) {
     return Vec8fb(sign_bit(a.get_low()), sign_bit(a.get_high()));
 }
 
 // Function sign_combine: changes the sign of a when b has the sign bit set
 // same as select(sign_bit(b), -a, a)
-static inline Vec8f sign_combine(Vec8f const & a, Vec8f const & b) {
+static inline Vec8f sign_combine(Vec8f const a, Vec8f const b) {
     return Vec8f(sign_combine(a.get_low(), b.get_low()), sign_combine(a.get_high(), b.get_high()));
 }
 
-// Function is_finite: gives true for elements that are normal, denormal or zero, 
+// Function is_finite: gives true for elements that are normal, denormal or zero,
 // false for INF and NAN
 // (the underscore in the name avoids a conflict with a macro in Intel's mathimf.h)
-static inline Vec8fb is_finite(Vec8f const & a) {
+static inline Vec8fb is_finite(Vec8f const a) {
     return Vec8fb(is_finite(a.get_low()), is_finite(a.get_high()));
 }
 
 // Function is_inf: gives true for elements that are +INF or -INF
 // false for finite numbers and NAN
 // (the underscore in the name avoids a conflict with a macro in Intel's mathimf.h)
-static inline Vec8fb is_inf(Vec8f const & a) {
+static inline Vec8fb is_inf(Vec8f const a) {
     return Vec8fb(is_inf(a.get_low()), is_inf(a.get_high()));
 }
 
 // Function is_nan: gives true for elements that are +NAN or -NAN
 // false for finite numbers and +/-INF
 // (the underscore in the name avoids a conflict with a macro in Intel's mathimf.h)
-static inline Vec8fb is_nan(Vec8f const & a) {
+static inline Vec8fb is_nan(Vec8f const a) {
     return Vec8fb(is_nan(a.get_low()), is_nan(a.get_high()));
 }
 
 // Function is_subnormal: gives true for elements that are denormal (subnormal)
 // false for finite numbers, zero, NAN and INF
-static inline Vec8fb is_subnormal(Vec8f const & a) {
+static inline Vec8fb is_subnormal(Vec8f const a) {
     return Vec8fb(is_subnormal(a.get_low()), is_subnormal(a.get_high()));
 }
 
 // Function is_zero_or_subnormal: gives true for elements that are zero or subnormal (denormal)
 // false for finite numbers, NAN and INF
-static inline Vec8fb is_zero_or_subnormal(Vec8f const & a) {
+static inline Vec8fb is_zero_or_subnormal(Vec8f const a) {
     return Vec8fb(is_zero_or_subnormal(a.get_low()), is_zero_or_subnormal(a.get_high()));
 }
 
 // Function infinite4f: returns a vector where all elements are +INF
 static inline Vec8f infinite8f() {
-    return constant8f<0x7F800000,0x7F800000,0x7F800000,0x7F800000,0x7F800000,0x7F800000,0x7F800000,0x7F800000>();
+    return Vec8f(infinite4f(),infinite4f());
 }
 
 // Function nan4f: returns a vector where all elements are +NAN (quiet)
-static inline Vec8f nan8f(int n = 0x10) {
+static inline Vec8f nan8f(uint32_t n = 0x10) {
     return Vec8f(nan4f(n), nan4f(n));
 }
 
 // change signs on vectors Vec8f
 // Each index i0 - i7 is 1 for changing sign on the corresponding element, 0 for no change
 template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7>
-static inline Vec8f change_sign(Vec8f const & a) {
+inline Vec8f change_sign(Vec8f const a) {
     if ((i0 | i1 | i2 | i3 | i4 | i5 | i6 | i7) == 0) return a;
     Vec4f lo = change_sign<i0,i1,i2,i3>(a.get_low());
     Vec4f hi = change_sign<i4,i5,i6,i7>(a.get_high());
@@ -1088,28 +1085,27 @@ static inline Vec8f change_sign(Vec8f const & a) {
 class Vec4d : public Vec256de {
 public:
     // Default constructor:
-    Vec4d() {
-    }
+    Vec4d() = default;
     // Constructor to broadcast the same value into all elements:
     Vec4d(double d) {
         y1 = y0 = _mm_set1_pd(d);
     }
     // Constructor to build from all elements:
     Vec4d(double d0, double d1, double d2, double d3) {
-        y0 = _mm_setr_pd(d0, d1); 
-        y1 = _mm_setr_pd(d2, d3); 
+        y0 = _mm_setr_pd(d0, d1);
+        y1 = _mm_setr_pd(d2, d3);
     }
     // Constructor to build from two Vec4f:
-    Vec4d(Vec2d const & a0, Vec2d const & a1) {
+    Vec4d(Vec2d const a0, Vec2d const a1) {
         y0 = a0;  y1 = a1;
     }
     // Constructor to convert from type Vec256de
-    Vec4d(Vec256de const & x) {
+    Vec4d(Vec256de const x) {
         y0 = x.get_low();
         y1 = x.get_high();
     }
     // Assignment operator to convert from type Vec256de
-    Vec4d & operator = (Vec256de const & x) {
+    Vec4d & operator = (Vec256de const x) {
         y0 = x.get_low();
         y1 = x.get_high();
         return *this;
@@ -1133,13 +1129,19 @@ public:
         _mm_storeu_pd(p,   y0);
         _mm_storeu_pd(p+2, y1);
     }
-    // Member function to store into array, aligned by 32
+    // Member function storing into array, aligned by 32
     // You may use store_a instead of store if you are certain that p points to an address
     // divisible by 32
     void store_a(double * p) const {
         _mm_store_pd(p,   y0);
         _mm_store_pd(p+2, y1);
     }
+    // Member function storing to aligned uncached memory (non-temporal store).
+    // Note: Will generate runtime error if p is not aligned by 32
+    void store_nt(double * p) const {
+        _mm_stream_pd(p,   y0);
+        _mm_stream_pd(p+2, y1);
+    }
     // Partial load. Load n elements and set the rest to 0
     Vec4d & load_partial(int n, double const * p) {
         if (n > 0 && n <= 2) {
@@ -1173,11 +1175,10 @@ public:
             y1 = Vec2d(0.0);
         }
         return *this;
-    }    
+    }
     // Member function to change a single element in vector
-    // Note: This function is inefficient. Use load function if changing more than one element
-    Vec4d const & insert(uint32_t index, double value) {
-        if (index < 2) {
+    Vec4d const insert(int index, double value) {
+        if ((uint32_t)index < 2) {
             y0 = Vec2d(y0).insert(index, value);
         }
         else {
@@ -1186,9 +1187,8 @@ public:
         return *this;
     }
     // Member function extract a single element from vector
-    // Note: This function is inefficient. Use store function if extracting more than one element
-    double extract(uint32_t index) const {
-        if (index < 2) {
+    double extract(int index) const {
+        if ((uint32_t)index < 2) {
             return Vec2d(y0).extract(index);
         }
         else {
@@ -1197,7 +1197,7 @@ public:
     }
     // Extract a single element. Use store function if extracting more than one element.
     // Operator [] can only read an element, not write.
-    double operator [] (uint32_t index) const {
+    double operator [] (int index) const {
         return extract(index);
     }
     // Member functions to split into two Vec2d:
@@ -1207,13 +1207,15 @@ public:
     Vec2d get_high() const {
         return y1;
     }
-    static int size () {
+    static constexpr int size() {
         return 4;
     }
+    static constexpr int elementtype() {
+        return 17;
+    }
 };
 
 
-
 /*****************************************************************************
 *
 *          Operators for Vec4d
@@ -1221,20 +1223,20 @@ public:
 *****************************************************************************/
 
 // vector operator + : add element by element
-static inline Vec4d operator + (Vec4d const & a, Vec4d const & b) {
+static inline Vec4d operator + (Vec4d const a, Vec4d const b) {
     return Vec4d(a.get_low() + b.get_low(), a.get_high() + b.get_high());
 }
 
 // vector operator + : add vector and scalar
-static inline Vec4d operator + (Vec4d const & a, double b) {
+static inline Vec4d operator + (Vec4d const a, double b) {
     return a + Vec4d(b);
 }
-static inline Vec4d operator + (double a, Vec4d const & b) {
+static inline Vec4d operator + (double a, Vec4d const b) {
     return Vec4d(a) + b;
 }
 
 // vector operator += : add
-static inline Vec4d & operator += (Vec4d & a, Vec4d const & b) {
+static inline Vec4d & operator += (Vec4d & a, Vec4d const b) {
     a = a + b;
     return a;
 }
@@ -1253,26 +1255,26 @@ static inline Vec4d & operator ++ (Vec4d & a) {
 }
 
 // vector operator - : subtract element by element
-static inline Vec4d operator - (Vec4d const & a, Vec4d const & b) {
+static inline Vec4d operator - (Vec4d const a, Vec4d const b) {
     return Vec4d(a.get_low() - b.get_low(), a.get_high() - b.get_high());
 }
 
 // vector operator - : subtract vector and scalar
-static inline Vec4d operator - (Vec4d const & a, double b) {
+static inline Vec4d operator - (Vec4d const a, double b) {
     return a - Vec4d(b);
 }
-static inline Vec4d operator - (double a, Vec4d const & b) {
+static inline Vec4d operator - (double a, Vec4d const b) {
     return Vec4d(a) - b;
 }
 
 // vector operator - : unary minus
 // Change sign bit, even for 0, INF and NAN
-static inline Vec4d operator - (Vec4d const & a) {
+static inline Vec4d operator - (Vec4d const a) {
     return Vec4d(-a.get_low(), -a.get_high());
 }
 
 // vector operator -= : subtract
-static inline Vec4d & operator -= (Vec4d & a, Vec4d const & b) {
+static inline Vec4d & operator -= (Vec4d & a, Vec4d const b) {
     a = a - b;
     return a;
 }
@@ -1291,118 +1293,118 @@ static inline Vec4d & operator -- (Vec4d & a) {
 }
 
 // vector operator * : multiply element by element
-static inline Vec4d operator * (Vec4d const & a, Vec4d const & b) {
+static inline Vec4d operator * (Vec4d const a, Vec4d const b) {
     return Vec4d(a.get_low() * b.get_low(), a.get_high() * b.get_high());
 }
 
 // vector operator * : multiply vector and scalar
-static inline Vec4d operator * (Vec4d const & a, double b) {
+static inline Vec4d operator * (Vec4d const a, double b) {
     return a * Vec4d(b);
 }
-static inline Vec4d operator * (double a, Vec4d const & b) {
+static inline Vec4d operator * (double a, Vec4d const b) {
     return Vec4d(a) * b;
 }
 
 // vector operator *= : multiply
-static inline Vec4d & operator *= (Vec4d & a, Vec4d const & b) {
+static inline Vec4d & operator *= (Vec4d & a, Vec4d const b) {
     a = a * b;
     return a;
 }
 
 // vector operator / : divide all elements by same integer
-static inline Vec4d operator / (Vec4d const & a, Vec4d const & b) {
+static inline Vec4d operator / (Vec4d const a, Vec4d const b) {
     return Vec4d(a.get_low() / b.get_low(), a.get_high() / b.get_high());
 }
 
 // vector operator / : divide vector and scalar
-static inline Vec4d operator / (Vec4d const & a, double b) {
+static inline Vec4d operator / (Vec4d const a, double b) {
     return a / Vec4d(b);
 }
-static inline Vec4d operator / (double a, Vec4d const & b) {
+static inline Vec4d operator / (double a, Vec4d const b) {
     return Vec4d(a) / b;
 }
 
 // vector operator /= : divide
-static inline Vec4d & operator /= (Vec4d & a, Vec4d const & b) {
+static inline Vec4d & operator /= (Vec4d & a, Vec4d const b) {
     a = a / b;
     return a;
 }
 
 // vector operator == : returns true for elements for which a == b
-static inline Vec4db operator == (Vec4d const & a, Vec4d const & b) {
+static inline Vec4db operator == (Vec4d const a, Vec4d const b) {
     return Vec4db(a.get_low() == b.get_low(), a.get_high() == b.get_high());
 }
 
 // vector operator != : returns true for elements for which a != b
-static inline Vec4db operator != (Vec4d const & a, Vec4d const & b) {
+static inline Vec4db operator != (Vec4d const a, Vec4d const b) {
     return Vec4db(a.get_low() != b.get_low(), a.get_high() != b.get_high());
 }
 
 // vector operator < : returns true for elements for which a < b
-static inline Vec4db operator < (Vec4d const & a, Vec4d const & b) {
+static inline Vec4db operator < (Vec4d const a, Vec4d const b) {
     return Vec4db(a.get_low() < b.get_low(), a.get_high() < b.get_high());
 }
 
 // vector operator <= : returns true for elements for which a <= b
-static inline Vec4db operator <= (Vec4d const & a, Vec4d const & b) {
+static inline Vec4db operator <= (Vec4d const a, Vec4d const b) {
     return Vec4db(a.get_low() <= b.get_low(), a.get_high() <= b.get_high());
 }
 
 // vector operator > : returns true for elements for which a > b
-static inline Vec4db operator > (Vec4d const & a, Vec4d const & b) {
+static inline Vec4db operator > (Vec4d const a, Vec4d const b) {
     return Vec4db(a.get_low() > b.get_low(), a.get_high() > b.get_high());
 }
 
 // vector operator >= : returns true for elements for which a >= b
-static inline Vec4db operator >= (Vec4d const & a, Vec4d const & b) {
+static inline Vec4db operator >= (Vec4d const a, Vec4d const b) {
     return Vec4db(a.get_low() >= b.get_low(), a.get_high() >= b.get_high());
 }
 
 // Bitwise logical operators
 
 // vector operator & : bitwise and
-static inline Vec4d operator & (Vec4d const & a, Vec4d const & b) {
+static inline Vec4d operator & (Vec4d const a, Vec4d const b) {
     return Vec4d(a.get_low() & b.get_low(), a.get_high() & b.get_high());
 }
 
 // vector operator &= : bitwise and
-static inline Vec4d & operator &= (Vec4d & a, Vec4d const & b) {
+static inline Vec4d & operator &= (Vec4d & a, Vec4d const b) {
     a = a & b;
     return a;
 }
 
 // vector operator & : bitwise and of Vec4d and Vec4db
-static inline Vec4d operator & (Vec4d const & a, Vec4db const & b) {
+static inline Vec4d operator & (Vec4d const a, Vec4db const b) {
     return Vec4d(a.get_low() & b.get_low(), a.get_high() & b.get_high());
 }
-static inline Vec4d operator & (Vec4db const & a, Vec4d const & b) {
+static inline Vec4d operator & (Vec4db const a, Vec4d const b) {
     return Vec4d(a.get_low() & b.get_low(), a.get_high() & b.get_high());
 }
 
 // vector operator | : bitwise or
-static inline Vec4d operator | (Vec4d const & a, Vec4d const & b) {
+static inline Vec4d operator | (Vec4d const a, Vec4d const b) {
     return Vec4d(a.get_low() | b.get_low(), a.get_high() | b.get_high());
 }
 
 // vector operator |= : bitwise or
-static inline Vec4d & operator |= (Vec4d & a, Vec4d const & b) {
+static inline Vec4d & operator |= (Vec4d & a, Vec4d const b) {
     a = a | b;
     return a;
 }
 
 // vector operator ^ : bitwise xor
-static inline Vec4d operator ^ (Vec4d const & a, Vec4d const & b) {
+static inline Vec4d operator ^ (Vec4d const a, Vec4d const b) {
     return Vec4d(a.get_low() ^ b.get_low(), a.get_high() ^ b.get_high());
 }
 
 // vector operator ^= : bitwise xor
-static inline Vec4d & operator ^= (Vec4d & a, Vec4d const & b) {
+static inline Vec4d & operator ^= (Vec4d & a, Vec4d const b) {
     a = a ^ b;
     return a;
 }
 
 // vector operator ! : logical not. Returns Boolean vector
-static inline Vec4db operator ! (Vec4d const & a) {
+static inline Vec4db operator ! (Vec4d const a) {
     return Vec4db(!a.get_low(), !a.get_high());
 }
 
@@ -1415,173 +1417,164 @@ static inline Vec4db operator ! (Vec4d const & a) {
 
 // Select between two operands. Corresponds to this pseudocode:
 // for (int i = 0; i < 2; i++) result[i] = s[i] ? a[i] : b[i];
-// Each byte in s must be either 0 (false) or 0xFFFFFFFFFFFFFFFF (true). 
+// Each byte in s must be either 0 (false) or 0xFFFFFFFFFFFFFFFF (true).
 // No other values are allowed.
-static inline Vec4d select (Vec4db const & s, Vec4d const & a, Vec4d const & b) {
+static inline Vec4d select (Vec4db const s, Vec4d const a, Vec4d const b) {
     return Vec4d(select(s.get_low(), a.get_low(), b.get_low()), select(s.get_high(), a.get_high(), b.get_high()));
 }
 
 // Conditional add: For all vector elements i: result[i] = f[i] ? (a[i] + b[i]) : a[i]
-static inline Vec4d if_add (Vec4db const & f, Vec4d const & a, Vec4d const & b) {
+static inline Vec4d if_add (Vec4db const f, Vec4d const a, Vec4d const b) {
     return a + (Vec4d(f) & b);
 }
 
-// Conditional multiply: For all vector elements i: result[i] = f[i] ? (a[i] * b[i]) : a[i]
-static inline Vec4d if_mul (Vec4db const & f, Vec4d const & a, Vec4d const & b) {
+// Conditional subtract
+static inline Vec4d if_sub (Vec4db const f, Vec4d const a, Vec4d const b) {
+    return a - (Vec4d(f) & b);
+}
+
+// Conditional multiply
+static inline Vec4d if_mul (Vec4db const f, Vec4d const a, Vec4d const b) {
     return a * select(f, b, 1.f);
 }
 
+// Conditional divide
+static inline Vec4d if_div (Vec4db const f, Vec4d const a, Vec4d const b) {
+    return a / select(f, b, 1.);
+}
+
+
 // General arithmetic functions, etc.
 
 // Horizontal add: Calculates the sum of all vector elements.
-static inline double horizontal_add (Vec4d const & a) {
+static inline double horizontal_add (Vec4d const a) {
     return horizontal_add(a.get_low() + a.get_high());
 }
 
 // function max: a > b ? a : b
-static inline Vec4d max(Vec4d const & a, Vec4d const & b) {
+static inline Vec4d max(Vec4d const a, Vec4d const b) {
     return Vec4d(max(a.get_low(),b.get_low()), max(a.get_high(),b.get_high()));
 }
 
 // function min: a < b ? a : b
-static inline Vec4d min(Vec4d const & a, Vec4d const & b) {
+static inline Vec4d min(Vec4d const a, Vec4d const b) {
     return Vec4d(min(a.get_low(),b.get_low()), min(a.get_high(),b.get_high()));
 }
+// NAN-safe versions of maximum and minimum are in vector_convert.h
 
 // function abs: absolute value
 // Removes sign bit, even for -0.0f, -INF and -NAN
-static inline Vec4d abs(Vec4d const & a) {
+static inline Vec4d abs(Vec4d const a) {
     return Vec4d(abs(a.get_low()), abs(a.get_high()));
 }
 
 // function sqrt: square root
-static inline Vec4d sqrt(Vec4d const & a) {
+static inline Vec4d sqrt(Vec4d const a) {
     return Vec4d(sqrt(a.get_low()), sqrt(a.get_high()));
 }
 
 // function square: a * a
-static inline Vec4d square(Vec4d const & a) {
+static inline Vec4d square(Vec4d const a) {
     return Vec4d(square(a.get_low()), square(a.get_high()));
 }
 
 // pow(Vec4d, int):
 // Raise floating point numbers to integer power n
-template <typename TT> static Vec4d pow(Vec4d const & a, TT const & n);
+template <typename TT> static Vec4d pow(Vec4d const a, TT const n);
 
 // Raise floating point numbers to integer power n
 template <>
-inline Vec4d pow<int>(Vec4d const & x0, int const & n) {
+inline Vec4d pow<int>(Vec4d const x0, int const n) {
     return pow_template_i<Vec4d>(x0, n);
 }
 
 // allow conversion from unsigned int
 template <>
-inline Vec4d pow<uint32_t>(Vec4d const & x0, uint32_t const & n) {
+inline Vec4d pow<uint32_t>(Vec4d const x0, uint32_t const n) {
     return pow_template_i<Vec4d>(x0, (int)n);
 }
 
-
 // Raise floating point numbers to integer power n, where n is a compile-time constant
+// implement as function pow(vector, const_int)
 template <int n>
-static inline Vec4d pow_n(Vec4d const & a) {
-    return Vec4d(pow_n<n>(a.get_low()), pow_n<n>(a.get_high()));
-}
-
-template <int n>
-static inline Vec4d pow(Vec4d const & a, Const_int_t<n>) {
-    return pow_n<n>(a);
+static inline Vec4d pow(Vec4d const a, Const_int_t<n>) {
+    return pow_n<Vec4d, n>(a);
 }
 
 
 // function round: round to nearest integer (even). (result as double vector)
-static inline Vec4d round(Vec4d const & a) {
+static inline Vec4d round(Vec4d const a) {
     return Vec4d(round(a.get_low()), round(a.get_high()));
 }
 
 // function truncate: round towards zero. (result as double vector)
-static inline Vec4d truncate(Vec4d const & a) {
+static inline Vec4d truncate(Vec4d const a) {
     return Vec4d(truncate(a.get_low()), truncate(a.get_high()));
 }
 
 // function floor: round towards minus infinity. (result as double vector)
-static inline Vec4d floor(Vec4d const & a) {
+static inline Vec4d floor(Vec4d const a) {
     return Vec4d(floor(a.get_low()), floor(a.get_high()));
 }
 
 // function ceil: round towards plus infinity. (result as double vector)
-static inline Vec4d ceil(Vec4d const & a) {
+static inline Vec4d ceil(Vec4d const a) {
     return Vec4d(ceil(a.get_low()), ceil(a.get_high()));
 }
 
-// function round_to_int: round to nearest integer (even). (result as integer vector)
-static inline Vec4i round_to_int(Vec4d const & a) {
-    // Note: assume MXCSR control register is set to rounding
-    return round_to_int(a.get_low(), a.get_high());
+// function round_to_int32: round to nearest integer (even). (result as integer vector)
+static inline Vec4i round_to_int32(Vec4d const a) {
+    return round_to_int32(a.get_low(), a.get_high());
 }
 
-// function truncate_to_int: round towards zero. (result as integer vector)
-static inline Vec4i truncate_to_int(Vec4d const & a) {
-    return truncate_to_int(a.get_low(), a.get_high());
+// function truncate_to_int32: round towards zero. (result as integer vector)
+static inline Vec4i truncate_to_int32(Vec4d const a) {
+    return truncate_to_int32(a.get_low(), a.get_high());
 }
 
-#ifdef VECTORI256_H  // 256 bit integer vectors are available
-
-// function truncate_to_int64: round towards zero. (inefficient)
-static inline Vec4q truncate_to_int64(Vec4d const & a) {
+// function truncatei: round towards zero. (inefficient)
+static inline Vec4q truncatei(Vec4d const a) {
     double aa[4];
     a.store(aa);
     return Vec4q(int64_t(aa[0]), int64_t(aa[1]), int64_t(aa[2]), int64_t(aa[3]));
 }
 
-// function truncate_to_int64_limited: round towards zero.
-// result as 64-bit integer vector, but with limited range
-static inline Vec4q truncate_to_int64_limited(Vec4d const & a) {
-    return Vec4q(truncate_to_int64_limited(a.get_low()), truncate_to_int64_limited(a.get_high()));
-}
-
-// function round_to_int64: round to nearest or even. (inefficient)
-static inline Vec4q round_to_int64(Vec4d const & a) {
-    return truncate_to_int64(round(a));
-}
-
-// function round_to_int64_limited: round to nearest integer
-// result as 64-bit integer vector, but with limited range
-static inline Vec4q round_to_int64_limited(Vec4d const & a) {
-    return Vec4q(round_to_int64_limited(a.get_low()), round_to_int64_limited(a.get_high()));
+// function roundi: round to nearest or even. (inefficient)
+static inline Vec4q roundi(Vec4d const a) {
+    return truncatei(round(a));
 }
 
 // function to_double: convert integer vector elements to double vector (inefficient)
-static inline Vec4d to_double(Vec4q const & a) {
+static inline Vec4d to_double(Vec4q const a) {
     int64_t aa[4];
     a.store(aa);
     return Vec4d(double(aa[0]), double(aa[1]), double(aa[2]), double(aa[3]));
 }
 
-// function to_double_limited: convert integer vector elements to double vector
-// limited to abs(x) < 2^31
-static inline Vec4d to_double_limited(Vec4q const & x) {
-    return Vec4d(to_double_limited(x.get_low()),to_double_limited(x.get_high()));
+// function to_double: convert unsigned integer vector elements to double vector (inefficient)
+static inline Vec4d to_double(Vec4uq const a) {
+    uint64_t aa[4];
+    a.store(aa);
+    return Vec4d(double(aa[0]), double(aa[1]), double(aa[2]), double(aa[3]));
 }
 
-#endif  // VECTORI256_H
-
 // function to_double: convert integer vector to double vector
-static inline Vec4d to_double(Vec4i const & a) {
+static inline Vec4d to_double(Vec4i const a) {
     return Vec4d(to_double_low(a), to_double_high(a));
 }
 
 // function compress: convert two Vec4d to one Vec8f
-static inline Vec8f compress (Vec4d const & low, Vec4d const & high) {
+static inline Vec8f compress (Vec4d const low, Vec4d const high) {
     return Vec8f(compress(low.get_low(), low.get_high()), compress(high.get_low(), high.get_high()));
 }
 
 // Function extend_low : convert Vec8f vector elements 0 - 3 to Vec4d
-static inline Vec4d extend_low (Vec8f const & a) {
+static inline Vec4d extend_low (Vec8f const a) {
     return Vec4d(extend_low(a.get_low()), extend_high(a.get_low()));
 }
 
 // Function extend_high : convert Vec8f vector elements 4 - 7 to Vec4d
-static inline Vec4d extend_high (Vec8f const & a) {
+static inline Vec4d extend_high (Vec8f const a) {
     return Vec4d(extend_low(a.get_high()), extend_high(a.get_high()));
 }
 
@@ -1589,41 +1582,39 @@ static inline Vec4d extend_high (Vec8f const & a) {
 // Fused multiply and add functions
 
 // Multiply and add
-static inline Vec4d mul_add(Vec4d const & a, Vec4d const & b, Vec4d const & c) {
+static inline Vec4d mul_add(Vec4d const a, Vec4d const b, Vec4d const c) {
     return Vec4d(mul_add(a.get_low(),b.get_low(),c.get_low()), mul_add(a.get_high(),b.get_high(),c.get_high()));
 }
 
 // Multiply and subtract
-static inline Vec4d mul_sub(Vec4d const & a, Vec4d const & b, Vec4d const & c) {
+static inline Vec4d mul_sub(Vec4d const a, Vec4d const b, Vec4d const c) {
     return Vec4d(mul_sub(a.get_low(),b.get_low(),c.get_low()), mul_sub(a.get_high(),b.get_high(),c.get_high()));
 }
 
 // Multiply and inverse subtract
-static inline Vec4d nmul_add(Vec4d const & a, Vec4d const & b, Vec4d const & c) {
+static inline Vec4d nmul_add(Vec4d const a, Vec4d const b, Vec4d const c) {
     return Vec4d(nmul_add(a.get_low(),b.get_low(),c.get_low()), nmul_add(a.get_high(),b.get_high(),c.get_high()));
 }
 
-// Multiply and subtract with extra precision on the intermediate calculations, 
+// Multiply and subtract with extra precision on the intermediate calculations,
 // even if FMA instructions not supported, using Veltkamp-Dekker split
-static inline Vec4d mul_sub_x(Vec4d const & a, Vec4d const & b, Vec4d const & c) {
+static inline Vec4d mul_sub_x(Vec4d const a, Vec4d const b, Vec4d const c) {
     return Vec4d(mul_sub_x(a.get_low(),b.get_low(),c.get_low()), mul_sub_x(a.get_high(),b.get_high(),c.get_high()));
 }
 
-
 // Math functions using fast bit manipulation
 
-#ifdef VECTORI256_H  // 256 bit integer vectors are available, AVX2
 // Extract the exponent as an integer
 // exponent(a) = floor(log2(abs(a)));
 // exponent(1.0) = 0, exponent(0.0) = -1023, exponent(INF) = +1024, exponent(NAN) = +1024
-static inline Vec4q exponent(Vec4d const & a) {
+static inline Vec4q exponent(Vec4d const a) {
     return Vec4q(exponent(a.get_low()), exponent(a.get_high()));
 }
 
 // Extract the fraction part of a floating point number
 // a = 2^exponent(a) * fraction(a), except for a = 0
-// fraction(1.0) = 1.0, fraction(5.0) = 1.25 
-static inline Vec4d fraction(Vec4d const & a) {
+// fraction(1.0) = 1.0, fraction(5.0) = 1.25
+static inline Vec4d fraction(Vec4d const a) {
     return Vec4d(fraction(a.get_low()), fraction(a.get_high()));
 }
 
@@ -1632,11 +1623,10 @@ static inline Vec4d fraction(Vec4d const & a) {
 // n >=  1024 gives +INF
 // n <= -1023 gives 0.0
 // This function will never produce denormals, and never raise exceptions
-static inline Vec4d exp2(Vec4q const & a) {
+static inline Vec4d exp2(Vec4q const a) {
     return Vec4d(exp2(a.get_low()), exp2(a.get_high()));
 }
-//static Vec4d exp2(Vec4d const & x); // defined in vectormath_exp.h
-#endif
+//static Vec4d exp2(Vec4d const x); // defined in vectormath_exp.h
 
 
 // Categorization functions
@@ -1644,43 +1634,43 @@ static inline Vec4d exp2(Vec4q const & a) {
 // Function sign_bit: gives true for elements that have the sign bit set
 // even for -0.0, -INF and -NAN
 // Note that sign_bit(Vec4d(-0.0)) gives true, while Vec4d(-0.0) < Vec4d(0.0) gives false
-static inline Vec4db sign_bit(Vec4d const & a) {
+static inline Vec4db sign_bit(Vec4d const a) {
     return Vec4db(sign_bit(a.get_low()), sign_bit(a.get_high()));
 }
 
 // Function sign_combine: changes the sign of a when b has the sign bit set
 // same as select(sign_bit(b), -a, a)
-static inline Vec4d sign_combine(Vec4d const & a, Vec4d const & b) {
+static inline Vec4d sign_combine(Vec4d const a, Vec4d const b) {
     return Vec4d(sign_combine(a.get_low(), b.get_low()), sign_combine(a.get_high(), b.get_high()));
 }
 
-// Function is_finite: gives true for elements that are normal, denormal or zero, 
+// Function is_finite: gives true for elements that are normal, denormal or zero,
 // false for INF and NAN
-static inline Vec4db is_finite(Vec4d const & a) {
+static inline Vec4db is_finite(Vec4d const a) {
     return Vec4db(is_finite(a.get_low()), is_finite(a.get_high()));
 }
 
 // Function is_inf: gives true for elements that are +INF or -INF
 // false for finite numbers and NAN
-static inline Vec4db is_inf(Vec4d const & a) {
+static inline Vec4db is_inf(Vec4d const a) {
     return Vec4db(is_inf(a.get_low()), is_inf(a.get_high()));
 }
 
 // Function is_nan: gives true for elements that are +NAN or -NAN
 // false for finite numbers and +/-INF
-static inline Vec4db is_nan(Vec4d const & a) {
+static inline Vec4db is_nan(Vec4d const a) {
     return Vec4db(is_nan(a.get_low()), is_nan(a.get_high()));
 }
 
 // Function is_subnormal: gives true for elements that are denormal (subnormal)
 // false for finite numbers, zero, NAN and INF
-static inline Vec4db is_subnormal(Vec4d const & a) {
+static inline Vec4db is_subnormal(Vec4d const a) {
     return Vec4db(is_subnormal(a.get_low()), is_subnormal(a.get_high()));
 }
 
 // Function is_zero_or_subnormal: gives true for elements that are zero or subnormal (denormal)
 // false for finite numbers, NAN and INF
-static inline Vec4db is_zero_or_subnormal(Vec4d const & a) {
+static inline Vec4db is_zero_or_subnormal(Vec4d const a) {
     return Vec4db(is_zero_or_subnormal(a.get_low()),is_zero_or_subnormal(a.get_high()));
 }
 
@@ -1690,14 +1680,14 @@ static inline Vec4d infinite4d() {
 }
 
 // Function nan2d: returns a vector where all elements are +NAN (quiet)
-static inline Vec4d nan4d(int n = 0x10) {
+static inline Vec4d nan4d(uint32_t n = 0x10) {
     return Vec4d(nan2d(n), nan2d(n));
 }
 
 // change signs on vectors Vec4d
 // Each index i0 - i3 is 1 for changing sign on the corresponding element, 0 for no change
 template <int i0, int i1, int i2, int i3>
-static inline Vec4d change_sign(Vec4d const & a) {
+inline Vec4d change_sign(Vec4d const a) {
     if ((i0 | i1 | i2 | i3) == 0) return a;
     Vec2d lo = change_sign<i0,i1>(a.get_low());
     Vec2d hi = change_sign<i2,i3>(a.get_high());
@@ -1711,42 +1701,57 @@ static inline Vec4d change_sign(Vec4d const & a) {
 *
 *****************************************************************************/
 
-static inline Vec256ie reinterpret_i (Vec256ie const & x) {
+static inline Vec256b reinterpret_i (Vec256b const x) {
     return x;
 }
 
-static inline Vec256ie reinterpret_i (Vec256fe  const & x) {
-    return Vec256ie(reinterpret_i(x.get_low()), reinterpret_i(x.get_high()));
+static inline Vec256b reinterpret_i (Vec256fe  const x) {
+    return Vec256b(reinterpret_i(x.get_low()), reinterpret_i(x.get_high()));
 }
 
-static inline Vec256ie reinterpret_i (Vec256de const & x) {
-    return Vec256ie(reinterpret_i(x.get_low()), reinterpret_i(x.get_high()));
+static inline Vec256b reinterpret_i (Vec256de const x) {
+    return Vec256b(reinterpret_i(x.get_low()), reinterpret_i(x.get_high()));
 }
 
-static inline Vec256fe  reinterpret_f (Vec256ie const & x) {
+static inline Vec256fe  reinterpret_f (Vec256b const x) {
     return Vec256fe(reinterpret_f(x.get_low()), reinterpret_f(x.get_high()));
 }
 
-static inline Vec256fe  reinterpret_f (Vec256fe  const & x) {
+static inline Vec256fe  reinterpret_f (Vec256fe  const x) {
     return x;
 }
 
-static inline Vec256fe  reinterpret_f (Vec256de const & x) {
+static inline Vec256fe  reinterpret_f (Vec256de const x) {
     return Vec256fe(reinterpret_f(x.get_low()), reinterpret_f(x.get_high()));
 }
 
-static inline Vec256de reinterpret_d (Vec256ie const & x) {
+static inline Vec256de reinterpret_d (Vec256b const x) {
     return Vec256de(reinterpret_d(x.get_low()), reinterpret_d(x.get_high()));
 }
 
-static inline Vec256de reinterpret_d (Vec256fe  const & x) {
+static inline Vec256de reinterpret_d (Vec256fe  const x) {
     return Vec256de(reinterpret_d(x.get_low()), reinterpret_d(x.get_high()));
 }
 
-static inline Vec256de reinterpret_d (Vec256de const & x) {
+static inline Vec256de reinterpret_d (Vec256de const x) {
     return x;
 }
 
+// extend vectors to double size by adding zeroes
+static inline Vec8f extend_z(Vec4f a) {
+    return Vec8f(a, _mm_setzero_ps());
+}
+static inline Vec4d extend_z(Vec2d a) {
+    return Vec4d(a, _mm_setzero_pd());
+}
+
+static inline Vec8fb extend_z(Vec4fb a) {
+    return Vec8fb(a, Vec4fb(false));
+}
+static inline Vec4db extend_z(Vec2db a) {
+    return Vec4db(a, Vec2db(false));
+}
+
 
 /*****************************************************************************
 *
@@ -1754,185 +1759,43 @@ static inline Vec256de reinterpret_d (Vec256de const & x) {
 *
 ******************************************************************************
 *
-* The permute function can reorder the elements of a vector and optionally
-* set some elements to zero. 
-*
-* The indexes are inserted as template parameters in <>. These indexes must be
-* constants. Each template parameter is an index to the element you want to 
-* select. An index of -1 will generate zero. An index of -256 means don't care.
-*
-* Example:
-* Vec4d a(10., 11., 12., 13.);    // a is (10, 11, 12, 13)
-* Vec4d b;
-* b = permute4d<1,0,-1,3>(a);     // b is (11, 10,  0, 13)
-*
+* These permute functions can reorder the elements of a vector and optionally
+* set some elements to zero. See Vectori128.h for description
 *
-* The blend function can mix elements from two different vectors and
-* optionally set some elements to zero. 
-*
-* The indexes are inserted as template parameters in <>. These indexes must be
-* constants. Each template parameter is an index to the element you want to 
-* select, where indexes 0 - 3 indicate an element from the first source
-* vector and indexes 4 - 7 indicate an element from the second source vector.
-* A negative index will generate zero.
-*
-*
-* Example:
-* Vec4d a(10., 11., 12., 13.);    // a is (10, 11, 12, 13)
-* Vec4d b(20., 21., 22., 23.);    // a is (20, 21, 22, 23)
-* Vec4d c;
-* c = blend4d<4,3,7,-1> (a,b);    // c is (20, 13, 23,  0)
 *****************************************************************************/
 
 // permute vector Vec4d
 template <int i0, int i1, int i2, int i3>
-static inline Vec4d permute4d(Vec4d const & a) {
-    return Vec4d(blend2d<i0,i1> (a.get_low(), a.get_high()), 
-           blend2d<i2,i3> (a.get_low(), a.get_high()));
-}
-
-// helper function used below
-template <int n>
-static inline Vec2d select4(Vec4d const & a, Vec4d const & b) {
-    switch (n) {
-    case 0:
-        return a.get_low();
-    case 1:
-        return a.get_high();
-    case 2:
-        return b.get_low();
-    case 3:
-        return b.get_high();
-    }
-    return _mm_setzero_pd();
+static inline Vec4d permute4(Vec4d const a) {
+    return Vec4d(blend2<i0,i1> (a.get_low(), a.get_high()),
+           blend2<i2,i3> (a.get_low(), a.get_high()));
 }
 
-// blend vectors Vec4d
-template <int i0, int i1, int i2, int i3>
-static inline Vec4d blend4d(Vec4d const & a, Vec4d const & b) {
-    const int j0 = i0 >= 0 ? i0/2 : i0;
-    const int j1 = i1 >= 0 ? i1/2 : i1;
-    const int j2 = i2 >= 0 ? i2/2 : i2;
-    const int j3 = i3 >= 0 ? i3/2 : i3;    
-    Vec2d x0, x1;
-
-    if (j0 == j1 || i0 < 0 || i1 < 0) {  // both from same
-        const int k0 = j0 >= 0 ? j0 : j1;
-        x0 = permute2d<i0 & -7, i1 & -7> (select4<k0> (a,b));
-    }
-    else {
-        x0 = blend2d<i0 & -7, (i1 & -7) | 2> (select4<j0>(a,b), select4<j1>(a,b));
-    }
-    if (j2 == j3 || i2 < 0 || i3 < 0) {  // both from same
-        const int k1 = j2 >= 0 ? j2 : j3;
-        x1 = permute2d<i2 & -7, i3 & -7> (select4<k1> (a,b));
-    }
-    else {
-        x1 = blend2d<i2 & -7, (i3 & -7) | 2> (select4<j2>(a,b), select4<j3>(a,b));
-    }
-    return Vec4d(x0,x1);
-}
-
-/*****************************************************************************
-*
-*          Vector Vec8f permute and blend functions
-*
-*****************************************************************************/
-
 // permute vector Vec8f
 template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7>
-static inline Vec8f permute8f(Vec8f const & a) {
-    return Vec8f(blend4f<i0,i1,i2,i3> (a.get_low(), a.get_high()), 
-                 blend4f<i4,i5,i6,i7> (a.get_low(), a.get_high()));
+static inline Vec8f permute8(Vec8f const a) {
+    return Vec8f(blend4<i0,i1,i2,i3> (a.get_low(), a.get_high()),
+        blend4<i4,i5,i6,i7> (a.get_low(), a.get_high()));
 }
 
-// helper function used below
-template <int n>
-static inline Vec4f select4(Vec8f const & a, Vec8f const & b) {
-    switch (n) {
-    case 0:
-        return a.get_low();
-    case 1:
-        return a.get_high();
-    case 2:
-        return b.get_low();
-    case 3:
-        return b.get_high();
-    }
-    return _mm_setzero_ps();
+
+// blend vectors Vec4d
+template <int i0, int i1, int i2, int i3>
+static inline Vec4d blend4(Vec4d const a, Vec4d const b) {
+    Vec2d x0 = blend_half<Vec4d, i0, i1>(a, b);
+    Vec2d x1 = blend_half<Vec4d, i2, i3>(a, b);
+    return Vec4d(x0, x1);
 }
 
 // blend vectors Vec8f
 template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7>
-static inline Vec8f blend8f(Vec8f const & a, Vec8f const & b) {
-    const int j0 = i0 >= 0 ? i0/4 : i0;
-    const int j1 = i1 >= 0 ? i1/4 : i1;
-    const int j2 = i2 >= 0 ? i2/4 : i2;
-    const int j3 = i3 >= 0 ? i3/4 : i3;
-    const int j4 = i4 >= 0 ? i4/4 : i4;
-    const int j5 = i5 >= 0 ? i5/4 : i5;
-    const int j6 = i6 >= 0 ? i6/4 : i6;
-    const int j7 = i7 >= 0 ? i7/4 : i7;
-    Vec4f x0, x1;
-
-    const int r0 = j0 >= 0 ? j0 : j1 >= 0 ? j1 : j2 >= 0 ? j2 : j3;
-    const int r1 = j4 >= 0 ? j4 : j5 >= 0 ? j5 : j6 >= 0 ? j6 : j7;
-    const int s0 = (j1 >= 0 && j1 != r0) ? j1 : (j2 >= 0 && j2 != r0) ? j2 : j3;
-    const int s1 = (j5 >= 0 && j5 != r1) ? j5 : (j6 >= 0 && j6 != r1) ? j6 : j7;
-
-    // Combine all the indexes into a single bitfield, with 4 bits for each
-    const int m1 = (i0&0xF) | (i1&0xF)<<4 | (i2&0xF)<<8 | (i3&0xF)<<12 | (i4&0xF)<<16 | (i5&0xF)<<20 | (i6&0xF)<<24 | (i7&0xF)<<28;
-
-    // Mask to zero out negative indexes
-    const int mz = (i0<0?0:0xF) | (i1<0?0:0xF)<<4 | (i2<0?0:0xF)<<8 | (i3<0?0:0xF)<<12 | (i4<0?0:0xF)<<16 | (i5<0?0:0xF)<<20 | (i6<0?0:0xF)<<24 | (i7<0?0:0xF)<<28;
-
-    if (r0 < 0) {
-        x0 = _mm_setzero_ps();
-    }
-    else if (((m1 ^ r0*0x4444) & 0xCCCC & mz) == 0) { 
-        // i0 - i3 all from same source
-        x0 = permute4f<i0 & -13, i1 & -13, i2 & -13, i3 & -13> (select4<r0> (a,b));
-    }
-    else if ((j2 < 0 || j2 == r0 || j2 == s0) && (j3 < 0 || j3 == r0 || j3 == s0)) { 
-        // i0 - i3 all from two sources
-        const int k0 =  i0 >= 0 ? i0 & 3 : i0;
-        const int k1 = (i1 >= 0 ? i1 & 3 : i1) | (j1 == s0 ? 4 : 0);
-        const int k2 = (i2 >= 0 ? i2 & 3 : i2) | (j2 == s0 ? 4 : 0);
-        const int k3 = (i3 >= 0 ? i3 & 3 : i3) | (j3 == s0 ? 4 : 0);
-        x0 = blend4f<k0,k1,k2,k3> (select4<r0>(a,b), select4<s0>(a,b));
-    }
-    else {
-        // i0 - i3 from three or four different sources
-        x0 = blend4f<0,1,6,7> (
-             blend4f<i0 & -13, (i1 & -13) | 4, -0x100, -0x100> (select4<j0>(a,b), select4<j1>(a,b)),
-             blend4f<-0x100, -0x100, i2 & -13, (i3 & -13) | 4> (select4<j2>(a,b), select4<j3>(a,b)));
-    }
-
-    if (r1 < 0) {
-        x1 = _mm_setzero_ps();
-    }
-    else if (((m1 ^ r1*0x44440000u) & 0xCCCC0000 & mz) == 0) { 
-        // i4 - i7 all from same source
-        x1 = permute4f<i4 & -13, i5 & -13, i6 & -13, i7 & -13> (select4<r1> (a,b));
-    }
-    else if ((j6 < 0 || j6 == r1 || j6 == s1) && (j7 < 0 || j7 == r1 || j7 == s1)) { 
-        // i4 - i7 all from two sources
-        const int k4 =  i4 >= 0 ? i4 & 3 : i4;
-        const int k5 = (i5 >= 0 ? i5 & 3 : i5) | (j5 == s1 ? 4 : 0);
-        const int k6 = (i6 >= 0 ? i6 & 3 : i6) | (j6 == s1 ? 4 : 0);
-        const int k7 = (i7 >= 0 ? i7 & 3 : i7) | (j7 == s1 ? 4 : 0);
-        x1 = blend4f<k4,k5,k6,k7> (select4<r1>(a,b), select4<s1>(a,b));
-    }
-    else {
-        // i4 - i7 from three or four different sources
-        x1 = blend4f<0,1,6,7> (
-             blend4f<i4 & -13, (i5 & -13) | 4, -0x100, -0x100> (select4<j4>(a,b), select4<j5>(a,b)),
-             blend4f<-0x100, -0x100, i6 & -13, (i7 & -13) | 4> (select4<j6>(a,b), select4<j7>(a,b)));
-    }
-
-    return Vec8f(x0,x1);
+static inline Vec8f blend8(Vec8f const a, Vec8f const b) {
+    Vec4f x0 = blend_half<Vec8f, i0, i1, i2, i3>(a, b);
+    Vec4f x1 = blend_half<Vec8f, i4, i5, i6, i7>(a, b);
+    return Vec8f(x0, x1);
 }
 
+
 /*****************************************************************************
 *
 *          Vector lookup functions
@@ -1942,47 +1805,32 @@ static inline Vec8f blend8f(Vec8f const & a, Vec8f const & b) {
 * These functions use vector elements as indexes into a table.
 * The table is given as one or more vectors or as an array.
 *
-* This can be used for several purposes:
-*  - table lookup
-*  - permute or blend with variable indexes
-*  - blend from more than two sources
-*  - gather non-contiguous data
-*
-* An index out of range may produce any value - the actual value produced is
-* implementation dependent and may be different for different instruction
-* sets. An index out of range does not produce an error message or exception.
-*
-* Example:
-* Vec4i a(2,0,0,3);               // index  a is (  2,   0,   0,   3)
-* Vec4f b(1.0f,1.1f,1.2f,1.3f);   // table  b is (1.0, 1.1, 1.2, 1.3)
-* Vec4f c;
-* c = lookup4 (a,b);              // result c is (1.2, 1.0, 1.0, 1.3)
-*
 *****************************************************************************/
 
-#ifdef VECTORI256_H  // Vec8i and Vec4q must be defined
-
-static inline Vec8f lookup8(Vec8i const & index, Vec8f const & table) {
+static inline Vec8f lookup8(Vec8i const index, Vec8f const table) {
     Vec4f  r0 = lookup8(index.get_low() , table.get_low(), table.get_high());
     Vec4f  r1 = lookup8(index.get_high(), table.get_low(), table.get_high());
     return Vec8f(r0, r1);
 }
 
 template <int n>
-static inline Vec8f lookup(Vec8i const & index, float const * table) {
-    if (n <= 0) return 0;
-    if (n <= 4) {
-        Vec4f table1 = Vec4f().load(table);        
-        return Vec8f(       
+static inline Vec8f lookup(Vec8i const index, float const * table) {
+    if constexpr (n <= 0) return 0;
+    if constexpr (n <= 4) {
+        Vec4f table1 = Vec4f().load(table);
+        return Vec8f(
             lookup4 (index.get_low(),  table1),
             lookup4 (index.get_high(), table1));
     }
-    if (n <= 8) {
+    if constexpr (n <= 8) {
         return lookup8(index, Vec8f().load(table));
     }
     // Limit index
     Vec8ui index1;
-    if ((n & (n-1)) == 0) {
+    if constexpr (n == INT_MAX) {
+        index1 = index;
+    }
+    else if constexpr ((n & (n-1)) == 0) {
         // n is a power of 2, make index modulo n
         index1 = Vec8ui(index) & (n-1);
     }
@@ -1994,35 +1842,56 @@ static inline Vec8f lookup(Vec8i const & index, float const * table) {
     table[index1[4]],table[index1[5]],table[index1[6]],table[index1[7]]);
 }
 
-static inline Vec4d lookup4(Vec4q const & index, Vec4d const & table) {
+static inline Vec4d lookup4(Vec4q const index, Vec4d const table) {
     Vec2d  r0 = lookup4(index.get_low() , table.get_low(), table.get_high());
     Vec2d  r1 = lookup4(index.get_high(), table.get_low(), table.get_high());
     return Vec4d(r0, r1);
 }
 
 template <int n>
-static inline Vec4d lookup(Vec4q const & index, double const * table) {
-    if (n <= 0) return 0;
-    if (n <= 2) {
-        Vec2d table1 = Vec2d().load(table);        
-        return Vec4d(       
+static inline Vec4d lookup(Vec4q const index, double const * table) {
+    if constexpr (n <= 0) return 0;
+    if constexpr (n <= 2) {
+        Vec2d table1 = Vec2d().load(table);
+        return Vec4d(
             lookup2 (index.get_low(),  table1),
             lookup2 (index.get_high(), table1));
     }
     // Limit index
     Vec8ui index1;
-    if ((n & (n-1)) == 0) {
+    if constexpr (n == INT_MAX) {
+        index1 = Vec8ui(index);
+    }
+    else if constexpr ((n & (n-1)) == 0) {
         // n is a power of 2, make index modulo n
-        index1 = Vec8ui(index) & constant8i<n-1, 0, n-1, 0, n-1, 0, n-1, 0>();
+        index1 = Vec8ui(index) & Vec8ui(n-1, 0, n-1, 0, n-1, 0, n-1, 0);
     }
     else {
         // n is not a power of 2, limit to n-1
-        index1 = min(Vec8ui(index), constant8i<n-1, 0, n-1, 0, n-1, 0, n-1, 0>() );
+        index1 = min(Vec8ui(index), Vec8ui(n-1, 0, n-1, 0, n-1, 0, n-1, 0));
     }
     Vec4q index2 = Vec4q(index1);
     return Vec4d(table[index2[0]],table[index2[1]],table[index2[2]],table[index2[3]]);
 }
-#endif  // VECTORI256_H
+
+
+/*****************************************************************************
+*
+*          Gather functions with fixed indexes
+*
+*****************************************************************************/
+// Load elements from array a with indices i0, i1, i2, i3, ..
+template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7>
+static inline Vec8f gather8f(void const * a) {
+    return reinterpret_f(gather8i<i0, i1, i2, i3, i4, i5, i6, i7>(a));
+}
+
+// Load elements from array a with indices i0, i1, i2, i3
+template <int i0, int i1, int i2, int i3>
+static inline Vec4d gather4d(void const * a) {
+    return reinterpret_d(gather4q<i0, i1, i2, i3>(a));
+}
+
 
 /*****************************************************************************
 *
@@ -2031,27 +1900,16 @@ static inline Vec4d lookup(Vec4q const & index, double const * table) {
 ******************************************************************************
 *
 * These functions write the elements of a vector to arbitrary positions in an
-* array in memory. Each vector element is written to an array position 
+* array in memory. Each vector element is written to an array position
 * determined by an index. An element is not written if the corresponding
 * index is out of range.
 * The indexes can be specified as constant template parameters or as an
 * integer vector.
-* 
-* The scatter functions are useful if the data are distributed in a sparce
-* manner into the array. If the array is dense then it is more efficient
-* to permute the data into the right positions and then write the whole
-* permuted vector into the array.
-*
-* Example:
-* Vec8d a(10,11,12,13,14,15,16,17);
-* double b[16] = {0};
-* scatter<0,2,14,10,1,-1,5,9>(a,b); 
-* // Now, b = {10,14,11,0,0,16,0,0,0,17,13,0,0,0,12,0}
 *
 *****************************************************************************/
 
 template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7>
-static inline void scatter(Vec8f const & data, float * array) {
+static inline void scatter(Vec8f const data, float * array) {
     const int index[8] = {i0,i1,i2,i3,i4,i5,i6,i7};
     for (int i = 0; i < 8; i++) {
         if (index[i] >= 0) array[index[i]] = data[i];
@@ -2059,55 +1917,33 @@ static inline void scatter(Vec8f const & data, float * array) {
 }
 
 template <int i0, int i1, int i2, int i3>
-static inline void scatter(Vec4d const & data, double * array) {
+static inline void scatter(Vec4d const data, double * array) {
     const int index[4] = {i0,i1,i2,i3};
     for (int i = 0; i < 4; i++) {
         if (index[i] >= 0) array[index[i]] = data[i];
     }
 }
 
-static inline void scatter(Vec8i const & index, uint32_t limit, Vec8f const & data, float * array) {
+// scatter functions with variable indexes
+
+static inline void scatter(Vec8i const index, uint32_t limit, Vec8f const data, float * destination) {
     for (int i = 0; i < 8; i++) {
-        if (uint32_t(index[i]) < limit) array[index[i]] = data[i];
+        if (uint32_t(index[i]) < limit) destination[index[i]] = data[i];
     }
 }
 
-static inline void scatter(Vec4q const & index, uint32_t limit, Vec4d const & data, double * array) {
+static inline void scatter(Vec4q const index, uint32_t limit, Vec4d const data, double * destination) {
     for (int i = 0; i < 4; i++) {
-        if (uint64_t(index[i]) < uint64_t(limit)) array[index[i]] = data[i];
+        if (uint64_t(index[i]) < uint64_t(limit)) destination[index[i]] = data[i];
     }
-} 
+}
 
-static inline void scatter(Vec4i const & index, uint32_t limit, Vec4d const & data, double * array) {
+static inline void scatter(Vec4i const index, uint32_t limit, Vec4d const data, double * destination) {
     for (int i = 0; i < 4; i++) {
-        if (uint32_t(index[i]) < limit) array[index[i]] = data[i];
+        if (uint32_t(index[i]) < limit) destination[index[i]] = data[i];
     }
-} 
-
-/*****************************************************************************
-*
-*          Horizontal scan functions
-*
-*****************************************************************************/
-
-// Get index to the first element that is true. Return -1 if all are false
-
-static inline int horizontal_find_first(Vec8fb const & x) {
-    return horizontal_find_first(Vec8ib(x));
-}
-
-static inline int horizontal_find_first(Vec4db const & x) {
-    return horizontal_find_first(Vec4qb(x));
-} 
-
-// Count the number of elements that are true
-static inline uint32_t horizontal_count(Vec8fb const & x) {
-    return horizontal_count(Vec8ib(x));
 }
 
-static inline uint32_t horizontal_count(Vec4db const & x) {
-    return horizontal_count(Vec4qb(x));
-}
 
 /*****************************************************************************
 *
@@ -2116,27 +1952,17 @@ static inline uint32_t horizontal_count(Vec4db const & x) {
 *****************************************************************************/
 
 // to_bits: convert boolean vector to integer bitfield
-static inline uint8_t to_bits(Vec8fb const & x) {
-    return to_bits(Vec8ib(x));
-}
-
-// to_Vec8fb: convert integer bitfield to boolean vector
-static inline Vec8fb to_Vec8fb(uint8_t x) {
-    return Vec8fb(to_Vec8ib(x));
+static inline uint8_t to_bits(Vec8fb const x) {
+    return to_bits(Vec8ib(reinterpret_i(x)));
 }
 
 // to_bits: convert boolean vector to integer bitfield
-static inline uint8_t to_bits(Vec4db const & x) {
-    return to_bits(Vec4qb(x));
-}
-
-// to_Vec4db: convert integer bitfield to boolean vector
-static inline Vec4db to_Vec4db(uint8_t x) {
-    return Vec4db(to_Vec4qb(x));
+static inline uint8_t to_bits(Vec4db const x) {
+    return to_bits(Vec4qb(reinterpret_i(x)));
 }
 
 #ifdef VCL_NAMESPACE
 }
 #endif
 
-#endif // VECTORF256_H
+#endif // VECTORF256E_H
diff --git a/EEDI3/vectorclass/vectorf512.h b/EEDI3/vectorclass/vectorf512.h
index 32a0ae9..ce4e283 100644
--- a/EEDI3/vectorclass/vectorf512.h
+++ b/EEDI3/vectorclass/vectorf512.h
@@ -1,16 +1,13 @@
 /****************************  vectorf512.h   *******************************
 * Author:        Agner Fog
 * Date created:  2014-07-23
-* Last modified: 2017-02-19
-* Version:       1.27
-* Project:       vector classes
+* Last modified: 2023-07-04
+* Version:       2.02.02
+* Project:       vector class library
 * Description:
-* Header file defining floating point vector classes as interface to intrinsic 
-* functions in x86 microprocessors with AVX512 and later instruction sets.
+* Header file defining 512-bit floating point vector classes
 *
-* Instructions:
-* Use Gnu, Intel or Microsoft C++ compiler. Compile for the desired 
-* instruction set, which must be at least AVX512F. 
+* Instructions: see vcl_manual.pdf
 *
 * The following vector classes are defined here:
 * Vec16f    Vector of  16  single precision floating point numbers
@@ -18,21 +15,27 @@
 * Vec8d     Vector of   8  double precision floating point numbers
 * Vec8db    Vector of   8  Booleans for use with Vec8d
 *
-* Each vector object is represented internally in the CPU as a 512-bit register.
+* Each vector object is represented internally in the CPU a 512-bit register.
 * This header file defines operators and functions for these vectors.
 *
-* For detailed instructions, see VectorClass.pdf
-*
-* (c) Copyright 2014-2017 GNU General Public License http://www.gnu.org/licenses
+* (c) Copyright 2014-2023 Agner Fog.
+* Apache License version 2.0 or later.
 *****************************************************************************/
 
-// check combination of header files
-#if defined (VECTORF512_H)
-#if    VECTORF512_H != 2
+#ifndef VECTORF512_H
+#define VECTORF512_H
+
+#ifndef VECTORCLASS_H
+#include "vectorclass.h"
+#endif
+
+#if VECTORCLASS_H < 20200
+#error Incompatible versions of vector class library mixed
+#endif
+
+#ifdef VECTORF512E_H
 #error Two different versions of vectorf512.h included
 #endif
-#else
-#define VECTORF512_H  2
 
 #include "vectori512.h"
 
@@ -40,307 +43,45 @@
 namespace VCL_NAMESPACE {
 #endif
 
-// Define missing intrinsic functions
-#if defined (GCC_VERSION) && GCC_VERSION < 41102 && !defined(__INTEL_COMPILER) && !defined(__clang__)
-
-static inline __m512 _mm512_castpd_ps(__m512d x) {
-    union {
-        __m512d a;
-        __m512  b;
-    } u;
-    u.a = x;
-    return u.b;
-}
-
-static inline __m512d _mm512_castps_pd(__m512 x) {
-    union {
-        __m512  a;
-        __m512d b;
-    } u;
-    u.a = x;
-    return u.b;
-}
-
-
-static inline __m512i _mm512_castps_si512(__m512 x) {
-    union {
-        __m512  a;
-        __m512i b;
-    } u;
-    u.a = x;
-    return u.b;
-}
-
-static inline __m512 _mm512_castsi512_ps(__m512i x) {
-    union {
-        __m512i a;
-        __m512  b;
-    } u;
-    u.a = x;
-    return u.b;
-}
-
-static inline __m512i _mm512_castpd_si512(__m512d x) {
-    union {
-        __m512d a;
-        __m512i b;
-    } u;
-    u.a = x;
-    return u.b;
-}
-
-static inline __m512d _mm512_castsi512_pd(__m512i x) {
-    union {
-        __m512i a;
-        __m512d b;
-    } u;
-    u.a = x;
-    return u.b;
-}
-
-static inline __m512 _mm512_castps256_ps512(__m256 x) {
-    union {
-        __m256 a;
-        __m512 b;
-    } u;
-    u.a = x;
-    return u.b;
-}
-
-static inline __m256 _mm512_castps512_ps256(__m512 x) {
-    union {
-        __m512 a;
-        __m256 b;
-    } u;
-    u.a = x;
-    return u.b;
-}
-
-static inline __m512d _mm512_castpd256_pd512(__m256d x) {
-    union {
-        __m256d a;
-        __m512d b;
-    } u;
-    u.a = x;
-    return u.b;
-}
-
-static inline __m256d _mm512_castpd512_pd256(__m512d x) {
-    union {
-        __m512d a;
-        __m256d b;
-    } u;
-    u.a = x;
-    return u.b;
-}
-
-#endif 
-
 
 /*****************************************************************************
 *
 *          Vec16fb: Vector of 16 Booleans for use with Vec16f
+*          Vec8db:  Vector of 8  Booleans for use with Vec8d
 *
 *****************************************************************************/
-class Vec16fb : public Vec16b {
-public:
-    // Default constructor:
-    Vec16fb () {
-    }
-    Vec16fb (Vec16b x) {
-        m16 = x;
-    }
-    // Constructor to build from all elements:
-    Vec16fb(bool x0, bool x1, bool x2, bool x3, bool x4, bool x5, bool x6, bool x7,
-        bool x8, bool x9, bool x10, bool x11, bool x12, bool x13, bool x14, bool x15) :
-        Vec16b(x0, x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12, x13, x14, x15) {
-    }
-    // Constructor to convert from type __mmask16 used in intrinsics:
-    Vec16fb (__mmask16 x) {
-        m16 = x;
-    }
-    // Constructor to broadcast single value:
-    Vec16fb(bool b) : Vec16b(b) {}
-private: // Prevent constructing from int, etc.
-    Vec16fb(int b);
-public:
-    // Constructor to make from two halves
-    Vec16fb (Vec8fb const & x0, Vec8fb const & x1) {
-        m16 = Vec16b(Vec8ib(x0), Vec8ib(x1));
-    }
-    // Assignment operator to convert from type __mmask16 used in intrinsics:
-    Vec16fb & operator = (__mmask16 x) {
-        m16 = x;
-        return *this;
-    }
-    // Assignment operator to broadcast scalar value:
-    Vec16fb & operator = (bool b) {
-        m16 = Vec16b(b);
-        return *this;
-    }
-private: // Prevent assigning int because of ambiguity
-    Vec16fb & operator = (int x);
-public:
-};
 
-// Define operators for Vec16fb
-
-// vector operator & : bitwise and
-static inline Vec16fb operator & (Vec16fb a, Vec16fb b) {
-    return Vec16b(a) & Vec16b(b);
-}
-static inline Vec16fb operator && (Vec16fb a, Vec16fb b) {
-    return a & b;
-}
+typedef Vec16b Vec16fb;
+typedef Vec8b  Vec8db;
 
-// vector operator | : bitwise or
-static inline Vec16fb operator | (Vec16fb a, Vec16fb b) {
-    return Vec16b(a) | Vec16b(b);
+#if INSTRSET == 9  // special cases of mixed compact and broad vectors
+inline Vec16b::Vec16b(Vec8ib const x0, Vec8ib const x1) {
+    mm = uint16_t(to_bits(x0) | uint16_t(to_bits(x1) << 8));
 }
-static inline Vec16fb operator || (Vec16fb a, Vec16fb b) {
-    return a | b;
+inline Vec16b::Vec16b(Vec8fb const x0, Vec8fb const x1) {
+    mm = uint16_t(to_bits(x0) | uint16_t(to_bits(x1) << 8));
 }
-
-// vector operator ^ : bitwise xor
-static inline Vec16fb operator ^ (Vec16fb a, Vec16fb b) {
-    return Vec16b(a) ^ Vec16b(b);
+inline Vec8b::Vec8b(Vec4qb const x0, Vec4qb const x1) {
+    mm = Vec8b_masktype(to_bits(x0) | (to_bits(x1) << 4));  // see definition of Vec8b_masktype in vectori128.h
 }
-
-// vector operator ~ : bitwise not
-static inline Vec16fb operator ~ (Vec16fb a) {
-    return ~Vec16b(a);
+inline Vec8b::Vec8b(Vec4db const x0, Vec4db const x1) {
+    mm = Vec8b_masktype(to_bits(x0) | (to_bits(x1) << 4));
 }
 
-// vector operator ! : element not
-static inline Vec16fb operator ! (Vec16fb a) {
-    return ~a;
+inline Vec8ib Vec16b::get_low() const {
+    return Vec8ib().load_bits(uint8_t(mm));
 }
-
-// vector operator &= : bitwise and
-static inline Vec16fb & operator &= (Vec16fb & a, Vec16fb b) {
-    a = a & b;
-    return a;
+inline Vec8ib Vec16b::get_high() const {
+    return Vec8ib().load_bits(uint8_t((uint16_t)mm >> 8u));
 }
-
-// vector operator |= : bitwise or
-static inline Vec16fb & operator |= (Vec16fb & a, Vec16fb b) {
-    a = a | b;
-    return a;
+inline Vec4qb Vec8b::get_low() const {
+    return Vec4qb().load_bits(uint8_t(mm & 0xF));
 }
-
-// vector operator ^= : bitwise xor
-static inline Vec16fb & operator ^= (Vec16fb & a, Vec16fb b) {
-    a = a ^ b;
-    return a;
+inline Vec4qb Vec8b::get_high() const {
+    return Vec4qb().load_bits(uint8_t(mm >> 4u));
 }
 
-
-/*****************************************************************************
-*
-*          Vec8db: Vector of 8 Booleans for use with Vec8d
-*
-*****************************************************************************/
-
-class Vec8db : public Vec8b {
-public:
-    // Default constructor:
-    Vec8db () {
-    }
-    Vec8db (Vec16b x) {
-        m16 = x;
-    }
-    // Constructor to build from all elements:
-    Vec8db(bool x0, bool x1, bool x2, bool x3, bool x4, bool x5, bool x6, bool x7) :
-        Vec8b(x0, x1, x2, x3, x4, x5, x6, x7) {
-    }
-    // Constructor to convert from type __mmask8 used in intrinsics:
-    Vec8db (__mmask8 x) {
-        m16 = x;
-    }
-    // Constructor to convert from type __mmask16 used in intrinsics:
-    Vec8db (__mmask16 x) {
-        m16 = x;
-    }
-    // Constructor to build from two halves
-    Vec8db (Vec4db const & x0, Vec4db const & x1) {
-        m16 = Vec8qb(Vec4qb(x0), Vec4qb(x1));
-    }
-    // Assignment operator to convert from type __mmask8 used in intrinsics:
-    Vec8db & operator = (__mmask8 x) {
-        m16 = (__mmask16)x;
-        return *this;
-    }
-    // Assignment operator to convert from type __mmask16 used in intrinsics:
-    Vec8db & operator = (__mmask16 x) {
-        m16 = x;
-        return *this;
-    }
-    // Constructor to broadcast single value:
-    Vec8db(bool b) : Vec8b(b) {}
-    // Assignment operator to broadcast scalar:
-    Vec8db & operator = (bool b) {
-        m16 = Vec8b(b);
-        return *this;
-    }
-private: // Prevent constructing from int, etc.
-    Vec8db(int b);
-    Vec8db & operator = (int x);
-public:
-    static int size () {
-        return 8;
-    }
-};
-
-// Define operators for Vec8db
-
-// vector operator & : bitwise and
-static inline Vec8db operator & (Vec8db a, Vec8db b) {
-    return Vec16b(a) & Vec16b(b);
-}
-static inline Vec8db operator && (Vec8db a, Vec8db b) {
-    return a & b;
-}
-
-// vector operator | : bitwise or
-static inline Vec8db operator | (Vec8db a, Vec8db b) {
-    return Vec16b(a) | Vec16b(b);
-}
-static inline Vec8db operator || (Vec8db a, Vec8db b) {
-    return a | b;
-}
-
-// vector operator ^ : bitwise xor
-static inline Vec8db operator ^ (Vec8db a, Vec8db b) {
-    return Vec16b(a) ^ Vec16b(b);
-}
-
-// vector operator ~ : bitwise not
-static inline Vec8db operator ~ (Vec8db a) {
-    return ~Vec16b(a);
-}
-
-// vector operator ! : element not
-static inline Vec8db operator ! (Vec8db a) {
-    return ~a;
-}
-
-// vector operator &= : bitwise and
-static inline Vec8db & operator &= (Vec8db & a, Vec8db b) {
-    a = a & b;
-    return a;
-}
-
-// vector operator |= : bitwise or
-static inline Vec8db & operator |= (Vec8db & a, Vec8db b) {
-    a = a | b;
-    return a;
-}
-
-// vector operator ^= : bitwise xor
-static inline Vec8db & operator ^= (Vec8db & a, Vec8db b) {
-    a = a ^ b;
-    return a;
-}
+#endif
 
 
 /*****************************************************************************
@@ -354,8 +95,7 @@ protected:
     __m512 zmm; // Float vector
 public:
     // Default constructor:
-    Vec16f() {
-    }
+    Vec16f() = default;
     // Constructor to broadcast the same value into all elements:
     Vec16f(float f) {
         zmm = _mm512_set1_ps(f);
@@ -363,18 +103,18 @@ public:
     // Constructor to build from all elements:
     Vec16f(float f0, float f1, float f2, float f3, float f4, float f5, float f6, float f7,
     float f8, float f9, float f10, float f11, float f12, float f13, float f14, float f15) {
-        zmm = _mm512_setr_ps(f0, f1, f2, f3, f4, f5, f6, f7, f8, f9, f10, f11, f12, f13, f14, f15); 
+        zmm = _mm512_setr_ps(f0, f1, f2, f3, f4, f5, f6, f7, f8, f9, f10, f11, f12, f13, f14, f15);
     }
     // Constructor to build from two Vec8f:
-    Vec16f(Vec8f const & a0, Vec8f const & a1) {
+    Vec16f(Vec8f const a0, Vec8f const a1) {
         zmm = _mm512_castpd_ps(_mm512_insertf64x4(_mm512_castps_pd(_mm512_castps256_ps512(a0)), _mm256_castps_pd(a1), 1));
     }
     // Constructor to convert from type __m512 used in intrinsics:
-    Vec16f(__m512 const & x) {
+    Vec16f(__m512 const x) {
         zmm = x;
     }
     // Assignment operator to convert from type __m512 used in intrinsics:
-    Vec16f & operator = (__m512 const & x) {
+    Vec16f & operator = (__m512 const x) {
         zmm = x;
         return *this;
     }
@@ -383,29 +123,30 @@ public:
         return zmm;
     }
     // Member function to load from array (unaligned)
-    Vec16f & load(void const * p) {
-        zmm = _mm512_loadu_ps((float const*)p);
+    Vec16f & load(float const * p) {
+        zmm = _mm512_loadu_ps(p);
         return *this;
     }
     // Member function to load from array, aligned by 64
-    // You may use load_a instead of load if you are certain that p points to an address
-    // divisible by 64.
-    Vec16f & load_a(void const * p) {
-        zmm = _mm512_load_ps((float const*)p);
+    // You may use load_a instead of load if you are certain that p points to an address divisible by 64
+    Vec16f & load_a(float const * p) {
+        zmm = _mm512_load_ps(p);
         return *this;
     }
     // Member function to store into array (unaligned)
     void store(float * p) const {
         _mm512_storeu_ps(p, zmm);
     }
-    // Member function to store into array, aligned by 64
-    // You may use store_a instead of store if you are certain that p points to an address
-    // divisible by 64.
+    // Member function storing into array, aligned by 64
+    // You may use store_a instead of store if you are certain that p points to an address divisible by 64
     void store_a(float * p) const {
         _mm512_store_ps(p, zmm);
     }
-    // Member function to store into array using a non-temporal memory hint, aligned by 64
-    void stream(float * p) const {
+    // Member function storing to aligned uncached memory (non-temporal store).
+    // This may be more efficient than store_a when storing large blocks of memory if it 
+    // is unlikely that the data will stay in the cache until it is read again.
+    // Note: Will generate runtime error if p is not aligned by 16
+    void store_nt(float * p) const {
         _mm512_stream_ps(p, zmm);
     }
     // Partial load. Load n elements and set the rest to 0
@@ -423,20 +164,18 @@ public:
         return *this;
     }
     // Member function to change a single element in vector
-    Vec16f const & insert(uint32_t index, float value) {
-        //zmm = _mm512_mask_set1_ps(zmm, __mmask16(1 << index), value);  // this intrinsic function does not exist (yet?)
-        zmm = _mm512_castsi512_ps(_mm512_mask_set1_epi32(_mm512_castps_si512(zmm), __mmask16(1 << index), *(int32_t*)&value));  // ignore warning
+    Vec16f const insert(int index, float value) {
+        zmm = _mm512_mask_broadcastss_ps(zmm, __mmask16(1u << index), _mm_set_ss(value));
         return *this;
     }
     // Member function extract a single element from vector
-    float extract(uint32_t index) const {
-        float a[16];
-        store(a);
-        return a[index & 15];
+    float extract(int index) const {
+        __m512 x = _mm512_maskz_compress_ps(__mmask16(1u << index), zmm);
+        return _mm512_cvtss_f32(x);
     }
     // Extract a single element. Use store function if extracting more than one element.
     // Operator [] can only read an element, not write.
-    float operator [] (uint32_t index) const {
+    float operator [] (int index) const {
         return extract(index);
     }
     // Member functions to split into two Vec4f:
@@ -446,9 +185,13 @@ public:
     Vec8f get_high() const {
         return _mm256_castpd_ps(_mm512_extractf64x4_pd(_mm512_castps_pd(zmm),1));
     }
-    static int size () {
+    static constexpr int size() {
         return 16;
     }
+    static constexpr int elementtype() {
+        return 16;
+    }
+    typedef __m512 registertype;
 };
 
 
@@ -459,20 +202,20 @@ public:
 *****************************************************************************/
 
 // vector operator + : add element by element
-static inline Vec16f operator + (Vec16f const & a, Vec16f const & b) {
+static inline Vec16f operator + (Vec16f const a, Vec16f const b) {
     return _mm512_add_ps(a, b);
 }
 
 // vector operator + : add vector and scalar
-static inline Vec16f operator + (Vec16f const & a, float b) {
+static inline Vec16f operator + (Vec16f const a, float b) {
     return a + Vec16f(b);
 }
-static inline Vec16f operator + (float a, Vec16f const & b) {
+static inline Vec16f operator + (float a, Vec16f const b) {
     return Vec16f(a) + b;
 }
 
 // vector operator += : add
-static inline Vec16f & operator += (Vec16f & a, Vec16f const & b) {
+static inline Vec16f & operator += (Vec16f & a, Vec16f const b) {
     a = a + b;
     return a;
 }
@@ -491,26 +234,26 @@ static inline Vec16f & operator ++ (Vec16f & a) {
 }
 
 // vector operator - : subtract element by element
-static inline Vec16f operator - (Vec16f const & a, Vec16f const & b) {
+static inline Vec16f operator - (Vec16f const a, Vec16f const b) {
     return _mm512_sub_ps(a, b);
 }
 
 // vector operator - : subtract vector and scalar
-static inline Vec16f operator - (Vec16f const & a, float b) {
+static inline Vec16f operator - (Vec16f const a, float b) {
     return a - Vec16f(b);
 }
-static inline Vec16f operator - (float a, Vec16f const & b) {
+static inline Vec16f operator - (float a, Vec16f const b) {
     return Vec16f(a) - b;
 }
 
 // vector operator - : unary minus
 // Change sign bit, even for 0, INF and NAN
-static inline Vec16f operator - (Vec16f const & a) {
+static inline Vec16f operator - (Vec16f const a) {
     return _mm512_castsi512_ps(Vec16i(_mm512_castps_si512(a)) ^ 0x80000000);
 }
 
 // vector operator -= : subtract
-static inline Vec16f & operator -= (Vec16f & a, Vec16f const & b) {
+static inline Vec16f & operator -= (Vec16f & a, Vec16f const b) {
     a = a - b;
     return a;
 }
@@ -529,123 +272,122 @@ static inline Vec16f & operator -- (Vec16f & a) {
 }
 
 // vector operator * : multiply element by element
-static inline Vec16f operator * (Vec16f const & a, Vec16f const & b) {
+static inline Vec16f operator * (Vec16f const a, Vec16f const b) {
     return _mm512_mul_ps(a, b);
 }
 
 // vector operator * : multiply vector and scalar
-static inline Vec16f operator * (Vec16f const & a, float b) {
+static inline Vec16f operator * (Vec16f const a, float b) {
     return a * Vec16f(b);
 }
-static inline Vec16f operator * (float a, Vec16f const & b) {
+static inline Vec16f operator * (float a, Vec16f const b) {
     return Vec16f(a) * b;
 }
 
 // vector operator *= : multiply
-static inline Vec16f & operator *= (Vec16f & a, Vec16f const & b) {
+static inline Vec16f & operator *= (Vec16f & a, Vec16f const b) {
     a = a * b;
     return a;
 }
 
 // vector operator / : divide all elements by same integer
-static inline Vec16f operator / (Vec16f const & a, Vec16f const & b) {
+static inline Vec16f operator / (Vec16f const a, Vec16f const b) {
     return _mm512_div_ps(a, b);
 }
 
 // vector operator / : divide vector and scalar
-static inline Vec16f operator / (Vec16f const & a, float b) {
+static inline Vec16f operator / (Vec16f const a, float b) {
     return a / Vec16f(b);
 }
-static inline Vec16f operator / (float a, Vec16f const & b) {
+static inline Vec16f operator / (float a, Vec16f const b) {
     return Vec16f(a) / b;
 }
 
 // vector operator /= : divide
-static inline Vec16f & operator /= (Vec16f & a, Vec16f const & b) {
+static inline Vec16f & operator /= (Vec16f & a, Vec16f const b) {
     a = a / b;
     return a;
 }
 
 // vector operator == : returns true for elements for which a == b
-static inline Vec16fb operator == (Vec16f const & a, Vec16f const & b) {
+static inline Vec16fb operator == (Vec16f const a, Vec16f const b) {
 //    return _mm512_cmpeq_ps_mask(a, b);
     return _mm512_cmp_ps_mask(a, b, 0);
 }
 
 // vector operator != : returns true for elements for which a != b
-static inline Vec16fb operator != (Vec16f const & a, Vec16f const & b) {
+static inline Vec16fb operator != (Vec16f const a, Vec16f const b) {
 //    return _mm512_cmpneq_ps_mask(a, b);
     return _mm512_cmp_ps_mask(a, b, 4);
 }
 
 // vector operator < : returns true for elements for which a < b
-static inline Vec16fb operator < (Vec16f const & a, Vec16f const & b) {
+static inline Vec16fb operator < (Vec16f const a, Vec16f const b) {
 //    return _mm512_cmplt_ps_mask(a, b);
     return _mm512_cmp_ps_mask(a, b, 1);
-
 }
 
 // vector operator <= : returns true for elements for which a <= b
-static inline Vec16fb operator <= (Vec16f const & a, Vec16f const & b) {
+static inline Vec16fb operator <= (Vec16f const a, Vec16f const b) {
 //    return _mm512_cmple_ps_mask(a, b);
     return _mm512_cmp_ps_mask(a, b, 2);
 }
 
 // vector operator > : returns true for elements for which a > b
-static inline Vec16fb operator > (Vec16f const & a, Vec16f const & b) {
-    return b < a;
+static inline Vec16fb operator > (Vec16f const a, Vec16f const b) {
+    return _mm512_cmp_ps_mask(a, b, 6+8);
 }
 
 // vector operator >= : returns true for elements for which a >= b
-static inline Vec16fb operator >= (Vec16f const & a, Vec16f const & b) {
-    return b <= a;
+static inline Vec16fb operator >= (Vec16f const a, Vec16f const b) {
+    return _mm512_cmp_ps_mask(a, b, 5+8);
 }
 
 // Bitwise logical operators
 
 // vector operator & : bitwise and
-static inline Vec16f operator & (Vec16f const & a, Vec16f const & b) {
+static inline Vec16f operator & (Vec16f const a, Vec16f const b) {
     return _mm512_castsi512_ps(Vec16i(_mm512_castps_si512(a)) & Vec16i(_mm512_castps_si512(b)));
 }
 
 // vector operator &= : bitwise and
-static inline Vec16f & operator &= (Vec16f & a, Vec16f const & b) {
+static inline Vec16f & operator &= (Vec16f & a, Vec16f const b) {
     a = a & b;
     return a;
 }
 
 // vector operator & : bitwise and of Vec16f and Vec16fb
-static inline Vec16f operator & (Vec16f const & a, Vec16fb const & b) {
+static inline Vec16f operator & (Vec16f const a, Vec16fb const b) {
     return _mm512_maskz_mov_ps(b, a);
 }
-static inline Vec16f operator & (Vec16fb const & a, Vec16f const & b) {
+static inline Vec16f operator & (Vec16fb const a, Vec16f const b) {
     return b & a;
 }
 
 // vector operator | : bitwise or
-static inline Vec16f operator | (Vec16f const & a, Vec16f const & b) {
+static inline Vec16f operator | (Vec16f const a, Vec16f const b) {
     return _mm512_castsi512_ps(Vec16i(_mm512_castps_si512(a)) | Vec16i(_mm512_castps_si512(b)));
 }
 
 // vector operator |= : bitwise or
-static inline Vec16f & operator |= (Vec16f & a, Vec16f const & b) {
+static inline Vec16f & operator |= (Vec16f & a, Vec16f const b) {
     a = a | b;
     return a;
 }
 
 // vector operator ^ : bitwise xor
-static inline Vec16f operator ^ (Vec16f const & a, Vec16f const & b) {
+static inline Vec16f operator ^ (Vec16f const a, Vec16f const b) {
     return _mm512_castsi512_ps(Vec16i(_mm512_castps_si512(a)) ^ Vec16i(_mm512_castps_si512(b)));
 }
 
 // vector operator ^= : bitwise xor
-static inline Vec16f & operator ^= (Vec16f & a, Vec16f const & b) {
+static inline Vec16f & operator ^= (Vec16f & a, Vec16f const b) {
     a = a ^ b;
     return a;
 }
 
 // vector operator ! : logical not. Returns Boolean vector
-static inline Vec16fb operator ! (Vec16f const & a) {
+static inline Vec16fb operator ! (Vec16f const a) {
     return a == Vec16f(0.0f);
 }
 
@@ -656,29 +398,151 @@ static inline Vec16fb operator ! (Vec16f const & a) {
 *
 *****************************************************************************/
 
-static inline Vec16f zero_16f() {
-    return _mm512_setzero_ps();
-}
-
 // Select between two operands. Corresponds to this pseudocode:
 // for (int i = 0; i < 8; i++) result[i] = s[i] ? a[i] : b[i];
-// Each byte in s must be either 0 (false) or 0xFFFFFFFF (true). No other values are allowed.
-static inline Vec16f select (Vec16fb const & s, Vec16f const & a, Vec16f const & b) {
+static inline Vec16f select (Vec16fb const s, Vec16f const a, Vec16f const b) {
     return _mm512_mask_mov_ps(b, s, a);
 }
 
 // Conditional add: For all vector elements i: result[i] = f[i] ? (a[i] + b[i]) : a[i]
-static inline Vec16f if_add (Vec16fb const & f, Vec16f const & a, Vec16f const & b) {
+static inline Vec16f if_add (Vec16fb const f, Vec16f const a, Vec16f const b) {
     return _mm512_mask_add_ps(a, f, a, b);
 }
 
-// Conditional multiply: For all vector elements i: result[i] = f[i] ? (a[i] * b[i]) : a[i]
-static inline Vec16f if_mul (Vec16fb const & f, Vec16f const & a, Vec16f const & b) {
+// Conditional subtract
+static inline Vec16f if_sub (Vec16fb const f, Vec16f const a, Vec16f const b) {
+    return _mm512_mask_sub_ps(a, f, a, b);
+}
+
+// Conditional multiply
+static inline Vec16f if_mul (Vec16fb const f, Vec16f const a, Vec16f const b) {
     return _mm512_mask_mul_ps(a, f, a, b);
 }
 
+// Conditional divide
+static inline Vec16f if_div (Vec16fb const f, Vec16f const a, Vec16f const b) {
+    return _mm512_mask_div_ps(a, f, a, b);
+}
+
+
+// sign functions
+
+// Function sign_bit: gives true for elements that have the sign bit set
+// even for -0.0f, -INF and -NAN
+// Note that sign_bit(Vec16f(-0.0f)) gives true, while Vec16f(-0.0f) < Vec16f(0.0f) gives false
+// (the underscore in the name avoids a conflict with a macro in Intel's mathimf.h)
+static inline Vec16fb sign_bit(Vec16f const a) {
+    Vec16i t1 = _mm512_castps_si512(a);    // reinterpret as 32-bit integer
+    return Vec16fb(t1 < 0);
+}
+
+// Function sign_combine: changes the sign of a when b has the sign bit set
+// same as select(sign_bit(b), -a, a)
+static inline Vec16f sign_combine(Vec16f const a, Vec16f const b) {
+    // return a ^ (b & Vec16f(-0.0f));
+    return _mm512_castsi512_ps (_mm512_ternarylogic_epi32(
+        _mm512_castps_si512(a), _mm512_castps_si512(b), Vec16i(0x80000000), 0x78));
+}
+
+// Categorization functions
+
+// Function is_finite: gives true for elements that are normal, subnormal or zero,
+// false for INF and NAN
+// (the underscore in the name avoids a conflict with a macro in Intel's mathimf.h)
+static inline Vec16fb is_finite(Vec16f const a) {
+#if INSTRSET >= 10  // __AVX512DQ__
+    __mmask16 f = _mm512_fpclass_ps_mask(a, 0x99);
+    return _mm512_knot(f);
+#else
+    Vec16i  t1 = _mm512_castps_si512(a);    // reinterpret as 32-bit integer
+    Vec16i  t2 = t1 << 1;                   // shift out sign bit
+    Vec16ib t3 = Vec16i(t2 & 0xFF000000) != 0xFF000000; // exponent field is not all 1s
+    return Vec16fb(t3);
+#endif
+}
+
+// Function is_inf: gives true for elements that are +INF or -INF
+// false for finite numbers and NAN
+// (the underscore in the name avoids a conflict with a macro in Intel's mathimf.h)
+static inline Vec16fb is_inf(Vec16f const a) {
+#if INSTRSET >= 10  // __AVX512DQ__
+    return _mm512_fpclass_ps_mask(a, 0x18);
+#else
+    Vec16i t1 = _mm512_castps_si512(a); // reinterpret as 32-bit integer
+    Vec16i t2 = t1 << 1;                // shift out sign bit
+    return Vec16fb(t2 == 0xFF000000);   // exponent is all 1s, fraction is 0
+#endif
+}
+
+// Function is_nan: gives true for elements that are +NAN or -NAN
+// false for finite numbers and +/-INF
+// (the underscore in the name avoids a conflict with a macro in Intel's mathimf.h)
+#if INSTRSET >= 10
+static inline Vec16fb is_nan(Vec16f const a) {
+    // assume that compiler does not optimize this away with -ffinite-math-only:
+    return _mm512_fpclass_ps_mask(a, 0x81);
+}
+//#elif defined(__GNUC__) && !defined(__INTEL_COMPILER) && !defined(__clang__)
+//__attribute__((optimize("-fno-unsafe-math-optimizations")))
+//static inline Vec16fb is_nan(Vec16f const a) {
+//    return a != a; // not safe with -ffinite-math-only compiler option
+//}
+#elif (defined(__GNUC__) || defined(__clang__)) && !defined(__INTEL_COMPILER)
+static inline Vec16fb is_nan(Vec16f const a) {
+    __m512 aa = a;
+    __mmask16 unordered;
+    __asm volatile("vcmpps $3, %1, %1, %0" : "=Yk" (unordered) :  "v" (aa) );
+    return Vec16fb(unordered);
+}
+#else
+static inline Vec16fb is_nan(Vec16f const a) {
+    // assume that compiler does not optimize this away with -ffinite-math-only:
+    return Vec16fb().load_bits(_mm512_cmp_ps_mask(a, a, 3)); // compare unordered
+    // return a != a; // This is not safe with -ffinite-math-only, -ffast-math, or /fp:fast compiler option
+}
+#endif
+
+
+// Function is_subnormal: gives true for elements that are subnormal
+// false for finite numbers, zero, NAN and INF
+static inline Vec16fb is_subnormal(Vec16f const a) {
+#if INSTRSET >= 10  // __AVX512DQ__
+    return _mm512_fpclass_ps_mask(a, 0x20);
+#else
+    Vec16i t1 = _mm512_castps_si512(a);    // reinterpret as 32-bit integer
+    Vec16i t2 = t1 << 1;                   // shift out sign bit
+    Vec16i t3 = 0xFF000000;                // exponent mask
+    Vec16i t4 = t2 & t3;                   // exponent
+    Vec16i t5 = _mm512_andnot_si512(t3,t2);// fraction
+    return Vec16fb(t4 == 0 && t5 != 0);     // exponent = 0 and fraction != 0
+#endif
+}
+
+// Function is_zero_or_subnormal: gives true for elements that are zero or subnormal
+// false for finite numbers, NAN and INF
+static inline Vec16fb is_zero_or_subnormal(Vec16f const a) {
+#if INSTRSET >= 10  // __AVX512DQ__
+    return _mm512_fpclass_ps_mask(a, 0x26);
+#else
+    Vec16i t = _mm512_castps_si512(a);            // reinterpret as 32-bit integer
+    t &= 0x7F800000;                       // isolate exponent
+    return Vec16fb(t == 0);                       // exponent = 0
+#endif
+}
+
+// change signs on vectors Vec16f
+// Each index i0 - i7 is 1 for changing sign on the corresponding element, 0 for no change
+template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7, int i8, int i9, int i10, int i11, int i12, int i13, int i14, int i15>
+static inline Vec16f change_sign(Vec16f const a) {
+    constexpr __mmask16 m = __mmask16((i0&1) | (i1&1)<<1 | (i2&1)<< 2 | (i3&1)<<3 | (i4&1)<<4 | (i5&1)<<5 | (i6&1)<<6 | (i7&1)<<7
+        | (i8&1)<<8 | (i9&1)<<9 | (i10&1)<<10 | (i11&1)<<11 | (i12&1)<<12 | (i13&1)<<13 | (i14&1)<<14 | (i15&1)<<15);
+    if constexpr ((uint16_t)m == 0) return a;
+    __m512 s = _mm512_castsi512_ps(_mm512_maskz_set1_epi32(m, 0x80000000));
+    return a ^ s;
+}
+
 // Horizontal add: Calculates the sum of all vector elements.
-static inline float horizontal_add (Vec16f const & a) {
+static inline float horizontal_add (Vec16f const a) {
 #if defined(__INTEL_COMPILER)
     return _mm512_reduce_add_ps(a);
 #else
@@ -687,142 +551,95 @@ static inline float horizontal_add (Vec16f const & a) {
 }
 
 // function max: a > b ? a : b
-static inline Vec16f max(Vec16f const & a, Vec16f const & b) {
+static inline Vec16f max(Vec16f const a, Vec16f const b) {
     return _mm512_max_ps(a,b);
 }
 
 // function min: a < b ? a : b
-static inline Vec16f min(Vec16f const & a, Vec16f const & b) {
+static inline Vec16f min(Vec16f const a, Vec16f const b) {
     return _mm512_min_ps(a,b);
 }
+// NAN-safe versions of maximum and minimum are in vector_convert.h
 
 // function abs: absolute value
-// Removes sign bit, even for -0.0f, -INF and -NAN
-static inline Vec16f abs(Vec16f const & a) {
-    union {
-        int32_t i;
-        float   f;
-    } u = {0x7FFFFFFF};
-    return a & Vec16f(u.f);
+static inline Vec16f abs(Vec16f const a) {
+#if INSTRSET >= 10  // AVX512DQ
+    return _mm512_range_ps(a, a, 8);
+#else
+    return a & Vec16f(_mm512_castsi512_ps(Vec16i(0x7FFFFFFF)));
+#endif
 }
 
 // function sqrt: square root
-static inline Vec16f sqrt(Vec16f const & a) {
+static inline Vec16f sqrt(Vec16f const a) {
     return _mm512_sqrt_ps(a);
 }
 
 // function square: a * a
-static inline Vec16f square(Vec16f const & a) {
+static inline Vec16f square(Vec16f const a) {
     return a * a;
 }
 
 // pow(Vec16f, int):
-template <typename TT> static Vec16f pow(Vec16f const & a, TT const & n);
+template <typename TT> static Vec16f pow(Vec16f const a, TT const n);
 
 // Raise floating point numbers to integer power n
 template <>
-inline Vec16f pow<int>(Vec16f const & x0, int const & n) {
+inline Vec16f pow<int>(Vec16f const x0, int const n) {
     return pow_template_i<Vec16f>(x0, n);
 }
 
 // allow conversion from unsigned int
 template <>
-inline Vec16f pow<uint32_t>(Vec16f const & x0, uint32_t const & n) {
+inline Vec16f pow<uint32_t>(Vec16f const x0, uint32_t const n) {
     return pow_template_i<Vec16f>(x0, (int)n);
 }
 
-
 // Raise floating point numbers to integer power n, where n is a compile-time constant
 template <int n>
-static inline Vec16f pow_n(Vec16f const & a) {
-    if (n < 0)    return Vec16f(1.0f) / pow_n<-n>(a);
-    if (n == 0)   return Vec16f(1.0f);
-    if (n >= 256) return pow(a, n);
-    Vec16f x = a;                      // a^(2^i)
-    Vec16f y;                          // accumulator
-    const int lowest = n - (n & (n-1));// lowest set bit in n
-    if (n & 1) y = x;
-    if (n < 2) return y;
-    x = x*x;                           // x^2
-    if (n & 2) {
-        if (lowest == 2) y = x; else y *= x;
-    }
-    if (n < 4) return y;
-    x = x*x;                           // x^4
-    if (n & 4) {
-        if (lowest == 4) y = x; else y *= x;
-    }
-    if (n < 8) return y;
-    x = x*x;                           // x^8
-    if (n & 8) {
-        if (lowest == 8) y = x; else y *= x;
-    }
-    if (n < 16) return y;
-    x = x*x;                           // x^16
-    if (n & 16) {
-        if (lowest == 16) y = x; else y *= x;
-    }
-    if (n < 32) return y;
-    x = x*x;                           // x^32
-    if (n & 32) {
-        if (lowest == 32) y = x; else y *= x;
-    }
-    if (n < 64) return y;
-    x = x*x;                           // x^64
-    if (n & 64) {
-        if (lowest == 64) y = x; else y *= x;
-    }
-    if (n < 128) return y;
-    x = x*x;                           // x^128
-    if (n & 128) {
-        if (lowest == 128) y = x; else y *= x;
-    }
-    return y;
-}
-
-template <int n>
-static inline Vec16f pow(Vec16f const & a, Const_int_t<n>) {
-    return pow_n<n>(a);
+static inline Vec16f pow(Vec16f const a, Const_int_t<n>) {
+    return pow_n<Vec16f, n>(a);
 }
 
-
 // function round: round to nearest integer (even). (result as float vector)
-static inline Vec16f round(Vec16f const & a) {
+static inline Vec16f round(Vec16f const a) {
     return _mm512_roundscale_ps(a, 0+8);
 }
 
 // function truncate: round towards zero. (result as float vector)
-static inline Vec16f truncate(Vec16f const & a) {
+static inline Vec16f truncate(Vec16f const a) {
     return _mm512_roundscale_ps(a, 3+8);
 }
 
 // function floor: round towards minus infinity. (result as float vector)
-static inline Vec16f floor(Vec16f const & a) {
+static inline Vec16f floor(Vec16f const a) {
     return _mm512_roundscale_ps(a, 1+8);
 }
 
 // function ceil: round towards plus infinity. (result as float vector)
-static inline Vec16f ceil(Vec16f const & a) {
+static inline Vec16f ceil(Vec16f const a) {
     return _mm512_roundscale_ps(a, 2+8);
 }
 
-// function round_to_int: round to nearest integer (even). (result as integer vector)
-static inline Vec16i round_to_int(Vec16f const & a) {
+// function roundi: round to nearest integer (even). (result as integer vector)
+static inline Vec16i roundi(Vec16f const a) {
     return _mm512_cvt_roundps_epi32(a, 0+8 /*_MM_FROUND_NO_EXC*/);
 }
+//static inline Vec16i round_to_int(Vec16f const a) {return roundi(a);} // deprecated
 
-// function truncate_to_int: round towards zero. (result as integer vector)
-static inline Vec16i truncate_to_int(Vec16f const & a) {
+// function truncatei: round towards zero. (result as integer vector)
+static inline Vec16i truncatei(Vec16f const a) {
     return _mm512_cvtt_roundps_epi32(a, 0+8 /*_MM_FROUND_NO_EXC*/);
 }
+//static inline Vec16i truncate_to_int(Vec16f const a) {return truncatei(a);} // deprecated
 
 // function to_float: convert integer vector to float vector
-static inline Vec16f to_float(Vec16i const & a) {
+static inline Vec16f to_float(Vec16i const a) {
     return _mm512_cvtepi32_ps(a);
 }
 
 // function to_float: convert unsigned integer vector to float vector
-static inline Vec16f to_float(Vec16ui const & a) {
+static inline Vec16f to_float(Vec16ui const a) {
     return _mm512_cvtepu32_ps(a);
 }
 
@@ -830,7 +647,7 @@ static inline Vec16f to_float(Vec16ui const & a) {
 
 // approximate reciprocal (Faster than 1.f / a.
 // relative accuracy better than 2^-11 without AVX512, 2^-14 with AVX512F, full precision with AVX512ER)
-static inline Vec16f approx_recipr(Vec16f const & a) {
+static inline Vec16f approx_recipr(Vec16f const a) {
 #ifdef __AVX512ER__  // AVX512ER instruction set includes fast reciprocal with better precision
     return _mm512_rcp28_round_ps(a, _MM_FROUND_NO_EXC);
 #else
@@ -840,7 +657,7 @@ static inline Vec16f approx_recipr(Vec16f const & a) {
 
 // approximate reciprocal squareroot (Faster than 1.f / sqrt(a).
 // Relative accuracy better than 2^-11 without AVX512, 2^-14 with AVX512F, full precision with AVX512ER)
-static inline Vec16f approx_rsqrt(Vec16f const & a) {
+static inline Vec16f approx_rsqrt(Vec16f const a) {
 #ifdef __AVX512ER__  // AVX512ER instruction set includes fast reciprocal squareroot with better precision
     return _mm512_rsqrt28_round_ps(a, _MM_FROUND_NO_EXC);
 #else
@@ -852,22 +669,23 @@ static inline Vec16f approx_rsqrt(Vec16f const & a) {
 // Fused multiply and add functions
 
 // Multiply and add
-static inline Vec16f mul_add(Vec16f const & a, Vec16f const & b, Vec16f const & c) {
+static inline Vec16f mul_add(Vec16f const a, Vec16f const b, Vec16f const c) {
     return _mm512_fmadd_ps(a, b, c);
 }
 
 // Multiply and subtract
-static inline Vec16f mul_sub(Vec16f const & a, Vec16f const & b, Vec16f const & c) {
+static inline Vec16f mul_sub(Vec16f const a, Vec16f const b, Vec16f const c) {
     return _mm512_fmsub_ps(a, b, c);
 }
 
 // Multiply and inverse subtract
-static inline Vec16f nmul_add(Vec16f const & a, Vec16f const & b, Vec16f const & c) {
+static inline Vec16f nmul_add(Vec16f const a, Vec16f const b, Vec16f const c) {
     return _mm512_fnmadd_ps(a, b, c);
 }
 
-// Multiply and subtract with extra precision on the intermediate calculations, 
-static inline Vec16f mul_sub_x(Vec16f const & a, Vec16f const & b, Vec16f const & c) {
+// Multiply and subtract with extra precision on the intermediate calculations,
+// Do not use mul_sub_x in general code because it is inaccurate in certain cases when FMA is not supported
+static inline Vec16f mul_sub_x(Vec16f const a, Vec16f const b, Vec16f const c) {
     return _mm512_fmsub_ps(a, b, c);
 }
 
@@ -877,8 +695,8 @@ static inline Vec16f mul_sub_x(Vec16f const & a, Vec16f const & b, Vec16f const
 // Extract the exponent as an integer
 // exponent(a) = floor(log2(abs(a)));
 // exponent(1.0f) = 0, exponent(0.0f) = -127, exponent(INF) = +128, exponent(NAN) = +128
-static inline Vec16i exponent(Vec16f const & a) {
-    // return round_to_int(Vec16i(_mm512_getexp_ps(a)));
+static inline Vec16i exponent(Vec16f const a) {
+    // return roundi(Vec16i(_mm512_getexp_ps(a)));
     Vec16ui t1 = _mm512_castps_si512(a);// reinterpret as 32-bit integers
     Vec16ui t2 = t1 << 1;               // shift out sign bit
     Vec16ui t3 = t2 >> 24;              // shift down logical to position 0
@@ -888,138 +706,24 @@ static inline Vec16i exponent(Vec16f const & a) {
 
 // Extract the fraction part of a floating point number
 // a = 2^exponent(a) * fraction(a), except for a = 0
-// fraction(1.0f) = 1.0f, fraction(5.0f) = 1.25f 
-static inline Vec16f fraction(Vec16f const & a) {
-#if 1
+// fraction(1.0f) = 1.0f, fraction(5.0f) = 1.25f
+static inline Vec16f fraction(Vec16f const a) {
     return _mm512_getmant_ps(a, _MM_MANT_NORM_1_2, _MM_MANT_SIGN_zero);
-#else
-    Vec8ui t1 = _mm512_castps_si512(a);   // reinterpret as 32-bit integer
-    Vec8ui t2 = (t1 & 0x007FFFFF) | 0x3F800000; // set exponent to 0 + bias
-    return _mm512_castsi512_ps(t2);
-#endif
 }
 
 // Fast calculation of pow(2,n) with n integer
 // n  =    0 gives 1.0f
 // n >=  128 gives +INF
 // n <= -127 gives 0.0f
-// This function will never produce denormals, and never raise exceptions
-static inline Vec16f exp2(Vec16i const & n) {
+// This function will never produce subnormals, and never raise exceptions
+static inline Vec16f exp2(Vec16i const n) {
     Vec16i t1 = max(n,  -0x7F);         // limit to allowed range
     Vec16i t2 = min(t1,  0x80);
     Vec16i t3 = t2 + 0x7F;              // add bias
     Vec16i t4 = t3 << 23;               // put exponent into position 23
     return _mm512_castsi512_ps(t4);     // reinterpret as float
 }
-//static Vec16f exp2(Vec16f const & x); // defined in vectormath_exp.h
-
-
-
-// Categorization functions
-
-// Function sign_bit: gives true for elements that have the sign bit set
-// even for -0.0f, -INF and -NAN
-// Note that sign_bit(Vec16f(-0.0f)) gives true, while Vec16f(-0.0f) < Vec16f(0.0f) gives false
-// (the underscore in the name avoids a conflict with a macro in Intel's mathimf.h)
-static inline Vec16fb sign_bit(Vec16f const & a) {
-    Vec16i t1 = _mm512_castps_si512(a);    // reinterpret as 32-bit integer
-    return Vec16fb(t1 < 0);
-}
-
-// Function sign_combine: changes the sign of a when b has the sign bit set
-// same as select(sign_bit(b), -a, a)
-static inline Vec16f sign_combine(Vec16f const & a, Vec16f const & b) {
-    union {
-        uint32_t i;
-        float    f;
-    } signmask = {0x80000000};
-    return a ^ (b & Vec16f(signmask.f));
-}
-
-// Function is_finite: gives true for elements that are normal, denormal or zero, 
-// false for INF and NAN
-// (the underscore in the name avoids a conflict with a macro in Intel's mathimf.h)
-static inline Vec16fb is_finite(Vec16f const & a) {
-#ifdef __AVX512DQ__
-    __mmask16 f = _mm512_fpclass_ps_mask(a, 0x99);
-    return _mm512_knot(f);
-#else
-    Vec16i  t1 = _mm512_castps_si512(a);    // reinterpret as 32-bit integer
-    Vec16i  t2 = t1 << 1;                   // shift out sign bit
-    Vec16ib t3 = Vec16i(t2 & 0xFF000000) != 0xFF000000; // exponent field is not all 1s
-    return Vec16fb(t3);
-#endif
-}
-
-// Function is_inf: gives true for elements that are +INF or -INF
-// false for finite numbers and NAN
-// (the underscore in the name avoids a conflict with a macro in Intel's mathimf.h)
-static inline Vec16fb is_inf(Vec16f const & a) {
-    Vec16i t1 = _mm512_castps_si512(a); // reinterpret as 32-bit integer
-    Vec16i t2 = t1 << 1;                // shift out sign bit
-    return Vec16fb(t2 == 0xFF000000);   // exponent is all 1s, fraction is 0
-}
-
-// Function is_nan: gives true for elements that are +NAN or -NAN
-// false for finite numbers and +/-INF
-// (the underscore in the name avoids a conflict with a macro in Intel's mathimf.h)
-static inline Vec16fb is_nan(Vec16f const & a) {
-    Vec16i t1 = _mm512_castps_si512(a); // reinterpret as 32-bit integer
-    Vec16i t2 = t1 << 1;                // shift out sign bit
-    Vec16i t3 = 0xFF000000;             // exponent mask
-    Vec16i t4 = t2 & t3;                // exponent
-    Vec16i t5 = _mm512_andnot_si512(t3,t2);// fraction
-    return Vec16fb(t4 == t3 && t5 != 0);// exponent = all 1s and fraction != 0
-}
-
-// Function is_subnormal: gives true for elements that are denormal (subnormal)
-// false for finite numbers, zero, NAN and INF
-static inline Vec16fb is_subnormal(Vec16f const & a) {
-    Vec16i t1 = _mm512_castps_si512(a);    // reinterpret as 32-bit integer
-    Vec16i t2 = t1 << 1;                   // shift out sign bit
-    Vec16i t3 = 0xFF000000;                // exponent mask
-    Vec16i t4 = t2 & t3;                   // exponent
-    Vec16i t5 = _mm512_andnot_si512(t3,t2);// fraction
-    return Vec16fb(t4 == 0 && t5 != 0);     // exponent = 0 and fraction != 0
-}
-
-// Function is_zero_or_subnormal: gives true for elements that are zero or subnormal (denormal)
-// false for finite numbers, NAN and INF
-static inline Vec16fb is_zero_or_subnormal(Vec16f const & a) {
-    Vec16i t = _mm512_castps_si512(a);            // reinterpret as 32-bit integer
-           t &= 0x7F800000;                       // isolate exponent
-    return Vec16fb(t == 0);                       // exponent = 0
-}
-
-// Function infinite4f: returns a vector where all elements are +INF
-static inline Vec16f infinite16f() {
-    union {
-        int32_t i;
-        float   f;
-    } inf = {0x7F800000};
-    return Vec16f(inf.f);
-}
-
-// Function nan4f: returns a vector where all elements are +NAN (quiet)
-static inline Vec16f nan16f(int n = 0x10) {
-    union {
-        int32_t i;
-        float   f;
-    } nanf = {0x7FC00000 + n};
-    return Vec16f(nanf.f);
-}
-
-// change signs on vectors Vec16f
-// Each index i0 - i7 is 1 for changing sign on the corresponding element, 0 for no change
-template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7, int i8, int i9, int i10, int i11, int i12, int i13, int i14, int i15>
-static inline Vec16f change_sign(Vec16f const & a) {
-    const __mmask16 m = __mmask16((i0&1) | (i1&1)<<1 | (i2&1)<< 2 | (i3&1)<<3 | (i4&1)<<4 | (i5&1)<<5 | (i6&1)<<6 | (i7&1)<<7
-        | (i8&1)<<8 | (i9&1)<<9 | (i10&1)<<10 | (i11&1)<<11 | (i12&1)<<12 | (i13&1)<<13 | (i14&1)<<14 | (i15&1)<<15);
-    if ((uint16_t)m == 0) return a;
-    __m512 s = _mm512_castsi512_ps(_mm512_maskz_set1_epi32(m, 0x80000000));
-    return a ^ s;
-}
-
+//static Vec16f exp2(Vec16f const x); // defined in vectormath_exp.h
 
 
 /*****************************************************************************
@@ -1033,26 +737,25 @@ protected:
     __m512d zmm; // double vector
 public:
     // Default constructor:
-    Vec8d() {
-    }
+    Vec8d() = default;
     // Constructor to broadcast the same value into all elements:
     Vec8d(double d) {
         zmm = _mm512_set1_pd(d);
     }
     // Constructor to build from all elements:
     Vec8d(double d0, double d1, double d2, double d3, double d4, double d5, double d6, double d7) {
-        zmm = _mm512_setr_pd(d0, d1, d2, d3, d4, d5, d6, d7); 
+        zmm = _mm512_setr_pd(d0, d1, d2, d3, d4, d5, d6, d7);
     }
     // Constructor to build from two Vec4d:
-    Vec8d(Vec4d const & a0, Vec4d const & a1) {
+    Vec8d(Vec4d const a0, Vec4d const a1) {
         zmm = _mm512_insertf64x4(_mm512_castpd256_pd512(a0), a1, 1);
     }
     // Constructor to convert from type __m512d used in intrinsics:
-    Vec8d(__m512d const & x) {
+    Vec8d(__m512d const x) {
         zmm = x;
     }
     // Assignment operator to convert from type __m512d used in intrinsics:
-    Vec8d & operator = (__m512d const & x) {
+    Vec8d & operator = (__m512d const x) {
         zmm = x;
         return *this;
     }
@@ -1076,12 +779,19 @@ public:
     void store(double * p) const {
         _mm512_storeu_pd(p, zmm);
     }
-    // Member function to store into array, aligned by 64
+    // Member function storing into array, aligned by 64
     // You may use store_a instead of store if you are certain that p points to an address
     // divisible by 64
     void store_a(double * p) const {
         _mm512_store_pd(p, zmm);
     }
+    // Member function storing to aligned uncached memory (non-temporal store).
+    // This may be more efficient than store_a when storing large blocks of memory if it 
+    // is unlikely that the data will stay in the cache until it is read again.
+    // Note: Will generate runtime error if p is not aligned by 16
+    void store_nt(double * p) const {
+        _mm512_stream_pd(p, zmm);
+    } 
     // Partial load. Load n elements and set the rest to 0
     Vec8d & load_partial(int n, double const * p) {
         zmm = _mm512_maskz_loadu_pd(__mmask16((1<<n)-1), p);
@@ -1097,22 +807,24 @@ public:
         return *this;
     }
     // Member function to change a single element in vector
-    // Note: This function is inefficient. Use load function if changing more than one element
-    Vec8d const & insert(uint32_t index, double value) {
-        //zmm = _mm512_mask_set1_pd(zmm, __mmask16(1 << index), value);  // this intrinsic function does not exist (yet?)
-        zmm = _mm512_castsi512_pd(_mm512_mask_set1_epi64(_mm512_castpd_si512(zmm), __mmask16(1 << index), *(int64_t*)&value)); // ignore warning
+    Vec8d const insert(int index, double value) {
+        zmm = _mm512_mask_broadcastsd_pd(zmm, __mmask8(1u << index), _mm_set_sd(value));
         return *this;
     }
     // Member function extract a single element from vector
-    double extract(uint32_t index) const {
+    double extract(int index) const {
+#if INSTRSET >= 10
+        __m512d x = _mm512_maskz_compress_pd(__mmask8(1u << index), zmm);
+        return _mm512_cvtsd_f64(x);
+#else
         double a[8];
         store(a);
-        return a[index & 7];        
+        return a[index & 7];
+#endif
     }
-
     // Extract a single element. Use store function if extracting more than one element.
     // Operator [] can only read an element, not write.
-    double operator [] (uint32_t index) const {
+    double operator [] (int index) const {
         return extract(index);
     }
     // Member functions to split into two Vec4d:
@@ -1122,13 +834,16 @@ public:
     Vec4d get_high() const {
         return _mm512_extractf64x4_pd(zmm,1);
     }
-    static int size () {
+    static constexpr int size() {
         return 8;
     }
+    static constexpr int elementtype() {
+        return 17;
+    }
+    typedef __m512d registertype;
 };
 
 
-
 /*****************************************************************************
 *
 *          Operators for Vec8d
@@ -1136,20 +851,20 @@ public:
 *****************************************************************************/
 
 // vector operator + : add element by element
-static inline Vec8d operator + (Vec8d const & a, Vec8d const & b) {
+static inline Vec8d operator + (Vec8d const a, Vec8d const b) {
     return _mm512_add_pd(a, b);
 }
 
 // vector operator + : add vector and scalar
-static inline Vec8d operator + (Vec8d const & a, double b) {
+static inline Vec8d operator + (Vec8d const a, double b) {
     return a + Vec8d(b);
 }
-static inline Vec8d operator + (double a, Vec8d const & b) {
+static inline Vec8d operator + (double a, Vec8d const b) {
     return Vec8d(a) + b;
 }
 
 // vector operator += : add
-static inline Vec8d & operator += (Vec8d & a, Vec8d const & b) {
+static inline Vec8d & operator += (Vec8d & a, Vec8d const b) {
     a = a + b;
     return a;
 }
@@ -1168,26 +883,26 @@ static inline Vec8d & operator ++ (Vec8d & a) {
 }
 
 // vector operator - : subtract element by element
-static inline Vec8d operator - (Vec8d const & a, Vec8d const & b) {
+static inline Vec8d operator - (Vec8d const a, Vec8d const b) {
     return _mm512_sub_pd(a, b);
 }
 
 // vector operator - : subtract vector and scalar
-static inline Vec8d operator - (Vec8d const & a, double b) {
+static inline Vec8d operator - (Vec8d const a, double b) {
     return a - Vec8d(b);
 }
-static inline Vec8d operator - (double a, Vec8d const & b) {
+static inline Vec8d operator - (double a, Vec8d const b) {
     return Vec8d(a) - b;
 }
 
 // vector operator - : unary minus
 // Change sign bit, even for 0, INF and NAN
-static inline Vec8d operator - (Vec8d const & a) {
+static inline Vec8d operator - (Vec8d const a) {
     return _mm512_castsi512_pd(Vec8q(_mm512_castpd_si512(a)) ^ Vec8q(0x8000000000000000));
 }
 
 // vector operator -= : subtract
-static inline Vec8d & operator -= (Vec8d & a, Vec8d const & b) {
+static inline Vec8d & operator -= (Vec8d & a, Vec8d const b) {
     a = a - b;
     return a;
 }
@@ -1206,150 +921,276 @@ static inline Vec8d & operator -- (Vec8d & a) {
 }
 
 // vector operator * : multiply element by element
-static inline Vec8d operator * (Vec8d const & a, Vec8d const & b) {
+static inline Vec8d operator * (Vec8d const a, Vec8d const b) {
     return _mm512_mul_pd(a, b);
 }
 
 // vector operator * : multiply vector and scalar
-static inline Vec8d operator * (Vec8d const & a, double b) {
+static inline Vec8d operator * (Vec8d const a, double b) {
     return a * Vec8d(b);
 }
-static inline Vec8d operator * (double a, Vec8d const & b) {
+static inline Vec8d operator * (double a, Vec8d const b) {
     return Vec8d(a) * b;
 }
 
 // vector operator *= : multiply
-static inline Vec8d & operator *= (Vec8d & a, Vec8d const & b) {
+static inline Vec8d & operator *= (Vec8d & a, Vec8d const b) {
     a = a * b;
     return a;
 }
 
 // vector operator / : divide all elements by same integer
-static inline Vec8d operator / (Vec8d const & a, Vec8d const & b) {
+static inline Vec8d operator / (Vec8d const a, Vec8d const b) {
     return _mm512_div_pd(a, b);
 }
 
 // vector operator / : divide vector and scalar
-static inline Vec8d operator / (Vec8d const & a, double b) {
+static inline Vec8d operator / (Vec8d const a, double b) {
     return a / Vec8d(b);
 }
-static inline Vec8d operator / (double a, Vec8d const & b) {
+static inline Vec8d operator / (double a, Vec8d const b) {
     return Vec8d(a) / b;
 }
 
 // vector operator /= : divide
-static inline Vec8d & operator /= (Vec8d & a, Vec8d const & b) {
+static inline Vec8d & operator /= (Vec8d & a, Vec8d const b) {
     a = a / b;
     return a;
 }
 
 // vector operator == : returns true for elements for which a == b
-static inline Vec8db operator == (Vec8d const & a, Vec8d const & b) {
+static inline Vec8db operator == (Vec8d const a, Vec8d const b) {
     return _mm512_cmp_pd_mask(a, b, 0);
 }
 
 // vector operator != : returns true for elements for which a != b
-static inline Vec8db operator != (Vec8d const & a, Vec8d const & b) {
+static inline Vec8db operator != (Vec8d const a, Vec8d const b) {
     return _mm512_cmp_pd_mask(a, b, 4);
 }
 
 // vector operator < : returns true for elements for which a < b
-static inline Vec8db operator < (Vec8d const & a, Vec8d const & b) {
+static inline Vec8db operator < (Vec8d const a, Vec8d const b) {
     return _mm512_cmp_pd_mask(a, b, 1);
 }
 
 // vector operator <= : returns true for elements for which a <= b
-static inline Vec8db operator <= (Vec8d const & a, Vec8d const & b) {
+static inline Vec8db operator <= (Vec8d const a, Vec8d const b) {
     return _mm512_cmp_pd_mask(a, b, 2);
 }
 
 // vector operator > : returns true for elements for which a > b
-static inline Vec8db operator > (Vec8d const & a, Vec8d const & b) {
-    return b < a;
+static inline Vec8db operator > (Vec8d const a, Vec8d const b) {
+    return _mm512_cmp_pd_mask(a, b, 6+8);
 }
 
 // vector operator >= : returns true for elements for which a >= b
-static inline Vec8db operator >= (Vec8d const & a, Vec8d const & b) {
-    return b <= a;
+static inline Vec8db operator >= (Vec8d const a, Vec8d const b) {
+    return _mm512_cmp_pd_mask(a, b, 5+8);
 }
 
 // Bitwise logical operators
 
 // vector operator & : bitwise and
-static inline Vec8d operator & (Vec8d const & a, Vec8d const & b) {
+static inline Vec8d operator & (Vec8d const a, Vec8d const b) {
     return _mm512_castsi512_pd(Vec8q(_mm512_castpd_si512(a)) & Vec8q(_mm512_castpd_si512(b)));
 }
 
 // vector operator &= : bitwise and
-static inline Vec8d & operator &= (Vec8d & a, Vec8d const & b) {
+static inline Vec8d & operator &= (Vec8d & a, Vec8d const b) {
     a = a & b;
     return a;
 }
 
 // vector operator & : bitwise and of Vec8d and Vec8db
-static inline Vec8d operator & (Vec8d const & a, Vec8db const & b) {
-    return _mm512_maskz_mov_pd(b, a);
+static inline Vec8d operator & (Vec8d const a, Vec8db const b) {
+    return _mm512_maskz_mov_pd((uint8_t)b, a);
 }
 
-static inline Vec8d operator & (Vec8db const & a, Vec8d const & b) {
+static inline Vec8d operator & (Vec8db const a, Vec8d const b) {
     return b & a;
 }
 
 // vector operator | : bitwise or
-static inline Vec8d operator | (Vec8d const & a, Vec8d const & b) {
+static inline Vec8d operator | (Vec8d const a, Vec8d const b) {
     return _mm512_castsi512_pd(Vec8q(_mm512_castpd_si512(a)) | Vec8q(_mm512_castpd_si512(b)));
 }
 
 // vector operator |= : bitwise or
-static inline Vec8d & operator |= (Vec8d & a, Vec8d const & b) {
+static inline Vec8d & operator |= (Vec8d & a, Vec8d const b) {
     a = a | b;
     return a;
 }
 
 // vector operator ^ : bitwise xor
-static inline Vec8d operator ^ (Vec8d const & a, Vec8d const & b) {
+static inline Vec8d operator ^ (Vec8d const a, Vec8d const b) {
     return _mm512_castsi512_pd(Vec8q(_mm512_castpd_si512(a)) ^ Vec8q(_mm512_castpd_si512(b)));
 }
 
 // vector operator ^= : bitwise xor
-static inline Vec8d & operator ^= (Vec8d & a, Vec8d const & b) {
+static inline Vec8d & operator ^= (Vec8d & a, Vec8d const b) {
     a = a ^ b;
     return a;
 }
 
 // vector operator ! : logical not. Returns Boolean vector
-static inline Vec8db operator ! (Vec8d const & a) {
+static inline Vec8db operator ! (Vec8d const a) {
     return a == Vec8d(0.0);
 }
 
 
-/*****************************************************************************
-*
-*          Functions for Vec8d
-*
-*****************************************************************************/
+/*****************************************************************************
+*
+*          Functions for Vec8d
+*
+*****************************************************************************/
+
+// Select between two operands. Corresponds to this pseudocode:
+// for (int i = 0; i < 2; i++) result[i] = s[i] ? a[i] : b[i];
+static inline Vec8d select (Vec8db const s, Vec8d const a, Vec8d const b) {
+    return _mm512_mask_mov_pd (b, (uint8_t)s, a);
+}
+
+// Conditional add: For all vector elements i: result[i] = f[i] ? (a[i] + b[i]) : a[i]
+static inline Vec8d if_add (Vec8db const f, Vec8d const a, Vec8d const b) {
+    return _mm512_mask_add_pd(a, (uint8_t)f, a, b);
+}
+
+// Conditional subtract
+static inline Vec8d if_sub (Vec8db const f, Vec8d const a, Vec8d const b) {
+    return _mm512_mask_sub_pd(a, (uint8_t)f, a, b);
+}
+
+// Conditional multiply
+static inline Vec8d if_mul (Vec8db const f, Vec8d const a, Vec8d const b) {
+    return _mm512_mask_mul_pd(a, (uint8_t)f, a, b);
+}
+
+// Conditional divide
+static inline Vec8d if_div (Vec8db const f, Vec8d const a, Vec8d const b) {
+    return _mm512_mask_div_pd(a, (uint8_t)f, a, b);
+}
+
+// Sign functions
+
+// Function sign_bit: gives true for elements that have the sign bit set
+// even for -0.0, -INF and -NAN
+static inline Vec8db sign_bit(Vec8d const a) {
+    Vec8q t1 = _mm512_castpd_si512(a);    // reinterpret as 64-bit integer
+    return Vec8db(t1 < 0);
+}
+
+// Function sign_combine: changes the sign of a when b has the sign bit set
+// same as select(sign_bit(b), -a, a)
+static inline Vec8d sign_combine(Vec8d const a, Vec8d const b) {
+    // return a ^ (b & Vec8d(-0.0));
+    return _mm512_castsi512_pd (_mm512_ternarylogic_epi64(
+        _mm512_castpd_si512(a), _mm512_castpd_si512(b), Vec8q(0x8000000000000000), 0x78));
+}
+
+// Categorization functions
+
+// Function is_finite: gives true for elements that are normal, subnormal or zero,
+// false for INF and NAN
+static inline Vec8db is_finite(Vec8d const a) {
+#if INSTRSET >= 10 // __AVX512DQ__
+    __mmask8 f = _mm512_fpclass_pd_mask(a, 0x99);
+    return __mmask8(_mm512_knot(f));
+#else
+    Vec8q  t1 = _mm512_castpd_si512(a); // reinterpret as 64-bit integer
+    Vec8q  t2 = t1 << 1;                // shift out sign bit
+    Vec8q  t3 = 0xFFE0000000000000ll;   // exponent mask
+    Vec8qb t4 = Vec8q(t2 & t3) != t3;   // exponent field is not all 1s
+    return Vec8db(t4);
+#endif
+}
+
+// Function is_inf: gives true for elements that are +INF or -INF
+// false for finite numbers and NAN
+static inline Vec8db is_inf(Vec8d const a) {
+#if INSTRSET >= 10  // __AVX512DQ__
+    return _mm512_fpclass_pd_mask(a, 0x18);
+#else
+    Vec8q t1 = _mm512_castpd_si512(a);           // reinterpret as 64-bit integer
+    Vec8q t2 = t1 << 1;                          // shift out sign bit
+    return Vec8db(t2 == 0xFFE0000000000000ll);   // exponent is all 1s, fraction is 0
+#endif
+}
+
+// Function is_nan: gives true for elements that are +NAN or -NAN
+// false for finite numbers and +/-INF
+// (the underscore in the name avoids a conflict with a macro in Intel's mathimf.h)
+#if INSTRSET >= 10
+static inline Vec8db is_nan(Vec8d const a) {
+    // assume that compiler does not optimize this away with -ffinite-math-only:
+    return _mm512_fpclass_pd_mask(a, 0x81);
+}
+//#elif defined(__GNUC__) && !defined(__INTEL_COMPILER) && !defined(__clang__)
+//__attribute__((optimize("-fno-unsafe-math-optimizations")))
+//static inline Vec8db is_nan(Vec8d const a) {
+//    return a != a; // not safe with -ffinite-math-only compiler option
+//}
+#elif (defined(__GNUC__) || defined(__clang__)) && !defined(__INTEL_COMPILER)
+static inline Vec8db is_nan(Vec8d const a) {
+    __m512d aa = a;
+    __mmask16 unordered;
+    __asm volatile("vcmppd $3, %1, %1, %0" : "=Yk" (unordered) :  "v" (aa) );
+    return Vec8db(unordered);
+}
+#else
+static inline Vec8db is_nan(Vec8d const a) {
+    // assume that compiler does not optimize this away with -ffinite-math-only:
+    return Vec8db().load_bits(_mm512_cmp_pd_mask(a, a, 3)); // compare unordered
+    // return a != a; // This is not safe with -ffinite-math-only, -ffast-math, or /fp:fast compiler option
+}
+#endif
+
 
-// Select between two operands. Corresponds to this pseudocode:
-// for (int i = 0; i < 2; i++) result[i] = s[i] ? a[i] : b[i];
-static inline Vec8d select (Vec8db const & s, Vec8d const & a, Vec8d const & b) {
-    return _mm512_mask_mov_pd (b, s, a);
+// Function is_subnormal: gives true for elements that are subnormal
+// false for finite numbers, zero, NAN and INF
+static inline Vec8db is_subnormal(Vec8d const a) {
+#if INSTRSET >= 10  // __AVX512DQ__
+    return _mm512_fpclass_pd_mask(a, 0x20);
+#else
+    Vec8q t1 = _mm512_castpd_si512(a); // reinterpret as 64-bit integer
+    Vec8q t2 = t1 << 1;                // shift out sign bit
+    Vec8q t3 = 0xFFE0000000000000ll;   // exponent mask
+    Vec8q t4 = t2 & t3;                // exponent
+    Vec8q t5 = _mm512_andnot_si512(t3,t2);// fraction
+    return Vec8db(t4 == 0 && t5 != 0); // exponent = 0 and fraction != 0
+#endif
 }
 
-// Conditional add: For all vector elements i: result[i] = f[i] ? (a[i] + b[i]) : a[i]
-static inline Vec8d if_add (Vec8db const & f, Vec8d const & a, Vec8d const & b) {
-    return _mm512_mask_add_pd(a, f, a, b);
+// Function is_zero_or_subnormal: gives true for elements that are zero or subnormal
+// false for finite numbers, NAN and INF
+static inline Vec8db is_zero_or_subnormal(Vec8d const a) {
+#if INSTRSET >= 10  // __AVX512DQ__
+    return _mm512_fpclass_pd_mask(a, 0x26);
+#else
+    Vec8q t = _mm512_castpd_si512(a);            // reinterpret as 32-bit integer
+    t &= 0x7FF0000000000000ll;             // isolate exponent
+    return Vec8db(t == 0);                       // exponent = 0
+#endif
 }
 
-// Conditional multiply: For all vector elements i: result[i] = f[i] ? (a[i] * b[i]) : a[i]
-static inline Vec8d if_mul (Vec8db const & f, Vec8d const & a, Vec8d const & b) {
-    return _mm512_mask_mul_pd(a, f, a, b);
+// change signs on vectors Vec8d
+// Each index i0 - i3 is 1 for changing sign on the corresponding element, 0 for no change
+template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7>
+static inline Vec8d change_sign(Vec8d const a) {
+    const __mmask16 m = __mmask16((i0&1) | (i1&1)<<1 | (i2&1)<< 2 | (i3&1)<<3 | (i4&1)<<4 | (i5&1)<<5 | (i6&1)<<6 | (i7&1)<<7);
+    if ((uint8_t)m == 0) return a;
+#ifdef __x86_64__
+    __m512d s = _mm512_castsi512_pd(_mm512_maskz_set1_epi64(m, 0x8000000000000000));
+#else  // 32 bit mode
+    __m512i v = Vec8q(0x8000000000000000);
+    __m512d s = _mm512_castsi512_pd(_mm512_maskz_mov_epi64(m, v));
+#endif
+    return a ^ s;
 }
 
-
 // General arithmetic functions, etc.
 
 // Horizontal add: Calculates the sum of all vector elements.
-static inline double horizontal_add (Vec8d const & a) {
+static inline double horizontal_add (Vec8d const a) {
 #if defined(__INTEL_COMPILER)
     return _mm512_reduce_add_pd(a);
 #else
@@ -1358,220 +1199,156 @@ static inline double horizontal_add (Vec8d const & a) {
 }
 
 // function max: a > b ? a : b
-static inline Vec8d max(Vec8d const & a, Vec8d const & b) {
+static inline Vec8d max(Vec8d const a, Vec8d const b) {
     return _mm512_max_pd(a,b);
 }
 
 // function min: a < b ? a : b
-static inline Vec8d min(Vec8d const & a, Vec8d const & b) {
+static inline Vec8d min(Vec8d const a, Vec8d const b) {
     return _mm512_min_pd(a,b);
 }
+// NAN-safe versions of maximum and minimum are in vector_convert.h
 
 // function abs: absolute value
-// Removes sign bit, even for -0.0f, -INF and -NAN
-static inline Vec8d abs(Vec8d const & a) {
-    return _mm512_castsi512_pd(Vec8q(_mm512_castpd_si512(a)) & Vec8q(0x7FFFFFFFFFFFFFFF));
+static inline Vec8d abs(Vec8d const a) {
+#if INSTRSET >= 10  // AVX512DQ
+    return _mm512_range_pd(a, a, 8);
+#else
+    return a & Vec8d(_mm512_castsi512_pd(Vec8q(0x7FFFFFFFFFFFFFFF)));
+#endif
 }
 
 // function sqrt: square root
-static inline Vec8d sqrt(Vec8d const & a) {
+static inline Vec8d sqrt(Vec8d const a) {
     return _mm512_sqrt_pd(a);
 }
 
 // function square: a * a
-static inline Vec8d square(Vec8d const & a) {
+static inline Vec8d square(Vec8d const a) {
     return a * a;
 }
 
-// pow(Vec8d, int):
-template <typename TT> static Vec8d pow(Vec8d const & a, TT const & n);
+// The purpose of this template is to prevent implicit conversion of a float
+// exponent to int when calling pow(vector, float) and vectormath_exp.h is not included
+template <typename TT> static Vec8d pow(Vec8d const a, TT const n); // = delete;
 
+// pow(Vec8d, int):
 // Raise floating point numbers to integer power n
 template <>
-inline Vec8d pow<int>(Vec8d const & x0, int const & n) {
+inline Vec8d pow<int>(Vec8d const x0, int const n) {
     return pow_template_i<Vec8d>(x0, n);
 }
 
 // allow conversion from unsigned int
 template <>
-inline Vec8d pow<uint32_t>(Vec8d const & x0, uint32_t const & n) {
+inline Vec8d pow<uint32_t>(Vec8d const x0, uint32_t const n) {
     return pow_template_i<Vec8d>(x0, (int)n);
 }
 
-
 // Raise floating point numbers to integer power n, where n is a compile-time constant
 template <int n>
-static inline Vec8d pow_n(Vec8d const & a) {
-    if (n < 0)    return Vec8d(1.0) / pow_n<-n>(a);
-    if (n == 0)   return Vec8d(1.0);
-    if (n >= 256) return pow(a, n);
-    Vec8d x = a;                       // a^(2^i)
-    Vec8d y;                           // accumulator
-    const int lowest = n - (n & (n-1));// lowest set bit in n
-    if (n & 1) y = x;
-    if (n < 2) return y;
-    x = x*x;                           // x^2
-    if (n & 2) {
-        if (lowest == 2) y = x; else y *= x;
-    }
-    if (n < 4) return y;
-    x = x*x;                           // x^4
-    if (n & 4) {
-        if (lowest == 4) y = x; else y *= x;
-    }
-    if (n < 8) return y;
-    x = x*x;                           // x^8
-    if (n & 8) {
-        if (lowest == 8) y = x; else y *= x;
-    }
-    if (n < 16) return y;
-    x = x*x;                           // x^16
-    if (n & 16) {
-        if (lowest == 16) y = x; else y *= x;
-    }
-    if (n < 32) return y;
-    x = x*x;                           // x^32
-    if (n & 32) {
-        if (lowest == 32) y = x; else y *= x;
-    }
-    if (n < 64) return y;
-    x = x*x;                           // x^64
-    if (n & 64) {
-        if (lowest == 64) y = x; else y *= x;
-    }
-    if (n < 128) return y;
-    x = x*x;                           // x^128
-    if (n & 128) {
-        if (lowest == 128) y = x; else y *= x;
-    }
-    return y;
-}
-
-template <int n>
-static inline Vec8d pow(Vec8d const & a, Const_int_t<n>) {
-    return pow_n<n>(a);
+static inline Vec8d pow(Vec8d const a, Const_int_t<n>) {
+    return pow_n<Vec8d, n>(a);
 }
 
 
 // function round: round to nearest integer (even). (result as double vector)
-static inline Vec8d round(Vec8d const & a) {
+static inline Vec8d round(Vec8d const a) {
     return _mm512_roundscale_pd(a, 0);
 }
 
 // function truncate: round towards zero. (result as double vector)
-static inline Vec8d truncate(Vec8d const & a) {
+static inline Vec8d truncate(Vec8d const a) {
     return _mm512_roundscale_pd(a, 3);
 }
 
 // function floor: round towards minus infinity. (result as double vector)
-static inline Vec8d floor(Vec8d const & a) {
+static inline Vec8d floor(Vec8d const a) {
     return _mm512_roundscale_pd(a, 1);
 }
 
 // function ceil: round towards plus infinity. (result as double vector)
-static inline Vec8d ceil(Vec8d const & a) {
+static inline Vec8d ceil(Vec8d const a) {
     return _mm512_roundscale_pd(a, 2);
 }
 
-// function round_to_int: round to nearest integer (even). (result as integer vector)
-static inline Vec8i round_to_int(Vec8d const & a) {
+// function round_to_int32: round to nearest integer (even). (result as integer vector)
+static inline Vec8i round_to_int32(Vec8d const a) {
     //return _mm512_cvtpd_epi32(a);
     return _mm512_cvt_roundpd_epi32(a, 0+8);
 }
+//static inline Vec8i round_to_int(Vec8d const a) {return round_to_int32(a);} // deprecated
+
 
-// function truncate_to_int: round towards zero. (result as integer vector)
-static inline Vec8i truncate_to_int(Vec8d const & a) {
+// function truncate_to_int32: round towards zero. (result as integer vector)
+static inline Vec8i truncate_to_int32(Vec8d const a) {
     return _mm512_cvttpd_epi32(a);
 }
+//static inline Vec8i truncate_to_int(Vec8d const a) {return truncate_to_int32(a);} // deprecated
 
-// function truncate_to_int64: round towards zero. (inefficient)
-static inline Vec8q truncate_to_int64(Vec8d const & a) {
-#ifdef __AVX512DQ__
+
+// function truncatei: round towards zero
+static inline Vec8q truncatei(Vec8d const a) {
+#if INSTRSET >= 10  // __AVX512DQ__
     return _mm512_cvttpd_epi64(a);
 #else
-    double aa[8];
+    double aa[8];            // inefficient
     a.store(aa);
     return Vec8q(int64_t(aa[0]), int64_t(aa[1]), int64_t(aa[2]), int64_t(aa[3]), int64_t(aa[4]), int64_t(aa[5]), int64_t(aa[6]), int64_t(aa[7]));
 #endif
 }
+//static inline Vec8q truncate_to_int64(Vec8d const a) {return truncatei(a);} // deprecated
 
-// function truncate_to_int64_limited: round towards zero.
-// result as 64-bit integer vector, but with limited range. Deprecated!
-static inline Vec8q truncate_to_int64_limited(Vec8d const & a) {
-#ifdef __AVX512DQ__
-    return truncate_to_int64(a);
-#else
-    // Note: assume MXCSR control register is set to rounding
-    Vec4q   b = _mm512_cvttpd_epi32(a);                    // round to 32-bit integers
-    __m512i c = permute8q<0,-256,1,-256,2,-256,3,-256>(Vec8q(b,b));      // get bits 64-127 to position 128-191, etc.
-    __m512i s = _mm512_srai_epi32(c, 31);                  // sign extension bits
-    return      _mm512_unpacklo_epi32(c, s);               // interleave with sign extensions
-#endif
-} 
-
-// function round_to_int64: round to nearest or even. (inefficient)
-static inline Vec8q round_to_int64(Vec8d const & a) {
-#ifdef __AVX512DQ__
+// function roundi: round to nearest or even
+static inline Vec8q roundi(Vec8d const a) {
+#if INSTRSET >= 10  // __AVX512DQ__
     return _mm512_cvtpd_epi64(a);
 #else
-    return truncate_to_int64(round(a));
-#endif
-}
-
-// function round_to_int64_limited: round to nearest integer (even)
-// result as 64-bit integer vector, but with limited range. Deprecated!
-static inline Vec8q round_to_int64_limited(Vec8d const & a) {
-#ifdef __AVX512DQ__
-    return round_to_int64(a);
-#else
-    Vec4q   b = _mm512_cvt_roundpd_epi32(a, 0+8);     // round to 32-bit integers   
-    __m512i c = permute8q<0,-256,1,-256,2,-256,3,-256>(Vec8q(b,b));  // get bits 64-127 to position 128-191, etc.
-    __m512i s = _mm512_srai_epi32(c, 31);                            // sign extension bits
-    return      _mm512_unpacklo_epi32(c, s);                         // interleave with sign extensions
+    return truncatei(round(a));
 #endif
 }
+//static inline Vec8q round_to_int64(Vec8d const a) {return roundi(a);} // deprecated
 
-// function to_double: convert integer vector elements to double vector (inefficient)
-static inline Vec8d to_double(Vec8q const & a) {
-#if defined (__AVX512DQ__)
+// function to_double: convert integer vector elements to double vector
+static inline Vec8d to_double(Vec8q const a) {
+#if INSTRSET >= 10 // __AVX512DQ__
     return _mm512_cvtepi64_pd(a);
 #else
-    int64_t aa[8];
+    int64_t aa[8];           // inefficient
     a.store(aa);
     return Vec8d(double(aa[0]), double(aa[1]), double(aa[2]), double(aa[3]), double(aa[4]), double(aa[5]), double(aa[6]), double(aa[7]));
 #endif
 }
 
-// function to_double_limited: convert integer vector elements to double vector
-// limited to abs(x) < 2^31. Deprecated!
-static inline Vec8d to_double_limited(Vec8q const & x) {
-#if defined (__AVX512DQ__)
-    return to_double(x);
+static inline Vec8d to_double(Vec8uq const a) {
+#if INSTRSET >= 10 // __AVX512DQ__
+    return _mm512_cvtepu64_pd(a);
 #else
-    Vec16i compressed = permute16i<0,2,4,6,8,10,12,14,-256,-256,-256,-256,-256,-256,-256,-256>(Vec16i(x));
-    return _mm512_cvtepi32_pd(compressed.get_low());
+    uint64_t aa[8];          // inefficient
+    a.store(aa);
+    return Vec8d(double(aa[0]), double(aa[1]), double(aa[2]), double(aa[3]), double(aa[4]), double(aa[5]), double(aa[6]), double(aa[7]));
 #endif
 }
 
 // function to_double: convert integer vector to double vector
-static inline Vec8d to_double(Vec8i const & a) {
+static inline Vec8d to_double(Vec8i const a) {
     return _mm512_cvtepi32_pd(a);
 }
 
 // function compress: convert two Vec8d to one Vec16f
-static inline Vec16f compress (Vec8d const & low, Vec8d const & high) {
+static inline Vec16f compress (Vec8d const low, Vec8d const high) {
     __m256 t1 = _mm512_cvtpd_ps(low);
     __m256 t2 = _mm512_cvtpd_ps(high);
     return Vec16f(t1, t2);
 }
 
 // Function extend_low : convert Vec16f vector elements 0 - 3 to Vec8d
-static inline Vec8d extend_low(Vec16f const & a) {
+static inline Vec8d extend_low(Vec16f const a) {
     return _mm512_cvtps_pd(_mm512_castps512_ps256(a));
 }
 
 // Function extend_high : convert Vec16f vector elements 4 - 7 to Vec8d
-static inline Vec8d extend_high (Vec16f const & a) {
+static inline Vec8d extend_high (Vec16f const a) {
     return _mm512_cvtps_pd(a.get_high());
 }
 
@@ -1579,22 +1356,22 @@ static inline Vec8d extend_high (Vec16f const & a) {
 // Fused multiply and add functions
 
 // Multiply and add
-static inline Vec8d mul_add(Vec8d const & a, Vec8d const & b, Vec8d const & c) {
+static inline Vec8d mul_add(Vec8d const a, Vec8d const b, Vec8d const c) {
     return _mm512_fmadd_pd(a, b, c);
 }
 
 // Multiply and subtract
-static inline Vec8d mul_sub(Vec8d const & a, Vec8d const & b, Vec8d const & c) {
+static inline Vec8d mul_sub(Vec8d const a, Vec8d const b, Vec8d const c) {
     return _mm512_fmsub_pd(a, b, c);
 }
 
 // Multiply and inverse subtract
-static inline Vec8d nmul_add(Vec8d const & a, Vec8d const & b, Vec8d const & c) {
+static inline Vec8d nmul_add(Vec8d const a, Vec8d const b, Vec8d const c) {
     return _mm512_fnmadd_pd(a, b, c);
 }
 
-// Multiply and subtract with extra precision on the intermediate calculations, 
-static inline Vec8d mul_sub_x(Vec8d const & a, Vec8d const & b, Vec8d const & c) {
+// Multiply and subtract with extra precision on the intermediate calculations. used internally in math functions
+static inline Vec8d mul_sub_x(Vec8d const a, Vec8d const b, Vec8d const c) {
     return _mm512_fmsub_pd(a, b, c);
 }
 
@@ -1604,7 +1381,7 @@ static inline Vec8d mul_sub_x(Vec8d const & a, Vec8d const & b, Vec8d const & c)
 // Extract the exponent as an integer
 // exponent(a) = floor(log2(abs(a)));
 // exponent(1.0) = 0, exponent(0.0) = -1023, exponent(INF) = +1024, exponent(NAN) = +1024
-static inline Vec8q exponent(Vec8d const & a) {
+static inline Vec8q exponent(Vec8d const a) {
     Vec8uq t1 = _mm512_castpd_si512(a);// reinterpret as 64-bit integer
     Vec8uq t2 = t1 << 1;               // shift out sign bit
     Vec8uq t3 = t2 >> 53;              // shift down logical to position 0
@@ -1614,8 +1391,8 @@ static inline Vec8q exponent(Vec8d const & a) {
 
 // Extract the fraction part of a floating point number
 // a = 2^exponent(a) * fraction(a), except for a = 0
-// fraction(1.0) = 1.0, fraction(5.0) = 1.25 
-static inline Vec8d fraction(Vec8d const & a) {
+// fraction(1.0) = 1.0, fraction(5.0) = 1.25
+static inline Vec8d fraction(Vec8d const a) {
     return _mm512_getmant_pd(a, _MM_MANT_NORM_1_2, _MM_MANT_SIGN_zero);
 }
 
@@ -1623,117 +1400,16 @@ static inline Vec8d fraction(Vec8d const & a) {
 // n  =     0 gives 1.0
 // n >=  1024 gives +INF
 // n <= -1023 gives 0.0
-// This function will never produce denormals, and never raise exceptions
-static inline Vec8d exp2(Vec8q const & n) {
+// This function will never produce subnormals, and never raise exceptions
+static inline Vec8d exp2(Vec8q const n) {
     Vec8q t1 = max(n,  -0x3FF);        // limit to allowed range
     Vec8q t2 = min(t1,  0x400);
     Vec8q t3 = t2 + 0x3FF;             // add bias
     Vec8q t4 = t3 << 52;               // put exponent into position 52
     return _mm512_castsi512_pd(t4);    // reinterpret as double
 }
-//static Vec8d exp2(Vec8d const & x); // defined in vectormath_exp.h
-
-
-// Categorization functions
-
-// Function sign_bit: gives true for elements that have the sign bit set
-// even for -0.0, -INF and -NAN
-// Note that sign_bit(Vec8d(-0.0)) gives true, while Vec8d(-0.0) < Vec8d(0.0) gives false
-static inline Vec8db sign_bit(Vec8d const & a) {
-    Vec8q t1 = _mm512_castpd_si512(a);    // reinterpret as 64-bit integer
-    return Vec8db(t1 < 0);
-}
-
-// Function sign_combine: changes the sign of a when b has the sign bit set
-// same as select(sign_bit(b), -a, a)
-static inline Vec8d sign_combine(Vec8d const & a, Vec8d const & b) {
-    union {
-        uint64_t i;
-        double f;
-    } u = {0x8000000000000000};  // mask for sign bit
-    return a ^ (b & Vec8d(u.f));
-}
-
-// Function is_finite: gives true for elements that are normal, denormal or zero, 
-// false for INF and NAN
-static inline Vec8db is_finite(Vec8d const & a) {
-#ifdef __AVX512DQ__
-    __mmask8 f = _mm512_fpclass_pd_mask(a, 0x99);
-    return _mm512_knot(f);
-#else
-    Vec8q  t1 = _mm512_castpd_si512(a); // reinterpret as 64-bit integer
-    Vec8q  t2 = t1 << 1;                // shift out sign bit
-    Vec8q  t3 = 0xFFE0000000000000ll;   // exponent mask
-    Vec8qb t4 = Vec8q(t2 & t3) != t3;   // exponent field is not all 1s
-    return Vec8db(t4);
-#endif
-}
-
-// Function is_inf: gives true for elements that are +INF or -INF
-// false for finite numbers and NAN
-static inline Vec8db is_inf(Vec8d const & a) {
-    Vec8q t1 = _mm512_castpd_si512(a);           // reinterpret as 64-bit integer
-    Vec8q t2 = t1 << 1;                          // shift out sign bit
-    return Vec8db(t2 == 0xFFE0000000000000ll);   // exponent is all 1s, fraction is 0
-}
-
-// Function is_nan: gives true for elements that are +NAN or -NAN
-// false for finite numbers and +/-INF
-static inline Vec8db is_nan(Vec8d const & a) {
-    Vec8q t1 = _mm512_castpd_si512(a); // reinterpret as 64-bit integer
-    Vec8q t2 = t1 << 1;                // shift out sign bit
-    Vec8q t3 = 0xFFE0000000000000ll;   // exponent mask
-    Vec8q t4 = t2 & t3;                // exponent
-    Vec8q t5 = _mm512_andnot_si512(t3,t2);// fraction
-    return Vec8db(t4 == t3 && t5 != 0);// exponent = all 1s and fraction != 0
-}
-
-// Function is_subnormal: gives true for elements that are denormal (subnormal)
-// false for finite numbers, zero, NAN and INF
-static inline Vec8db is_subnormal(Vec8d const & a) {
-    Vec8q t1 = _mm512_castpd_si512(a); // reinterpret as 64-bit integer
-    Vec8q t2 = t1 << 1;                // shift out sign bit
-    Vec8q t3 = 0xFFE0000000000000ll;   // exponent mask
-    Vec8q t4 = t2 & t3;                // exponent
-    Vec8q t5 = _mm512_andnot_si512(t3,t2);// fraction
-    return Vec8db(t4 == 0 && t5 != 0); // exponent = 0 and fraction != 0
-}
-
-// Function is_zero_or_subnormal: gives true for elements that are zero or subnormal (denormal)
-// false for finite numbers, NAN and INF
-static inline Vec8db is_zero_or_subnormal(Vec8d const & a) {
-    Vec8q t = _mm512_castpd_si512(a);            // reinterpret as 32-bit integer
-          t &= 0x7FF0000000000000ll;             // isolate exponent
-    return Vec8db(t == 0);                       // exponent = 0
-}
-
-// Function infinite2d: returns a vector where all elements are +INF
-static inline Vec8d infinite8d() {
-    union {
-        uint64_t i;
-        double f;
-    } u = {0x7FF0000000000000};
-    return Vec8d(u.f);
-}
-
-// Function nan8d: returns a vector where all elements are +NAN (quiet NAN)
-static inline Vec8d nan8d(int n = 0x10) {
-    union {
-        uint64_t i;
-        double f;
-    } u = {0x7FF8000000000000 + uint64_t(n)};
-    return Vec8d(u.f);
-}
+//static Vec8d exp2(Vec8d const x);    // defined in vectormath_exp.h
 
-// change signs on vectors Vec8d
-// Each index i0 - i3 is 1 for changing sign on the corresponding element, 0 for no change
-template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7>
-static inline Vec8d change_sign(Vec8d const & a) {
-    const __mmask16 m = __mmask16((i0&1) | (i1&1)<<1 | (i2&1)<< 2 | (i3&1)<<3 | (i4&1)<<4 | (i5&1)<<5 | (i6&1)<<6 | (i7&1)<<7);
-    if ((uint8_t)m == 0) return a;
-    __m512d s = _mm512_castsi512_pd(_mm512_maskz_set1_epi64(m, 0x8000000000000000));
-    return a ^ s;
-}
 
 
 /*****************************************************************************
@@ -1744,42 +1420,81 @@ static inline Vec8d change_sign(Vec8d const & a) {
 
 // AVX512 requires gcc version 4.9 or higher. Apparently the problem with mangling intrinsic vector types no longer exists in gcc 4.x
 
-static inline __m512i reinterpret_i (__m512i const & x) {
+static inline __m512i reinterpret_i (__m512i const x) {
     return x;
 }
 
-static inline __m512i reinterpret_i (__m512  const & x) {
+static inline __m512i reinterpret_i (__m512  const x) {
     return _mm512_castps_si512(x);
 }
 
-static inline __m512i reinterpret_i (__m512d const & x) {
+static inline __m512i reinterpret_i (__m512d const x) {
     return _mm512_castpd_si512(x);
 }
 
-static inline __m512  reinterpret_f (__m512i const & x) {
+static inline __m512  reinterpret_f (__m512i const x) {
     return _mm512_castsi512_ps(x);
 }
 
-static inline __m512  reinterpret_f (__m512  const & x) {
+static inline __m512  reinterpret_f (__m512  const x) {
     return x;
 }
 
-static inline __m512  reinterpret_f (__m512d const & x) {
+static inline __m512  reinterpret_f (__m512d const x) {
     return _mm512_castpd_ps(x);
 }
 
-static inline __m512d reinterpret_d (__m512i const & x) {
+static inline __m512d reinterpret_d (__m512i const x) {
     return _mm512_castsi512_pd(x);
 }
 
-static inline __m512d reinterpret_d (__m512  const & x) {
+static inline __m512d reinterpret_d (__m512  const x) {
     return _mm512_castps_pd(x);
 }
 
-static inline __m512d reinterpret_d (__m512d const & x) {
+static inline __m512d reinterpret_d (__m512d const x) {
     return x;
 }
 
+#if defined(__GNUC__) && __GNUC__ <= 9 // GCC v. 9 is missing the _mm512_zextps256_ps512 intrinsic
+// extend vectors to double size by adding zeroes
+static inline Vec16f extend_z(Vec8f a) {
+    return Vec16f(a, Vec8f(0));
+}
+static inline Vec8d extend_z(Vec4d a) {
+    return Vec8d(a, Vec4d(0));
+}
+#else
+// extend vectors to double size by adding zeroes
+static inline Vec16f extend_z(Vec8f a) {
+    return _mm512_zextps256_ps512(a);
+}
+static inline Vec8d extend_z(Vec4d a) {
+    return _mm512_zextpd256_pd512(a);
+}
+#endif
+
+// Function infinite4f: returns a vector where all elements are +INF
+static inline Vec16f infinite16f() {
+    return reinterpret_f(Vec16i(0x7F800000));
+}
+
+// Function nan4f: returns a vector where all elements are +NAN (quiet)
+static inline Vec16f nan16f(uint32_t n = 0x100) {
+    return nan_vec<Vec16f>(n);
+}
+
+// Function infinite2d: returns a vector where all elements are +INF
+static inline Vec8d infinite8d() {
+    return reinterpret_d(Vec8q(0x7FF0000000000000));
+}
+
+// Function nan8d: returns a vector where all elements are +NAN (quiet NAN)
+static inline Vec8d nan8d(uint32_t n = 0x10) {
+    return nan_vec<Vec8d>(n);
+}
+
+
 /*****************************************************************************
 *
 *          Vector permute functions
@@ -1787,169 +1502,166 @@ static inline __m512d reinterpret_d (__m512d const & x) {
 ******************************************************************************
 *
 * These permute functions can reorder the elements of a vector and optionally
-* set some elements to zero. 
+* set some elements to zero. See Vectori128.h for description
 *
-* The indexes are inserted as template parameters in <>. These indexes must be
-* constants. Each template parameter is an index to the element you want to select.
-* An index of -1 will generate zero. An index of -256 means don't care.
-*
-* Example:
-* Vec8d a(10,11,12,13,14,15,16,17);      // a is (10,11,12,13,14,15,16,17)
-* Vec8d b;
-* b = permute8d<0,2,7,7,-1,-1,1,1>(a);   // b is (10,12,17,17, 0, 0,11,11)
-*
-* A lot of the code here is metaprogramming aiming to find the instructions
-* that best fit the template parameters and instruction set. The metacode
-* will be reduced out to leave only a few vector instructions in release
-* mode with optimization on.
 *****************************************************************************/
 
 // Permute vector of 8 64-bit integers.
-// Index -1 gives 0, index -256 means don't care.
 template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7>
-static inline Vec8d permute8d(Vec8d const & a) {
-
-    // Combine indexes into a single bitfield, with 4 bits for each
-    const int m1 = (i0&7) | (i1&7)<<4 | (i2&7)<< 8 | (i3&7)<<12 | (i4&7)<<16 | (i5&7)<<20 | (i6&7)<<24 | (i7&7)<<28;
-
-    // Mask to zero out negative indexes
-    const int mz = (i0<0?0:0xF) | (i1<0?0:0xF0) | (i2<0?0:0xF00) | (i3<0?0:0xF000) | (i4<0?0:0xF0000) | (i5<0?0:0xF00000) | (i6<0?0:0xF000000) | (i7<0?0:0xF0000000);
-    const int m2 = m1 & mz;
+static inline Vec8d permute8(Vec8d const a) {
+    int constexpr indexs[8] = { i0, i1, i2, i3, i4, i5, i6, i7 }; // indexes as array
+    __m512d y = a;  // result
+    // get flags for possibilities that fit the permutation pattern
+    constexpr uint64_t flags = perm_flags<Vec8d>(indexs);
 
-    // zeroing needed
-    const bool dozero = ((i0|i1|i2|i3|i4|i5|i6|i7) & 0x80) != 0;
+    static_assert((flags & perm_outofrange) == 0, "Index out of range in permute function");
 
-    // special case: all zero
-    if (mz == 0) return  _mm512_setzero_pd();
+    if constexpr ((flags & perm_allzero) != 0) return _mm512_setzero_pd();  // just return zero
 
-    // mask for elements not zeroed
-    const __mmask16  z = __mmask16((i0>=0)<<0 | (i1>=0)<<1 | (i2>=0)<<2 | (i3>=0)<<3 | (i4>=0)<<4 | (i5>=0)<<5 | (i6>=0)<<6 | (i7>=0)<<7);
-    // same with 2 bits for each element
-    const __mmask16 zz = __mmask16((i0>=0?3:0) | (i1>=0?0xC:0) | (i2>=0?0x30:0) | (i3>=0?0xC0:0) | (i4>=0?0x300:0) | (i5>=0?0xC00:0) | (i6>=0?0x3000:0) | (i7>=0?0xC000:0));
+    if constexpr ((flags & perm_perm) != 0) {              // permutation needed
 
-    if (((m1 ^ 0x76543210) & mz) == 0) {
-        // no shuffling
-        if (dozero) {
-            // zero some elements
-            return _mm512_maskz_mov_pd(z, a);
+        if constexpr ((flags & perm_largeblock) != 0) {    // use larger permutation
+            constexpr EList<int, 4> L = largeblock_perm<8>(indexs); // permutation pattern
+            constexpr uint8_t  ppat = (L.a[0] & 3) | (L.a[1]<<2 & 0xC) | (L.a[2]<<4 & 0x30) | (L.a[3]<<6 & 0xC0);
+            y = _mm512_shuffle_f64x2(a, a, ppat);
         }
-        return a;                                 // do nothing
-    }
-
-    if (((m1 ^ 0x66442200) & 0x66666666 & mz) == 0) {
-        // no exchange of data between the four 128-bit lanes
-        const int pat = ((m2 | m2 >> 8 | m2 >> 16 | m2 >> 24) & 0x11) * 0x01010101;
-        const int pmask = ((pat & 1) * 10 + 4) | ((((pat >> 4) & 1) * 10 + 4) << 4);
-        if (((m1 ^ pat) & mz & 0x11111111) == 0) {
-            // same permute pattern in all lanes
-            if (dozero) {  // permute within lanes and zero
-                return _mm512_castsi512_pd(_mm512_maskz_shuffle_epi32(zz, _mm512_castpd_si512(a), (_MM_PERM_ENUM)pmask));
+        else if constexpr ((flags & perm_same_pattern) != 0) {  // same pattern in all lanes
+            if constexpr ((flags & perm_punpckh) != 0) {   // fits punpckhi
+                y = _mm512_unpackhi_pd(y, y);
             }
-            else {  // permute within lanes
-                return _mm512_castsi512_pd(_mm512_shuffle_epi32(_mm512_castpd_si512(a), (_MM_PERM_ENUM)pmask));
+            else if constexpr ((flags & perm_punpckl)!=0){ // fits punpcklo
+                y = _mm512_unpacklo_pd(y, y);
+            }
+            else { // general permute within lanes
+                constexpr uint8_t mm0 = (i0&1) | (i1&1)<<1 | (i2&1)<<2 | (i3&1)<<3 | (i4&1)<<4 | (i5&1)<<5 | (i6&1)<<6 | (i7&1)<<7;
+                y = _mm512_permute_pd(a, mm0);             // select within same lane
             }
         }
-        // different permute patterns in each lane. It's faster to do a full permute than four masked permutes within lanes
-    }
-    if ((((m1 ^ 0x10101010) & 0x11111111 & mz) == 0) 
-    &&  ((m1 ^ (m1 >> 4)) & 0x06060606 & mz & (mz >> 4)) == 0) {
-        // permute lanes only. no permutation within each lane
-        const int m3 = m2 | (m2 >> 4);
-        const int s = ((m3 >> 1) & 3) | (((m3 >> 9) & 3) << 2) | (((m3 >> 17) & 3) << 4) | (((m3 >> 25) & 3) << 6);
-        if (dozero) {
-            // permute lanes and zero some 64-bit elements
-            return  _mm512_maskz_shuffle_f64x2(z, a, a, (_MM_PERM_ENUM)s);
-        }
-        else {
-            // permute lanes
-            return _mm512_shuffle_f64x2(a, a, (_MM_PERM_ENUM)s);
+        else {  // different patterns in all lanes
+            if constexpr ((flags & perm_rotate_big) != 0) { // fits big rotate
+                constexpr uint8_t rot = uint8_t(flags >> perm_rot_count); // rotation count
+                y = _mm512_castsi512_pd(_mm512_alignr_epi64 (_mm512_castpd_si512(y), _mm512_castpd_si512(y), rot));
+            }
+            else if constexpr ((flags & perm_broadcast) != 0) {  // broadcast one element
+                constexpr int e = flags >> perm_rot_count;
+                if constexpr(e != 0) {
+                    y = _mm512_castsi512_pd(_mm512_alignr_epi64(_mm512_castpd_si512(y), _mm512_castpd_si512(y), e));
+                }
+                y = _mm512_broadcastsd_pd(_mm512_castpd512_pd128(y));
+            }
+            else if constexpr ((flags & perm_compress) != 0) {
+                y = _mm512_maskz_compress_pd(__mmask8(compress_mask(indexs)), y); // compress
+                if constexpr ((flags & perm_addz2) == 0) return y;
+            }
+            else if constexpr ((flags & perm_expand) != 0) {
+                y = _mm512_maskz_expand_pd(__mmask8(expand_mask(indexs)), y); // expand
+                if constexpr ((flags & perm_addz2) == 0) return y;
+            }
+            else if constexpr ((flags & perm_cross_lane) == 0) {  // no lane crossing
+                if constexpr ((flags & perm_zeroing) == 0) {      // no zeroing. use vpermilps
+                    const __m512i pmask = constant16ui <i0<<1, 0, i1<<1, 0, i2<<1, 0, i3<<1, 0, i4<<1, 0, i5<<1, 0, i6<<1, 0, i7<<1, 0>();
+                    return _mm512_permutevar_pd(a, pmask);
+                }
+                else { // with zeroing. pshufb may be marginally better because it needs no extra zero mask
+                    constexpr EList <int8_t, 64> bm = pshufb_mask<Vec8q>(indexs);
+                    return _mm512_castsi512_pd(_mm512_shuffle_epi8(_mm512_castpd_si512(y), Vec8q().load(bm.a)));
+                }
+            }
+            else {
+                // full permute needed
+                const __m512i pmask = constant16ui <
+                    i0 & 7, 0, i1 & 7, 0, i2 & 7, 0, i3 & 7, 0, i4 & 7, 0, i5 & 7, 0, i6 & 7, 0, i7 & 7, 0>();
+                y = _mm512_permutexvar_pd(pmask, y);
+            }
         }
     }
-    // full permute needed
-    const __m512i pmask = constant16i<i0&7, 0, i1&7, 0, i2&7, 0, i3&7, 0, i4&7, 0, i5&7, 0, i6&7, 0, i7&7, 0>();
-    if (dozero) {
-        // full permute and zeroing
-        return _mm512_maskz_permutexvar_pd(z, pmask, a);
-    }
-    else {    
-        return _mm512_permutexvar_pd(pmask, a);
+    if constexpr ((flags & perm_zeroing) != 0) { // additional zeroing needed
+        y = _mm512_maskz_mov_pd(zero_mask<8>(indexs), y);
     }
+    return y;
 }
 
 
-
 // Permute vector of 16 32-bit integers.
-// Index -1 gives 0, index -256 means don't care.
 template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7, int i8, int i9, int i10, int i11, int i12, int i13, int i14, int i15>
-static inline Vec16f permute16f(Vec16f const & a) {
-
-    // Combine indexes into a single bitfield, with 4 bits for each
-    const uint64_t m1 = (i0&15) | (i1&15)<<4 | (i2&15)<< 8 | (i3&15)<<12 | (i4&15)<<16 | (i5&15)<<20 | (i6&15)<<24 | (i7&15LL)<<28   // 15LL avoids sign extension of (int32_t | int64_t)
-        | (i8&15LL)<<32 | (i9&15LL)<<36 | (i10&15LL)<<40 | (i11&15LL)<<44 | (i12&15LL)<<48 | (i13&15LL)<<52 | (i14&15LL)<<56 | (i15&15LL)<<60;
-
-    // Mask to zero out negative indexes
-    const uint64_t mz = (i0<0?0:0xF) | (i1<0?0:0xF0) | (i2<0?0:0xF00) | (i3<0?0:0xF000) | (i4<0?0:0xF0000) | (i5<0?0:0xF00000) | (i6<0?0:0xF000000) | (i7<0?0:0xF0000000ULL) | (i8<0?0:0xF00000000) 
-        | (i9<0?0:0xF000000000) | (i10<0?0:0xF0000000000) | (i11<0?0:0xF00000000000) | (i12<0?0:0xF000000000000) | (i13<0?0:0xF0000000000000) | (i14<0?0:0xF00000000000000) | (i15<0?0:0xF000000000000000);
+static inline Vec16f permute16(Vec16f const a) {
+    int constexpr indexs[16] = {  // indexes as array
+        i0, i1, i2, i3, i4, i5, i6, i7, i8, i9, i10, i11, i12, i13, i14, i15 };
+    __m512 y = a;  // result
+    // get flags for possibilities that fit the permutation pattern
+    constexpr uint64_t flags = perm_flags<Vec16f>(indexs);
 
-    const uint64_t m2 = m1 & mz;
+    static_assert((flags & perm_outofrange) == 0, "Index out of range in permute function");
 
-    // zeroing needed
-    const bool dozero = ((i0|i1|i2|i3|i4|i5|i6|i7|i8|i9|i10|i11|i12|i13|i14|i15) & 0x80) != 0;
+    if constexpr ((flags & perm_allzero) != 0) return _mm512_setzero_ps();  // just return zero
 
-    // special case: all zero
-    if (mz == 0) return  _mm512_setzero_ps();
+    if constexpr ((flags & perm_perm) != 0) {              // permutation needed
 
-    // mask for elements not zeroed
-    const __mmask16 z = __mmask16((i0>=0)<<0 | (i1>=0)<<1 | (i2>=0)<<2 | (i3>=0)<<3 | (i4>=0)<<4 | (i5>=0)<<5 | (i6>=0)<<6 | (i7>=0)<<7
-        | (i8>=0)<<8 | (i9>=0)<<9 | (i10>=0)<<10 | (i11>=0)<<11 | (i12>=0)<<12 | (i13>=0)<<13 | (i14>=0)<<14 | (i15>=0)<<15);
-
-    if (((m1 ^ 0xFEDCBA9876543210) & mz) == 0) {
-        // no shuffling
-        if (dozero) {
-            // zero some elements
-            return _mm512_maskz_mov_ps(z, a);
+        if constexpr ((flags & perm_largeblock) != 0) {    // use larger permutation
+            constexpr EList<int, 8> L = largeblock_perm<16>(indexs); // permutation pattern
+            y = _mm512_castpd_ps(
+                permute8 <L.a[0], L.a[1], L.a[2], L.a[3], L.a[4], L.a[5], L.a[6], L.a[7]>
+                (Vec8d(_mm512_castps_pd(a))));
+            if (!(flags & perm_addz)) return y;            // no remaining zeroing
         }
-        return a;                                 // do nothing
-    }
-
-    if (((m1 ^ 0xCCCC888844440000) & 0xCCCCCCCCCCCCCCCC & mz) == 0) {
-        // no exchange of data between the four 128-bit lanes
-        const uint64_t pat = ((m2 | (m2 >> 16) | (m2 >> 32) | (m2 >> 48)) & 0x3333) * 0x0001000100010001;
-        const int pmask = (pat & 3) | (((pat >> 4) & 3) << 2) | (((pat >> 8) & 3) << 4) | (((pat >> 12) & 3) << 6);
-        if (((m1 ^ pat) & 0x3333333333333333 & mz) == 0) {
-            // same permute pattern in all lanes
-            if (dozero) {  // permute within lanes and zero
-                return _mm512_castsi512_ps(_mm512_maskz_shuffle_epi32(z, _mm512_castps_si512(a), (_MM_PERM_ENUM)pmask));
+        else if constexpr ((flags & perm_same_pattern) != 0) {  // same pattern in all lanes
+            if constexpr ((flags & perm_punpckh) != 0) {   // fits punpckhi
+                y = _mm512_unpackhi_ps(y, y);
             }
-            else {  // permute within lanes
-                return _mm512_castsi512_ps(_mm512_shuffle_epi32(_mm512_castps_si512(a), (_MM_PERM_ENUM)pmask));
+            else if constexpr ((flags & perm_punpckl)!=0){ // fits punpcklo
+                y = _mm512_unpacklo_ps(y, y);
+            }
+            else { // general permute within lanes
+                y = _mm512_permute_ps(a, uint8_t(flags >> perm_ipattern));
             }
         }
-        // different permute patterns in each lane. It's faster to do a full permute than four masked permutes within lanes
-    }
-    const uint64_t lane = (m2 | m2 >> 4 | m2 >> 8 | m2 >> 12) & 0x000C000C000C000C;
-    if ((((m1 ^ 0x3210321032103210) & 0x3333333333333333 & mz) == 0) 
-    &&  ((m1 ^ (lane * 0x1111)) & 0xCCCCCCCCCCCCCCCC & mz) == 0) {
-        // permute lanes only. no permutation within each lane
-        const uint64_t s = ((lane >> 2) & 3) | (((lane >> 18) & 3) << 2) | (((lane >> 34) & 3) << 4) | (((lane >> 50) & 3) << 6);
-        if (dozero) {
-            // permute lanes and zero some 64-bit elements
-            return  _mm512_maskz_shuffle_f32x4(z, a, a, (_MM_PERM_ENUM)s);
-        }
-        else {
-            // permute lanes
-            return _mm512_shuffle_f32x4(a, a, (_MM_PERM_ENUM)s);
+        else {  // different patterns in all lanes
+            if constexpr ((flags & perm_rotate_big) != 0) { // fits big rotate
+                constexpr uint8_t rot = uint8_t(flags >> perm_rot_count); // rotation count
+                y = _mm512_castsi512_ps(_mm512_alignr_epi32(_mm512_castps_si512(y), _mm512_castps_si512(y), rot));
+            }
+            else if constexpr ((flags & perm_broadcast) != 0) {  // broadcast one element
+                constexpr int e = flags >> perm_rot_count;       // element index
+                if constexpr(e != 0) {
+                    y = _mm512_castsi512_ps(_mm512_alignr_epi32(_mm512_castps_si512(y), _mm512_castps_si512(y), e));
+                }
+                y = _mm512_broadcastss_ps(_mm512_castps512_ps128(y));
+            }
+            else if constexpr ((flags & perm_zext) != 0) {       // zero extension
+                y = _mm512_castsi512_ps(_mm512_cvtepu32_epi64(_mm512_castsi512_si256(_mm512_castps_si512(y))));
+                if constexpr ((flags & perm_addz2) == 0) return y;
+            }
+            else if constexpr ((flags & perm_compress) != 0) {
+                y = _mm512_maskz_compress_ps(__mmask16(compress_mask(indexs)), y); // compress
+                if constexpr ((flags & perm_addz2) == 0) return y;
+            }
+            else if constexpr ((flags & perm_expand) != 0) {
+                y = _mm512_maskz_expand_ps(__mmask16(expand_mask(indexs)), y); // expand
+                if constexpr ((flags & perm_addz2) == 0) return y;
+            }
+            else if constexpr ((flags & perm_cross_lane) == 0) {  // no lane crossing
+                if constexpr ((flags & perm_zeroing) == 0) {      // no zeroing. use vpermilps
+                    const __m512i pmask = constant16ui <i0, i1, i2, i3, i4, i5, i6, i7, i8, i9, i10, i11, i12, i13, i14, i15>();
+                    return _mm512_permutevar_ps(a, pmask);
+                }
+                else { // with zeroing. pshufb may be marginally better because it needs no extra zero mask
+                    constexpr EList <int8_t, 64> bm = pshufb_mask<Vec16i>(indexs);
+                    return _mm512_castsi512_ps(_mm512_shuffle_epi8(_mm512_castps_si512(a), Vec16i().load(bm.a)));
+                }
+            }
+            else {
+                // full permute needed
+                const __m512i pmaskf = constant16ui <
+                    i0 & 15, i1 & 15, i2 & 15, i3 & 15, i4 & 15, i5 & 15, i6 & 15, i7 & 15,
+                    i8 & 15, i9 & 15, i10 & 15, i11 & 15, i12 & 15, i13 & 15, i14 & 15, i15 & 15>();
+                y = _mm512_permutexvar_ps(pmaskf, a);
+            }
         }
     }
-    // full permute needed
-    const __m512i pmask = constant16i<i0&15, i1&15, i2&15, i3&15, i4&15, i5&15, i6&15, i7&15, i8&15, i9&15, i10&15, i11&15, i12&15, i13&15, i14&15, i15&15>();
-    if (dozero) {
-        // full permute and zeroing
-        return _mm512_maskz_permutexvar_ps(z, pmask, a);
-    }
-    else {    
-        return _mm512_permutexvar_ps(pmask, a);
+    if constexpr ((flags & perm_zeroing) != 0) { // additional zeroing needed
+        y = _mm512_maskz_mov_ps(zero_mask<16>(indexs), y);
     }
+    return y;
 }
 
 
@@ -1957,201 +1669,154 @@ static inline Vec16f permute16f(Vec16f const & a) {
 *
 *          Vector blend functions
 *
-******************************************************************************
-*
-* These blend functions can mix elements from two different vectors and
-* optionally set some elements to zero. 
-*
-* The indexes are inserted as template parameters in <>. These indexes must be
-* constants. Each template parameter is an index to the element you want to 
-* select, where higher indexes indicate an element from the second source
-* vector. For example, if each vector has 8 elements, then indexes 0 - 7
-* will select an element from the first vector and indexes 8 - 15 will select 
-* an element from the second vector. A negative index will generate zero.
-*
-* Example:
-* Vec8d a(100,101,102,103,104,105,106,107); // a is (100, 101, 102, 103, 104, 105, 106, 107)
-* Vec8d b(200,201,202,203,204,205,206,207); // b is (200, 201, 202, 203, 204, 205, 206, 207)
-* Vec8d c;
-* c = blend8d<1,0,9,8,7,-1,15,15> (a,b);    // c is (101, 100, 201, 200, 107,   0, 207, 207)
-*
-* A lot of the code here is metaprogramming aiming to find the instructions
-* that best fit the template parameters and instruction set. The metacode
-* will be reduced out to leave only a few vector instructions in release
-* mode with optimization on.
 *****************************************************************************/
 
+template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7>
+static inline Vec8d blend8(Vec8d const a, Vec8d const b) {
+    int constexpr indexs[8] = { i0, i1, i2, i3, i4, i5, i6, i7 }; // indexes as array
+    __m512d y = a;                                         // result
+    constexpr uint64_t flags = blend_flags<Vec8d>(indexs); // get flags for possibilities that fit the index pattern
 
-template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7> 
-static inline Vec8d blend8d(Vec8d const & a, Vec8d const & b) {  
-
-    // Combine indexes into a single bitfield, with 4 bits for each
-    const int m1 = (i0&0xF) | (i1&0xF)<<4 | (i2&0xF)<< 8 | (i3&0xF)<<12 | (i4&0xF)<<16 | (i5&0xF)<<20 | (i6&0xF)<<24 | (i7&0xF)<<28;
-
-    // Mask to zero out negative indexes
-    const int mz = (i0<0?0:0xF) | (i1<0?0:0xF0) | (i2<0?0:0xF00) | (i3<0?0:0xF000) | (i4<0?0:0xF0000) | (i5<0?0:0xF00000) | (i6<0?0:0xF000000) | (i7<0?0:0xF0000000);
-    const int m2 = m1 & mz;
-
-    // zeroing needed
-    const bool dozero = ((i0|i1|i2|i3|i4|i5|i6|i7) & 0x80) != 0;
-
-    // mask for elements not zeroed
-    const __mmask16 z = __mmask16((i0>=0)<<0 | (i1>=0)<<1 | (i2>=0)<<2 | (i3>=0)<<3 | (i4>=0)<<4 | (i5>=0)<<5 | (i6>=0)<<6 | (i7>=0)<<7);
+    static_assert((flags & blend_outofrange) == 0, "Index out of range in blend function");
 
-    // special case: all zero
-    if (mz == 0) return  _mm512_setzero_pd();
+    if constexpr ((flags & blend_allzero) != 0) return _mm512_setzero_pd();  // just return zero
 
-    // special case: all from a
-    if ((m1 & 0x88888888 & mz) == 0) {
-        return permute8d <i0, i1, i2, i3, i4, i5, i6, i7> (a);
+    if constexpr ((flags & blend_b) == 0) {                // nothing from b. just permute a
+        return permute8 <i0, i1, i2, i3, i4, i5, i6, i7> (a);
     }
-
-    // special case: all from b
-    if ((~m1 & 0x88888888 & mz) == 0) {
-        return permute8d <i0^8, i1^8, i2^8, i3^8, i4^8, i5^8, i6^8, i7^8> (b);
+    if constexpr ((flags & blend_a) == 0) {                // nothing from a. just permute b
+        constexpr EList<int, 16> L = blend_perm_indexes<8, 2>(indexs); // get permutation indexes
+        return permute8 < L.a[8], L.a[9], L.a[10], L.a[11], L.a[12], L.a[13], L.a[14], L.a[15] > (b);
     }
-
-    // special case: blend without permute
-    if (((m1 ^ 0x76543210) & 0x77777777 & mz) == 0) {
-        __mmask16 blendmask = __mmask16((i0&8)>>3 | (i1&8)>>2 | (i2&8)>>1 | (i3&8)>>0 | (i4&8)<<1 | (i5&8)<<2 | (i6&8)<<3 | (i7&8)<<4 );
-        __m512d t = _mm512_mask_blend_pd(blendmask, a, b);
-        if (dozero) {
-            t = _mm512_maskz_mov_pd(z, t);
+    if constexpr ((flags & (blend_perma | blend_permb)) == 0) { // no permutation, only blending
+        constexpr uint8_t mb = (uint8_t)make_bit_mask<8, 0x303>(indexs);  // blend mask
+        y = _mm512_mask_mov_pd (a, mb, b);
+    }
+    else if constexpr ((flags & blend_largeblock) != 0) {  // blend and permute 128-bit blocks
+        constexpr EList<int, 4> L = largeblock_perm<8>(indexs); // get 128-bit blend pattern
+        constexpr uint8_t shuf = (L.a[0] & 3) | (L.a[1] & 3) << 2 | (L.a[2] & 3) << 4 | (L.a[3] & 3) << 6;
+        if constexpr (make_bit_mask<8, 0x103>(indexs) == 0) { // fits vshufi64x2 (a,b)
+            y = _mm512_shuffle_f64x2(a, b, shuf);
         }
-        return t;
-    }
-    // special case: all data stay within their lane
-    if (((m1 ^ 0x66442200) & 0x66666666 & mz) == 0) {
-
-        // mask for elements from a and b
-        const uint32_t mb = ((i0&8)?0xF:0) | ((i1&8)?0xF0:0) | ((i2&8)?0xF00:0) | ((i3&8)?0xF000:0) | ((i4&8)?0xF0000:0) | ((i5&8)?0xF00000:0) | ((i6&8)?0xF000000:0) | ((i7&8)?0xF0000000:0);
-        const uint32_t mbz = mb & mz;     // mask for nonzero elements from b
-        const uint32_t maz = ~mb & mz;    // mask for nonzero elements from a
-        const uint32_t m1a = m1 & maz;
-        const uint32_t m1b = m1 & mbz;
-        const uint32_t pata = ((m1a | m1a >> 8 | m1a >> 16 | m1a >> 24) & 0xFF) * 0x01010101;  // permute pattern for elements from a
-        const uint32_t patb = ((m1b | m1b >> 8 | m1b >> 16 | m1b >> 24) & 0xFF) * 0x01010101;  // permute pattern for elements from b
-
-        if (((m1 ^ pata) & 0x11111111 & maz) == 0 && ((m1 ^ patb) & 0x11111111 & mbz) == 0) {
-            // Same permute pattern in all lanes:
-            // todo!: make special case for PSHUFD
-
-            // This code generates two instructions instead of one, but we are avoiding the slow lane-crossing instruction,
-            // and we are saving 64 bytes of data cache.
-            // 1. Permute a, zero elements not from a (using _mm512_maskz_shuffle_epi32)
-            __m512d ta = permute8d< (maz&0xF)?i0&7:-1, (maz&0xF0)?i1&7:-1, (maz&0xF00)?i2&7:-1, (maz&0xF000)?i3&7:-1, 
-                (maz&0xF0000)?i4&7:-1, (maz&0xF00000)?i5&7:-1, (maz&0xF000000)?i6&7:-1, (maz&0xF0000000)?i7&7:-1> (a);
-            // write mask for elements from b
-            const __mmask16 sb = ((mbz&0xF)?3:0) | ((mbz&0xF0)?0xC:0) | ((mbz&0xF00)?0x30:0) | ((mbz&0xF000)?0xC0:0) | ((mbz&0xF0000)?0x300:0) | ((mbz&0xF00000)?0xC00:0) | ((mbz&0xF000000)?0x3000:0) | ((mbz&0xF0000000)?0xC000:0);
-            // permute index for elements from b
-            const int pi = ((patb & 1) * 10 + 4) | ((((patb >> 4) & 1) * 10 + 4) << 4);
-            // 2. Permute elements from b and combine with elements from a through write mask
-            return _mm512_castsi512_pd(_mm512_mask_shuffle_epi32(_mm512_castpd_si512(ta), sb, _mm512_castpd_si512(b), (_MM_PERM_ENUM)pi));
+        else if constexpr (make_bit_mask<8, 0x203>(indexs) == 0) { // fits vshufi64x2 (b,a)
+            y = _mm512_shuffle_f64x2(b, a, shuf);
+        }
+        else {
+            constexpr EList <int64_t, 8> bm = perm_mask_broad<Vec8q>(indexs);
+            y = _mm512_permutex2var_pd(a, Vec8q().load(bm.a), b);
         }
-        // not same permute pattern in all lanes. use full permute
     }
-    // general case: full permute
-    const __m512i pmask = constant16i<i0&0xF, 0, i1&0xF, 0, i2&0xF, 0, i3&0xF, 0, i4&0xF, 0, i5&0xF, 0, i6&0xF, 0, i7&0xF, 0>();
-    if (dozero) {
-        return _mm512_maskz_permutex2var_pd(z, a, pmask, b);
+    // check if pattern fits special cases
+    else if constexpr ((flags & blend_punpcklab) != 0) {
+        y = _mm512_unpacklo_pd (a, b);
     }
-    else {
-        return _mm512_permutex2var_pd(a, pmask, b);
+    else if constexpr ((flags & blend_punpcklba) != 0) {
+        y = _mm512_unpacklo_pd (b, a);
+    }
+    else if constexpr ((flags & blend_punpckhab) != 0) {
+        y = _mm512_unpackhi_pd (a, b);
+    }
+    else if constexpr ((flags & blend_punpckhba) != 0) {
+        y = _mm512_unpackhi_pd (b, a);
+    }
+    else if constexpr ((flags & blend_shufab) != 0) {      // use floating point instruction shufpd
+        y = _mm512_shuffle_pd(a, b, uint8_t(flags >> blend_shufpattern));
+    }
+    else if constexpr ((flags & blend_shufba) != 0) {      // use floating point instruction shufpd
+        y = _mm512_shuffle_pd(b, a, uint8_t(flags >> blend_shufpattern));
+    }
+    else { // No special cases
+        constexpr EList <int64_t, 8> bm = perm_mask_broad<Vec8q>(indexs);
+        y = _mm512_permutex2var_pd(a, Vec8q().load(bm.a), b);
     }
+    if constexpr ((flags & blend_zeroing) != 0) {          // additional zeroing needed
+        y = _mm512_maskz_mov_pd(zero_mask<8>(indexs), y);
+    }
+    return y;
 }
 
 
-template <int i0,  int i1,  int i2,  int i3,  int i4,  int i5,  int i6,  int i7, 
-          int i8,  int i9,  int i10, int i11, int i12, int i13, int i14, int i15 > 
-static inline Vec16f blend16f(Vec16f const & a, Vec16f const & b) {  
-
-    // Combine indexes into a single bitfield, with 4 bits for each indicating shuffle, but not source
-    const uint64_t m1 = (i0&0xF) | (i1&0xF)<<4 | (i2&0xF)<<8 | (i3&0xF)<<12 | (i4&0xF)<<16 | (i5&0xF)<<20 | (i6&0xF)<<24 | (i7&0xFLL)<<28
-        | (i8&0xFLL)<<32 | (i9&0xFLL)<<36 | (i10&0xFLL)<<40 | (i11&0xFLL)<<44 | (i12&0xFLL)<<48 | (i13&0xFLL)<<52 | (i14&0xFLL)<<56 | (i15&0xFLL)<<60;
-
-    // Mask to zero out negative indexes
-    const uint64_t mz = (i0<0?0:0xF) | (i1<0?0:0xF0) | (i2<0?0:0xF00) | (i3<0?0:0xF000) | (i4<0?0:0xF0000) | (i5<0?0:0xF00000) | (i6<0?0:0xF000000) | (i7<0?0:0xF0000000ULL)
-        | (i8<0?0:0xF00000000) | (i9<0?0:0xF000000000) | (i10<0?0:0xF0000000000) | (i11<0?0:0xF00000000000) | (i12<0?0:0xF000000000000) | (i13<0?0:0xF0000000000000) | (i14<0?0:0xF00000000000000) | (i15<0?0:0xF000000000000000);
-    const uint64_t m2 = m1 & mz;
+template <int i0,  int i1,  int i2,  int i3,  int i4,  int i5,  int i6,  int i7,
+          int i8,  int i9,  int i10, int i11, int i12, int i13, int i14, int i15 >
+static inline Vec16f blend16(Vec16f const a, Vec16f const b) {
+    int constexpr indexs[16] = { i0, i1, i2, i3, i4, i5, i6, i7, i8, i9, i10, i11, i12, i13, i14, i15}; // indexes as array
+    __m512 y = a;                                          // result
+    constexpr uint64_t flags = blend_flags<Vec16f>(indexs);// get flags for possibilities that fit the index pattern
 
-    // collect bit 4 of each index = select source
-    const uint64_t ms = ((i0&16)?0xF:0) | ((i1&16)?0xF0:0) | ((i2&16)?0xF00:0) | ((i3&16)?0xF000:0) | ((i4&16)?0xF0000:0) | ((i5&16)?0xF00000:0) | ((i6&16)?0xF000000:0) | ((i7&16)?0xF0000000ULL:0)
-        | ((i8&16)?0xF00000000:0) | ((i9&16)?0xF000000000:0) | ((i10&16)?0xF0000000000:0) | ((i11&16)?0xF00000000000:0) | ((i12&16)?0xF000000000000:0) | ((i13&16)?0xF0000000000000:0) | ((i14&16)?0xF00000000000000:0) | ((i15&16)?0xF000000000000000:0);
+    static_assert((flags & blend_outofrange) == 0, "Index out of range in blend function");
 
-    // zeroing needed
-    const bool dozero = ((i0|i1|i2|i3|i4|i5|i6|i7|i8|i9|i10|i11|i12|i13|i14|i15) & 0x80) != 0;
+    if constexpr ((flags & blend_allzero) != 0) return _mm512_setzero_ps();  // just return zero
 
-    // mask for elements not zeroed
-    const __mmask16 z = __mmask16((i0>=0)<<0 | (i1>=0)<<1 | (i2>=0)<<2 | (i3>=0)<<3 | (i4>=0)<<4 | (i5>=0)<<5 | (i6>=0)<<6 | (i7>=0)<<7 
-        | (i8>=0)<<8 | (i9>=0)<<9 | (i10>=0)<<10 | (i11>=0)<<11 | (i12>=0)<<12 | (i13>=0)<<13 | (i14>=0)<<14 | (i15>=0)<<15);
-
-    // special case: all zero
-    if (mz == 0) return  _mm512_setzero_ps();
-
-    // special case: all from a
-    if ((ms & mz) == 0) {
-        return permute16f<i0,i1,i2,i3,i4,i5,i6,i7,i8,i9,i10,i11,i12,i13,i14,i15> (a);
+    if constexpr ((flags & blend_b) == 0) {                // nothing from b. just permute a
+        return permute16 <i0, i1, i2, i3, i4, i5, i6, i7, i8, i9, i10, i11, i12, i13, i14, i15> (a);
     }
-
-    // special case: all from b
-    if ((~ms & mz) == 0) {
-        return permute16f<i0^16,i1^16,i2^16,i3^16,i4^16,i5^16,i6^16,i7^16,i8^16,i9^16,i10^16,i11^16,i12^16,i13^16,i14^16,i15^16 > (b);
+    if constexpr ((flags & blend_a) == 0) {                // nothing from a. just permute b
+        constexpr EList<int, 32> L = blend_perm_indexes<16, 2>(indexs); // get permutation indexes
+        return permute16 <
+            L.a[16], L.a[17], L.a[18], L.a[19], L.a[20], L.a[21], L.a[22], L.a[23],
+            L.a[24], L.a[25], L.a[26], L.a[27], L.a[28], L.a[29], L.a[30], L.a[31] > (b);
     }
-
-    // special case: blend without permute
-    if (((m1 ^ 0xFEDCBA9876543210) & mz) == 0) {
-        __mmask16 blendmask = __mmask16((i0&16)>>4 | (i1&16)>>3 | (i2&16)>>2 | (i3&16)>>1 | (i4&16) | (i5&16)<<1 | (i6&16)<<2 | (i7&16)<<3
-            | (i8&16)<<4 | (i9&16)<<5 | (i10&16)<<6 | (i11&16)<<7 | (i12&16)<<8 | (i13&16)<<9 | (i14&16)<<10 | (i15&16)<<11);
-        __m512 t = _mm512_mask_blend_ps(blendmask, a, b);
-        if (dozero) {
-            t = _mm512_maskz_mov_ps(z, t);
+    if constexpr ((flags & (blend_perma | blend_permb)) == 0) { // no permutation, only blending
+        constexpr uint16_t mb = (uint16_t)make_bit_mask<16, 0x304>(indexs);  // blend mask
+        y = _mm512_mask_mov_ps(a, mb, b);
+    }
+    else if constexpr ((flags & blend_largeblock) != 0) {  // blend and permute 64-bit blocks
+        constexpr EList<int, 8> L = largeblock_perm<16>(indexs); // get 64-bit blend pattern
+        y = _mm512_castpd_ps(blend8 <
+            L.a[0], L.a[1], L.a[2], L.a[3], L.a[4], L.a[5], L.a[6], L.a[7] >
+            (Vec8d(_mm512_castps_pd(a)), Vec8d(_mm512_castps_pd(b))));
+        if (!(flags & blend_addz)) return y;               // no remaining zeroing
+    }
+    else if constexpr ((flags & blend_same_pattern) != 0) {
+        // same pattern in all 128-bit lanes. check if pattern fits special cases
+        if constexpr ((flags & blend_punpcklab) != 0) {
+            y = _mm512_unpacklo_ps(a, b);
+        }
+        else if constexpr ((flags & blend_punpcklba) != 0) {
+            y = _mm512_unpacklo_ps(b, a);
+        }
+        else if constexpr ((flags & blend_punpckhab) != 0) {
+            y = _mm512_unpackhi_ps(a, b);
         }
-        return t;
-    }
-
-    // special case: all data stay within their lane
-    if (((m1 ^ 0xCCCC888844440000) & 0xCCCCCCCCCCCCCCCC & mz) == 0) {
-
-        // mask for elements from a and b
-        const uint64_t mb  = ms;
-        const uint64_t mbz = mb & mz;     // mask for nonzero elements from b
-        const uint64_t maz = ~mb & mz;    // mask for nonzero elements from a
-        const uint64_t m1a = m1 & maz;
-        const uint64_t m1b = m1 & mbz;
-        const uint64_t pata = ((m1a | m1a >> 16 | m1a >> 32 | m1a >> 48) & 0xFFFF) * 0x0001000100010001;  // permute pattern for elements from a
-        const uint64_t patb = ((m1b | m1b >> 16 | m1b >> 32 | m1b >> 48) & 0xFFFF) * 0x0001000100010001;  // permute pattern for elements from b
-
-        if (((m1 ^ pata) & 0x3333333333333333 & maz) == 0 && ((m1 ^ patb) & 0x3333333333333333 & mbz) == 0) {
-            // Same permute pattern in all lanes:
-            // todo!: special case for SHUFPS
-
-            // This code generates two instructions instead of one, but we are avoiding the slow lane-crossing instruction,
-            // and we are saving 64 bytes of data cache.
-            // 1. Permute a, zero elements not from a (using _mm512_maskz_shuffle_epi32)
-            __m512 ta = permute16f< (maz&0xF)?i0&15:-1, (maz&0xF0)?i1&15:-1, (maz&0xF00)?i2&15:-1, (maz&0xF000)?i3&15:-1, 
-                (maz&0xF0000)?i4&15:-1, (maz&0xF00000)?i5&15:-1, (maz&0xF000000)?i6&15:-1, (maz&0xF0000000)?i7&15:-1,
-                (maz&0xF00000000)?i8&15:-1, (maz&0xF000000000)?i9&15:-1, (maz&0xF0000000000)?i10&15:-1, (maz&0xF00000000000)?i11&15:-1, 
-                (maz&0xF000000000000)?i12&15:-1, (maz&0xF0000000000000)?i13&15:-1, (maz&0xF00000000000000)?i14&15:-1, (maz&0xF000000000000000)?i15&15:-1> (a);
-            // write mask for elements from b
-            const __mmask16 sb = ((mbz&0xF)?1:0) | ((mbz&0xF0)?0x2:0) | ((mbz&0xF00)?0x4:0) | ((mbz&0xF000)?0x8:0) | ((mbz&0xF0000)?0x10:0) | ((mbz&0xF00000)?0x20:0) | ((mbz&0xF000000)?0x40:0) | ((mbz&0xF0000000)?0x80:0) 
-                | ((mbz&0xF00000000)?0x100:0) | ((mbz&0xF000000000)?0x200:0) | ((mbz&0xF0000000000)?0x400:0) | ((mbz&0xF00000000000)?0x800:0) | ((mbz&0xF000000000000)?0x1000:0) | ((mbz&0xF0000000000000)?0x2000:0) | ((mbz&0xF00000000000000)?0x4000:0) | ((mbz&0xF000000000000000)?0x8000:0);
-            // permute index for elements from b
-            const int pi = (patb & 3) | (((patb >> 4) & 3) << 2) | (((patb >> 8) & 3) << 4) | (((patb >> 12) & 3) << 6);
-            // 2. Permute elements from b and combine with elements from a through write mask
-            return _mm512_castsi512_ps(_mm512_mask_shuffle_epi32(_mm512_castps_si512(ta), sb, _mm512_castps_si512(b), (_MM_PERM_ENUM)pi));
+        else if constexpr ((flags & blend_punpckhba) != 0) {
+            y = _mm512_unpackhi_ps(b, a);
+        }
+        else if constexpr ((flags & blend_shufab) != 0) {  // use floating point instruction shufpd
+            y = _mm512_shuffle_ps(a, b, uint8_t(flags >> blend_shufpattern));
+        }
+        else if constexpr ((flags & blend_shufba) != 0) {  // use floating point instruction shufpd
+            y = _mm512_shuffle_ps(b, a, uint8_t(flags >> blend_shufpattern));
+        }
+        else {
+            // Use vshufps twice. This generates two instructions in the dependency chain,
+            // but we are avoiding the slower lane-crossing instruction, and saving 64
+            // bytes of data cache.
+            auto shuf = [](int const (&a)[16]) constexpr { // get pattern for vpshufd
+                int pat[4] = {-1,-1,-1,-1};
+                for (int i = 0; i < 16; i++) {
+                    int ix = a[i];
+                    if (ix >= 0 && pat[i&3] < 0) {
+                        pat[i&3] = ix;
+                    }
+                }
+                return (pat[0] & 3) | (pat[1] & 3) << 2 | (pat[2] & 3) << 4 | (pat[3] & 3) << 6;
+            };
+            constexpr uint8_t  pattern = uint8_t(shuf(indexs));                     // permute pattern
+            constexpr uint16_t froma = (uint16_t)make_bit_mask<16, 0x004>(indexs);  // elements from a
+            constexpr uint16_t fromb = (uint16_t)make_bit_mask<16, 0x304>(indexs);  // elements from b
+            y = _mm512_maskz_shuffle_ps(   froma, a, a, pattern);
+            y = _mm512_mask_shuffle_ps (y, fromb, b, b, pattern);
+            return y;  // we have already zeroed any unused elements
         }
-        // not same permute pattern in all lanes. use full permute
     }
-
-    // general case: full permute
-    const __m512i pmask = constant16i<i0&0x1F, i1&0x1F, i2&0x1F, i3&0x1F, i4&0x1F, i5&0x1F, i6&0x1F, i7&0x1F, 
-        i8&0x1F, i9&0x1F, i10&0x1F, i11&0x1F, i12&0x1F, i13&0x1F, i14&0x1F, i15&0x1F>();
-    if (dozero) {
-        return _mm512_maskz_permutex2var_ps(z, a, pmask, b);        
+    else { // No special cases
+        constexpr EList <int32_t, 16> bm = perm_mask_broad<Vec16i>(indexs);
+        y = _mm512_permutex2var_ps(a, Vec16i().load(bm.a), b);
     }
-    else {
-        return _mm512_permutex2var_ps(a, pmask, b);
+    if constexpr ((flags & blend_zeroing) != 0) {          // additional zeroing needed
+        y = _mm512_maskz_mov_ps(zero_mask<16>(indexs), y);
     }
+    return y;
 }
 
 
@@ -2164,43 +1829,27 @@ static inline Vec16f blend16f(Vec16f const & a, Vec16f const & b) {
 * These functions use vector elements as indexes into a table.
 * The table is given as one or more vectors or as an array.
 *
-* This can be used for several purposes:
-*  - table lookup
-*  - permute or blend with variable indexes
-*  - blend from more than two sources
-*  - gather non-contiguous data
-*
-* An index out of range may produce any value - the actual value produced is
-* implementation dependent and may be different for different instruction
-* sets. An index out of range does not produce an error message or exception.
-*
-* Example:
-* Vec8d a(2,0,0,6,4,3,5,0);                 // index a is (  2,   0,   0,   6,   4,   3,   5,   0)
-* Vec8d b(100,101,102,103,104,105,106,107); // table b is (100, 101, 102, 103, 104, 105, 106, 107)
-* Vec8d c;
-* c = lookup8 (a,b);                        // c is       (102, 100, 100, 106, 104, 103, 105, 100)
-*
 *****************************************************************************/
 
-static inline Vec16f lookup16(Vec16i const & index, Vec16f const & table) {
+static inline Vec16f lookup16(Vec16i const index, Vec16f const table) {
     return _mm512_permutexvar_ps(index, table);
 }
 
 template <int n>
-static inline Vec16f lookup(Vec16i const & index, float const * table) {
-    if (n <= 0) return 0;
-    if (n <= 16) {
+static inline Vec16f lookup(Vec16i const index, float const * table) {
+    if constexpr (n <= 0) return 0;
+    if constexpr (n <= 16) {
         Vec16f table1 = Vec16f().load((float*)table);
         return lookup16(index, table1);
     }
-    if (n <= 32) {
+    if constexpr (n <= 32) {
         Vec16f table1 = Vec16f().load((float*)table);
         Vec16f table2 = Vec16f().load((float*)table + 16);
         return _mm512_permutex2var_ps(table1, index, table2);
     }
     // n > 32. Limit index
     Vec16ui index1;
-    if ((n & (n-1)) == 0) {
+    if constexpr ((n & (n-1)) == 0) {
         // n is a power of 2, make index modulo n
         index1 = Vec16ui(index) & (n-1);
     }
@@ -2212,25 +1861,25 @@ static inline Vec16f lookup(Vec16i const & index, float const * table) {
 }
 
 
-static inline Vec8d lookup8(Vec8q const & index, Vec8d const & table) {
+static inline Vec8d lookup8(Vec8q const index, Vec8d const table) {
     return _mm512_permutexvar_pd(index, table);
 }
 
 template <int n>
-static inline Vec8d lookup(Vec8q const & index, double const * table) {
-    if (n <= 0) return 0;
-    if (n <= 8) {
+static inline Vec8d lookup(Vec8q const index, double const * table) {
+    if constexpr (n <= 0) return 0;
+    if constexpr (n <= 8) {
         Vec8d table1 = Vec8d().load((double*)table);
         return lookup8(index, table1);
     }
-    if (n <= 16) {
+    if constexpr (n <= 16) {
         Vec8d table1 = Vec8d().load((double*)table);
         Vec8d table2 = Vec8d().load((double*)table + 8);
         return _mm512_permutex2var_pd(table1, index, table2);
     }
     // n > 16. Limit index
     Vec8uq index1;
-    if ((n & (n-1)) == 0) {
+    if constexpr ((n & (n-1)) == 0) {
         // n is a power of 2, make index modulo n
         index1 = Vec8uq(index) & (n-1);
     }
@@ -2248,57 +1897,30 @@ static inline Vec8d lookup(Vec8q const & index, double const * table) {
 *
 *****************************************************************************/
 // Load elements from array a with indices i0,i1,i2,i3,i4,i5,i6,i7,i8,i9,i10,i11,i12,i13,i14,i15
-template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7, 
+template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7,
 int i8, int i9, int i10, int i11, int i12, int i13, int i14, int i15>
 static inline Vec16f gather16f(void const * a) {
-    Static_error_check<(i0|i1|i2|i3|i4|i5|i6|i7|i8|i9|i10|i11|i12|i13|i14|i15)>=0> Negative_array_index;  // Error message if index is negative
-    // find smallest and biggest index, using only compile-time constant expressions
-    const int i01min   = i0  < i1  ? i0  : i1;
-    const int i23min   = i2  < i3  ? i2  : i3;
-    const int i45min   = i4  < i5  ? i4  : i5;
-    const int i67min   = i6  < i7  ? i6  : i7;
-    const int i89min   = i8  < i9  ? i8  : i9;
-    const int i1011min = i10 < i11 ? i10 : i11;
-    const int i1213min = i12 < i13 ? i12 : i13;
-    const int i1415min = i14 < i15 ? i14 : i15;
-    const int i0_3min   = i01min   < i23min    ? i01min   : i23min;
-    const int i4_7min   = i45min   < i67min    ? i45min   : i67min;
-    const int i8_11min  = i89min   < i1011min  ? i89min   : i1011min;
-    const int i12_15min = i1213min < i1415min  ? i1213min : i1415min;
-    const int i0_7min   = i0_3min  < i4_7min   ? i0_3min  : i4_7min;
-    const int i8_15min  = i8_11min < i12_15min ? i8_11min : i12_15min;
-    const int imin      = i0_7min  < i8_15min  ? i0_7min  : i8_15min;
-    const int i01max   = i0  > i1  ? i0  : i1;
-    const int i23max   = i2  > i3  ? i2  : i3;
-    const int i45max   = i4  > i5  ? i4  : i5;
-    const int i67max   = i6  > i7  ? i6  : i7;
-    const int i89max   = i8  > i9  ? i8  : i9;
-    const int i1011max = i10 > i11 ? i10 : i11;
-    const int i1213max = i12 > i13 ? i12 : i13;
-    const int i1415max = i14 > i15 ? i14 : i15;
-    const int i0_3max   = i01max   > i23max    ? i01max   : i23max;
-    const int i4_7max   = i45max   > i67max    ? i45max   : i67max;
-    const int i8_11max  = i89max   > i1011max  ? i89max   : i1011max;
-    const int i12_15max = i1213max > i1415max  ? i1213max : i1415max;
-    const int i0_7max   = i0_3max  > i4_7max   ? i0_3max  : i4_7max;
-    const int i8_15max  = i8_11max > i12_15max ? i8_11max : i12_15max;
-    const int imax      = i0_7max  > i8_15max  ? i0_7max  : i8_15max;
-    if (imax - imin <= 15) {
+    int constexpr indexs[16] = { i0, i1, i2, i3, i4, i5, i6, i7, i8, i9, i10, i11, i12, i13, i14, i15 };
+    constexpr int imin = min_index(indexs);
+    constexpr int imax = max_index(indexs);
+    static_assert(imin >= 0, "Negative index in gather function");
+
+    if constexpr (imax - imin <= 15) {
         // load one contiguous block and permute
-        if (imax > 15) {
+        if constexpr (imax > 15) {
             // make sure we don't read past the end of the array
             Vec16f b = Vec16f().load((float const *)a + imax-15);
-            return permute16f<i0-imax+15, i1-imax+15, i2-imax+15, i3-imax+15, i4-imax+15, i5-imax+15, i6-imax+15, i7-imax+15,
+            return permute16<i0-imax+15, i1-imax+15, i2-imax+15, i3-imax+15, i4-imax+15, i5-imax+15, i6-imax+15, i7-imax+15,
                 i8-imax+15, i9-imax+15, i10-imax+15, i11-imax+15, i12-imax+15, i13-imax+15, i14-imax+15, i15-imax+15> (b);
         }
         else {
             Vec16f b = Vec16f().load((float const *)a + imin);
-            return permute16f<i0-imin, i1-imin, i2-imin, i3-imin, i4-imin, i5-imin, i6-imin, i7-imin,
+            return permute16<i0-imin, i1-imin, i2-imin, i3-imin, i4-imin, i5-imin, i6-imin, i7-imin,
                 i8-imin, i9-imin, i10-imin, i11-imin, i12-imin, i13-imin, i14-imin, i15-imin> (b);
         }
     }
-    if ((i0<imin+16  || i0>imax-16)  && (i1<imin+16  || i1>imax-16)  && (i2<imin+16  || i2>imax-16)  && (i3<imin+16  || i3>imax-16)
-    &&  (i4<imin+16  || i4>imax-16)  && (i5<imin+16  || i5>imax-16)  && (i6<imin+16  || i6>imax-16)  && (i7<imin+16  || i7>imax-16)    
+    if constexpr ((i0<imin+16  || i0>imax-16)  && (i1<imin+16  || i1>imax-16)  && (i2<imin+16  || i2>imax-16)  && (i3<imin+16  || i3>imax-16)
+    &&  (i4<imin+16  || i4>imax-16)  && (i5<imin+16  || i5>imax-16)  && (i6<imin+16  || i6>imax-16)  && (i7<imin+16  || i7>imax-16)
     &&  (i8<imin+16  || i8>imax-16)  && (i9<imin+16  || i9>imax-16)  && (i10<imin+16 || i10>imax-16) && (i11<imin+16 || i11>imax-16)
     &&  (i12<imin+16 || i12>imax-16) && (i13<imin+16 || i13>imax-16) && (i14<imin+16 || i14>imax-16) && (i15<imin+16 || i15>imax-16) ) {
         // load two contiguous blocks and blend
@@ -2320,7 +1942,7 @@ static inline Vec16f gather16f(void const * a) {
         const int j13 = i13<imin+16 ? i13-imin : 31-imax+i13;
         const int j14 = i14<imin+16 ? i14-imin : 31-imax+i14;
         const int j15 = i15<imin+16 ? i15-imin : 31-imax+i15;
-        return blend16f<j0,j1,j2,j3,j4,j5,j6,j7,j8,j9,j10,j11,j12,j13,j14,j15>(b, c);
+        return blend16<j0,j1,j2,j3,j4,j5,j6,j7,j8,j9,j10,j11,j12,j13,j14,j15>(b, c);
     }
     // use gather instruction
     return _mm512_i32gather_ps(Vec16i(i0,i1,i2,i3,i4,i5,i6,i7,i8,i9,i10,i11,i12,i13,i14,i15), (const float *)a, 4);
@@ -2329,35 +1951,24 @@ static inline Vec16f gather16f(void const * a) {
 
 template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7>
 static inline Vec8d gather8d(void const * a) {
-    Static_error_check<(i0|i1|i2|i3|i4|i5|i6|i7)>=0> Negative_array_index;  // Error message if index is negative
-
-    const int i01min = i0 < i1 ? i0 : i1;
-    const int i23min = i2 < i3 ? i2 : i3;
-    const int i45min = i4 < i5 ? i4 : i5;
-    const int i67min = i6 < i7 ? i6 : i7;
-    const int i0123min = i01min < i23min ? i01min : i23min;
-    const int i4567min = i45min < i67min ? i45min : i67min;
-    const int imin = i0123min < i4567min ? i0123min : i4567min;
-    const int i01max = i0 > i1 ? i0 : i1;
-    const int i23max = i2 > i3 ? i2 : i3;
-    const int i45max = i4 > i5 ? i4 : i5;
-    const int i67max = i6 > i7 ? i6 : i7;
-    const int i0123max = i01max > i23max ? i01max : i23max;
-    const int i4567max = i45max > i67max ? i45max : i67max;
-    const int imax = i0123max > i4567max ? i0123max : i4567max;
-    if (imax - imin <= 7) {
+    int constexpr indexs[8] = { i0, i1, i2, i3, i4, i5, i6, i7 }; // indexes as array
+    constexpr int imin = min_index(indexs);
+    constexpr int imax = max_index(indexs);
+    static_assert(imin >= 0, "Negative index in gather function");
+
+    if constexpr (imax - imin <= 7) {
         // load one contiguous block and permute
-        if (imax > 7) {
+        if constexpr (imax > 7) {
             // make sure we don't read past the end of the array
             Vec8d b = Vec8d().load((double const *)a + imax-7);
-            return permute8d<i0-imax+7, i1-imax+7, i2-imax+7, i3-imax+7, i4-imax+7, i5-imax+7, i6-imax+7, i7-imax+7> (b);
+            return permute8<i0-imax+7, i1-imax+7, i2-imax+7, i3-imax+7, i4-imax+7, i5-imax+7, i6-imax+7, i7-imax+7> (b);
         }
         else {
             Vec8d b = Vec8d().load((double const *)a + imin);
-            return permute8d<i0-imin, i1-imin, i2-imin, i3-imin, i4-imin, i5-imin, i6-imin, i7-imin> (b);
+            return permute8<i0-imin, i1-imin, i2-imin, i3-imin, i4-imin, i5-imin, i6-imin, i7-imin> (b);
         }
     }
-    if ((i0<imin+8 || i0>imax-8) && (i1<imin+8 || i1>imax-8) && (i2<imin+8 || i2>imax-8) && (i3<imin+8 || i3>imax-8)
+    if constexpr ((i0<imin+8 || i0>imax-8) && (i1<imin+8 || i1>imax-8) && (i2<imin+8 || i2>imax-8) && (i3<imin+8 || i3>imax-8)
     &&  (i4<imin+8 || i4>imax-8) && (i5<imin+8 || i5>imax-8) && (i6<imin+8 || i6>imax-8) && (i7<imin+8 || i7>imax-8)) {
         // load two contiguous blocks and blend
         Vec8d b = Vec8d().load((double const *)a + imin);
@@ -2370,7 +1981,7 @@ static inline Vec8d gather8d(void const * a) {
         const int j5 = i5<imin+8 ? i5-imin : 15-imax+i5;
         const int j6 = i6<imin+8 ? i6-imin : 15-imax+i6;
         const int j7 = i7<imin+8 ? i7-imin : 15-imax+i7;
-        return blend8d<j0, j1, j2, j3, j4, j5, j6, j7>(b, c);
+        return blend8<j0, j1, j2, j3, j4, j5, j6, j7>(b, c);
     }
     // use gather instruction
     return _mm512_i64gather_pd(Vec8q(i0,i1,i2,i3,i4,i5,i6,i7), (const double *)a, 8);
@@ -2383,110 +1994,56 @@ static inline Vec8d gather8d(void const * a) {
 ******************************************************************************
 *
 * These functions write the elements of a vector to arbitrary positions in an
-* array in memory. Each vector element is written to an array position 
+* array in memory. Each vector element is written to an array position
 * determined by an index. An element is not written if the corresponding
 * index is out of range.
 * The indexes can be specified as constant template parameters or as an
 * integer vector.
-* 
-* The scatter functions are useful if the data are distributed in a sparce
-* manner into the array. If the array is dense then it is more efficient
-* to permute the data into the right positions and then write the whole
-* permuted vector into the array.
-*
-* Example:
-* Vec8d a(10,11,12,13,14,15,16,17);
-* double b[16] = {0};
-* scatter<0,2,14,10,1,-1,5,9>(a,b); 
-* // Now, b = {10,14,11,0,0,16,0,0,0,17,13,0,0,0,12,0}
 *
 *****************************************************************************/
 
 template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7,
     int i8, int i9, int i10, int i11, int i12, int i13, int i14, int i15>
-    static inline void scatter(Vec16f const & data, float * array) {
-    __m512i indx = constant16i<i0,i1,i2,i3,i4,i5,i6,i7,i8,i9,i10,i11,i12,i13,i14,i15>();
+    static inline void scatter(Vec16f const data, float * array) {
+    __m512i indx = constant16ui<i0,i1,i2,i3,i4,i5,i6,i7,i8,i9,i10,i11,i12,i13,i14,i15>();
     Vec16fb mask(i0>=0, i1>=0, i2>=0, i3>=0, i4>=0, i5>=0, i6>=0, i7>=0,
         i8>=0, i9>=0, i10>=0, i11>=0, i12>=0, i13>=0, i14>=0, i15>=0);
     _mm512_mask_i32scatter_ps(array, mask, indx, data, 4);
 }
 
 template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7>
-static inline void scatter(Vec8d const & data, double * array) {
-    __m256i indx = constant8i<i0,i1,i2,i3,i4,i5,i6,i7>();
+static inline void scatter(Vec8d const data, double * array) {
+    __m256i indx = constant8ui<i0,i1,i2,i3,i4,i5,i6,i7>();
     Vec8db mask(i0>=0, i1>=0, i2>=0, i3>=0, i4>=0, i5>=0, i6>=0, i7>=0);
     _mm512_mask_i32scatter_pd(array, mask, indx, data, 8);
 }
 
-static inline void scatter(Vec16i const & index, uint32_t limit, Vec16f const & data, float * array) {
-    Vec16fb mask = Vec16ui(index) < limit;
-    _mm512_mask_i32scatter_ps(array, mask, index, data, 4);
-}
-
-static inline void scatter(Vec8q const & index, uint32_t limit, Vec8d const & data, double * array) {
-    Vec8db mask = Vec8uq(index) < uint64_t(limit);
-    _mm512_mask_i64scatter_pd(array, mask, index, data, 8);
-}
-
-static inline void scatter(Vec8i const & index, uint32_t limit, Vec8d const & data, double * array) {
-#if defined (__AVX512VL__)
-    __mmask16 mask = _mm256_cmplt_epu32_mask(index, Vec8ui(limit));
-#else
-    __mmask16 mask = _mm512_cmplt_epu32_mask(_mm512_castsi256_si512(index), _mm512_castsi256_si512(Vec8ui(limit)));
-#endif
-    _mm512_mask_i32scatter_pd(array, mask, index, data, 8);
-}
-
-
-/*****************************************************************************
-*
-*          Horizontal scan functions
-*
-*****************************************************************************/
-
-// Get index to the first element that is true. Return -1 if all are false
-static inline int horizontal_find_first(Vec16fb const & x) {
-    return horizontal_find_first(Vec16ib(x));
-}
-
-static inline int horizontal_find_first(Vec8db const & x) {
-    return horizontal_find_first(Vec8qb(x));
-}
-
-// Count the number of elements that are true
-static inline uint32_t horizontal_count(Vec16fb const & x) {
-    return horizontal_count(Vec16ib(x));
-}
-
-static inline uint32_t horizontal_count(Vec8db const & x) {
-    return horizontal_count(Vec8qb(x));
-}
 
 /*****************************************************************************
 *
-*          Boolean <-> bitfield conversion functions
+*          Scatter functions with variable indexes
 *
 *****************************************************************************/
 
-// to_bits: convert boolean vector to integer bitfield
-static inline uint16_t to_bits(Vec16fb x) {
-    return to_bits(Vec16ib(x));
+static inline void scatter(Vec16i const index, uint32_t limit, Vec16f const data, float * destination) {
+    Vec16fb mask = Vec16ui(index) < limit;
+    _mm512_mask_i32scatter_ps(destination, mask, index, data, 4);
 }
 
-// to_Vec16fb: convert integer bitfield to boolean vector
-static inline Vec16fb to_Vec16fb(uint16_t x) {
-    return Vec16fb(to_Vec16ib(x));
+static inline void scatter(Vec8q const index, uint32_t limit, Vec8d const data, double * destination) {
+    Vec8db mask = Vec8uq(index) < uint64_t(limit);
+    _mm512_mask_i64scatter_pd(destination, (uint8_t)mask, index, data, 8);
 }
 
-// to_bits: convert boolean vector to integer bitfield
-static inline uint8_t to_bits(Vec8db x) {
-    return to_bits(Vec8qb(x));
+static inline void scatter(Vec8i const index, uint32_t limit, Vec8d const data, double * destination) {
+#if INSTRSET >= 10 // __AVX512VL__, __AVX512DQ__
+    __mmask8 mask = _mm256_cmplt_epu32_mask(index, Vec8ui(limit));
+#else
+    __mmask16 mask = _mm512_cmplt_epu32_mask(_mm512_castsi256_si512(index), _mm512_castsi256_si512(Vec8ui(limit)));
+#endif
+    _mm512_mask_i32scatter_pd(destination, (__mmask8)mask, index, data, 8);
 }
 
-// to_Vec8db: convert integer bitfield to boolean vector
-static inline Vec8db to_Vec8db(uint8_t x) {
-    return Vec8db(to_Vec8qb(x));
-}
 
 #ifdef VCL_NAMESPACE
 }
diff --git a/EEDI3/vectorclass/vectorf512e.h b/EEDI3/vectorclass/vectorf512e.h
index c4f5003..dec7dd5 100644
--- a/EEDI3/vectorclass/vectorf512e.h
+++ b/EEDI3/vectorclass/vectorf512e.h
@@ -1,16 +1,14 @@
 /****************************  vectorf512.h   *******************************
 * Author:        Agner Fog
 * Date created:  2014-07-23
-* Last modified: 2017-02-19
-* Version:       1.27
-* Project:       vector classes
+* Last modified: 2023-07-04
+* Version:       2.02.02
+* Project:       vector class library
 * Description:
-* Header file defining floating point vector classes as interface to intrinsic 
-* functions in x86 microprocessors with AVX512 and later instruction sets.
+* Header file defining 512-bit floating point vector classes
+* Emulated for processors without AVX512 instruction set
 *
-* Instructions:
-* Use Gnu, Intel or Microsoft C++ compiler. Compile for the desired 
-* instruction set, which must be at least AVX512F. 
+* Instructions: see vcl_manual.pdf
 *
 * The following vector classes are defined here:
 * Vec16f    Vector of  16  single precision floating point numbers
@@ -18,21 +16,27 @@
 * Vec8d     Vector of   8  double precision floating point numbers
 * Vec8db    Vector of   8  Booleans for use with Vec8d
 *
-* Each vector object is represented internally in the CPU as a 512-bit register.
+* Each vector object is represented internally in the CPU as two 256-bit registers.
 * This header file defines operators and functions for these vectors.
 *
-* For detailed instructions, see VectorClass.pdf
-*
-* (c) Copyright 2014-2017 GNU General Public License http://www.gnu.org/licenses
+* (c) Copyright 2014-2023 Agner Fog.
+* Apache License version 2.0 or later.
 *****************************************************************************/
 
-// check combination of header files
+#ifndef VECTORF512E_H
+#define VECTORF512E_H
+
+#ifndef VECTORCLASS_H
+#include "vectorclass.h"
+#endif
+
+#if VECTORCLASS_H < 20200
+#error Incompatible versions of vector class library mixed
+#endif
+
 #if defined (VECTORF512_H)
-#if    VECTORF512_H != 1
 #error Two different versions of vectorf512.h included
 #endif
-#else
-#define VECTORF512_H 1
 
 #include "vectori512e.h"
 
@@ -42,28 +46,32 @@ namespace VCL_NAMESPACE {
 
 /*****************************************************************************
 *
-*          Vec16fb: Vector of 16 Booleans for use with Vec16f
+*          Vec16fb: Vector of 16 broad booleans for use with Vec16f
 *
 *****************************************************************************/
 class Vec16fb : public Vec16b {
 public:
     // Default constructor:
-    Vec16fb () {
-    }
+    Vec16fb () = default;
     // Constructor to build from all elements:
     Vec16fb(bool x0, bool x1, bool x2, bool x3, bool x4, bool x5, bool x6, bool x7,
         bool x8, bool x9, bool x10, bool x11, bool x12, bool x13, bool x14, bool x15) :
         Vec16b(x0, x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12, x13, x14, x15) {
     }
     // Constructor from Vec16b
-    Vec16fb (Vec16b const & x) {
+    Vec16fb (Vec16b const x) {
         z0 = x.get_low();
         z1 = x.get_high();
     }
     // Constructor from two Vec8fb
-    Vec16fb (Vec8fb const & x0, Vec8fb const & x1) {
+    Vec16fb (Vec8fb const x0, Vec8fb const x1) {
+#ifdef VECTORF256E_H
+        z0 = reinterpret_i(x0);
+        z1 = reinterpret_i(x1);
+#else
         z0 = x0;
         z1 = x1;
+#endif
     }
     // Constructor to broadcast scalar value:
     Vec16fb(bool b) : Vec16b(b) {
@@ -73,11 +81,6 @@ public:
         *this = Vec16b(b);
         return *this;
     }
-private: // Prevent constructing from int, etc.
-    Vec16fb(int b);
-    Vec16fb & operator = (int x);
-public:
-
     // Get low and high half
     Vec8fb get_low() const {
         return reinterpret_f(Vec8i(z0));
@@ -85,55 +88,74 @@ public:
     Vec8fb get_high() const {
         return reinterpret_f(Vec8i(z1));
     }
+    // Member function to change a bitfield to a boolean vector
+    Vec16fb & load_bits(uint16_t a) {
+        z0 = Vec8ib().load_bits(uint8_t(a));
+        z1 = Vec8ib().load_bits(uint8_t(a>>8));
+        return *this;
+    }
+    // Prevent constructing from int, etc.
+    Vec16fb(int b) = delete;
+    Vec16fb & operator = (int x) = delete;
 };
 
 // Define operators for Vec16fb
 
 // vector operator & : bitwise and
-static inline Vec16fb operator & (Vec16fb const & a, Vec16fb const & b) {
+static inline Vec16fb operator & (Vec16fb const a, Vec16fb const b) {
     return Vec16fb(a.get_low() & b.get_low(), a.get_high() & b.get_high());
 }
-static inline Vec16fb operator && (Vec16fb const & a, Vec16fb const & b) {
+static inline Vec16fb operator && (Vec16fb const a, Vec16fb const b) {
     return a & b;
 }
 
 // vector operator | : bitwise or
-static inline Vec16fb operator | (Vec16fb const & a, Vec16fb const & b) {
+static inline Vec16fb operator | (Vec16fb const a, Vec16fb const b) {
     return Vec16fb(a.get_low() | b.get_low(), a.get_high() | b.get_high());
 }
-static inline Vec16fb operator || (Vec16fb const & a, Vec16fb const & b) {
+static inline Vec16fb operator || (Vec16fb const a, Vec16fb const b) {
     return a | b;
 }
 
 // vector operator ^ : bitwise xor
-static inline Vec16fb operator ^ (Vec16fb const & a, Vec16fb const & b) {
+static inline Vec16fb operator ^ (Vec16fb const a, Vec16fb const b) {
     return Vec16fb(a.get_low() ^ b.get_low(), a.get_high() ^ b.get_high());
 }
 
+// vector operator == : xnor
+static inline Vec16fb operator == (Vec16fb const a, Vec16fb const b) {
+    return Vec16fb(Vec16fb(a) ^ Vec16fb(~b));
+}
+
+// vector operator != : xor
+static inline Vec16fb operator != (Vec16fb const a, Vec16fb const b) {
+    return Vec16fb(a ^ b);
+}
+
 // vector operator ~ : bitwise not
-static inline Vec16fb operator ~ (Vec16fb const & a) {
+static inline Vec16fb operator ~ (Vec16fb const a) {
     return Vec16fb(~a.get_low(), ~a.get_high());
 }
 
 // vector operator ! : element not
-static inline Vec16fb operator ! (Vec16fb const & a) {
+static inline Vec16fb operator ! (Vec16fb const a) {
     return ~a;
 }
 
 // vector operator &= : bitwise and
-static inline Vec16fb & operator &= (Vec16fb & a, Vec16fb const & b) {
+static inline Vec16fb & operator &= (Vec16fb & a, Vec16fb const b) {
     a = a & b;
     return a;
 }
 
 // vector operator |= : bitwise or
-static inline Vec16fb & operator |= (Vec16fb & a, Vec16fb const & b) {
+static inline Vec16fb & operator |= (Vec16fb & a, Vec16fb const b) {
     a = a | b;
     return a;
 }
 
 // vector operator ^= : bitwise xor
-static inline Vec16fb & operator ^= (Vec16fb & a, Vec16fb const & b) {
+static inline Vec16fb & operator ^= (Vec16fb & a, Vec16fb const b) {
     a = a ^ b;
     return a;
 }
@@ -141,29 +163,33 @@ static inline Vec16fb & operator ^= (Vec16fb & a, Vec16fb const & b) {
 
 /*****************************************************************************
 *
-*          Vec8db: Vector of 8 Booleans for use with Vec8d
+*          Vec8db: Vector of 8 broad booleans for use with Vec8d
 *
 *****************************************************************************/
 
 class Vec8db : public Vec512b {
 public:
     // Default constructor:
-    Vec8db () {
-    }
+    Vec8db () = default;
     // Constructor to build from all elements:
     Vec8db(bool x0, bool x1, bool x2, bool x3, bool x4, bool x5, bool x6, bool x7) {
         z0 = Vec4qb(x0, x1, x2, x3);
         z1 = Vec4qb(x4, x5, x6, x7);
     }
     // Construct from Vec512b
-    Vec8db (Vec512b const & x) {
+    Vec8db (Vec512b const x) {
         z0 = x.get_low();
         z1 = x.get_high();
     }
     // Constructor from two Vec4db
-    Vec8db (Vec4db const & x0, Vec4db const & x1) {
+    Vec8db (Vec4db const x0, Vec4db const x1) {
+#ifdef VECTORF256E_H
+        z0 = reinterpret_i(x0);
+        z1 = reinterpret_i(x1);
+#else
         z0 = x0;
         z1 = x1;
+#endif
     }
     // Constructor to broadcast single value:
     Vec8db(bool b) {
@@ -174,13 +200,7 @@ public:
         *this = Vec8db(b);
         return *this;
     }
-private: 
-    // Prevent constructing from int, etc. because of ambiguity
-    Vec8db(int b);
-    // Prevent assigning int because of ambiguity
-    Vec8db & operator = (int x);
-public:
-    Vec8db & insert (int index, bool a) {
+    Vec8db & insert(int index, bool a) {
         if (index < 4) {
             z0 = Vec4q(z0).insert(index, -(int64_t)a);
         }
@@ -190,8 +210,8 @@ public:
         return *this;
     }
     // Member function extract a single element from vector
-    bool extract(uint32_t index) const {
-        if (index < 4) {
+    bool extract(int index) const {
+        if ((uint32_t)index < 4) {
             return Vec4q(z0).extract(index) != 0;
         }
         else {
@@ -199,7 +219,7 @@ public:
         }
     }
     // Extract a single element. Operator [] can only read an element, not write.
-    bool operator [] (uint32_t index) const {
+    bool operator [] (int index) const {
         return extract(index);
     }
     // Get low and high half
@@ -209,58 +229,81 @@ public:
     Vec4db get_high() const {
         return reinterpret_d(Vec4q(z1));
     }
-    static int size () {
+    // Member function to change a bitfield to a boolean vector
+    Vec8db & load_bits(uint8_t a) {
+        z0 = Vec4qb().load_bits(a);
+        z1 = Vec4qb().load_bits(uint8_t(a>>4u));
+        return *this;
+    }
+    static constexpr int size() {
         return 8;
     }
+    static constexpr int elementtype() {
+        return 3;
+    }
+    // Prevent constructing from int, etc. because of ambiguity
+    Vec8db(int b) = delete;
+    // Prevent assigning int because of ambiguity
+    Vec8db & operator = (int x) = delete;
 };
 
 // Define operators for Vec8db
 
 // vector operator & : bitwise and
-static inline Vec8db operator & (Vec8db const & a, Vec8db const & b) {
+static inline Vec8db operator & (Vec8db const a, Vec8db const b) {
     return Vec8db(a.get_low() & b.get_low(), a.get_high() & b.get_high());
 }
-static inline Vec8db operator && (Vec8db const & a, Vec8db const & b) {
+static inline Vec8db operator && (Vec8db const a, Vec8db const b) {
     return a & b;
 }
 
 // vector operator | : bitwise or
-static inline Vec8db operator | (Vec8db const & a, Vec8db const & b) {
+static inline Vec8db operator | (Vec8db const a, Vec8db const b) {
     return Vec8db(a.get_low() | b.get_low(), a.get_high() | b.get_high());
 }
-static inline Vec8db operator || (Vec8db const & a, Vec8db const & b) {
+static inline Vec8db operator || (Vec8db const a, Vec8db const b) {
     return a | b;
 }
 
 // vector operator ^ : bitwise xor
-static inline Vec8db operator ^ (Vec8db const & a, Vec8db const & b) {
+static inline Vec8db operator ^ (Vec8db const a, Vec8db const b) {
     return Vec8db(a.get_low() ^ b.get_low(), a.get_high() ^ b.get_high());
 }
 
+// vector operator == : xnor
+static inline Vec8db operator == (Vec8db const a, Vec8db const b) {
+    return Vec8db(Vec8db(a) ^ Vec8db(~b));
+}
+
+// vector operator != : xor
+static inline Vec8db operator != (Vec8db const a, Vec8db const b) {
+    return Vec8db(a ^ b);
+}
+
 // vector operator ~ : bitwise not
-static inline Vec8db operator ~ (Vec8db const & a) {
+static inline Vec8db operator ~ (Vec8db const a) {
     return Vec8db(~a.get_low(), ~a.get_high());
 }
 
 // vector operator ! : element not
-static inline Vec8db operator ! (Vec8db const & a) {
+static inline Vec8db operator ! (Vec8db const a) {
     return ~a;
 }
 
 // vector operator &= : bitwise and
-static inline Vec8db & operator &= (Vec8db & a, Vec8db const & b) {
+static inline Vec8db & operator &= (Vec8db & a, Vec8db const b) {
     a = a & b;
     return a;
 }
 
 // vector operator |= : bitwise or
-static inline Vec8db & operator |= (Vec8db & a, Vec8db const & b) {
+static inline Vec8db & operator |= (Vec8db & a, Vec8db const b) {
     a = a | b;
     return a;
 }
 
 // vector operator ^= : bitwise xor
-static inline Vec8db & operator ^= (Vec8db & a, Vec8db const & b) {
+static inline Vec8db & operator ^= (Vec8db & a, Vec8db const b) {
     a = a ^ b;
     return a;
 }
@@ -278,8 +321,7 @@ protected:
     Vec8f z1;
 public:
     // Default constructor:
-    Vec16f() {
-    }
+    Vec16f() = default;
     // Constructor to broadcast the same value into all elements:
     Vec16f(float f) {
         z0 = z1 = Vec8f(f);
@@ -291,7 +333,7 @@ public:
         z1 = Vec8f(f8, f9, f10, f11, f12, f13, f14, f15);
     }
     // Constructor to build from two Vec8f:
-    Vec16f(Vec8f const & a0, Vec8f const & a1) {
+    Vec16f(Vec8f const a0, Vec8f const a1) {
         z0 = a0;
         z1 = a1;
     }
@@ -309,8 +351,7 @@ public:
         return *this;
     }
     // Member function to load from array, aligned by 64
-    // You may use load_a instead of load if you are certain that p points to an address
-    // divisible by 64.
+    // You may use load_a instead of load if you are certain that p points to an address divisible by 64
     Vec16f & load_a(float const * p) {
         z0 = Vec8f().load_a(p);
         z1 = Vec8f().load_a(p+8);
@@ -322,12 +363,19 @@ public:
         Vec8f(z1).store(p+8);
     }
     // Member function to store into array, aligned by 64
-    // You may use store_a instead of store if you are certain that p points to an address
-    // divisible by 64.
+    // You may use store_a instead of store if you are certain that p points to an address divisible by 64
     void store_a(float * p) const {
         Vec8f(z0).store_a(p);
         Vec8f(z1).store_a(p+8);
     }
+    // Member function storing to aligned uncached memory (non-temporal store).
+    // This may be more efficient than store_a when storing large blocks of memory if it 
+    // is unlikely that the data will stay in the cache until it is read again.
+    // Note: Will generate runtime error if p is not aligned by 64
+    void store_nt(float * p) const {
+        Vec8f(z0).store_nt(p);
+        Vec8f(z1).store_nt(p+8);
+    }
     // Partial load. Load n elements and set the rest to 0
     Vec16f & load_partial(int n, float const * p) {
         if (n < 8) {
@@ -362,8 +410,8 @@ public:
         return *this;
     }
     // Member function to change a single element in vector
-    Vec16f const & insert(uint32_t index, float value) {
-        if (index < 8) {
+    Vec16f const insert(int index, float value) {
+        if ((uint32_t)index < 8) {
             z0 = Vec8f(z0).insert(index, value);
         }
         else {
@@ -372,17 +420,20 @@ public:
         return *this;
     }
     // Member function extract a single element from vector
-    float extract(uint32_t index) const {
+    float extract(int index) const {
         float a[16];
         store(a);
         return a[index & 15];
     }
     // Extract a single element. Use store function if extracting more than one element.
     // Operator [] can only read an element, not write.
-    float operator [] (uint32_t index) const {
+    float operator [] (int index) const {
         return extract(index);
     }
-    static int size () {
+    static constexpr int size() {
+        return 16;
+    }
+    static constexpr int elementtype() {
         return 16;
     }
 };
@@ -395,20 +446,20 @@ public:
 *****************************************************************************/
 
 // vector operator + : add element by element
-static inline Vec16f operator + (Vec16f const & a, Vec16f const & b) {
+static inline Vec16f operator + (Vec16f const a, Vec16f const b) {
     return Vec16f(a.get_low() + b.get_low(), a.get_high() + b.get_high());
 }
 
 // vector operator + : add vector and scalar
-static inline Vec16f operator + (Vec16f const & a, float b) {
+static inline Vec16f operator + (Vec16f const a, float b) {
     return a + Vec16f(b);
 }
-static inline Vec16f operator + (float a, Vec16f const & b) {
+static inline Vec16f operator + (float a, Vec16f const b) {
     return Vec16f(a) + b;
 }
 
 // vector operator += : add
-static inline Vec16f & operator += (Vec16f & a, Vec16f const & b) {
+static inline Vec16f & operator += (Vec16f & a, Vec16f const b) {
     a = a + b;
     return a;
 }
@@ -427,26 +478,26 @@ static inline Vec16f & operator ++ (Vec16f & a) {
 }
 
 // vector operator - : subtract element by element
-static inline Vec16f operator - (Vec16f const & a, Vec16f const & b) {
+static inline Vec16f operator - (Vec16f const a, Vec16f const b) {
     return Vec16f(a.get_low() - b.get_low(), a.get_high() - b.get_high());
 }
 
 // vector operator - : subtract vector and scalar
-static inline Vec16f operator - (Vec16f const & a, float b) {
+static inline Vec16f operator - (Vec16f const a, float b) {
     return a - Vec16f(b);
 }
-static inline Vec16f operator - (float a, Vec16f const & b) {
+static inline Vec16f operator - (float a, Vec16f const b) {
     return Vec16f(a) - b;
 }
 
 // vector operator - : unary minus
 // Change sign bit, even for 0, INF and NAN
-static inline Vec16f operator - (Vec16f const & a) {
+static inline Vec16f operator - (Vec16f const a) {
     return Vec16f(-a.get_low(), -a.get_high());
 }
 
 // vector operator -= : subtract
-static inline Vec16f & operator -= (Vec16f & a, Vec16f const & b) {
+static inline Vec16f & operator -= (Vec16f & a, Vec16f const b) {
     a = a - b;
     return a;
 }
@@ -465,118 +516,118 @@ static inline Vec16f & operator -- (Vec16f & a) {
 }
 
 // vector operator * : multiply element by element
-static inline Vec16f operator * (Vec16f const & a, Vec16f const & b) {
+static inline Vec16f operator * (Vec16f const a, Vec16f const b) {
     return Vec16f(a.get_low() * b.get_low(), a.get_high() * b.get_high());
 }
 
 // vector operator * : multiply vector and scalar
-static inline Vec16f operator * (Vec16f const & a, float b) {
+static inline Vec16f operator * (Vec16f const a, float b) {
     return a * Vec16f(b);
 }
-static inline Vec16f operator * (float a, Vec16f const & b) {
+static inline Vec16f operator * (float a, Vec16f const b) {
     return Vec16f(a) * b;
 }
 
 // vector operator *= : multiply
-static inline Vec16f & operator *= (Vec16f & a, Vec16f const & b) {
+static inline Vec16f & operator *= (Vec16f & a, Vec16f const b) {
     a = a * b;
     return a;
 }
 
 // vector operator / : divide all elements by same integer
-static inline Vec16f operator / (Vec16f const & a, Vec16f const & b) {
+static inline Vec16f operator / (Vec16f const a, Vec16f const b) {
     return Vec16f(a.get_low() / b.get_low(), a.get_high() / b.get_high());
 }
 
 // vector operator / : divide vector and scalar
-static inline Vec16f operator / (Vec16f const & a, float b) {
+static inline Vec16f operator / (Vec16f const a, float b) {
     return a / Vec16f(b);
 }
-static inline Vec16f operator / (float a, Vec16f const & b) {
+static inline Vec16f operator / (float a, Vec16f const b) {
     return Vec16f(a) / b;
 }
 
 // vector operator /= : divide
-static inline Vec16f & operator /= (Vec16f & a, Vec16f const & b) {
+static inline Vec16f & operator /= (Vec16f & a, Vec16f const b) {
     a = a / b;
     return a;
 }
 
 // vector operator == : returns true for elements for which a == b
-static inline Vec16fb operator == (Vec16f const & a, Vec16f const & b) {
+static inline Vec16fb operator == (Vec16f const a, Vec16f const b) {
     return Vec16fb(a.get_low() == b.get_low(), a.get_high() == b.get_high());
 }
 
 // vector operator != : returns true for elements for which a != b
-static inline Vec16fb operator != (Vec16f const & a, Vec16f const & b) {
+static inline Vec16fb operator != (Vec16f const a, Vec16f const b) {
     return Vec16fb(a.get_low() != b.get_low(), a.get_high() != b.get_high());
 }
 
 // vector operator < : returns true for elements for which a < b
-static inline Vec16fb operator < (Vec16f const & a, Vec16f const & b) {
+static inline Vec16fb operator < (Vec16f const a, Vec16f const b) {
     return Vec16fb(a.get_low() < b.get_low(), a.get_high() < b.get_high());
 }
 
 // vector operator <= : returns true for elements for which a <= b
-static inline Vec16fb operator <= (Vec16f const & a, Vec16f const & b) {
+static inline Vec16fb operator <= (Vec16f const a, Vec16f const b) {
     return Vec16fb(a.get_low() <= b.get_low(), a.get_high() <= b.get_high());
 }
 
 // vector operator > : returns true for elements for which a > b
-static inline Vec16fb operator > (Vec16f const & a, Vec16f const & b) {
+static inline Vec16fb operator > (Vec16f const a, Vec16f const b) {
     return b < a;
 }
 
 // vector operator >= : returns true for elements for which a >= b
-static inline Vec16fb operator >= (Vec16f const & a, Vec16f const & b) {
+static inline Vec16fb operator >= (Vec16f const a, Vec16f const b) {
     return b <= a;
 }
 
 // Bitwise logical operators
 
 // vector operator & : bitwise and
-static inline Vec16f operator & (Vec16f const & a, Vec16f const & b) {
+static inline Vec16f operator & (Vec16f const a, Vec16f const b) {
     return Vec16f(a.get_low() & b.get_low(), a.get_high() & b.get_high());
 }
 
 // vector operator &= : bitwise and
-static inline Vec16f & operator &= (Vec16f & a, Vec16f const & b) {
+static inline Vec16f & operator &= (Vec16f & a, Vec16f const b) {
     a = a & b;
     return a;
 }
 
 // vector operator & : bitwise and of Vec16f and Vec16fb
-static inline Vec16f operator & (Vec16f const & a, Vec16fb const & b) {
+static inline Vec16f operator & (Vec16f const a, Vec16fb const b) {
     return Vec16f(a.get_low() & b.get_low(), a.get_high() & b.get_high());
 }
-static inline Vec16f operator & (Vec16fb const & a, Vec16f const & b) {
+static inline Vec16f operator & (Vec16fb const a, Vec16f const b) {
     return b & a;
 }
 
 // vector operator | : bitwise or
-static inline Vec16f operator | (Vec16f const & a, Vec16f const & b) {
+static inline Vec16f operator | (Vec16f const a, Vec16f const b) {
     return Vec16f(a.get_low() | b.get_low(), a.get_high() | b.get_high());
 }
 
 // vector operator |= : bitwise or
-static inline Vec16f & operator |= (Vec16f & a, Vec16f const & b) {
+static inline Vec16f & operator |= (Vec16f & a, Vec16f const b) {
     a = a | b;
     return a;
 }
 
 // vector operator ^ : bitwise xor
-static inline Vec16f operator ^ (Vec16f const & a, Vec16f const & b) {
+static inline Vec16f operator ^ (Vec16f const a, Vec16f const b) {
     return Vec16f(a.get_low() ^ b.get_low(), a.get_high() ^ b.get_high());
 }
 
 // vector operator ^= : bitwise xor
-static inline Vec16f & operator ^= (Vec16f & a, Vec16f const & b) {
+static inline Vec16f & operator ^= (Vec16f & a, Vec16f const b) {
     a = a ^ b;
     return a;
 }
 
 // vector operator ! : logical not. Returns Boolean vector
-static inline Vec16fb operator ! (Vec16f const & a) {
+static inline Vec16fb operator ! (Vec16f const a) {
     return Vec16fb(!a.get_low(), !a.get_high());
 }
 
@@ -590,70 +641,80 @@ static inline Vec16fb operator ! (Vec16f const & a) {
 // Select between two operands. Corresponds to this pseudocode:
 // for (int i = 0; i < 8; i++) result[i] = s[i] ? a[i] : b[i];
 // Each byte in s must be either 0 (false) or 0xFFFFFFFF (true). No other values are allowed.
-static inline Vec16f select (Vec16fb const & s, Vec16f const & a, Vec16f const & b) {
+static inline Vec16f select (Vec16fb const s, Vec16f const a, Vec16f const b) {
     return Vec16f(select(s.get_low(), a.get_low(), b.get_low()), select(s.get_high(), a.get_high(), b.get_high()));
 }
 
 // Conditional add: For all vector elements i: result[i] = f[i] ? (a[i] + b[i]) : a[i]
-static inline Vec16f if_add (Vec16fb const & f, Vec16f const & a, Vec16f const & b) {
+static inline Vec16f if_add (Vec16fb const f, Vec16f const a, Vec16f const b) {
     return Vec16f(if_add(f.get_low(), a.get_low(), b.get_low()), if_add(f.get_high(), a.get_high(), b.get_high()));
 }
 
-// Conditional multiply: For all vector elements i: result[i] = f[i] ? (a[i] * b[i]) : a[i]
-static inline Vec16f if_mul (Vec16fb const & f, Vec16f const & a, Vec16f const & b) {
+// Conditional subtract
+static inline Vec16f if_sub (Vec16fb const f, Vec16f const a, Vec16f const b) {
+    return Vec16f(if_sub(f.get_low(), a.get_low(), b.get_low()), if_sub(f.get_high(), a.get_high(), b.get_high()));
+}
+
+// Conditional multiply
+static inline Vec16f if_mul (Vec16fb const f, Vec16f const a, Vec16f const b) {
     return Vec16f(if_mul(f.get_low(), a.get_low(), b.get_low()), if_mul(f.get_high(), a.get_high(), b.get_high()));
 }
 
+// Conditional divide
+static inline Vec16f if_div (Vec16fb const f, Vec16f const a, Vec16f const b) {
+    return Vec16f(if_div(f.get_low(), a.get_low(), b.get_low()), if_div(f.get_high(), a.get_high(), b.get_high()));
+}
+
 // Horizontal add: Calculates the sum of all vector elements.
-static inline float horizontal_add (Vec16f const & a) {
+static inline float horizontal_add (Vec16f const a) {
     return horizontal_add(a.get_low() + a.get_high());
 }
 
 // function max: a > b ? a : b
-static inline Vec16f max(Vec16f const & a, Vec16f const & b) {
+static inline Vec16f max(Vec16f const a, Vec16f const b) {
     return Vec16f(max(a.get_low(), b.get_low()), max(a.get_high(), b.get_high()));
 }
 
 // function min: a < b ? a : b
-static inline Vec16f min(Vec16f const & a, Vec16f const & b) {
+static inline Vec16f min(Vec16f const a, Vec16f const b) {
     return Vec16f(min(a.get_low(), b.get_low()), min(a.get_high(), b.get_high()));
 }
+// NAN-safe versions of maximum and minimum are in vector_convert.h
 
 // function abs: absolute value
 // Removes sign bit, even for -0.0f, -INF and -NAN
-static inline Vec16f abs(Vec16f const & a) {
+static inline Vec16f abs(Vec16f const a) {
     return Vec16f(abs(a.get_low()), abs(a.get_high()));
 }
 
 // function sqrt: square root
-static inline Vec16f sqrt(Vec16f const & a) {
+static inline Vec16f sqrt(Vec16f const a) {
     return Vec16f(sqrt(a.get_low()), sqrt(a.get_high()));
 }
 
 // function square: a * a
-static inline Vec16f square(Vec16f const & a) {
+static inline Vec16f square(Vec16f const a) {
     return a * a;
 }
 
 // pow(Vec16f, int):
-template <typename TT> static Vec16f pow(Vec16f const & a, TT const & n);
+template <typename TT> static Vec16f pow(Vec16f const a, TT const n);
 
 // Raise floating point numbers to integer power n
 template <>
-inline Vec16f pow<int>(Vec16f const & x0, int const & n) {
+inline Vec16f pow<int>(Vec16f const x0, int const n) {
     return pow_template_i<Vec16f>(x0, n);
 }
 
 // allow conversion from unsigned int
 template <>
-inline Vec16f pow<uint32_t>(Vec16f const & x0, uint32_t const & n) {
+inline Vec16f pow<uint32_t>(Vec16f const x0, uint32_t const n) {
     return pow_template_i<Vec16f>(x0, (int)n);
 }
 
-
 // Raise floating point numbers to integer power n, where n is a compile-time constant
 template <int n>
-static inline Vec16f pow_n(Vec16f const & a) {
+static inline Vec16f pow_n(Vec16f const a) {
     if (n < 0)    return Vec16f(1.0f) / pow_n<-n>(a);
     if (n == 0)   return Vec16f(1.0f);
     if (n >= 256) return pow(a, n);
@@ -700,48 +761,50 @@ static inline Vec16f pow_n(Vec16f const & a) {
 }
 
 template <int n>
-static inline Vec16f pow(Vec16f const & a, Const_int_t<n>) {
+static inline Vec16f pow(Vec16f const a, Const_int_t<n>) {
     return pow_n<n>(a);
 }
 
 
 // function round: round to nearest integer (even). (result as float vector)
-static inline Vec16f round(Vec16f const & a) {
+static inline Vec16f round(Vec16f const a) {
     return Vec16f(round(a.get_low()), round(a.get_high()));
 }
 
 // function truncate: round towards zero. (result as float vector)
-static inline Vec16f truncate(Vec16f const & a) {
+static inline Vec16f truncate(Vec16f const a) {
     return Vec16f(truncate(a.get_low()), truncate(a.get_high()));
 }
 
 // function floor: round towards minus infinity. (result as float vector)
-static inline Vec16f floor(Vec16f const & a) {
+static inline Vec16f floor(Vec16f const a) {
     return Vec16f(floor(a.get_low()), floor(a.get_high()));
 }
 
 // function ceil: round towards plus infinity. (result as float vector)
-static inline Vec16f ceil(Vec16f const & a) {
+static inline Vec16f ceil(Vec16f const a) {
     return Vec16f(ceil(a.get_low()), ceil(a.get_high()));
 }
 
-// function round_to_int: round to nearest integer (even). (result as integer vector)
-static inline Vec16i round_to_int(Vec16f const & a) {
-    return Vec16i(round_to_int(a.get_low()), round_to_int(a.get_high()));
+// function roundi: round to nearest integer (even). (result as integer vector)
+static inline Vec16i roundi(Vec16f const a) {
+    return Vec16i(roundi(a.get_low()), roundi(a.get_high()));
 }
+//static inline Vec16i round_to_int(Vec16f const a) {return roundi(a);} // deprecated
 
-// function truncate_to_int: round towards zero. (result as integer vector)
-static inline Vec16i truncate_to_int(Vec16f const & a) {
-    return Vec16i(truncate_to_int(a.get_low()), truncate_to_int(a.get_high()));
+// function truncatei: round towards zero. (result as integer vector)
+static inline Vec16i truncatei(Vec16f const a) {
+    return Vec16i(truncatei(a.get_low()), truncatei(a.get_high()));
 }
+//static inline Vec16i truncate_to_int(Vec16f const a) {return truncatei(a);} // deprecated
 
 // function to_float: convert integer vector to float vector
-static inline Vec16f to_float(Vec16i const & a) {
+static inline Vec16f to_float(Vec16i const a) {
     return Vec16f(to_float(a.get_low()), to_float(a.get_high()));
 }
 
 // function to_float: convert unsigned integer vector to float vector
-static inline Vec16f to_float(Vec16ui const & a) {
+static inline Vec16f to_float(Vec16ui const a) {
     return Vec16f(to_float(a.get_low()), to_float(a.get_high()));
 }
 
@@ -750,37 +813,36 @@ static inline Vec16f to_float(Vec16ui const & a) {
 
 // approximate reciprocal (Faster than 1.f / a.
 // relative accuracy better than 2^-11 without AVX512, 2^-14 with AVX512)
-static inline Vec16f approx_recipr(Vec16f const & a) {
+static inline Vec16f approx_recipr(Vec16f const a) {
     return Vec16f(approx_recipr(a.get_low()), approx_recipr(a.get_high()));
 }
 
 // approximate reciprocal squareroot (Faster than 1.f / sqrt(a).
 // Relative accuracy better than 2^-11 without AVX512, 2^-14 with AVX512)
-static inline Vec16f approx_rsqrt(Vec16f const & a) {
+static inline Vec16f approx_rsqrt(Vec16f const a) {
     return Vec16f(approx_rsqrt(a.get_low()), approx_rsqrt(a.get_high()));
 }
 
-
 // Fused multiply and add functions
 
 // Multiply and add
-static inline Vec16f mul_add(Vec16f const & a, Vec16f const & b, Vec16f const & c) {
+static inline Vec16f mul_add(Vec16f const a, Vec16f const b, Vec16f const c) {
     return Vec16f(mul_add(a.get_low(), b.get_low(), c.get_low()), mul_add(a.get_high(), b.get_high(), c.get_high()));
 }
 
 // Multiply and subtract
-static inline Vec16f mul_sub(Vec16f const & a, Vec16f const & b, Vec16f const & c) {
+static inline Vec16f mul_sub(Vec16f const a, Vec16f const b, Vec16f const c) {
     return Vec16f(mul_sub(a.get_low(), b.get_low(), c.get_low()), mul_sub(a.get_high(), b.get_high(), c.get_high()));
 }
 
 // Multiply and inverse subtract
-static inline Vec16f nmul_add(Vec16f const & a, Vec16f const & b, Vec16f const & c) {
+static inline Vec16f nmul_add(Vec16f const a, Vec16f const b, Vec16f const c) {
     return Vec16f(nmul_add(a.get_low(), b.get_low(), c.get_low()), nmul_add(a.get_high(), b.get_high(), c.get_high()));
 }
 
-// Multiply and subtract with extra precision on the intermediate calculations, 
+// Multiply and subtract with extra precision on the intermediate calculations,
 // even if FMA instructions not supported, using Veltkamp-Dekker split
-static inline Vec16f mul_sub_x(Vec16f const & a, Vec16f const & b, Vec16f const & c) {
+static inline Vec16f mul_sub_x(Vec16f const a, Vec16f const b, Vec16f const c) {
     return Vec16f(mul_sub_x(a.get_low(), b.get_low(), c.get_low()), mul_sub_x(a.get_high(), b.get_high(), c.get_high()));
 }
 
@@ -790,14 +852,14 @@ static inline Vec16f mul_sub_x(Vec16f const & a, Vec16f const & b, Vec16f const
 // Extract the exponent as an integer
 // exponent(a) = floor(log2(abs(a)));
 // exponent(1.0f) = 0, exponent(0.0f) = -127, exponent(INF) = +128, exponent(NAN) = +128
-static inline Vec16i exponent(Vec16f const & a) {
+static inline Vec16i exponent(Vec16f const a) {
     return Vec16i(exponent(a.get_low()), exponent(a.get_high()));
 }
 
 // Extract the fraction part of a floating point number
 // a = 2^exponent(a) * fraction(a), except for a = 0
-// fraction(1.0f) = 1.0f, fraction(5.0f) = 1.25f 
-static inline Vec16f fraction(Vec16f const & a) {
+// fraction(1.0f) = 1.0f, fraction(5.0f) = 1.25f
+static inline Vec16f fraction(Vec16f const a) {
     return Vec16f(fraction(a.get_low()), fraction(a.get_high()));
 }
 
@@ -806,10 +868,10 @@ static inline Vec16f fraction(Vec16f const & a) {
 // n >=  128 gives +INF
 // n <= -127 gives 0.0f
 // This function will never produce denormals, and never raise exceptions
-static inline Vec16f exp2(Vec16i const & n) {
+static inline Vec16f exp2(Vec16i const n) {
     return Vec16f(exp2(n.get_low()), exp2(n.get_high()));
 }
-//static Vec16f exp2(Vec16f const & x); // defined in vectormath_exp.h
+//static Vec16f exp2(Vec16f const x); // defined in vectormath_exp.h
 
 
 // Categorization functions
@@ -818,46 +880,46 @@ static inline Vec16f exp2(Vec16i const & n) {
 // even for -0.0f, -INF and -NAN
 // Note that sign_bit(Vec16f(-0.0f)) gives true, while Vec16f(-0.0f) < Vec16f(0.0f) gives false
 // (the underscore in the name avoids a conflict with a macro in Intel's mathimf.h)
-static inline Vec16fb sign_bit(Vec16f const & a) {
+static inline Vec16fb sign_bit(Vec16f const a) {
     return Vec16fb(sign_bit(a.get_low()), sign_bit(a.get_high()));
 }
 
 // Function sign_combine: changes the sign of a when b has the sign bit set
 // same as select(sign_bit(b), -a, a)
-static inline Vec16f sign_combine(Vec16f const & a, Vec16f const & b) {
+static inline Vec16f sign_combine(Vec16f const a, Vec16f const b) {
     return Vec16f(sign_combine(a.get_low(), b.get_low()), sign_combine(a.get_high(), b.get_high()));
 }
 
-// Function is_finite: gives true for elements that are normal, denormal or zero, 
+// Function is_finite: gives true for elements that are normal, denormal or zero,
 // false for INF and NAN
 // (the underscore in the name avoids a conflict with a macro in Intel's mathimf.h)
-static inline Vec16fb is_finite(Vec16f const & a) {
+static inline Vec16fb is_finite(Vec16f const a) {
     return Vec16fb(is_finite(a.get_low()), is_finite(a.get_high()));
 }
 
 // Function is_inf: gives true for elements that are +INF or -INF
 // false for finite numbers and NAN
 // (the underscore in the name avoids a conflict with a macro in Intel's mathimf.h)
-static inline Vec16fb is_inf(Vec16f const & a) {
+static inline Vec16fb is_inf(Vec16f const a) {
     return Vec16fb(is_inf(a.get_low()), is_inf(a.get_high()));
 }
 
 // Function is_nan: gives true for elements that are +NAN or -NAN
 // false for finite numbers and +/-INF
 // (the underscore in the name avoids a conflict with a macro in Intel's mathimf.h)
-static inline Vec16fb is_nan(Vec16f const & a) {
+static inline Vec16fb is_nan(Vec16f const a) {
     return Vec16fb(is_nan(a.get_low()), is_nan(a.get_high()));
 }
 
 // Function is_subnormal: gives true for elements that are denormal (subnormal)
 // false for finite numbers, zero, NAN and INF
-static inline Vec16fb is_subnormal(Vec16f const & a) {
+static inline Vec16fb is_subnormal(Vec16f const a) {
     return Vec16fb(is_subnormal(a.get_low()), is_subnormal(a.get_high()));
 }
 
 // Function is_zero_or_subnormal: gives true for elements that are zero or subnormal (denormal)
 // false for finite numbers, NAN and INF
-static inline Vec16fb is_zero_or_subnormal(Vec16f const & a) {
+static inline Vec16fb is_zero_or_subnormal(Vec16f const a) {
     return Vec16fb(is_zero_or_subnormal(a.get_low()), is_zero_or_subnormal(a.get_high()));
 }
 
@@ -868,20 +930,21 @@ static inline Vec16f infinite16f() {
 }
 
 // Function nan4f: returns a vector where all elements are +NAN (quiet)
-static inline Vec16f nan16f(int n = 0x10) {
+static inline Vec16f nan16f(uint32_t n = 0x10) {
     Vec8f nan = nan8f(n);
     return Vec16f(nan, nan);
 }
 
 // change signs on vectors Vec16f
 // Each index i0 - i7 is 1 for changing sign on the corresponding element, 0 for no change
+// ("static" is removed from change_sign templates because it seems to generate problems for
+// the Clang compiler with nested template calls. "static" is probably superfluous anyway.)
 template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7, int i8, int i9, int i10, int i11, int i12, int i13, int i14, int i15>
-static inline Vec16f change_sign(Vec16f const & a) {
+inline Vec16f change_sign(Vec16f const a) {
     return Vec16f(change_sign<i0,i1,i2,i3,i4,i5,i6,i7>(a.get_low()), change_sign<i8,i9,i10,i11,i12,i13,i14,i15>(a.get_high()));
 }
 
 
-
 /*****************************************************************************
 *
 *          Vec8d: Vector of 8 double precision floating point values
@@ -894,8 +957,7 @@ protected:
     Vec4d z1;
 public:
     // Default constructor:
-    Vec8d() {
-    }
+    Vec8d() = default;
     // Constructor to broadcast the same value into all elements:
     Vec8d(double d) {
         z0 = z1 = Vec4d(d);
@@ -906,7 +968,7 @@ public:
         z1 = Vec4d(d4, d5, d6, d7);
     }
     // Constructor to build from two Vec4d:
-    Vec8d(Vec4d const & a0, Vec4d const & a1) {
+    Vec8d(Vec4d const a0, Vec4d const a1) {
         z0 = a0;
         z1 = a1;
     }
@@ -917,8 +979,7 @@ public:
         return *this;
     }
     // Member function to load from array, aligned by 64
-    // You may use load_a instead of load if you are certain that p points to an address
-    // divisible by 64
+    // You may use load_a instead of load if you are certain that p points to an address divisible by 64
     Vec8d & load_a(double const * p) {
         z0.load_a(p);
         z1.load_a(p+4);
@@ -930,12 +991,19 @@ public:
         z1.store(p+4);
     }
     // Member function to store into array, aligned by 64
-    // You may use store_a instead of store if you are certain that p points to an address
-    // divisible by 64
+    // You may use store_a instead of store if you are certain that p points to an address divisible by 64
     void store_a(double * p) const {
         z0.store_a(p);
         z1.store_a(p+4);
     }
+    // Member function storing to aligned uncached memory (non-temporal store).
+    // This may be more efficient than store_a when storing large blocks of memory if it 
+    // is unlikely that the data will stay in the cache until it is read again.
+    // Note: Will generate runtime error if p is not aligned by 64
+    void store_nt(double * p) const {
+        z0.store_nt(p);
+        z1.store_nt(p+4);
+    }
     // Partial load. Load n elements and set the rest to 0
     Vec8d & load_partial(int n, double const * p) {
         if (n < 4) {
@@ -970,9 +1038,8 @@ public:
         return *this;
     }
     // Member function to change a single element in vector
-    // Note: This function is inefficient. Use load function if changing more than one element
-    Vec8d const & insert(uint32_t index, double value) {
-        if (index < 4) {
+    Vec8d const insert(int index, double value) {
+        if ((uint32_t)index < 4) {
             z0.insert(index, value);
         }
         else {
@@ -981,15 +1048,15 @@ public:
         return *this;
     }
     // Member function extract a single element from vector
-    double extract(uint32_t index) const {
+    double extract(int index) const {
         double a[8];
         store(a);
-        return a[index & 7];        
+        return a[index & 7];
     }
 
     // Extract a single element. Use store function if extracting more than one element.
     // Operator [] can only read an element, not write.
-    double operator [] (uint32_t index) const {
+    double operator [] (int index) const {
         return extract(index);
     }
     // Member functions to split into two Vec4d:
@@ -999,13 +1066,15 @@ public:
     Vec4d get_high() const {
         return z1;
     }
-    static int size () {
+    static constexpr int size() {
         return 8;
     }
+    static constexpr int elementtype() {
+        return 17;
+    }
 };
 
 
-
 /*****************************************************************************
 *
 *          Operators for Vec8d
@@ -1013,20 +1082,20 @@ public:
 *****************************************************************************/
 
 // vector operator + : add element by element
-static inline Vec8d operator + (Vec8d const & a, Vec8d const & b) {
+static inline Vec8d operator + (Vec8d const a, Vec8d const b) {
     return Vec8d(a.get_low() + b.get_low(), a.get_high() + b.get_high());
 }
 
 // vector operator + : add vector and scalar
-static inline Vec8d operator + (Vec8d const & a, double b) {
+static inline Vec8d operator + (Vec8d const a, double b) {
     return a + Vec8d(b);
 }
-static inline Vec8d operator + (double a, Vec8d const & b) {
+static inline Vec8d operator + (double a, Vec8d const b) {
     return Vec8d(a) + b;
 }
 
 // vector operator += : add
-static inline Vec8d & operator += (Vec8d & a, Vec8d const & b) {
+static inline Vec8d & operator += (Vec8d & a, Vec8d const b) {
     a = a + b;
     return a;
 }
@@ -1045,26 +1114,26 @@ static inline Vec8d & operator ++ (Vec8d & a) {
 }
 
 // vector operator - : subtract element by element
-static inline Vec8d operator - (Vec8d const & a, Vec8d const & b) {
+static inline Vec8d operator - (Vec8d const a, Vec8d const b) {
     return Vec8d(a.get_low() - b.get_low(), a.get_high() - b.get_high());
 }
 
 // vector operator - : subtract vector and scalar
-static inline Vec8d operator - (Vec8d const & a, double b) {
+static inline Vec8d operator - (Vec8d const a, double b) {
     return a - Vec8d(b);
 }
-static inline Vec8d operator - (double a, Vec8d const & b) {
+static inline Vec8d operator - (double a, Vec8d const b) {
     return Vec8d(a) - b;
 }
 
 // vector operator - : unary minus
 // Change sign bit, even for 0, INF and NAN
-static inline Vec8d operator - (Vec8d const & a) {
+static inline Vec8d operator - (Vec8d const a) {
     return Vec8d(-a.get_low(), -a.get_high());
 }
 
 // vector operator -= : subtract
-static inline Vec8d & operator -= (Vec8d & a, Vec8d const & b) {
+static inline Vec8d & operator -= (Vec8d & a, Vec8d const b) {
     a = a - b;
     return a;
 }
@@ -1083,123 +1152,122 @@ static inline Vec8d & operator -- (Vec8d & a) {
 }
 
 // vector operator * : multiply element by element
-static inline Vec8d operator * (Vec8d const & a, Vec8d const & b) {
+static inline Vec8d operator * (Vec8d const a, Vec8d const b) {
     return Vec8d(a.get_low() * b.get_low(), a.get_high() * b.get_high());
 }
 
 // vector operator * : multiply vector and scalar
-static inline Vec8d operator * (Vec8d const & a, double b) {
+static inline Vec8d operator * (Vec8d const a, double b) {
     return a * Vec8d(b);
 }
-static inline Vec8d operator * (double a, Vec8d const & b) {
+static inline Vec8d operator * (double a, Vec8d const b) {
     return Vec8d(a) * b;
 }
 
 // vector operator *= : multiply
-static inline Vec8d & operator *= (Vec8d & a, Vec8d const & b) {
+static inline Vec8d & operator *= (Vec8d & a, Vec8d const b) {
     a = a * b;
     return a;
 }
 
 // vector operator / : divide all elements by same integer
-static inline Vec8d operator / (Vec8d const & a, Vec8d const & b) {
+static inline Vec8d operator / (Vec8d const a, Vec8d const b) {
     return Vec8d(a.get_low() / b.get_low(), a.get_high() / b.get_high());
 }
 
 // vector operator / : divide vector and scalar
-static inline Vec8d operator / (Vec8d const & a, double b) {
+static inline Vec8d operator / (Vec8d const a, double b) {
     return a / Vec8d(b);
 }
-static inline Vec8d operator / (double a, Vec8d const & b) {
+static inline Vec8d operator / (double a, Vec8d const b) {
     return Vec8d(a) / b;
 }
 
 // vector operator /= : divide
-static inline Vec8d & operator /= (Vec8d & a, Vec8d const & b) {
+static inline Vec8d & operator /= (Vec8d & a, Vec8d const b) {
     a = a / b;
     return a;
 }
 
 // vector operator == : returns true for elements for which a == b
-static inline Vec8db operator == (Vec8d const & a, Vec8d const & b) {
+static inline Vec8db operator == (Vec8d const a, Vec8d const b) {
     return Vec8db(a.get_low() == b.get_low(), a.get_high() == b.get_high());
 }
 
 // vector operator != : returns true for elements for which a != b
-static inline Vec8db operator != (Vec8d const & a, Vec8d const & b) {
+static inline Vec8db operator != (Vec8d const a, Vec8d const b) {
     return Vec8db(a.get_low() != b.get_low(), a.get_high() != b.get_high());
 }
 
 // vector operator < : returns true for elements for which a < b
-static inline Vec8db operator < (Vec8d const & a, Vec8d const & b) {
+static inline Vec8db operator < (Vec8d const a, Vec8d const b) {
     return Vec8db(a.get_low() < b.get_low(), a.get_high() < b.get_high());
 }
 
 // vector operator <= : returns true for elements for which a <= b
-static inline Vec8db operator <= (Vec8d const & a, Vec8d const & b) {
+static inline Vec8db operator <= (Vec8d const a, Vec8d const b) {
     return Vec8db(a.get_low() <= b.get_low(), a.get_high() <= b.get_high());
 }
 
 // vector operator > : returns true for elements for which a > b
-static inline Vec8db operator > (Vec8d const & a, Vec8d const & b) {
+static inline Vec8db operator > (Vec8d const a, Vec8d const b) {
     return b < a;
 }
 
 // vector operator >= : returns true for elements for which a >= b
-static inline Vec8db operator >= (Vec8d const & a, Vec8d const & b) {
+static inline Vec8db operator >= (Vec8d const a, Vec8d const b) {
     return b <= a;
 }
 
 // Bitwise logical operators
 
 // vector operator & : bitwise and
-static inline Vec8d operator & (Vec8d const & a, Vec8d const & b) {
+static inline Vec8d operator & (Vec8d const a, Vec8d const b) {
     return Vec8d(a.get_low() & b.get_low(), a.get_high() & b.get_high());
 }
 
 // vector operator &= : bitwise and
-static inline Vec8d & operator &= (Vec8d & a, Vec8d const & b) {
+static inline Vec8d & operator &= (Vec8d & a, Vec8d const b) {
     a = a & b;
     return a;
 }
 
 // vector operator & : bitwise and of Vec8d and Vec8db
-static inline Vec8d operator & (Vec8d const & a, Vec8db const & b) {
+static inline Vec8d operator & (Vec8d const a, Vec8db const b) {
     return Vec8d(a.get_low() & b.get_low(), a.get_high() & b.get_high());
 }
 
-static inline Vec8d operator & (Vec8db const & a, Vec8d const & b) {
+static inline Vec8d operator & (Vec8db const a, Vec8d const b) {
     return b & a;
 }
 
 // vector operator | : bitwise or
-static inline Vec8d operator | (Vec8d const & a, Vec8d const & b) {
+static inline Vec8d operator | (Vec8d const a, Vec8d const b) {
     return Vec8d(a.get_low() | b.get_low(), a.get_high() | b.get_high());
 }
 
 // vector operator |= : bitwise or
-static inline Vec8d & operator |= (Vec8d & a, Vec8d const & b) {
+static inline Vec8d & operator |= (Vec8d & a, Vec8d const b) {
     a = a | b;
     return a;
 }
 
 // vector operator ^ : bitwise xor
-static inline Vec8d operator ^ (Vec8d const & a, Vec8d const & b) {
+static inline Vec8d operator ^ (Vec8d const a, Vec8d const b) {
     return Vec8d(a.get_low() ^ b.get_low(), a.get_high() ^ b.get_high());
 }
 
 // vector operator ^= : bitwise xor
-static inline Vec8d & operator ^= (Vec8d & a, Vec8d const & b) {
+static inline Vec8d & operator ^= (Vec8d & a, Vec8d const b) {
     a = a ^ b;
     return a;
 }
 
 // vector operator ! : logical not. Returns Boolean vector
-static inline Vec8db operator ! (Vec8d const & a) {
+static inline Vec8db operator ! (Vec8d const a) {
     return Vec8db(!a.get_low(), !a.get_high());
 }
 
-
 /*****************************************************************************
 *
 *          Functions for Vec8d
@@ -1208,73 +1276,83 @@ static inline Vec8db operator ! (Vec8d const & a) {
 
 // Select between two operands. Corresponds to this pseudocode:
 // for (int i = 0; i < 2; i++) result[i] = s[i] ? a[i] : b[i];
-static inline Vec8d select (Vec8db const & s, Vec8d const & a, Vec8d const & b) {
+static inline Vec8d select (Vec8db const s, Vec8d const a, Vec8d const b) {
     return Vec8d(select(s.get_low(), a.get_low(), b.get_low()), select(s.get_high(), a.get_high(), b.get_high()));
 }
 
 // Conditional add: For all vector elements i: result[i] = f[i] ? (a[i] + b[i]) : a[i]
-static inline Vec8d if_add (Vec8db const & f, Vec8d const & a, Vec8d const & b) {
+static inline Vec8d if_add (Vec8db const f, Vec8d const a, Vec8d const b) {
     return Vec8d(if_add(f.get_low(), a.get_low(), b.get_low()), if_add(f.get_high(), a.get_high(), b.get_high()));
 }
 
-// Conditional multiply: For all vector elements i: result[i] = f[i] ? (a[i] * b[i]) : a[i]
-static inline Vec8d if_mul (Vec8db const & f, Vec8d const & a, Vec8d const & b) {
+// Conditional subtract
+static inline Vec8d if_sub (Vec8db const f, Vec8d const a, Vec8d const b) {
+    return Vec8d(if_sub(f.get_low(), a.get_low(), b.get_low()), if_sub(f.get_high(), a.get_high(), b.get_high()));
+}
+
+// Conditional multiply
+static inline Vec8d if_mul (Vec8db const f, Vec8d const a, Vec8d const b) {
     return Vec8d(if_mul(f.get_low(), a.get_low(), b.get_low()), if_mul(f.get_high(), a.get_high(), b.get_high()));
 }
 
+// Conditional divide
+static inline Vec8d if_div (Vec8db const f, Vec8d const a, Vec8d const b) {
+    return Vec8d(if_div(f.get_low(), a.get_low(), b.get_low()), if_div(f.get_high(), a.get_high(), b.get_high()));
+}
 
 // General arithmetic functions, etc.
 
 // Horizontal add: Calculates the sum of all vector elements.
-static inline double horizontal_add (Vec8d const & a) {
+static inline double horizontal_add (Vec8d const a) {
     return horizontal_add(a.get_low() + a.get_high());
 }
 
 // function max: a > b ? a : b
-static inline Vec8d max(Vec8d const & a, Vec8d const & b) {
+static inline Vec8d max(Vec8d const a, Vec8d const b) {
     return Vec8d(max(a.get_low(), b.get_low()), max(a.get_high(), b.get_high()));
 }
 
 // function min: a < b ? a : b
-static inline Vec8d min(Vec8d const & a, Vec8d const & b) {
+static inline Vec8d min(Vec8d const a, Vec8d const b) {
     return Vec8d(min(a.get_low(), b.get_low()), min(a.get_high(), b.get_high()));
 }
+// NAN-safe versions of maximum and minimum are in vector_convert.h
 
 // function abs: absolute value
 // Removes sign bit, even for -0.0f, -INF and -NAN
-static inline Vec8d abs(Vec8d const & a) {
+static inline Vec8d abs(Vec8d const a) {
     return Vec8d(abs(a.get_low()), abs(a.get_high()));
 }
 
 // function sqrt: square root
-static inline Vec8d sqrt(Vec8d const & a) {
+static inline Vec8d sqrt(Vec8d const a) {
     return Vec8d(sqrt(a.get_low()), sqrt(a.get_high()));
 }
 
 // function square: a * a
-static inline Vec8d square(Vec8d const & a) {
+static inline Vec8d square(Vec8d const a) {
     return a * a;
 }
 
 // pow(Vec8d, int):
-template <typename TT> static Vec8d pow(Vec8d const & a, TT const & n);
+template <typename TT> static Vec8d pow(Vec8d const a, TT const n);
 
 // Raise floating point numbers to integer power n
 template <>
-inline Vec8d pow<int>(Vec8d const & x0, int const & n) {
+inline Vec8d pow<int>(Vec8d const x0, int const n) {
     return pow_template_i<Vec8d>(x0, n);
 }
 
 // allow conversion from unsigned int
 template <>
-inline Vec8d pow<uint32_t>(Vec8d const & x0, uint32_t const & n) {
+inline Vec8d pow<uint32_t>(Vec8d const x0, uint32_t const n) {
     return pow_template_i<Vec8d>(x0, (int)n);
 }
 
 
 // Raise floating point numbers to integer power n, where n is a compile-time constant
 template <int n>
-static inline Vec8d pow_n(Vec8d const & a) {
+static inline Vec8d pow_n(Vec8d const a) {
     if (n < 0)    return Vec8d(1.0) / pow_n<-n>(a);
     if (n == 0)   return Vec8d(1.0);
     if (n >= 256) return pow(a, n);
@@ -1321,135 +1399,122 @@ static inline Vec8d pow_n(Vec8d const & a) {
 }
 
 template <int n>
-static inline Vec8d pow(Vec8d const & a, Const_int_t<n>) {
+static inline Vec8d pow(Vec8d const a, Const_int_t<n>) {
     return pow_n<n>(a);
 }
 
 
 // function round: round to nearest integer (even). (result as double vector)
-static inline Vec8d round(Vec8d const & a) {
+static inline Vec8d round(Vec8d const a) {
     return Vec8d(round(a.get_low()), round(a.get_high()));
 }
 
 // function truncate: round towards zero. (result as double vector)
-static inline Vec8d truncate(Vec8d const & a) {
+static inline Vec8d truncate(Vec8d const a) {
     return Vec8d(truncate(a.get_low()), truncate(a.get_high()));
 }
 
 // function floor: round towards minus infinity. (result as double vector)
-static inline Vec8d floor(Vec8d const & a) {
+static inline Vec8d floor(Vec8d const a) {
     return Vec8d(floor(a.get_low()), floor(a.get_high()));
 }
 
 // function ceil: round towards plus infinity. (result as double vector)
-static inline Vec8d ceil(Vec8d const & a) {
+static inline Vec8d ceil(Vec8d const a) {
     return Vec8d(ceil(a.get_low()), ceil(a.get_high()));
 }
 
-// function round_to_int: round to nearest integer (even). (result as integer vector)
-static inline Vec8i round_to_int(Vec8d const & a) {
+// function round_to_int32: round to nearest integer (even). (result as integer vector)
+static inline Vec8i round_to_int32(Vec8d const a) {
     // Note: assume MXCSR control register is set to rounding
-    return Vec8i(round_to_int(a.get_low()), round_to_int(a.get_high()));
+    return Vec8i(round_to_int32(a.get_low()), round_to_int32(a.get_high()));
 }
+//static inline Vec8i round_to_int(Vec8d const a) {return round_to_int32(a);} // deprecated
 
-// function truncate_to_int: round towards zero. (result as integer vector)
-static inline Vec8i truncate_to_int(Vec8d const & a) {
-    return Vec8i(truncate_to_int(a.get_low()), truncate_to_int(a.get_high()));
+// function truncate_to_int32: round towards zero. (result as integer vector)
+static inline Vec8i truncate_to_int32(Vec8d const a) {
+    return Vec8i(truncate_to_int32(a.get_low()), truncate_to_int32(a.get_high()));
 }
+//static inline Vec8i truncate_to_int(Vec8d const a) {return truncate_to_int32(a);} // deprecated
 
-// function truncate_to_int64: round towards zero. (inefficient)
-static inline Vec8q truncate_to_int64(Vec8d const & a) {
-    return Vec8q(truncate_to_int64(a.get_low()), truncate_to_int64(a.get_high()));
+// function truncatei: round towards zero. (inefficient)
+static inline Vec8q truncatei(Vec8d const a) {
+    return Vec8q(truncatei(a.get_low()), truncatei(a.get_high()));
 }
+//static inline Vec8q truncate_to_int64(Vec8d const a) {return truncatei(a);} // deprecated
 
-// function truncate_to_int64_limited: round towards zero.
-// result as 64-bit integer vector, but with limited range
-static inline Vec8q truncate_to_int64_limited(Vec8d const & a) {
-    // Note: assume MXCSR control register is set to rounding
-    return Vec8q(truncate_to_int64_limited(a.get_low()), truncate_to_int64_limited(a.get_high()));
-} 
-
-// function round_to_int64: round to nearest or even. (inefficient)
-static inline Vec8q round_to_int64(Vec8d const & a) {
-    return Vec8q(round_to_int64(a.get_low()), round_to_int64(a.get_high()));
-}
-
-// function round_to_int64_limited: round to nearest integer (even)
-// result as 64-bit integer vector, but with limited range
-static inline Vec8q round_to_int64_limited(Vec8d const & a) {
-    // Note: assume MXCSR control register is set to rounding
-    return Vec8q(round_to_int64_limited(a.get_low()), round_to_int64_limited(a.get_high()));
+// function roundi: round to nearest or even. (inefficient)
+static inline Vec8q roundi(Vec8d const a) {
+    return Vec8q(roundi(a.get_low()), roundi(a.get_high()));
 }
+//static inline Vec8q round_to_int64(Vec8d const a) {return roundi(a);} // deprecated
 
 // function to_double: convert integer vector elements to double vector (inefficient)
-static inline Vec8d to_double(Vec8q const & a) {
+static inline Vec8d to_double(Vec8q const a) {
     return Vec8d(to_double(a.get_low()), to_double(a.get_high()));
 }
 
-// function to_double_limited: convert integer vector elements to double vector
-// limited to abs(x) < 2^31
-static inline Vec8d to_double_limited(Vec8q const & a) {
-    return Vec8d(to_double_limited(a.get_low()), to_double_limited(a.get_high()));
+// function to_double: convert unsigned integer vector elements to double vector (inefficient)
+static inline Vec8d to_double(Vec8uq const a) {
+    return Vec8d(to_double(a.get_low()), to_double(a.get_high()));
 }
 
 // function to_double: convert integer vector to double vector
-static inline Vec8d to_double(Vec8i const & a) {
+static inline Vec8d to_double(Vec8i const a) {
     return Vec8d(to_double(a.get_low()), to_double(a.get_high()));
 }
 
 // function compress: convert two Vec8d to one Vec16f
-static inline Vec16f compress (Vec8d const & low, Vec8d const & high) {
+static inline Vec16f compress (Vec8d const low, Vec8d const high) {
     return Vec16f(compress(low.get_low(), low.get_high()), compress(high.get_low(), high.get_high()));
 }
 
 // Function extend_low : convert Vec16f vector elements 0 - 3 to Vec8d
-static inline Vec8d extend_low(Vec16f const & a) {
+static inline Vec8d extend_low(Vec16f const a) {
     return Vec8d(extend_low(a.get_low()), extend_high(a.get_low()));
 }
 
 // Function extend_high : convert Vec16f vector elements 4 - 7 to Vec8d
-static inline Vec8d extend_high (Vec16f const & a) {
+static inline Vec8d extend_high (Vec16f const a) {
     return Vec8d(extend_low(a.get_high()), extend_high(a.get_high()));
 }
 
-
 // Fused multiply and add functions
 
 // Multiply and add
-static inline Vec8d mul_add(Vec8d const & a, Vec8d const & b, Vec8d const & c) {
+static inline Vec8d mul_add(Vec8d const a, Vec8d const b, Vec8d const c) {
     return Vec8d(mul_add(a.get_low(), b.get_low(), c.get_low()), mul_add(a.get_high(), b.get_high(), c.get_high()));
 }
 
 // Multiply and subtract
-static inline Vec8d mul_sub(Vec8d const & a, Vec8d const & b, Vec8d const & c) {
+static inline Vec8d mul_sub(Vec8d const a, Vec8d const b, Vec8d const c) {
     return Vec8d(mul_sub(a.get_low(), b.get_low(), c.get_low()), mul_sub(a.get_high(), b.get_high(), c.get_high()));
 }
 
 // Multiply and inverse subtract
-static inline Vec8d nmul_add(Vec8d const & a, Vec8d const & b, Vec8d const & c) {
+static inline Vec8d nmul_add(Vec8d const a, Vec8d const b, Vec8d const c) {
     return Vec8d(nmul_add(a.get_low(), b.get_low(), c.get_low()), nmul_add(a.get_high(), b.get_high(), c.get_high()));
 }
 
-// Multiply and subtract with extra precision on the intermediate calculations, 
+// Multiply and subtract with extra precision on the intermediate calculations,
 // even if FMA instructions not supported, using Veltkamp-Dekker split
-static inline Vec8d mul_sub_x(Vec8d const & a, Vec8d const & b, Vec8d const & c) {
+static inline Vec8d mul_sub_x(Vec8d const a, Vec8d const b, Vec8d const c) {
     return Vec8d(mul_sub_x(a.get_low(), b.get_low(), c.get_low()), mul_sub_x(a.get_high(), b.get_high(), c.get_high()));
 }
 
-
 // Math functions using fast bit manipulation
 
 // Extract the exponent as an integer
 // exponent(a) = floor(log2(abs(a)));
 // exponent(1.0) = 0, exponent(0.0) = -1023, exponent(INF) = +1024, exponent(NAN) = +1024
-static inline Vec8q exponent(Vec8d const & a) {
+static inline Vec8q exponent(Vec8d const a) {
     return Vec8q(exponent(a.get_low()), exponent(a.get_high()));
 }
 
 // Extract the fraction part of a floating point number
 // a = 2^exponent(a) * fraction(a), except for a = 0
-// fraction(1.0) = 1.0, fraction(5.0) = 1.25 
-static inline Vec8d fraction(Vec8d const & a) {
+// fraction(1.0) = 1.0, fraction(5.0) = 1.25
+static inline Vec8d fraction(Vec8d const a) {
     return Vec8d(fraction(a.get_low()), fraction(a.get_high()));
 }
 
@@ -1458,10 +1523,10 @@ static inline Vec8d fraction(Vec8d const & a) {
 // n >=  1024 gives +INF
 // n <= -1023 gives 0.0
 // This function will never produce denormals, and never raise exceptions
-static inline Vec8d exp2(Vec8q const & n) {
+static inline Vec8d exp2(Vec8q const n) {
     return Vec8d(exp2(n.get_low()), exp2(n.get_high()));
 }
-//static Vec8d exp2(Vec8d const & x); // defined in vectormath_exp.h
+//static Vec8d exp2(Vec8d const x); // defined in vectormath_exp.h
 
 
 // Categorization functions
@@ -1469,43 +1534,43 @@ static inline Vec8d exp2(Vec8q const & n) {
 // Function sign_bit: gives true for elements that have the sign bit set
 // even for -0.0, -INF and -NAN
 // Note that sign_bit(Vec8d(-0.0)) gives true, while Vec8d(-0.0) < Vec8d(0.0) gives false
-static inline Vec8db sign_bit(Vec8d const & a) {
+static inline Vec8db sign_bit(Vec8d const a) {
     return Vec8db(sign_bit(a.get_low()), sign_bit(a.get_high()));
 }
 
 // Function sign_combine: changes the sign of a when b has the sign bit set
 // same as select(sign_bit(b), -a, a)
-static inline Vec8d sign_combine(Vec8d const & a, Vec8d const & b) {
+static inline Vec8d sign_combine(Vec8d const a, Vec8d const b) {
     return Vec8d(sign_combine(a.get_low(), b.get_low()), sign_combine(a.get_high(), b.get_high()));
 }
 
-// Function is_finite: gives true for elements that are normal, denormal or zero, 
+// Function is_finite: gives true for elements that are normal, denormal or zero,
 // false for INF and NAN
-static inline Vec8db is_finite(Vec8d const & a) {
+static inline Vec8db is_finite(Vec8d const a) {
     return Vec8db(is_finite(a.get_low()), is_finite(a.get_high()));
 }
 
 // Function is_inf: gives true for elements that are +INF or -INF
 // false for finite numbers and NAN
-static inline Vec8db is_inf(Vec8d const & a) {
+static inline Vec8db is_inf(Vec8d const a) {
     return Vec8db(is_inf(a.get_low()), is_inf(a.get_high()));
 }
 
 // Function is_nan: gives true for elements that are +NAN or -NAN
 // false for finite numbers and +/-INF
-static inline Vec8db is_nan(Vec8d const & a) {
+static inline Vec8db is_nan(Vec8d const a) {
     return Vec8db(is_nan(a.get_low()), is_nan(a.get_high()));
 }
 
 // Function is_subnormal: gives true for elements that are denormal (subnormal)
 // false for finite numbers, zero, NAN and INF
-static inline Vec8db is_subnormal(Vec8d const & a) {
+static inline Vec8db is_subnormal(Vec8d const a) {
     return Vec8db(is_subnormal(a.get_low()), is_subnormal(a.get_high()));
 }
 
 // Function is_zero_or_subnormal: gives true for elements that are zero or subnormal (denormal)
 // false for finite numbers, NAN and INF
-static inline Vec8db is_zero_or_subnormal(Vec8d const & a) {
+static inline Vec8db is_zero_or_subnormal(Vec8d const a) {
     return Vec8db(is_zero_or_subnormal(a.get_low()), is_zero_or_subnormal(a.get_high()));
 }
 
@@ -1516,7 +1581,7 @@ static inline Vec8d infinite8d() {
 }
 
 // Function nan8d: returns a vector where all elements are +NAN (quiet NAN)
-static inline Vec8d nan8d(int n = 0x10) {
+static inline Vec8d nan8d(uint32_t n = 0x10) {
     Vec4d nan = nan4d(n);
     return Vec8d(nan, nan);
 }
@@ -1524,7 +1589,7 @@ static inline Vec8d nan8d(int n = 0x10) {
 // change signs on vectors Vec8d
 // Each index i0 - i3 is 1 for changing sign on the corresponding element, 0 for no change
 template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7>
-static inline Vec8d change_sign(Vec8d const & a) {
+inline Vec8d change_sign(Vec8d const a) {
     return Vec8d(change_sign<i0,i1,i2,i3>(a.get_low()), change_sign<i4,i5,i6,i7>(a.get_high()));
 }
 
@@ -1535,42 +1600,56 @@ static inline Vec8d change_sign(Vec8d const & a) {
 *
 *****************************************************************************/
 
-static inline Vec512ie reinterpret_i (Vec512ie const & x) {
+static inline Vec512b reinterpret_i (Vec512b const x) {
     return x;
 }
 
-static inline Vec512ie reinterpret_i (Vec16f  const & x) {
-    return Vec512ie(reinterpret_i(x.get_low()), reinterpret_i(x.get_high()));
+static inline Vec512b reinterpret_i (Vec16f  const x) {
+    return Vec512b(reinterpret_i(x.get_low()), reinterpret_i(x.get_high()));
 }
 
-static inline Vec512ie reinterpret_i (Vec8d const & x) {
-    return Vec512ie(reinterpret_i(x.get_low()), reinterpret_i(x.get_high()));
+static inline Vec512b reinterpret_i (Vec8d const x) {
+    return Vec512b(reinterpret_i(x.get_low()), reinterpret_i(x.get_high()));
 }
 
-static inline Vec16f  reinterpret_f (Vec512ie const & x) {
+static inline Vec16f  reinterpret_f (Vec512b const x) {
     return Vec16f(Vec8f(reinterpret_f(x.get_low())), Vec8f(reinterpret_f(x.get_high())));
 }
 
-static inline Vec16f  reinterpret_f (Vec16f  const & x) {
+static inline Vec16f  reinterpret_f (Vec16f  const x) {
     return x;
 }
 
-static inline Vec16f  reinterpret_f (Vec8d const & x) {
+static inline Vec16f  reinterpret_f (Vec8d const x) {
     return Vec16f(Vec8f(reinterpret_f(x.get_low())), Vec8f(reinterpret_f(x.get_high())));
 }
 
-static inline Vec8d reinterpret_d (Vec512ie const & x) {
+static inline Vec8d reinterpret_d (Vec512b const x) {
     return Vec8d(Vec4d(reinterpret_d(x.get_low())), Vec4d(reinterpret_d(x.get_high())));
 }
 
-static inline Vec8d reinterpret_d (Vec16f  const & x) {
+static inline Vec8d reinterpret_d (Vec16f  const x) {
     return Vec8d(Vec4d(reinterpret_d(x.get_low())), Vec4d(reinterpret_d(x.get_high())));
 }
 
-static inline Vec8d reinterpret_d (Vec8d const & x) {
+static inline Vec8d reinterpret_d (Vec8d const x) {
     return x;
 }
 
+// extend vectors to double size by adding zeroes
+static inline Vec16f extend_z(Vec8f a) {
+    return Vec16f(a, Vec8f(0));
+}
+static inline Vec8d extend_z(Vec4d a) {
+    return Vec8d(a, Vec4d(0));
+} 
+static inline Vec16fb extend_z(Vec8fb a) {
+    return Vec16fb(a, Vec8fb(false));
+}
+static inline Vec8db extend_z(Vec4db a) {
+    return Vec8db(a, Vec4db(false));
+} 
+
 
 /*****************************************************************************
 *
@@ -1579,37 +1658,24 @@ static inline Vec8d reinterpret_d (Vec8d const & x) {
 ******************************************************************************
 *
 * These permute functions can reorder the elements of a vector and optionally
-* set some elements to zero. 
-*
-* The indexes are inserted as template parameters in <>. These indexes must be
-* constants. Each template parameter is an index to the element you want to select.
-* An index of -1 will generate zero. An index of -256 means don't care.
+* set some elements to zero. See Vectori128.h for description
 *
-* Example:
-* Vec8d a(10,11,12,13,14,15,16,17);      // a is (10,11,12,13,14,15,16,17)
-* Vec8d b;
-* b = permute8d<0,2,7,7,-1,-1,1,1>(a);   // b is (10,12,17,17, 0, 0,11,11)
-*
-* A lot of the code here is metaprogramming aiming to find the instructions
-* that best fit the template parameters and instruction set. The metacode
-* will be reduced out to leave only a few vector instructions in release
-* mode with optimization on.
 *****************************************************************************/
 
 // Permute vector of 8 double
 // Index -1 gives 0, index -256 means don't care.
 template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7>
-static inline Vec8d permute8d(Vec8d const & a) {
-    return Vec8d(blend4d<i0,i1,i2,i3> (a.get_low(), a.get_high()),
-                 blend4d<i4,i5,i6,i7> (a.get_low(), a.get_high()));
+static inline Vec8d permute8(Vec8d const a) {
+    return Vec8d(blend4<i0,i1,i2,i3> (a.get_low(), a.get_high()),
+                 blend4<i4,i5,i6,i7> (a.get_low(), a.get_high()));
 }
 
 // Permute vector of 16 float
 // Index -1 gives 0, index -256 means don't care.
 template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7, int i8, int i9, int i10, int i11, int i12, int i13, int i14, int i15>
-static inline Vec16f permute16f(Vec16f const & a) {
-    return Vec16f(blend8f<i0,i1,i2 ,i3 ,i4 ,i5 ,i6 ,i7 > (a.get_low(), a.get_high()),
-                  blend8f<i8,i9,i10,i11,i12,i13,i14,i15> (a.get_low(), a.get_high()));
+static inline Vec16f permute16(Vec16f const a) {
+    return Vec16f(blend8<i0,i1,i2 ,i3 ,i4 ,i5 ,i6 ,i7 > (a.get_low(), a.get_high()),
+                  blend8<i8,i9,i10,i11,i12,i13,i14,i15> (a.get_low(), a.get_high()));
 }
 
 
@@ -1617,229 +1683,22 @@ static inline Vec16f permute16f(Vec16f const & a) {
 *
 *          Vector blend functions
 *
-******************************************************************************
-*
-* These blend functions can mix elements from two different vectors and
-* optionally set some elements to zero. 
-*
-* The indexes are inserted as template parameters in <>. These indexes must be
-* constants. Each template parameter is an index to the element you want to 
-* select, where higher indexes indicate an element from the second source
-* vector. For example, if each vector has 8 elements, then indexes 0 - 7
-* will select an element from the first vector and indexes 8 - 15 will select 
-* an element from the second vector. A negative index will generate zero.
-*
-* Example:
-* Vec8d a(100,101,102,103,104,105,106,107); // a is (100, 101, 102, 103, 104, 105, 106, 107)
-* Vec8d b(200,201,202,203,204,205,206,207); // b is (200, 201, 202, 203, 204, 205, 206, 207)
-* Vec8d c;
-* c = blend8d<1,0,9,8,7,-1,15,15> (a,b);    // c is (101, 100, 201, 200, 107,   0, 207, 207)
-*
-* A lot of the code here is metaprogramming aiming to find the instructions
-* that best fit the template parameters and instruction set. The metacode
-* will be reduced out to leave only a few vector instructions in release
-* mode with optimization on.
 *****************************************************************************/
 
-// helper function used below
-template <int n>
-static inline Vec4d select4(Vec8d const & a, Vec8d const & b) {
-    switch (n) {
-    case 0:
-        return a.get_low();
-    case 1:
-        return a.get_high();
-    case 2:
-        return b.get_low();
-    case 3:
-        return b.get_high();
-    }
-    return Vec4d(0.);
-}
-
 // blend vectors Vec8d
-template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7> 
-static inline Vec8d blend8d(Vec8d const & a, Vec8d const & b) {  
-    const int j0 = i0 >= 0 ? i0/4 : i0;
-    const int j1 = i1 >= 0 ? i1/4 : i1;
-    const int j2 = i2 >= 0 ? i2/4 : i2;
-    const int j3 = i3 >= 0 ? i3/4 : i3;
-    const int j4 = i4 >= 0 ? i4/4 : i4;
-    const int j5 = i5 >= 0 ? i5/4 : i5;
-    const int j6 = i6 >= 0 ? i6/4 : i6;
-    const int j7 = i7 >= 0 ? i7/4 : i7;
-    Vec4d x0, x1;
-
-    const int r0 = j0 >= 0 ? j0 : j1 >= 0 ? j1 : j2 >= 0 ? j2 : j3;
-    const int r1 = j4 >= 0 ? j4 : j5 >= 0 ? j5 : j6 >= 0 ? j6 : j7;
-    const int s0 = (j1 >= 0 && j1 != r0) ? j1 : (j2 >= 0 && j2 != r0) ? j2 : j3;
-    const int s1 = (j5 >= 0 && j5 != r1) ? j5 : (j6 >= 0 && j6 != r1) ? j6 : j7;
-
-    // Combine all the indexes into a single bitfield, with 4 bits for each
-    const int m1 = (i0&0xF) | (i1&0xF)<<4 | (i2&0xF)<<8 | (i3&0xF)<<12 | (i4&0xF)<<16 | (i5&0xF)<<20 | (i6&0xF)<<24 | (i7&0xF)<<28;
-
-    // Mask to zero out negative indexes
-    const int mz = (i0<0?0:0xF) | (i1<0?0:0xF)<<4 | (i2<0?0:0xF)<<8 | (i3<0?0:0xF)<<12 | (i4<0?0:0xF)<<16 | (i5<0?0:0xF)<<20 | (i6<0?0:0xF)<<24 | (i7<0?0:0xF)<<28;
-
-    if (r0 < 0) {
-        x0 = Vec4d(0.);
-    }
-    else if (((m1 ^ r0*0x4444) & 0xCCCC & mz) == 0) { 
-        // i0 - i3 all from same source
-        x0 = permute4d<i0 & -13, i1 & -13, i2 & -13, i3 & -13> (select4<r0> (a,b));
-    }
-    else if ((j2 < 0 || j2 == r0 || j2 == s0) && (j3 < 0 || j3 == r0 || j3 == s0)) { 
-        // i0 - i3 all from two sources
-        const int k0 =  i0 >= 0 ? i0 & 3 : i0;
-        const int k1 = (i1 >= 0 ? i1 & 3 : i1) | (j1 == s0 ? 4 : 0);
-        const int k2 = (i2 >= 0 ? i2 & 3 : i2) | (j2 == s0 ? 4 : 0);
-        const int k3 = (i3 >= 0 ? i3 & 3 : i3) | (j3 == s0 ? 4 : 0);
-        x0 = blend4d<k0,k1,k2,k3> (select4<r0>(a,b), select4<s0>(a,b));
-    }
-    else {
-        // i0 - i3 from three or four different sources
-        x0 = blend4d<0,1,6,7> (
-             blend4d<i0 & -13, (i1 & -13) | 4, -0x100, -0x100> (select4<j0>(a,b), select4<j1>(a,b)),
-             blend4d<-0x100, -0x100, i2 & -13, (i3 & -13) | 4> (select4<j2>(a,b), select4<j3>(a,b)));
-    }
-
-    if (r1 < 0) {
-        x1 = Vec4d(0.);
-    }
-    else if (((m1 ^ uint32_t(r1)*0x44440000u) & 0xCCCC0000 & mz) == 0) { 
-        // i4 - i7 all from same source
-        x1 = permute4d<i4 & -13, i5 & -13, i6 & -13, i7 & -13> (select4<r1> (a,b));
-    }
-    else if ((j6 < 0 || j6 == r1 || j6 == s1) && (j7 < 0 || j7 == r1 || j7 == s1)) { 
-        // i4 - i7 all from two sources
-        const int k4 =  i4 >= 0 ? i4 & 3 : i4;
-        const int k5 = (i5 >= 0 ? i5 & 3 : i5) | (j5 == s1 ? 4 : 0);
-        const int k6 = (i6 >= 0 ? i6 & 3 : i6) | (j6 == s1 ? 4 : 0);
-        const int k7 = (i7 >= 0 ? i7 & 3 : i7) | (j7 == s1 ? 4 : 0);
-        x1 = blend4d<k4,k5,k6,k7> (select4<r1>(a,b), select4<s1>(a,b));
-    }
-    else {
-        // i4 - i7 from three or four different sources
-        x1 = blend4d<0,1,6,7> (
-             blend4d<i4 & -13, (i5 & -13) | 4, -0x100, -0x100> (select4<j4>(a,b), select4<j5>(a,b)),
-             blend4d<-0x100, -0x100, i6 & -13, (i7 & -13) | 4> (select4<j6>(a,b), select4<j7>(a,b)));
-    }
-
-    return Vec8d(x0,x1);
+template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7>
+static inline Vec8d blend8(Vec8d const a, Vec8d const b) {
+    Vec4d x0 = blend_half<Vec8d, i0, i1, i2, i3>(a, b);
+    Vec4d x1 = blend_half<Vec8d, i4, i5, i6, i7>(a, b);
+    return Vec8d(x0, x1);
 }
 
-// helper function used below
-template <int n>
-static inline Vec8f select4(Vec16f const & a, Vec16f const & b) {
-    switch (n) {
-    case 0:
-        return a.get_low();
-    case 1:
-        return a.get_high();
-    case 2:
-        return b.get_low();
-    case 3:
-        return b.get_high();
-    }
-    return Vec8f(0.f);
-}
-
-template <int i0,  int i1,  int i2,  int i3,  int i4,  int i5,  int i6,  int i7, 
-          int i8,  int i9,  int i10, int i11, int i12, int i13, int i14, int i15 > 
-static inline Vec16f blend16f(Vec16f const & a, Vec16f const & b) {
-
-    const int j0  = i0  >= 0 ? i0 /8 : i0;
-    const int j1  = i1  >= 0 ? i1 /8 : i1;
-    const int j2  = i2  >= 0 ? i2 /8 : i2;
-    const int j3  = i3  >= 0 ? i3 /8 : i3;
-    const int j4  = i4  >= 0 ? i4 /8 : i4;
-    const int j5  = i5  >= 0 ? i5 /8 : i5;
-    const int j6  = i6  >= 0 ? i6 /8 : i6;
-    const int j7  = i7  >= 0 ? i7 /8 : i7;
-    const int j8  = i8  >= 0 ? i8 /8 : i8;
-    const int j9  = i9  >= 0 ? i9 /8 : i9;
-    const int j10 = i10 >= 0 ? i10/8 : i10;
-    const int j11 = i11 >= 0 ? i11/8 : i11;
-    const int j12 = i12 >= 0 ? i12/8 : i12;
-    const int j13 = i13 >= 0 ? i13/8 : i13;
-    const int j14 = i14 >= 0 ? i14/8 : i14;
-    const int j15 = i15 >= 0 ? i15/8 : i15;
-
-    Vec8f x0, x1;
-
-    const int r0 = j0 >= 0 ? j0 : j1 >= 0 ? j1 : j2  >= 0 ? j2  : j3  >= 0 ? j3  : j4  >= 0 ? j4  : j5  >= 0 ? j5  : j6  >= 0 ? j6  : j7;
-    const int r1 = j8 >= 0 ? j8 : j9 >= 0 ? j9 : j10 >= 0 ? j10 : j11 >= 0 ? j11 : j12 >= 0 ? j12 : j13 >= 0 ? j13 : j14 >= 0 ? j14 : j15;
-    const int s0 = (j1 >= 0 && j1 != r0) ? j1 : (j2 >= 0 && j2 != r0) ? j2  : (j3 >= 0 && j3 != r0) ? j3 : (j4 >= 0 && j4 != r0) ? j4 : (j5 >= 0 && j5 != r0) ? j5 : (j6 >= 0 && j6 != r0) ? j6 : j7;
-    const int s1 = (j9 >= 0 && j9 != r1) ? j9 : (j10>= 0 && j10!= r1) ? j10 : (j11>= 0 && j11!= r1) ? j11: (j12>= 0 && j12!= r1) ? j12: (j13>= 0 && j13!= r1) ? j13: (j14>= 0 && j14!= r1) ? j14: j15;
-
-    if (r0 < 0) {
-        x0 = Vec8f(0.f);
-    }
-    else if (r0 == s0) {
-        // i0 - i7 all from same source
-        x0 = permute8f<i0&-25, i1&-25, i2&-25, i3&-25, i4&-25, i5&-25, i6&-25, i7&-25> (select4<r0> (a,b));
-    }
-    else if ((j2<0||j2==r0||j2==s0) && (j3<0||j3==r0||j3==s0) && (j4<0||j4==r0||j4==s0) && (j5<0||j5==r0||j5==s0) && (j6<0||j6==r0||j6==s0) && (j7<0||j7==r0||j7==s0)) {
-        // i0 - i7 all from two sources
-        const int k0 =  i0 >= 0 ? (i0 & 7) : i0;
-        const int k1 = (i1 >= 0 ? (i1 & 7) : i1) | (j1 == s0 ? 8 : 0);
-        const int k2 = (i2 >= 0 ? (i2 & 7) : i2) | (j2 == s0 ? 8 : 0);
-        const int k3 = (i3 >= 0 ? (i3 & 7) : i3) | (j3 == s0 ? 8 : 0);
-        const int k4 = (i4 >= 0 ? (i4 & 7) : i4) | (j4 == s0 ? 8 : 0);
-        const int k5 = (i5 >= 0 ? (i5 & 7) : i5) | (j5 == s0 ? 8 : 0);
-        const int k6 = (i6 >= 0 ? (i6 & 7) : i6) | (j6 == s0 ? 8 : 0);
-        const int k7 = (i7 >= 0 ? (i7 & 7) : i7) | (j7 == s0 ? 8 : 0);
-        x0 = blend8f<k0,k1,k2,k3,k4,k5,k6,k7> (select4<r0>(a,b), select4<s0>(a,b));
-    }
-    else {
-        // i0 - i7 from three or four different sources
-        const int n0 = j0 >= 0 ? j0 /2*8 + 0 : j0;
-        const int n1 = j1 >= 0 ? j1 /2*8 + 1 : j1;
-        const int n2 = j2 >= 0 ? j2 /2*8 + 2 : j2;
-        const int n3 = j3 >= 0 ? j3 /2*8 + 3 : j3;
-        const int n4 = j4 >= 0 ? j4 /2*8 + 4 : j4;
-        const int n5 = j5 >= 0 ? j5 /2*8 + 5 : j5;
-        const int n6 = j6 >= 0 ? j6 /2*8 + 6 : j6;
-        const int n7 = j7 >= 0 ? j7 /2*8 + 7 : j7;
-        x0 = blend8f<n0, n1, n2, n3, n4, n5, n6, n7> (
-             blend8f< j0   & 2 ? -256 : i0 &15,  j1   & 2 ? -256 : i1 &15,  j2   & 2 ? -256 : i2 &15,  j3   & 2 ? -256 : i3 &15,  j4   & 2 ? -256 : i4 &15,  j5   & 2 ? -256 : i5 &15,  j6   & 2 ? -256 : i6 &15,  j7   & 2 ? -256 : i7 &15> (a.get_low(),a.get_high()),
-             blend8f<(j0^2)& 6 ? -256 : i0 &15, (j1^2)& 6 ? -256 : i1 &15, (j2^2)& 6 ? -256 : i2 &15, (j3^2)& 6 ? -256 : i3 &15, (j4^2)& 6 ? -256 : i4 &15, (j5^2)& 6 ? -256 : i5 &15, (j6^2)& 6 ? -256 : i6 &15, (j7^2)& 6 ? -256 : i7 &15> (b.get_low(),b.get_high()));
-    }
-
-    if (r1 < 0) {
-        x1 = Vec8f(0.f);
-    }
-    else if (r1 == s1) {
-        // i8 - i15 all from same source
-        x1 = permute8f<i8&-25, i9&-25, i10&-25, i11&-25, i12&-25, i13&-25, i14&-25, i15&-25> (select4<r1> (a,b));
-    }
-    else if ((j10<0||j10==r1||j10==s1) && (j11<0||j11==r1||j11==s1) && (j12<0||j12==r1||j12==s1) && (j13<0||j13==r1||j13==s1) && (j14<0||j14==r1||j14==s1) && (j15<0||j15==r1||j15==s1)) {
-        // i8 - i15 all from two sources
-        const int k8 =  i8 >= 0 ? (i8 & 7) : i8;
-        const int k9 = (i9 >= 0 ? (i9 & 7) : i9 ) | (j9 == s1 ? 8 : 0);
-        const int k10= (i10>= 0 ? (i10& 7) : i10) | (j10== s1 ? 8 : 0);
-        const int k11= (i11>= 0 ? (i11& 7) : i11) | (j11== s1 ? 8 : 0);
-        const int k12= (i12>= 0 ? (i12& 7) : i12) | (j12== s1 ? 8 : 0);
-        const int k13= (i13>= 0 ? (i13& 7) : i13) | (j13== s1 ? 8 : 0);
-        const int k14= (i14>= 0 ? (i14& 7) : i14) | (j14== s1 ? 8 : 0);
-        const int k15= (i15>= 0 ? (i15& 7) : i15) | (j15== s1 ? 8 : 0);
-        x1 = blend8f<k8,k9,k10,k11,k12,k13,k14,k15> (select4<r1>(a,b), select4<s1>(a,b));
-    }
-    else {
-        // i8 - i15 from three or four different sources
-        const int n8 = j8 >= 0 ? j8 /2*8 + 0 : j8 ;
-        const int n9 = j9 >= 0 ? j9 /2*8 + 1 : j9 ;
-        const int n10= j10>= 0 ? j10/2*8 + 2 : j10;
-        const int n11= j11>= 0 ? j11/2*8 + 3 : j11;
-        const int n12= j12>= 0 ? j12/2*8 + 4 : j12;
-        const int n13= j13>= 0 ? j13/2*8 + 5 : j13;
-        const int n14= j14>= 0 ? j14/2*8 + 6 : j14;
-        const int n15= j15>= 0 ? j15/2*8 + 7 : j15;
-        x1 = blend8f<n8, n9, n10, n11, n12, n13, n14, n15> (
-             blend8f< j8   & 2 ? -256 : i8 &15,  j9   & 2 ? -256 : i9 &15,  j10   & 2 ? -256 : i10 &15,  j11   & 2 ? -256 : i11 &15,  j12   & 2 ? -256 : i12 &15,  j13   & 2 ? -256 : i13 &15,  j14   & 2 ? -256 : i14 &15,  j15   & 2 ? -256 : i15 &15> (a.get_low(),a.get_high()),
-             blend8f<(j8^2)& 6 ? -256 : i8 &15, (j9^2)& 6 ? -256 : i9 &15, (j10^2)& 6 ? -256 : i10 &15, (j11^2)& 6 ? -256 : i11 &15, (j12^2)& 6 ? -256 : i12 &15, (j13^2)& 6 ? -256 : i13 &15, (j14^2)& 6 ? -256 : i14 &15, (j15^2)& 6 ? -256 : i15 &15> (b.get_low(),b.get_high()));
-    }
-    return Vec16f(x0,x1);
+template <int i0,  int i1,  int i2,  int i3,  int i4,  int i5,  int i6,  int i7,
+          int i8,  int i9,  int i10, int i11, int i12, int i13, int i14, int i15 >
+static inline Vec16f blend16(Vec16f const a, Vec16f const b) {
+    Vec8f x0 = blend_half<Vec16f, i0, i1, i2, i3, i4, i5, i6, i7>(a, b);
+    Vec8f x1 = blend_half<Vec16f, i8, i9, i10, i11, i12, i13, i14, i15>(a, b);
+    return Vec16f(x0, x1);
 }
 
 
@@ -1852,45 +1711,23 @@ static inline Vec16f blend16f(Vec16f const & a, Vec16f const & b) {
 * These functions use vector elements as indexes into a table.
 * The table is given as one or more vectors or as an array.
 *
-* This can be used for several purposes:
-*  - table lookup
-*  - permute or blend with variable indexes
-*  - blend from more than two sources
-*  - gather non-contiguous data
-*
-* An index out of range may produce any value - the actual value produced is
-* implementation dependent and may be different for different instruction
-* sets. An index out of range does not produce an error message or exception.
-*
-* Example:
-* Vec8d a(2,0,0,6,4,3,5,0);                 // index a is (  2,   0,   0,   6,   4,   3,   5,   0)
-* Vec8d b(100,101,102,103,104,105,106,107); // table b is (100, 101, 102, 103, 104, 105, 106, 107)
-* Vec8d c;
-* c = lookup8 (a,b);                        // c is       (102, 100, 100, 106, 104, 103, 105, 100)
-*
 *****************************************************************************/
 
-static inline Vec16f lookup16(Vec16i const & index, Vec16f const & table) {
+static inline Vec16f lookup16(Vec16i const index, Vec16f const table) {
     float tab[16];
     table.store(tab);
-    Vec8f t0 = lookup<16>(index.get_low(), tab);
-    Vec8f t1 = lookup<16>(index.get_high(), tab);
+    Vec8f t0 = reinterpret_f(lookup<16>(index.get_low(), tab));
+    Vec8f t1 = reinterpret_f(lookup<16>(index.get_high(), tab));
     return Vec16f(t0, t1);
 }
 
 template <int n>
-static inline Vec16f lookup(Vec16i const & index, float const * table) {
-    if (n <=  0) return 0;
-    if (n <=  8) {
-        Vec8f table1 = Vec8f().load(table);        
-        return Vec16f(       
-            lookup8 (index.get_low(),  table1),
-            lookup8 (index.get_high(), table1));
-    }
-    if (n <= 16) return lookup16(index, Vec16f().load(table));
+static inline Vec16f lookup(Vec16i const index, float const * table) {
+    if constexpr (n <=  0) return 0;
+    if constexpr (n <= 16) return lookup16(index, Vec16f().load(table));
     // n > 16. Limit index
     Vec16ui i1;
-    if ((n & (n-1)) == 0) {
+    if constexpr ((n & (n-1)) == 0) {
         // n is a power of 2, make index modulo n
         i1 = Vec16ui(index) & (n-1);
     }
@@ -1903,30 +1740,23 @@ static inline Vec16f lookup(Vec16i const & index, float const * table) {
         t[i1[8]],t[i1[9]],t[i1[10]],t[i1[11]],t[i1[12]],t[i1[13]],t[i1[14]],t[i1[15]]);
 }
 
-
-static inline Vec8d lookup8(Vec8q const & index, Vec8d const & table) {
+static inline Vec8d lookup8(Vec8q const index, Vec8d const table) {
     double tab[8];
     table.store(tab);
-    Vec4d t0 = lookup<8>(index.get_low(), tab);
-    Vec4d t1 = lookup<8>(index.get_high(), tab);
+    Vec4d t0 = reinterpret_d(lookup<8>(index.get_low(), tab));
+    Vec4d t1 = reinterpret_d(lookup<8>(index.get_high(), tab));
     return Vec8d(t0, t1);
-} 
+}
 
 template <int n>
-static inline Vec8d lookup(Vec8q const & index, double const * table) {
-    if (n <= 0) return 0;
-    if (n <= 4) {
-        Vec4d table1 = Vec4d().load(table);        
-        return Vec8d(       
-            lookup4 (index.get_low(),  table1),
-            lookup4 (index.get_high(), table1));
-    }
-    if (n <= 8) {
+static inline Vec8d lookup(Vec8q const index, double const * table) {
+    if constexpr (n <= 0) return 0;
+    if constexpr (n <= 8) {
         return lookup8(index, Vec8d().load(table));
     }
     // n > 8. Limit index
     Vec8uq i1;
-    if ((n & (n-1)) == 0) {
+    if constexpr ((n & (n-1)) == 0) {
         // n is a power of 2, make index modulo n
         i1 = Vec8uq(index) & (n-1);
     }
@@ -1943,58 +1773,32 @@ static inline Vec8d lookup(Vec8q const & index, double const * table) {
 *          Gather functions with fixed indexes
 *
 *****************************************************************************/
+
 // Load elements from array a with indices i0,i1,i2,i3,i4,i5,i6,i7,i8,i9,i10,i11,i12,i13,i14,i15
-template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7, 
+template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7,
 int i8, int i9, int i10, int i11, int i12, int i13, int i14, int i15>
 static inline Vec16f gather16f(void const * a) {
-    Static_error_check<(i0|i1|i2|i3|i4|i5|i6|i7|i8|i9|i10|i11|i12|i13|i14|i15)>=0> Negative_array_index;  // Error message if index is negative
-    // find smallest and biggest index, using only compile-time constant expressions
-    const int i01min   = i0  < i1  ? i0  : i1;
-    const int i23min   = i2  < i3  ? i2  : i3;
-    const int i45min   = i4  < i5  ? i4  : i5;
-    const int i67min   = i6  < i7  ? i6  : i7;
-    const int i89min   = i8  < i9  ? i8  : i9;
-    const int i1011min = i10 < i11 ? i10 : i11;
-    const int i1213min = i12 < i13 ? i12 : i13;
-    const int i1415min = i14 < i15 ? i14 : i15;
-    const int i0_3min   = i01min   < i23min    ? i01min   : i23min;
-    const int i4_7min   = i45min   < i67min    ? i45min   : i67min;
-    const int i8_11min  = i89min   < i1011min  ? i89min   : i1011min;
-    const int i12_15min = i1213min < i1415min  ? i1213min : i1415min;
-    const int i0_7min   = i0_3min  < i4_7min   ? i0_3min  : i4_7min;
-    const int i8_15min  = i8_11min < i12_15min ? i8_11min : i12_15min;
-    const int imin      = i0_7min  < i8_15min  ? i0_7min  : i8_15min;
-    const int i01max   = i0  > i1  ? i0  : i1;
-    const int i23max   = i2  > i3  ? i2  : i3;
-    const int i45max   = i4  > i5  ? i4  : i5;
-    const int i67max   = i6  > i7  ? i6  : i7;
-    const int i89max   = i8  > i9  ? i8  : i9;
-    const int i1011max = i10 > i11 ? i10 : i11;
-    const int i1213max = i12 > i13 ? i12 : i13;
-    const int i1415max = i14 > i15 ? i14 : i15;
-    const int i0_3max   = i01max   > i23max    ? i01max   : i23max;
-    const int i4_7max   = i45max   > i67max    ? i45max   : i67max;
-    const int i8_11max  = i89max   > i1011max  ? i89max   : i1011max;
-    const int i12_15max = i1213max > i1415max  ? i1213max : i1415max;
-    const int i0_7max   = i0_3max  > i4_7max   ? i0_3max  : i4_7max;
-    const int i8_15max  = i8_11max > i12_15max ? i8_11max : i12_15max;
-    const int imax      = i0_7max  > i8_15max  ? i0_7max  : i8_15max;
-    if (imax - imin <= 15) {
+    int constexpr indexs[16] = { i0, i1, i2, i3, i4, i5, i6, i7, i8, i9, i10, i11, i12, i13, i14, i15 };
+    constexpr int imin = min_index(indexs);
+    constexpr int imax = max_index(indexs);
+    static_assert(imin >= 0, "Negative index in gather function");
+
+    if constexpr (imax - imin <= 15) {
         // load one contiguous block and permute
-        if (imax > 15) {
+        if constexpr (imax > 15) {
             // make sure we don't read past the end of the array
             Vec16f b = Vec16f().load((float const *)a + imax-15);
-            return permute16f<i0-imax+15, i1-imax+15, i2-imax+15, i3-imax+15, i4-imax+15, i5-imax+15, i6-imax+15, i7-imax+15,
+            return permute16<i0-imax+15, i1-imax+15, i2-imax+15, i3-imax+15, i4-imax+15, i5-imax+15, i6-imax+15, i7-imax+15,
                 i8-imax+15, i9-imax+15, i10-imax+15, i11-imax+15, i12-imax+15, i13-imax+15, i14-imax+15, i15-imax+15> (b);
         }
         else {
             Vec16f b = Vec16f().load((float const *)a + imin);
-            return permute16f<i0-imin, i1-imin, i2-imin, i3-imin, i4-imin, i5-imin, i6-imin, i7-imin,
+            return permute16<i0-imin, i1-imin, i2-imin, i3-imin, i4-imin, i5-imin, i6-imin, i7-imin,
                 i8-imin, i9-imin, i10-imin, i11-imin, i12-imin, i13-imin, i14-imin, i15-imin> (b);
         }
     }
-    if ((i0<imin+16  || i0>imax-16)  && (i1<imin+16  || i1>imax-16)  && (i2<imin+16  || i2>imax-16)  && (i3<imin+16  || i3>imax-16)
-    &&  (i4<imin+16  || i4>imax-16)  && (i5<imin+16  || i5>imax-16)  && (i6<imin+16  || i6>imax-16)  && (i7<imin+16  || i7>imax-16)    
+    if constexpr ((i0<imin+16  || i0>imax-16)  && (i1<imin+16  || i1>imax-16)  && (i2<imin+16  || i2>imax-16)  && (i3<imin+16  || i3>imax-16)
+    &&  (i4<imin+16  || i4>imax-16)  && (i5<imin+16  || i5>imax-16)  && (i6<imin+16  || i6>imax-16)  && (i7<imin+16  || i7>imax-16)
     &&  (i8<imin+16  || i8>imax-16)  && (i9<imin+16  || i9>imax-16)  && (i10<imin+16 || i10>imax-16) && (i11<imin+16 || i11>imax-16)
     &&  (i12<imin+16 || i12>imax-16) && (i13<imin+16 || i13>imax-16) && (i14<imin+16 || i14>imax-16) && (i15<imin+16 || i15>imax-16) ) {
         // load two contiguous blocks and blend
@@ -2016,7 +1820,7 @@ static inline Vec16f gather16f(void const * a) {
         const int j13 = i13<imin+16 ? i13-imin : 31-imax+i13;
         const int j14 = i14<imin+16 ? i14-imin : 31-imax+i14;
         const int j15 = i15<imin+16 ? i15-imin : 31-imax+i15;
-        return blend16f<j0,j1,j2,j3,j4,j5,j6,j7,j8,j9,j10,j11,j12,j13,j14,j15>(b, c);
+        return blend16<j0,j1,j2,j3,j4,j5,j6,j7,j8,j9,j10,j11,j12,j13,j14,j15>(b, c);
     }
     // use lookup function
     return lookup<imax+1>(Vec16i(i0,i1,i2,i3,i4,i5,i6,i7,i8,i9,i10,i11,i12,i13,i14,i15), (const float *)a);
@@ -2025,35 +1829,24 @@ static inline Vec16f gather16f(void const * a) {
 
 template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7>
 static inline Vec8d gather8d(void const * a) {
-    Static_error_check<(i0|i1|i2|i3|i4|i5|i6|i7)>=0> Negative_array_index;  // Error message if index is negative
-
-    const int i01min = i0 < i1 ? i0 : i1;
-    const int i23min = i2 < i3 ? i2 : i3;
-    const int i45min = i4 < i5 ? i4 : i5;
-    const int i67min = i6 < i7 ? i6 : i7;
-    const int i0123min = i01min < i23min ? i01min : i23min;
-    const int i4567min = i45min < i67min ? i45min : i67min;
-    const int imin = i0123min < i4567min ? i0123min : i4567min;
-    const int i01max = i0 > i1 ? i0 : i1;
-    const int i23max = i2 > i3 ? i2 : i3;
-    const int i45max = i4 > i5 ? i4 : i5;
-    const int i67max = i6 > i7 ? i6 : i7;
-    const int i0123max = i01max > i23max ? i01max : i23max;
-    const int i4567max = i45max > i67max ? i45max : i67max;
-    const int imax = i0123max > i4567max ? i0123max : i4567max;
-    if (imax - imin <= 7) {
+    int constexpr indexs[8] = { i0, i1, i2, i3, i4, i5, i6, i7 }; // indexes as array
+    constexpr int imin = min_index(indexs);
+    constexpr int imax = max_index(indexs);
+    static_assert(imin >= 0, "Negative index in gather function");
+
+    if constexpr (imax - imin <= 7) {
         // load one contiguous block and permute
-        if (imax > 7) {
+        if constexpr (imax > 7) {
             // make sure we don't read past the end of the array
             Vec8d b = Vec8d().load((double const *)a + imax-7);
-            return permute8d<i0-imax+7, i1-imax+7, i2-imax+7, i3-imax+7, i4-imax+7, i5-imax+7, i6-imax+7, i7-imax+7> (b);
+            return permute8<i0-imax+7, i1-imax+7, i2-imax+7, i3-imax+7, i4-imax+7, i5-imax+7, i6-imax+7, i7-imax+7> (b);
         }
         else {
             Vec8d b = Vec8d().load((double const *)a + imin);
-            return permute8d<i0-imin, i1-imin, i2-imin, i3-imin, i4-imin, i5-imin, i6-imin, i7-imin> (b);
+            return permute8<i0-imin, i1-imin, i2-imin, i3-imin, i4-imin, i5-imin, i6-imin, i7-imin> (b);
         }
     }
-    if ((i0<imin+8 || i0>imax-8) && (i1<imin+8 || i1>imax-8) && (i2<imin+8 || i2>imax-8) && (i3<imin+8 || i3>imax-8)
+    if constexpr ((i0<imin+8 || i0>imax-8) && (i1<imin+8 || i1>imax-8) && (i2<imin+8 || i2>imax-8) && (i3<imin+8 || i3>imax-8)
     &&  (i4<imin+8 || i4>imax-8) && (i5<imin+8 || i5>imax-8) && (i6<imin+8 || i6>imax-8) && (i7<imin+8 || i7>imax-8)) {
         // load two contiguous blocks and blend
         Vec8d b = Vec8d().load((double const *)a + imin);
@@ -2066,7 +1859,7 @@ static inline Vec8d gather8d(void const * a) {
         const int j5 = i5<imin+8 ? i5-imin : 15-imax+i5;
         const int j6 = i6<imin+8 ? i6-imin : 15-imax+i6;
         const int j7 = i7<imin+8 ? i7-imin : 15-imax+i7;
-        return blend8d<j0, j1, j2, j3, j4, j5, j6, j7>(b, c);
+        return blend8<j0, j1, j2, j3, j4, j5, j6, j7>(b, c);
     }
     // use lookup function
     return lookup<imax+1>(Vec8q(i0,i1,i2,i3,i4,i5,i6,i7), (const double *)a);
@@ -2080,28 +1873,17 @@ static inline Vec8d gather8d(void const * a) {
 ******************************************************************************
 *
 * These functions write the elements of a vector to arbitrary positions in an
-* array in memory. Each vector element is written to an array position 
+* array in memory. Each vector element is written to an array position
 * determined by an index. An element is not written if the corresponding
 * index is out of range.
 * The indexes can be specified as constant template parameters or as an
 * integer vector.
-* 
-* The scatter functions are useful if the data are distributed in a sparce
-* manner into the array. If the array is dense then it is more efficient
-* to permute the data into the right positions and then write the whole
-* permuted vector into the array.
-*
-* Example:
-* Vec8d a(10,11,12,13,14,15,16,17);
-* double b[16] = {0};
-* scatter<0,2,14,10,1,-1,5,9>(a,b); 
-* // Now, b = {10,14,11,0,0,16,0,0,0,17,13,0,0,0,12,0}
 *
 *****************************************************************************/
 
 template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7,
     int i8, int i9, int i10, int i11, int i12, int i13, int i14, int i15>
-    static inline void scatter(Vec16f const & data, float * array) {
+    static inline void scatter(Vec16f const data, float * array) {
     const int index[16] = {i0,i1,i2,i3,i4,i5,i6,i7,i8,i9,i10,i11,i12,i13,i14,i15};
     for (int i = 0; i < 16; i++) {
         if (index[i] >= 0) array[index[i]] = data[i];
@@ -2109,65 +1891,37 @@ template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7,
 }
 
 template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7>
-static inline void scatter(Vec8d const & data, double * array) {
+static inline void scatter(Vec8d const data, double * array) {
     const int index[8] = {i0,i1,i2,i3,i4,i5,i6,i7};
     for (int i = 0; i < 8; i++) {
         if (index[i] >= 0) array[index[i]] = data[i];
     }
 }
 
-static inline void scatter(Vec16i const & index, uint32_t limit, Vec16f const & data, float * array) {
+// Scatter functions with variable indexes:
+
+static inline void scatter(Vec16i const index, uint32_t limit, Vec16f const data, float * destination) {
+    uint32_t ix[16];  index.store(ix);
     for (int i = 0; i < 16; i++) {
-        if (uint32_t(index[i]) < limit) array[index[i]] = data[i];
+        if (ix[i] < limit) destination[ix[i]] = data[i];
     }
 }
 
-static inline void scatter(Vec8q const & index, uint32_t limit, Vec8d const & data, double * array) {
+static inline void scatter(Vec8q const index, uint32_t limit, Vec8d const data, double * destination) {
+    uint64_t ix[8];  index.store(ix);
     for (int i = 0; i < 8; i++) {
-        if (uint64_t(index[i]) < uint64_t(limit)) array[index[i]] = data[i];
+        if (ix[i] < limit) destination[ix[i]] = data[i];
     }
 }
 
-static inline void scatter(Vec8i const & index, uint32_t limit, Vec8d const & data, double * array) {
+static inline void scatter(Vec8i const index, uint32_t limit, Vec8d const data, double * destination) {
+    uint32_t ix[8];  index.store(ix);
     for (int i = 0; i < 8; i++) {
-        if (uint32_t(index[i]) < limit) array[index[i]] = data[i];
+        if (ix[i] < limit) destination[ix[i]] = data[i];
     }
 }
 
 
-/*****************************************************************************
-*
-*          Horizontal scan functions
-*
-*****************************************************************************/
-
-// Get index to the first element that is true. Return -1 if all are false
-static inline int horizontal_find_first(Vec16fb const & x) {
-    int a1 = horizontal_find_first(x.get_low());
-    if (a1 >= 0) return a1;
-    int a2 = horizontal_find_first(x.get_high());
-    if (a2 < 0) return a2;
-    return a2 + 8;
-}
-
-static inline int horizontal_find_first(Vec8db const & x) {
-    int a1 = horizontal_find_first(x.get_low());
-    if (a1 >= 0) return a1;
-    int a2 = horizontal_find_first(x.get_high());
-    if (a2 < 0) return a2;
-    return a2 + 4;
-}
-
-// count the number of true elements
-static inline uint32_t horizontal_count(Vec16fb const & x) {
-    return horizontal_count(x.get_low()) + horizontal_count(x.get_high());
-}
-
-static inline uint32_t horizontal_count(Vec8db const & x) {
-    return horizontal_count(x.get_low()) + horizontal_count(x.get_high());
-}
-
-
 /*****************************************************************************
 *
 *          Boolean <-> bitfield conversion functions
@@ -2175,27 +1929,17 @@ static inline uint32_t horizontal_count(Vec8db const & x) {
 *****************************************************************************/
 
 // to_bits: convert boolean vector to integer bitfield
-static inline uint16_t to_bits(Vec16fb const & x) {
+static inline uint16_t to_bits(Vec16fb const x) {
     return to_bits(Vec16ib(x));
 }
 
-// to_Vec16fb: convert integer bitfield to boolean vector
-static inline Vec16fb to_Vec16fb(uint16_t x) {
-    return Vec16fb(to_Vec16ib(x));
-}
-
 // to_bits: convert boolean vector to integer bitfield
-static inline uint8_t to_bits(Vec8db const & x) {
+static inline uint8_t to_bits(Vec8db const x) {
     return to_bits(Vec8qb(x));
 }
 
-// to_Vec8db: convert integer bitfield to boolean vector
-static inline Vec8db to_Vec8db(uint8_t x) {
-    return Vec8db(to_Vec8qb(x));
-}
-
 #ifdef VCL_NAMESPACE
 }
 #endif
 
-#endif // VECTORF512_H
+#endif // VECTORF512E_H
diff --git a/EEDI3/vectorclass/vectorfp16.h b/EEDI3/vectorclass/vectorfp16.h
new file mode 100644
index 0000000..6178ef7
--- /dev/null
+++ b/EEDI3/vectorclass/vectorfp16.h
@@ -0,0 +1,2672 @@
+/****************************  vectorfp16.h   *******************************
+* Author:        Agner Fog
+* Date created:  2022-05-03
+* Last modified: 2023-11-07
+* Version:       2.02.02
+* Project:       vector class library
+* Description:
+* Header file defining half precision floating point vector classes
+* Instruction sets AVX512_FP16 and AVX512VL required
+*
+* Instructions: see vcl_manual.pdf
+*
+* The following vector classes are defined here:
+* Vec8h     Vector of  8 half precision floating point numbers in 128 bit vector
+* Vec16h    Vector of 16 half precision floating point numbers in 256 bit vector
+* Vec32h    Vector of 32 half precision floating point numbers in 512 bit vector
+*
+* This header file defines operators and functions for these vectors.
+*
+* You need a compiler supporting the AVX512_FP16 instruction to compile for this.
+* This code works with the following compilers:
+* clang++ version 14.0
+* g++ version 12.1 with binutils version 2.34
+* Intel c++ compiler version 2022.0
+*
+* (c) Copyright 2012-2023 Agner Fog.
+* Apache License version 2.0 or later.
+*****************************************************************************/
+
+#ifndef VECTORFP16_H
+#define VECTORFP16_H
+
+#ifndef VECTORCLASS_H
+#include "vectorclass.h"
+#endif
+
+#if VECTORCLASS_H < 20200
+#error Incompatible versions of vector class library mixed
+#endif
+
+#if INSTRSET < 10 || !defined(__AVX512FP16__)
+// half precision instructions not supported. Use emulation
+#include "vectorfp16e.h"
+#else
+
+#ifdef VCL_NAMESPACE
+namespace VCL_NAMESPACE {
+#endif
+
+// type Float16 emulates _Float16 in vectorfp16e.h if _Float16 not defined
+#ifdef __STDCPP_FLOAT16_T__
+typedef std::float16_t Float16; 
+#else
+typedef _Float16 Float16;
+#endif
+
+
+/*****************************************************************************
+*
+*          Vec8hb: Vector of 8 Booleans for use with Vec8h
+*
+*****************************************************************************/
+
+typedef Vec8b Vec8hb;  // compact boolean vector
+
+
+/*****************************************************************************
+*
+*          Vec8h: Vector of 8 half precision floating point values
+*
+*****************************************************************************/
+
+class Vec8h {
+protected:
+    __m128h xmm; // Float vector
+public:
+    // Default constructor:
+    Vec8h() = default;
+    // Constructor to broadcast the same value into all elements:
+    Vec8h(_Float16 f) {
+        xmm = _mm_set1_ph (f);
+    }
+    // Constructor to build from all elements:
+    Vec8h(_Float16 f0, _Float16 f1, _Float16 f2, _Float16 f3, _Float16 f4, _Float16 f5, _Float16 f6, _Float16 f7) {
+        xmm = _mm_setr_ph (f0, f1, f2, f3, f4, f5, f6, f7);
+    }
+    // Constructor to convert from type __m128 used in intrinsics:
+    Vec8h(__m128h const x) {
+        xmm = x;
+    }
+    // Assignment operator to convert from type __m128 used in intrinsics:
+    Vec8h & operator = (__m128h const x) {
+        xmm = x;
+        return *this;
+    }
+    // Type cast operator to convert to __m128 used in intrinsics
+    operator __m128h() const {
+        return xmm;
+    }
+    // Member function to load from array (unaligned)
+    Vec8h & load(void const * p) {
+        xmm = _mm_loadu_ph (p);
+        return *this;
+    }
+    // Member function to load from array, aligned by 16
+    // You may use load_a instead of load if you are certain that p points to an address
+    // divisible by 16. In most cases there is no difference in speed between load and load_a
+    Vec8h & load_a(void const * p) {
+        xmm = _mm_load_ph (p);
+        return *this;
+    }
+    // Member function to store into array (unaligned)
+    void store(void * p) const {
+        _mm_storeu_ph (p, xmm);
+    }
+    // Member function storing into array, aligned by 16
+    // You may use store_a instead of store if you are certain that p points to an address
+    // divisible by 16.
+    void store_a(void * p) const {
+        _mm_store_ph (p, xmm);
+    }
+    // Member function storing to aligned uncached memory (non-temporal store).
+    // This may be more efficient than store_a when storing large blocks of memory if it 
+    // is unlikely that the data will stay in the cache until it is read again.
+    // Note: Will generate runtime error if p is not aligned by 16
+    void store_nt(void * p) const {
+        _mm_stream_ps((float*)p, _mm_castph_ps(xmm));
+    }
+    // Partial load. Load n elements and set the rest to 0
+    Vec8h & load_partial(int n, void const * p) {
+        xmm = _mm_castsi128_ph(_mm_maskz_loadu_epi16(__mmask8((1u << n) - 1), p));
+        return *this;
+    }
+    // Partial store. Store n elements
+    void store_partial(int n, void * p) const {
+        _mm_mask_storeu_epi16(p, __mmask8((1u << n) - 1), _mm_castph_si128(xmm));
+    }
+    // cut off vector to n elements. The last 8-n elements are set to zero
+    Vec8h & cutoff(int n) {
+        xmm = _mm_castsi128_ph(_mm_maskz_mov_epi16(__mmask8((1u << n) - 1), _mm_castph_si128(xmm)));
+        return *this;
+    }
+    // Member function to change a single element in vector
+    Vec8h const insert(int index, _Float16 a) {
+        __m128h aa = _mm_set1_ph (a);
+        xmm = _mm_castsi128_ph(_mm_mask_mov_epi16(_mm_castph_si128(xmm), __mmask8(1u << index), _mm_castph_si128(aa)));
+        return *this;
+    }
+    // Member function extract a single element from vector
+    _Float16 extract(int index) const {
+#if INSTRSET >= 10 && defined (__AVX512VBMI2__)
+        __m128i x = _mm_maskz_compress_epi16(__mmask8(1u << index), _mm_castph_si128(xmm));
+        return _mm_cvtsh_h(_mm_castsi128_ph(x));
+#elif 0
+        union {
+            __m128h v;
+            _Float16 f[8];
+        } y;
+        y.v = xmm;
+        return y.f[index & 7];
+#else
+        Vec4ui x = _mm_maskz_compress_epi32(__mmask8(1u << (index >> 1)), _mm_castph_si128(xmm));  // extract int32_t
+        x >>= (index & 1) << 4;  // get upper 16 bits if index odd
+        return _mm_cvtsh_h(_mm_castsi128_ph(x));
+#endif
+    }
+    // Extract a single element. Use store function if extracting more than one element.
+    // Operator [] can only read an element, not write.
+    _Float16 operator [] (int index) const {
+        return extract(index);
+    }
+    static constexpr int size() {
+        return 8;
+    }
+    static constexpr int elementtype() {
+        return 15;
+    }
+    typedef __m128h registertype;
+};
+
+
+/*****************************************************************************
+*
+*          Operators for Vec8h
+*
+*****************************************************************************/
+
+// vector operator + : add element by element
+static inline Vec8h operator + (Vec8h const a, Vec8h const b) {
+    return _mm_add_ph(a, b);
+}
+
+// vector operator + : add vector and scalar
+static inline Vec8h operator + (Vec8h const a, _Float16 b) {
+    return a + Vec8h(b);
+}
+static inline Vec8h operator + (_Float16 a, Vec8h const b) {
+    return Vec8h(a) + b;
+}
+
+// vector operator += : add
+static inline Vec8h & operator += (Vec8h & a, Vec8h const b) {
+    a = a + b;
+    return a;
+}
+
+// postfix operator ++
+static inline Vec8h operator ++ (Vec8h & a, int) {
+    Vec8h a0 = a;
+    a = a + _Float16(1); // 1.0f16 not supported by g++ version 12.1
+    return a0;
+}
+
+// prefix operator ++
+static inline Vec8h & operator ++ (Vec8h & a) {
+    a = a +  _Float16(1);
+    return a;
+}
+
+// vector operator - : subtract element by element
+static inline Vec8h operator - (Vec8h const a, Vec8h const b) {
+    return _mm_sub_ph(a, b);
+}
+
+// vector operator - : subtract vector and scalar
+static inline Vec8h operator - (Vec8h const a, _Float16 b) {
+    return a - Vec8h(b);
+}
+static inline Vec8h operator - (_Float16 a, Vec8h const b) {
+    return Vec8h(a) - b;
+}
+
+// vector operator - : unary minus
+// Change sign bit, even for 0, INF and NAN
+static inline Vec8h operator - (Vec8h const a) {
+    return _mm_castps_ph(_mm_xor_ps(_mm_castph_ps(a), _mm_castsi128_ps(_mm_set1_epi32(0x80008000))));
+}
+
+// vector operator -= : subtract
+static inline Vec8h & operator -= (Vec8h & a, Vec8h const b) {
+    a = a - b;
+    return a;
+}
+
+// postfix operator --
+static inline Vec8h operator -- (Vec8h & a, int) {
+    Vec8h a0 = a;
+    a = a -  _Float16(1);
+    return a0;
+}
+
+// prefix operator --
+static inline Vec8h & operator -- (Vec8h & a) {
+    a = a -  _Float16(1);
+    return a;
+}
+
+// vector operator * : multiply element by element
+static inline Vec8h operator * (Vec8h const a, Vec8h const b) {
+    return _mm_mul_ph(a, b);
+}
+
+// vector operator * : multiply vector and scalar
+static inline Vec8h operator * (Vec8h const a, _Float16 b) {
+    return a * Vec8h(b);
+}
+static inline Vec8h operator * (_Float16 a, Vec8h const b) {
+    return Vec8h(a) * b;
+}
+
+// vector operator *= : multiply
+static inline Vec8h & operator *= (Vec8h & a, Vec8h const b) {
+    a = a * b;
+    return a;
+}
+
+// vector operator / : divide all elements by same integer
+static inline Vec8h operator / (Vec8h const a, Vec8h const b) {
+    return _mm_div_ph(a, b);
+}
+
+// vector operator / : divide vector and scalar
+static inline Vec8h operator / (Vec8h const a, _Float16 b) {
+    return a / Vec8h(b);
+}
+static inline Vec8h operator / (_Float16 a, Vec8h const b) {
+    return Vec8h(a) / b;
+}
+
+// vector operator /= : divide
+static inline Vec8h & operator /= (Vec8h & a, Vec8h const b) {
+    a = a / b;
+    return a;
+}
+
+// vector operator == : returns true for elements for which a == b
+static inline Vec8hb operator == (Vec8h const a, Vec8h const b) {
+    return _mm_cmp_ph_mask(a, b, 0);
+}
+
+// vector operator != : returns true for elements for which a != b
+static inline Vec8hb operator != (Vec8h const a, Vec8h const b) {
+    return _mm_cmp_ph_mask(a, b, 4);
+}
+
+// vector operator < : returns true for elements for which a < b
+static inline Vec8hb operator < (Vec8h const a, Vec8h const b) {
+    return _mm_cmp_ph_mask(a, b, 1);
+}
+
+// vector operator <= : returns true for elements for which a <= b
+static inline Vec8hb operator <= (Vec8h const a, Vec8h const b) {
+    return _mm_cmp_ph_mask(a, b, 2);
+}
+
+// vector operator > : returns true for elements for which a > b
+static inline Vec8hb operator > (Vec8h const a, Vec8h const b) {
+    return _mm_cmp_ph_mask(a, b, 6+8);
+}
+
+// vector operator >= : returns true for elements for which a >= b
+static inline Vec8hb operator >= (Vec8h const a, Vec8h const b) {
+    return _mm_cmp_ph_mask(a, b, 5+8);
+}
+
+// Bitwise logical operators
+
+// vector operator & : bitwise and
+static inline Vec8h operator & (Vec8h const a, Vec8h const b) {
+    return _mm_castps_ph(_mm_and_ps(_mm_castph_ps(a), _mm_castph_ps(b)));
+}
+
+// vector operator &= : bitwise and
+static inline Vec8h & operator &= (Vec8h & a, Vec8h const b) {
+    a = a & b;
+    return a;
+}
+
+// vector operator & : bitwise and of Vec8h and Vec8hb
+static inline Vec8h operator & (Vec8h const a, Vec8hb const b) {
+    return _mm_castsi128_ph(_mm_maskz_mov_epi16(b, _mm_castph_si128(a)));
+}
+static inline Vec8h operator & (Vec8hb const a, Vec8h const b) {
+    return b & a;
+}
+
+// vector operator | : bitwise or
+static inline Vec8h operator | (Vec8h const a, Vec8h const b) {
+    return _mm_castps_ph(_mm_or_ps(_mm_castph_ps(a), _mm_castph_ps(b)));
+}
+
+// vector operator |= : bitwise or
+static inline Vec8h & operator |= (Vec8h & a, Vec8h const b) {
+    a = a | b;
+    return a;
+}
+
+// vector operator ^ : bitwise xor
+static inline Vec8h operator ^ (Vec8h const a, Vec8h const b) {
+    return _mm_castps_ph(_mm_xor_ps(_mm_castph_ps(a), _mm_castph_ps(b)));
+}
+
+// vector operator ^= : bitwise xor
+static inline Vec8h & operator ^= (Vec8h & a, Vec8h const b) {
+    a = a ^ b;
+    return a;
+}
+
+// vector operator ! : logical not. Returns Boolean vector
+static inline Vec8hb operator ! (Vec8h const a) {
+    return a == Vec8h(0.0);
+}
+
+
+/*****************************************************************************
+*
+*          Functions for Vec8h
+*
+*****************************************************************************/
+
+// Select between two operands. Corresponds to this pseudocode:
+// for (int i = 0; i < 4; i++) result[i] = s[i] ? a[i] : b[i];
+static inline Vec8h select(Vec8hb const s, Vec8h const a, Vec8h const b) {
+    return _mm_castsi128_ph(_mm_mask_mov_epi16(_mm_castph_si128(b), s, _mm_castph_si128(a)));
+}
+
+// Conditional add: For all vector elements i: result[i] = f[i] ? (a[i] + b[i]) : a[i]
+static inline Vec8h if_add(Vec8hb const f, Vec8h const a, Vec8h const b) {
+    return _mm_mask_add_ph (a, f, a, b);
+}
+
+// Conditional subtract: For all vector elements i: result[i] = f[i] ? (a[i] - b[i]) : a[i]
+static inline Vec8h if_sub(Vec8hb const f, Vec8h const a, Vec8h const b) {
+    return _mm_mask_sub_ph (a, f, a, b);
+}
+
+// Conditional multiply: For all vector elements i: result[i] = f[i] ? (a[i] * b[i]) : a[i]
+static inline Vec8h if_mul(Vec8hb const f, Vec8h const a, Vec8h const b) {
+    return _mm_mask_mul_ph (a, f, a, b);
+}
+
+// Conditional divide: For all vector elements i: result[i] = f[i] ? (a[i] / b[i]) : a[i]
+static inline Vec8h if_div(Vec8hb const f, Vec8h const a, Vec8h const b) {
+    return _mm_mask_div_ph (a, f, a, b);
+}
+
+// Sign functions
+
+// Function sign_bit: gives true for elements that have the sign bit set
+// even for -0.0f, -INF and -NAN
+// Note that sign_bit(Vec8h(-0.0f16)) gives true, while Vec8h(-0.0f16) < Vec8h(0.0f16) gives false
+// (the underscore in the name avoids a conflict with a macro in Intel's mathimf.h)
+static inline Vec8hb sign_bit(Vec8h const a) {
+    Vec8s t1 = _mm_castph_si128(a);    // reinterpret as 16-bit integer
+    Vec8s t2 = t1 >> 15;               // extend sign bit
+    return t2 != 0;
+}
+
+// Function sign_combine: changes the sign of a when b has the sign bit set
+// same as select(sign_bit(b), -a, a)
+static inline Vec8h sign_combine(Vec8h const a, Vec8h const b) {
+    return a ^ (b & Vec8h(_Float16(-0.0)));
+}
+
+// Categorization functions
+
+// Function is_finite: gives true for elements that are normal, subnormal or zero,
+// false for INF and NAN
+// (the underscore in the name avoids a conflict with a macro in Intel's mathimf.h)
+static inline Vec8hb is_finite(Vec8h const a) {
+    return __mmask8(_mm_fpclass_ph_mask(a, 0x99) ^ 0xFF);
+}
+
+// Function is_inf: gives true for elements that are +INF or -INF
+// false for finite numbers and NAN
+// (the underscore in the name avoids a conflict with a macro in Intel's mathimf.h)
+static inline Vec8hb is_inf(Vec8h const a) {
+    return __mmask8(_mm_fpclass_ph_mask(a, 0x18));
+}
+
+// Function is_nan: gives true for elements that are +NAN or -NAN
+// false for finite numbers and +/-INF
+// (the underscore in the name avoids a conflict with a macro in Intel's mathimf.h)
+static inline Vec8hb is_nan(Vec8h const a) {
+    // assume that compiler does not optimize this away with -ffinite-math-only:
+    return Vec4fb(_mm_fpclass_ph_mask(a, 0x81));
+}
+
+// Function is_subnormal: gives true for elements that are subnormal
+// false for finite numbers, zero, NAN and INF
+static inline Vec8hb is_subnormal(Vec8h const a) {
+    return Vec8hb(_mm_fpclass_ph_mask(a, 0x20));
+}
+
+// Function is_zero_or_subnormal: gives true for elements that are zero or subnormal
+// false for finite numbers, NAN and INF
+static inline Vec8hb is_zero_or_subnormal(Vec8h const a) {
+    return Vec8hb(_mm_fpclass_ph_mask(a, 0x26));
+}
+
+// Function infinite8h: returns a vector where all elements are +INF
+static inline Vec8h infinite8h() {
+    return _mm_castsi128_ph(_mm_set1_epi16(0x7C00));
+}
+
+// template for producing quiet NAN
+template <>
+Vec8h nan_vec<Vec8h>(uint32_t payload) {
+    if constexpr (Vec8h::elementtype() == 15) {  // _Float16
+        union {
+            uint16_t i;
+            _Float16 f;
+        } uf;
+        uf.i = 0x7E00 | (payload & 0x01FF);
+        return Vec8h(uf.f);
+    }
+} 
+
+// Function nan8h: returns a vector where all elements are NAN (quiet)
+static inline Vec8h nan8h(uint32_t n = 0x10) {
+    return nan_vec<Vec8h>(n);
+}
+
+// This function returns the code hidden in a NAN. The sign bit is ignored
+static inline Vec8us nan_code(Vec8h const x) {
+    Vec8us a = Vec8us(_mm_castph_si128(x));
+    Vec8us const n = 0x3FF;
+    return select(is_nan(x), a & n, Vec8us(0));
+}
+
+
+// General arithmetic functions, etc.
+
+// Horizontal add: Calculates the sum of all vector elements.
+static inline _Float16 horizontal_add(Vec8h const a) {
+    //return _mm_reduce_add_ph(a);
+    __m128h b = _mm_castps_ph(_mm_movehl_ps(_mm_castph_ps(a), _mm_castph_ps(a)));
+    __m128h c = _mm_add_ph(a, b);
+    __m128h d = _mm_castps_ph(_mm_movehdup_ps( _mm_castph_ps(c)));    
+    __m128h e = _mm_add_ph(c, d);
+    __m128h f = _mm_castsi128_ph(_mm_shufflelo_epi16(_mm_castph_si128(e), 1));
+    __m128h g = _mm_add_sh(e, f);
+    return _mm_cvtsh_h(g);
+}
+
+#if MAX_VECTOR_SIZE >= 256
+// same, with high precision
+static inline float horizontal_add_x(Vec8h const a) {
+    //Vec8f b = _mm256_cvtph_ps(a); // declaration of _mm256_cvtph_ps has __m128i parameter because it was defined before __m128h was defined
+    Vec8f b = _mm256_cvtph_ps(_mm_castph_si128(a));
+    return horizontal_add(b);
+}
+#endif
+
+// function max: a > b ? a : b
+static inline Vec8h max(Vec8h const a, Vec8h const b) {
+    return _mm_max_ph(a, b);
+}
+
+// function min: a < b ? a : b
+static inline Vec8h min(Vec8h const a, Vec8h const b) {
+    return _mm_min_ph(a, b);
+}
+// NAN-safe versions of maximum and minimum are in vector_convert.h
+
+// function abs: absolute value
+static inline Vec8h abs(Vec8h const a) {
+    return _mm_abs_ph(a);
+}
+
+// function sqrt: square root
+static inline Vec8h sqrt(Vec8h const a) {
+    return _mm_sqrt_ph(a);
+}
+
+// function square: a * a
+static inline Vec8h square(Vec8h const a) {
+    return a * a;
+}
+
+// The purpose of this template is to prevent implicit conversion of a float
+// exponent to int when calling pow(vector, float) and vectormath_exp.h is not included
+template <typename TT> static Vec8h pow(Vec8h const a, TT const n);  // = delete
+
+// Raise floating point numbers to integer power n
+template <>
+inline Vec8h pow<int>(Vec8h const x0, int const n) {
+    return pow_template_i<Vec8h>(x0, n);
+}
+
+// allow conversion from unsigned int
+template <>
+inline Vec8h pow<uint32_t>(Vec8h const x0, uint32_t const n) {
+    return pow_template_i<Vec8h>(x0, (int)n);
+}
+
+// Raise floating point numbers to integer power n, where n is a compile-time constant:
+// Template in vectorf28.h is used
+//template <typename V, int n>
+//static inline V pow_n(V const a);
+
+// implement as function pow(vector, const_int)
+template <int n>
+static inline Vec8h pow(Vec8h const a, Const_int_t<n>) {
+    return pow_n<Vec8h, n>(a);
+}
+
+static inline Vec8h round(Vec8h const a) {
+    return _mm_roundscale_ph (a, 8);
+}
+
+// function truncate: round towards zero. (result as float vector)
+static inline Vec8h truncate(Vec8h const a) {
+    return _mm_roundscale_ph(a, 3 + 8);
+}
+
+// function floor: round towards minus infinity. (result as float vector)
+static inline Vec8h floor(Vec8h const a) {
+    return _mm_roundscale_ph(a, 1 + 8);
+}
+
+// function ceil: round towards plus infinity. (result as float vector)
+static inline Vec8h ceil(Vec8h const a) {
+    return _mm_roundscale_ph(a, 2 + 8);
+}
+
+// function roundi: round to nearest integer (even). (result as integer vector)
+static inline Vec8s roundi(Vec8h const a) {
+    // Note: assume MXCSR control register is set to rounding
+    return _mm_cvtph_epi16(a);
+}
+
+// function truncatei: round towards zero. (result as integer vector)
+static inline Vec8s truncatei(Vec8h const a) {
+    return _mm_cvttph_epi16(a);
+}
+
+// function to_float: convert integer vector to float vector
+static inline Vec8h to_float16(Vec8s const a) {
+    return _mm_cvtepi16_ph(a);
+}
+
+// function to_float: convert unsigned integer vector to float vector
+static inline Vec8h to_float16(Vec8us const a) {
+    return _mm_cvtepu16_ph(a);
+}
+
+// Approximate math functions
+
+// reciprocal (almost exact)
+static inline Vec8h approx_recipr(Vec8h const a) {
+    return _mm_rcp_ph (a);
+}
+
+// reciprocal squareroot (almost exact)
+static inline Vec8h approx_rsqrt(Vec8h const a) {
+    return _mm_rsqrt_ph(a);
+}
+
+// Fused multiply and add functions
+
+// Multiply and add. a*b+c
+static inline Vec8h mul_add(Vec8h const a, Vec8h const b, Vec8h const c) {
+    return _mm_fmadd_ph(a, b, c);
+}
+
+// Multiply and subtract. a*b-c
+static inline Vec8h mul_sub(Vec8h const a, Vec8h const b, Vec8h const c) {
+    return _mm_fmsub_ph(a, b, c);
+}
+
+// Multiply and inverse subtract
+static inline Vec8h nmul_add(Vec8h const a, Vec8h const b, Vec8h const c) {
+    return _mm_fnmadd_ph(a, b, c);
+}
+
+// Math functions using fast bit manipulation
+
+// Extract the exponent as an integer
+// exponent(a) = floor(log2(abs(a)));
+// exponent(1.0f) = 0, exponent(0.0f) = -127, exponent(INF) = +128, exponent(NAN) = +128
+static inline Vec8s exponent(Vec8h const a) {
+    Vec8us t1 = _mm_castph_si128(a);   // reinterpret as 16-bit integer
+    Vec8us t2 = t1 << 1;               // shift out sign bit
+    Vec8us t3 = t2 >> 11;              // shift down logical to position 0
+    Vec8s  t4 = Vec8s(t3) - 0x0F;      // subtract bias from exponent
+    return t4;
+}
+
+// Extract the fraction part of a floating point number
+// a = 2^exponent(a) * fraction(a), except for a = 0
+// fraction(1.0f) = 1.0f, fraction(5.0f) = 1.25f
+// NOTE: The name fraction clashes with an ENUM in MAC XCode CarbonCore script.h !
+static inline Vec8h fraction(Vec8h const a) {
+    return _mm_getmant_ph(a, _MM_MANT_NORM_1_2, _MM_MANT_SIGN_zero);
+}
+
+// Fast calculation of pow(2,n) with n integer
+// n  =    0 gives 1.0f
+// n >=  16 gives +INF
+// n <= -15 gives 0.0f
+// This function will never produce subnormals, and never raise exceptions
+static inline Vec8h exp2(Vec8s const n) {
+    Vec8s t1 = max(n, -15);            // limit to allowed range
+    Vec8s t2 = min(t1, 16);
+    Vec8s t3 = t2 + 15;                // add bias
+    Vec8s t4 = t3 << 10;               // put exponent into position 10
+    return _mm_castsi128_ph(t4);       // reinterpret as float
+}
+//static Vec8h exp2(Vec8h const x);    // defined in vectormath_exp.h ??
+
+
+// change signs on vectors Vec8h
+// Each index i0 - i7 is 1 for changing sign on the corresponding element, 0 for no change
+template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7>
+static inline Vec8h change_sign(Vec8h const a) {
+    if constexpr ((i0 | i1 | i2 | i3 | i4 | i5 | i6 | i7) == 0) return a;
+    __m128i mask = constant4ui<
+        (i0 ? 0x8000 : 0) | (i1 ? 0x80000000 : 0), 
+        (i2 ? 0x8000 : 0) | (i3 ? 0x80000000 : 0), 
+        (i4 ? 0x8000 : 0) | (i5 ? 0x80000000 : 0), 
+        (i6 ? 0x8000 : 0) | (i7 ? 0x80000000 : 0) >();
+    return  _mm_castps_ph(_mm_xor_ps(_mm_castph_ps(a), _mm_castsi128_ps(mask)));     // flip sign bits
+}
+
+/*****************************************************************************
+*
+*          conversion of precision
+*
+*****************************************************************************/
+
+// conversions Vec8h <-> Vec4f
+// extend precision: Vec8h -> Vec4f. upper half ignored
+static inline Vec4f convert8h_4f (Vec8h h) {
+    return _mm_cvtph_ps(_mm_castph_si128(h));
+}
+
+// reduce precision: Vec4f -> Vec8h. upper half zero
+static inline Vec8h convert4f_8h (Vec4f f) {
+    return _mm_castsi128_ph(_mm_cvtps_ph(f, 0));
+}
+
+#if MAX_VECTOR_SIZE >= 256
+// conversions Vec8h <-> Vec8f
+// extend precision: Vec8h -> Vec8f
+static inline Vec8f to_float (Vec8h h) {
+    return _mm256_cvtph_ps(_mm_castph_si128(h));
+}
+
+// reduce precision: Vec8f -> Vec8h
+static inline Vec8h to_float16 (Vec8f f) {
+    return _mm_castsi128_ph(_mm256_cvtps_ph(f, 0));
+} 
+#endif
+
+/*****************************************************************************
+*
+*          Functions for reinterpretation between vector types
+*
+*****************************************************************************/
+
+static inline __m128i reinterpret_i(__m128h const x) {
+    return _mm_castph_si128(x);
+}
+
+static inline __m128h  reinterpret_h(__m128i const x) {
+    return _mm_castsi128_ph(x);
+}
+
+static inline __m128  reinterpret_f(__m128h const x) {
+    return _mm_castph_ps(x);
+}
+
+static inline __m128d reinterpret_d(__m128h const x) {
+    return _mm_castph_pd(x);
+}
+
+
+
+
+/*****************************************************************************
+*
+*          Vector permute and blend functions
+*
+******************************************************************************
+*
+* The permute function can reorder the elements of a vector and optionally
+* set some elements to zero.
+*
+* See vectori128.h for details
+*
+*****************************************************************************/
+// permute vector Vec8h
+template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7>
+static inline Vec8h permute8(Vec8h const a) {
+    return _mm_castsi128_ph (permute8<i0, i1, i2, i3, i4, i5, i6, i7>(Vec8s(_mm_castph_si128(a))));
+}
+
+
+/*****************************************************************************
+*
+*          Vector blend functions
+*
+*****************************************************************************/
+
+// permute and blend Vec8h
+template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7>
+static inline Vec8h blend8(Vec8h const a, Vec8h const b) {
+    return _mm_castsi128_ph (blend8<i0, i1, i2, i3, i4, i5, i6, i7>(Vec8s(_mm_castph_si128(a)), Vec8s(_mm_castph_si128(b))));
+}
+
+
+/*****************************************************************************
+*
+*          Vector lookup functions
+*
+******************************************************************************
+*
+* These functions use vector elements as indexes into a table.
+* The table is given as one or more vectors or as an array.
+*
+*****************************************************************************/
+
+static inline Vec8h lookup8 (Vec8s const index, Vec8h const table) {
+    return _mm_castsi128_ph(lookup8(index, Vec8s(_mm_castph_si128(table))));
+}
+
+static inline Vec8h lookup16(Vec8s const index, Vec8h const table0, Vec8h const table1) {
+    return _mm_castsi128_ph(lookup16(index, Vec8s(_mm_castph_si128(table0)), Vec8s(_mm_castph_si128(table1))));
+}
+
+template <int n>
+static inline Vec8h lookup(Vec8s const index, void const * table) {
+    return _mm_castsi128_ph(lookup<n>(index, (void const *)(table)));
+}
+
+
+/*****************************************************************************
+*
+*          256 bit vectors
+*
+*****************************************************************************/
+
+#if MAX_VECTOR_SIZE >= 256
+
+
+/*****************************************************************************
+*
+*          Vec16hb: Vector of 16 Booleans for use with Vec16h
+*
+*****************************************************************************/
+
+typedef Vec16b Vec16hb;  // compact boolean vector
+
+
+/*****************************************************************************
+*
+*          Vec16h: Vector of 16 half precision floating point values
+*
+*****************************************************************************/
+
+class Vec16h {
+protected:
+    __m256h ymm; // Float vector
+public:
+    // Default constructor:
+    Vec16h() = default;
+    // Constructor to broadcast the same value into all elements:
+    Vec16h(_Float16 f) {
+        ymm = _mm256_set1_ph (f);
+    }
+    // Constructor to build from all elements:
+    Vec16h(_Float16 f0, _Float16 f1, _Float16 f2, _Float16 f3, _Float16 f4, _Float16 f5, _Float16 f6, _Float16 f7,
+    _Float16 f8, _Float16 f9, _Float16 f10, _Float16 f11, _Float16 f12, _Float16 f13, _Float16 f14, _Float16 f15) {
+        ymm = _mm256_setr_ph (f0, f1, f2, f3, f4, f5, f6, f7, f8, f9, f10, f11, f12, f13, f14, f15);
+    }
+    // Constructor to build from two Vec8h:
+    Vec16h(Vec8h const a0, Vec8h const a1) {     
+        ymm = _mm256_castps_ph(_mm256_insertf128_ps(_mm256_castps128_ps256(_mm_castph_ps(a0)),_mm_castph_ps(a1),1));
+    }
+    // Constructor to convert from type __m256h used in intrinsics:
+    Vec16h(__m256h const x) {
+        ymm = x;
+    }
+    // Assignment operator to convert from type __m256h used in intrinsics:
+    Vec16h & operator = (__m256h const x) {
+        ymm = x;
+        return *this;
+    }
+    // Type cast operator to convert to __m256h used in intrinsics
+    operator __m256h() const {
+        return ymm;
+    }
+    // Member function to load from array (unaligned)
+    Vec16h & load(void const * p) {
+        ymm = _mm256_loadu_ph (p);
+        return *this;
+    }
+    // Member function to load from array, aligned by 32
+    // You may use load_a instead of load if you are certain that p points to an address
+    // divisible by 32. In most cases there is no difference in speed between load and load_a
+    Vec16h & load_a(void const * p) {
+        ymm = _mm256_load_ph (p);
+        return *this;
+    }
+    // Member function to store into array (unaligned)
+    void store(void * p) const {
+        _mm256_storeu_ph (p, ymm);
+    }
+    // Member function storing into array, aligned by 32
+    // You may use store_a instead of store if you are certain that p points to an address
+    // divisible by 32.
+    void store_a(void * p) const {
+        _mm256_store_ph (p, ymm);
+    }
+    // Member function storing to aligned uncached memory (non-temporal store).
+    // This may be more efficient than store_a when storing large blocks of memory if it 
+    // is unlikely that the data will stay in the cache until it is read again.
+    // Note: Will generate runtime error if p is not aligned by 32
+    void store_nt(void * p) const {
+        _mm256_stream_ps((float*)p, _mm256_castph_ps(ymm));
+    }
+    // Partial load. Load n elements and set the rest to 0
+    Vec16h & load_partial(int n, void const * p) {
+        ymm = _mm256_castsi256_ph(_mm256_maskz_loadu_epi16(__mmask16((1u << n) - 1), p));
+        return *this;
+    }
+    // Partial store. Store n elements
+    void store_partial(int n, void * p) const {
+        _mm256_mask_storeu_epi16(p, __mmask16((1u << n) - 1), _mm256_castph_si256(ymm));
+    }
+    // cut off vector to n elements. The last 8-n elements are set to zero
+    Vec16h & cutoff(int n) {
+        ymm = _mm256_castsi256_ph(_mm256_maskz_mov_epi16(__mmask16((1u << n) - 1), _mm256_castph_si256(ymm)));
+        return *this;
+    }
+    // Member function to change a single element in vector
+    Vec16h const insert(int index, _Float16 a) {
+        __m256h aa = _mm256_set1_ph (a);
+        ymm = _mm256_castsi256_ph(_mm256_mask_mov_epi16(_mm256_castph_si256(ymm), __mmask16(1u << index), _mm256_castph_si256(aa)));
+        return *this;
+    }
+    // Member function extract a single element from vector
+    _Float16 extract(int index) const {
+#if INSTRSET >= 10 && defined (__AVX512VBMI2__)
+        __m256i x = _mm256_maskz_compress_epi16(__mmask16(1u << index), _mm256_castph_si256(ymm));
+        return _mm256_cvtsh_h(_mm256_castsi256_ph(x));
+#elif 0
+        union {
+            __m256h v;
+            _Float16 f[16];
+        } y;
+        y.v = ymm;
+        return y.f[index & 15];
+#else
+        Vec8ui x = _mm256_maskz_compress_epi32(__mmask16(1u << (index >> 1)), _mm256_castph_si256(ymm));  // extract int32_t
+        x >>= uint32_t((index & 1) << 4);  // get upper 16 bits if index odd
+        return _mm256_cvtsh_h(_mm256_castsi256_ph(x));
+#endif
+    }
+    // Extract a single element. Use store function if extracting more than one element.
+    // Operator [] can only read an element, not write.
+    _Float16 operator [] (int index) const {
+        return extract(index);
+    }
+    Vec8h get_low() const {
+        return _mm256_castph256_ph128(ymm);
+    }
+    Vec8h get_high() const {
+        return _mm_castps_ph(_mm256_extractf128_ps(_mm256_castph_ps(ymm),1));
+    }
+    static constexpr int size() {
+        return 16;
+    }
+    static constexpr int elementtype() {
+        return 15;
+    }
+    typedef __m256h registertype;
+};
+
+
+/*****************************************************************************
+*
+*          Operators for Vec16h
+*
+*****************************************************************************/
+
+// vector operator + : add element by element
+static inline Vec16h operator + (Vec16h const a, Vec16h const b) {
+    return _mm256_add_ph(a, b);
+}
+
+// vector operator + : add vector and scalar
+static inline Vec16h operator + (Vec16h const a, _Float16 b) {
+    return a + Vec16h(b);
+}
+static inline Vec16h operator + (_Float16 a, Vec16h const b) {
+    return Vec16h(a) + b;
+}
+
+// vector operator += : add
+static inline Vec16h & operator += (Vec16h & a, Vec16h const b) {
+    a = a + b;
+    return a;
+}
+
+// postfix operator ++
+static inline Vec16h operator ++ (Vec16h & a, int) {
+    Vec16h a0 = a;
+    a = a +  _Float16(1);
+    return a0;
+}
+
+// prefix operator ++
+static inline Vec16h & operator ++ (Vec16h & a) {
+    a = a +  _Float16(1);
+    return a;
+}
+
+// vector operator - : subtract element by element
+static inline Vec16h operator - (Vec16h const a, Vec16h const b) {
+    return _mm256_sub_ph(a, b);
+}
+
+// vector operator - : subtract vector and scalar
+static inline Vec16h operator - (Vec16h const a, float b) {
+    return a - Vec16h(b);
+}
+static inline Vec16h operator - (float a, Vec16h const b) {
+    return Vec16h(a) - b;
+}
+
+// vector operator - : unary minus
+// Change sign bit, even for 0, INF and NAN
+static inline Vec16h operator - (Vec16h const a) {
+    return _mm256_castps_ph(_mm256_xor_ps(_mm256_castph_ps(a), _mm256_castsi256_ps(_mm256_set1_epi32(0x80008000))));
+}
+
+// vector operator -= : subtract
+static inline Vec16h & operator -= (Vec16h & a, Vec16h const b) {
+    a = a - b;
+    return a;
+}
+
+// postfix operator --
+static inline Vec16h operator -- (Vec16h & a, int) {
+    Vec16h a0 = a;
+    a = a -  _Float16(1);
+    return a0;
+}
+
+// prefix operator --
+static inline Vec16h & operator -- (Vec16h & a) {
+    a = a -  _Float16(1);
+    return a;
+}
+
+// vector operator * : multiply element by element
+static inline Vec16h operator * (Vec16h const a, Vec16h const b) {
+    return _mm256_mul_ph(a, b);
+}
+
+// vector operator * : multiply vector and scalar
+static inline Vec16h operator * (Vec16h const a, _Float16 b) {
+    return a * Vec16h(b);
+}
+static inline Vec16h operator * (_Float16 a, Vec16h const b) {
+    return Vec16h(a) * b;
+}
+
+// vector operator *= : multiply
+static inline Vec16h & operator *= (Vec16h & a, Vec16h const b) {
+    a = a * b;
+    return a;
+}
+
+// vector operator / : divide all elements by same integer
+static inline Vec16h operator / (Vec16h const a, Vec16h const b) {
+    return _mm256_div_ph(a, b);
+}
+
+// vector operator / : divide vector and scalar
+static inline Vec16h operator / (Vec16h const a, _Float16 b) {
+    return a / Vec16h(b);
+}
+static inline Vec16h operator / (_Float16 a, Vec16h const b) {
+    return Vec16h(a) / b;
+}
+
+// vector operator /= : divide
+static inline Vec16h & operator /= (Vec16h & a, Vec16h const b) {
+    a = a / b;
+    return a;
+}
+
+// vector operator == : returns true for elements for which a == b
+static inline Vec16hb operator == (Vec16h const a, Vec16h const b) {
+    return _mm256_cmp_ph_mask(a, b, 0);
+}
+
+// vector operator != : returns true for elements for which a != b
+static inline Vec16hb operator != (Vec16h const a, Vec16h const b) {
+    return _mm256_cmp_ph_mask(a, b, 4);
+}
+
+// vector operator < : returns true for elements for which a < b
+static inline Vec16hb operator < (Vec16h const a, Vec16h const b) {
+    return _mm256_cmp_ph_mask(a, b, 1);
+}
+
+// vector operator <= : returns true for elements for which a <= b
+static inline Vec16hb operator <= (Vec16h const a, Vec16h const b) {
+    return _mm256_cmp_ph_mask(a, b, 2);
+}
+
+// vector operator > : returns true for elements for which a > b
+static inline Vec16hb operator > (Vec16h const a, Vec16h const b) {
+    return _mm256_cmp_ph_mask(a, b, 6+8);
+}
+
+// vector operator >= : returns true for elements for which a >= b
+static inline Vec16hb operator >= (Vec16h const a, Vec16h const b) {
+    return _mm256_cmp_ph_mask(a, b, 5+8);
+}
+
+// Bitwise logical operators
+
+// vector operator & : bitwise and
+static inline Vec16h operator & (Vec16h const a, Vec16h const b) {
+    return _mm256_castps_ph(_mm256_and_ps(_mm256_castph_ps(a), _mm256_castph_ps(b)));
+}
+
+// vector operator &= : bitwise and
+static inline Vec16h & operator &= (Vec16h & a, Vec16h const b) {
+    a = a & b;
+    return a;
+}
+
+// vector operator & : bitwise and of Vec16h and Vec16hb
+static inline Vec16h operator & (Vec16h const a, Vec16hb const b) {
+    return _mm256_castsi256_ph(_mm256_maskz_mov_epi16(b, _mm256_castph_si256(a)));
+}
+static inline Vec16h operator & (Vec16hb const a, Vec16h const b) {
+    return b & a;
+}
+
+// vector operator | : bitwise or
+static inline Vec16h operator | (Vec16h const a, Vec16h const b) {
+    return _mm256_castps_ph(_mm256_or_ps(_mm256_castph_ps(a), _mm256_castph_ps(b)));
+}
+
+// vector operator |= : bitwise or
+static inline Vec16h & operator |= (Vec16h & a, Vec16h const b) {
+    a = a | b;
+    return a;
+}
+
+// vector operator ^ : bitwise xor
+static inline Vec16h operator ^ (Vec16h const a, Vec16h const b) {
+    return _mm256_castps_ph(_mm256_xor_ps(_mm256_castph_ps(a), _mm256_castph_ps(b)));
+}
+
+// vector operator ^= : bitwise xor
+static inline Vec16h & operator ^= (Vec16h & a, Vec16h const b) {
+    a = a ^ b;
+    return a;
+}
+
+// vector operator ! : logical not. Returns Boolean vector
+static inline Vec16hb operator ! (Vec16h const a) {
+    return a == Vec16h(0.0);
+}
+
+
+/*****************************************************************************
+*
+*          Functions for Vec16h
+*
+*****************************************************************************/
+
+// Select between two operands. Corresponds to this pseudocode:
+// for (int i = 0; i < 4; i++) result[i] = s[i] ? a[i] : b[i];
+static inline Vec16h select(Vec16hb const s, Vec16h const a, Vec16h const b) {
+    return _mm256_castsi256_ph(_mm256_mask_mov_epi16(_mm256_castph_si256(b), s, _mm256_castph_si256(a)));
+}
+
+// Conditional add: For all vector elements i: result[i] = f[i] ? (a[i] + b[i]) : a[i]
+static inline Vec16h if_add(Vec16hb const f, Vec16h const a, Vec16h const b) {
+    return _mm256_mask_add_ph (a, f, a, b);
+}
+
+// Conditional subtract: For all vector elements i: result[i] = f[i] ? (a[i] - b[i]) : a[i]
+static inline Vec16h if_sub(Vec16hb const f, Vec16h const a, Vec16h const b) {
+    return _mm256_mask_sub_ph (a, f, a, b);
+}
+
+// Conditional multiply: For all vector elements i: result[i] = f[i] ? (a[i] * b[i]) : a[i]
+static inline Vec16h if_mul(Vec16hb const f, Vec16h const a, Vec16h const b) {
+    return _mm256_mask_mul_ph (a, f, a, b);
+}
+
+// Conditional divide: For all vector elements i: result[i] = f[i] ? (a[i] / b[i]) : a[i]
+static inline Vec16h if_div(Vec16hb const f, Vec16h const a, Vec16h const b) {
+    return _mm256_mask_div_ph (a, f, a, b);
+}
+
+// Sign functions
+
+// Function sign_bit: gives true for elements that have the sign bit set
+// even for -0.0f, -INF and -NAN
+// Note that sign_bit(Vec16h(-0.0f16)) gives true, while Vec16h(-0.0f16) < Vec16h(0.0f16) gives false
+// (the underscore in the name avoids a conflict with a macro in Intel's mathimf.h)
+static inline Vec16hb sign_bit(Vec16h const a) {
+    Vec16s t1 = _mm256_castph_si256(a);    // reinterpret as 16-bit integer
+    Vec16s t2 = t1 >> 15;                  // extend sign bit
+    return t2 != 0;
+}
+
+// Function sign_combine: changes the sign of a when b has the sign bit set
+// same as select(sign_bit(b), -a, a)
+static inline Vec16h sign_combine(Vec16h const a, Vec16h const b) {
+    return a ^ (b & Vec16h(_Float16(-0.0)));
+}
+
+// Categorization functions
+
+// Function is_finite: gives true for elements that are normal, subnormal or zero,
+// false for INF and NAN
+// (the underscore in the name avoids a conflict with a macro in Intel's mathimf.h)
+static inline Vec16hb is_finite(Vec16h const a) {
+    return __mmask16(_mm256_fpclass_ph_mask(a, 0x99) ^ 0xFFFF);
+}
+
+// Function is_inf: gives true for elements that are +INF or -INF
+// false for finite numbers and NAN
+// (the underscore in the name avoids a conflict with a macro in Intel's mathimf.h)
+static inline Vec16hb is_inf(Vec16h const a) {
+    return __mmask16(_mm256_fpclass_ph_mask(a, 0x18));
+}
+
+// Function is_nan: gives true for elements that are +NAN or -NAN
+// false for finite numbers and +/-INF
+// (the underscore in the name avoids a conflict with a macro in Intel's mathimf.h)
+static inline Vec16hb is_nan(Vec16h const a) {
+    // assume that compiler does not optimize this away with -ffinite-math-only:
+    return Vec16sb(_mm256_fpclass_ph_mask(a, 0x81));
+}
+
+// Function is_subnormal: gives true for elements that are subnormal
+// false for finite numbers, zero, NAN and INF
+static inline Vec16hb is_subnormal(Vec16h const a) {
+    return Vec16hb(_mm256_fpclass_ph_mask(a, 0x20));
+}
+
+// Function is_zero_or_subnormal: gives true for elements that are zero or subnormal
+// false for finite numbers, NAN and INF
+static inline Vec16hb is_zero_or_subnormal(Vec16h const a) {
+    return Vec16hb(_mm256_fpclass_ph_mask(a, 0x26));
+}
+
+// Function infinite16h: returns a vector where all elements are +INF
+static inline Vec16h infinite16h() {
+    return _mm256_castsi256_ph(_mm256_set1_epi16(0x7C00));
+}
+
+// template for producing quiet NAN
+template <>
+Vec16h nan_vec<Vec16h>(uint32_t payload) {
+    if constexpr (Vec16h::elementtype() == 15) {  // _Float16
+        union {
+            uint16_t i;
+            _Float16 f;
+        } uf;
+        uf.i = 0x7E00 | (payload & 0x01FF);
+        return Vec16h(uf.f);
+    }
+} 
+
+// Function nan16h: returns a vector where all elements are NAN (quiet)
+static inline Vec16h nan16h(uint32_t n = 0x10) {
+    return nan_vec<Vec16h>(n);
+}
+
+// This function returns the code hidden in a NAN. The sign bit is ignored
+static inline Vec16us nan_code(Vec16h const x) {
+    Vec16us a = Vec16us(_mm256_castph_si256(x));
+    Vec16us const n = 0x3FF;
+    return select(is_nan(x), a & n, Vec16us(0));
+}
+
+
+// General arithmetic functions, etc.
+
+// Horizontal add: Calculates the sum of all vector elements.
+static inline _Float16 horizontal_add(Vec16h const a) {
+    return horizontal_add(a.get_low()+a.get_high());
+}
+#if MAX_VECTOR_SIZE >= 512
+// same, with high precision
+static inline float horizontal_add_x(Vec16h const a) {
+    Vec16f b =  _mm512_cvtph_ps(_mm256_castph_si256(a));
+    return horizontal_add(b);
+}
+#endif
+
+// function max: a > b ? a : b
+static inline Vec16h max(Vec16h const a, Vec16h const b) {
+    return _mm256_max_ph(a, b);
+}
+
+// function min: a < b ? a : b
+static inline Vec16h min(Vec16h const a, Vec16h const b) {
+    return _mm256_min_ph(a, b);
+}
+// NAN-safe versions of maximum and minimum are in vector_convert.h
+
+// function abs: absolute value
+static inline Vec16h abs(Vec16h const a) {
+    return _mm256_abs_ph(a);
+}
+
+// function sqrt: square root
+static inline Vec16h sqrt(Vec16h const a) {
+    return _mm256_sqrt_ph(a);
+}
+
+// function square: a * a
+static inline Vec16h square(Vec16h const a) {
+    return a * a;
+}
+
+// The purpose of this template is to prevent implicit conversion of a float
+// exponent to int when calling pow(vector, float) and vectormath_exp.h is not included
+template <typename TT> static Vec16h pow(Vec16h const a, TT const n);  // = delete
+
+// Raise floating point numbers to integer power n
+template <>
+inline Vec16h pow<int>(Vec16h const x0, int const n) {
+    return pow_template_i<Vec16h>(x0, n);
+}
+
+// allow conversion from unsigned int
+template <>
+inline Vec16h pow<uint32_t>(Vec16h const x0, uint32_t const n) {
+    return pow_template_i<Vec16h>(x0, (int)n);
+}
+
+// Raise floating point numbers to integer power n, where n is a compile-time constant:
+// Template in vectorf28.h is used
+//template <typename V, int n>
+//static inline V pow_n(V const a);
+
+// implement as function pow(vector, const_int)
+template <int n>
+Vec16h pow(Vec16h const a, Const_int_t<n>) {
+    return pow_n<Vec16h, n>(a);
+}
+
+
+static inline Vec16h round(Vec16h const a) {
+    return _mm256_roundscale_ph (a, 8);
+}
+
+// function truncate: round towards zero. (result as float vector)
+static inline Vec16h truncate(Vec16h const a) {
+    return _mm256_roundscale_ph(a, 3 + 8);
+}
+
+// function floor: round towards minus infinity. (result as float vector)
+static inline Vec16h floor(Vec16h const a) {
+    return _mm256_roundscale_ph(a, 1 + 8);
+}
+
+// function ceil: round towards plus infinity. (result as float vector)
+static inline Vec16h ceil(Vec16h const a) {
+    return _mm256_roundscale_ph(a, 2 + 8);
+}
+
+// function roundi: round to nearest integer (even). (result as integer vector)
+static inline Vec16s roundi(Vec16h const a) {
+    // Note: assume MXCSR control register is set to rounding
+    return _mm256_cvtph_epi16(a);
+}
+
+// function truncatei: round towards zero. (result as integer vector)
+static inline Vec16s truncatei(Vec16h const a) {
+    return _mm256_cvttph_epi16(a);
+}
+
+// function to_float: convert integer vector to float vector
+static inline Vec16h to_float16(Vec16s const a) {
+    return _mm256_cvtepi16_ph(a);
+}
+
+// function to_float: convert unsigned integer vector to float vector
+static inline Vec16h to_float16(Vec16us const a) {
+    return _mm256_cvtepu16_ph(a);
+}
+
+// Approximate math functions
+
+// reciprocal (almost exact)
+static inline Vec16h approx_recipr(Vec16h const a) {
+    return _mm256_rcp_ph (a);
+}
+
+// reciprocal squareroot (almost exact)
+static inline Vec16h approx_rsqrt(Vec16h const a) {
+    return _mm256_rsqrt_ph(a);
+}
+
+// Fused multiply and add functions
+
+// Multiply and add. a*b+c
+static inline Vec16h mul_add(Vec16h const a, Vec16h const b, Vec16h const c) {
+    return _mm256_fmadd_ph(a, b, c);
+}
+
+// Multiply and subtract. a*b-c
+static inline Vec16h mul_sub(Vec16h const a, Vec16h const b, Vec16h const c) {
+    return _mm256_fmsub_ph(a, b, c);
+}
+
+// Multiply and inverse subtract
+static inline Vec16h nmul_add(Vec16h const a, Vec16h const b, Vec16h const c) {
+    return _mm256_fnmadd_ph(a, b, c);
+}
+
+// Math functions using fast bit manipulation
+
+// Extract the exponent as an integer
+// exponent(a) = floor(log2(abs(a)));
+// exponent(1.0f) = 0, exponent(0.0f) = -127, exponent(INF) = +128, exponent(NAN) = +128
+static inline Vec16s exponent(Vec16h const a) {
+    Vec16us t1 = _mm256_castph_si256(a);   // reinterpret as 16-bit integer
+    Vec16us t2 = t1 << 1;                  // shift out sign bit
+    Vec16us t3 = t2 >> 11;                 // shift down logical to position 0
+    Vec16s  t4 = Vec16s(t3) - 0x0F;        // subtract bias from exponent
+    return t4;
+}
+
+// Extract the fraction part of a floating point number
+// a = 2^exponent(a) * fraction(a), except for a = 0
+// fraction(1.0f) = 1.0f, fraction(5.0f) = 1.25f
+// NOTE: The name fraction clashes with an ENUM in MAC XCode CarbonCore script.h !
+static inline Vec16h fraction(Vec16h const a) {
+    return _mm256_getmant_ph(a, _MM_MANT_NORM_1_2, _MM_MANT_SIGN_zero);
+}
+
+// Fast calculation of pow(2,n) with n integer
+// n  =    0 gives 1.0f
+// n >=  16 gives +INF
+// n <= -15 gives 0.0f
+// This function will never produce subnormals, and never raise exceptions
+static inline Vec16h exp2(Vec16s const n) {
+    Vec16s t1 = max(n, -15);            // limit to allowed range
+    Vec16s t2 = min(t1, 16);
+    Vec16s t3 = t2 + 15;                // add bias
+    Vec16s t4 = t3 << 10;               // put exponent into position 10
+    return _mm256_castsi256_ph(t4);     // reinterpret as float
+}
+//static Vec16h exp2(Vec16h const x);    // defined in vectormath_exp.h ??
+
+
+// change signs on vectors Vec16h
+// Each index i0 - i15 is 1 for changing sign on the corresponding element, 0 for no change
+template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7, 
+int i8, int i9, int i10, int i11, int i12, int i13, int i14, int i15>
+Vec16h change_sign(Vec16h const a) {
+    if constexpr ((i0 | i1 | i2 | i3 | i4 | i5 | i6 | i7 | i8 | i9 | i10 | i11 | i12 | i13 | i14 | i15) == 0) return a;
+    __m256i mask = constant8ui<
+        (i0  ? 0x8000 : 0) | (i1  ? 0x80000000 : 0), 
+        (i2  ? 0x8000 : 0) | (i3  ? 0x80000000 : 0), 
+        (i4  ? 0x8000 : 0) | (i5  ? 0x80000000 : 0), 
+        (i6  ? 0x8000 : 0) | (i7  ? 0x80000000 : 0), 
+        (i8  ? 0x8000 : 0) | (i9  ? 0x80000000 : 0), 
+        (i10 ? 0x8000 : 0) | (i11 ? 0x80000000 : 0), 
+        (i12 ? 0x8000 : 0) | (i13 ? 0x80000000 : 0), 
+        (i14 ? 0x8000 : 0) | (i15 ? 0x80000000 : 0) >();
+    return  _mm256_castps_ph(_mm256_xor_ps(_mm256_castph_ps(a), _mm256_castsi256_ps(mask)));     // flip sign bits
+}
+
+/*****************************************************************************
+*
+*          conversions Vec16h <-> Vec16f
+*
+*****************************************************************************/
+#if MAX_VECTOR_SIZE >= 512
+// extend precision: Vec8h -> Vec8f
+static inline Vec16f to_float (Vec16h h) {
+    return _mm512_cvtph_ps(_mm256_castph_si256(h));
+}
+
+// reduce precision: Vec8f -> Vec8h
+static inline Vec16h to_float16 (Vec16f f) {
+    return _mm256_castsi256_ph(_mm512_cvtps_ph(f, 0));
+}
+#endif
+
+/*****************************************************************************
+*
+*          Functions for reinterpretation between vector types
+*
+*****************************************************************************/
+
+static inline __m256i reinterpret_i(__m256h const x) {
+    return _mm256_castph_si256(x);
+}
+
+static inline __m256h reinterpret_h(__m256i const x) {
+    return _mm256_castsi256_ph(x);
+}
+
+static inline __m256  reinterpret_f(__m256h const x) {
+    return _mm256_castph_ps(x);
+}
+
+static inline __m256d reinterpret_d(__m256h const x) {
+    return _mm256_castph_pd(x);
+}
+
+static inline Vec16h extend_z(Vec8h a) {
+    //return _mm256_zextsi128_si256(a);
+    return _mm256_zextph128_ph256(a);
+}
+
+
+/*****************************************************************************
+*
+*          Vector permute and blend functions
+*
+******************************************************************************
+*
+* The permute function can reorder the elements of a vector and optionally
+* set some elements to zero.
+*
+* See vectori128.h for details
+*
+*****************************************************************************/
+// permute vector Vec16h
+template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7, 
+int i8, int i9, int i10, int i11, int i12, int i13, int i14, int i15>
+Vec16h permute16(Vec16h const a) {
+    return _mm256_castsi256_ph (
+    permute16<i0, i1, i2, i3, i4, i5, i6, i7, i8, i9, i10, i11, i12, i13, i14, i15> (
+    Vec16s(_mm256_castph_si256(a))));
+}
+
+
+/*****************************************************************************
+*
+*          Vector blend functions
+*
+*****************************************************************************/
+
+// permute and blend Vec16h
+template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7, 
+int i8, int i9, int i10, int i11, int i12, int i13, int i14, int i15>
+Vec16h blend16(Vec16h const a, Vec16h const b) {
+    return _mm256_castsi256_ph (
+    blend16<i0, i1, i2, i3, i4, i5, i6, i7, i8, i9, i10, i11, i12, i13, i14, i15> (
+    Vec16s(_mm256_castph_si256(a)), Vec16s(_mm256_castph_si256(b))));
+}
+
+
+/*****************************************************************************
+*
+*          Vector lookup functions
+*
+******************************************************************************
+*
+* These functions use vector elements as indexes into a table.
+* The table is given as one or more vectors or as an array.
+*
+*****************************************************************************/
+
+static inline Vec16h lookup16 (Vec16s const index, Vec16h const table) {
+    return _mm256_castsi256_ph(lookup16(index, Vec16s(_mm256_castph_si256(table))));
+}
+
+template <int n>
+Vec16h lookup(Vec16s const index, void const * table) {
+    return _mm256_castsi256_ph(lookup<n>(index, (void const *)(table)));
+}
+
+
+#endif // MAX_VECTOR_SIZE >= 256
+
+
+
+/*****************************************************************************
+*
+*          512 bit vectors
+*
+*****************************************************************************/
+
+#if MAX_VECTOR_SIZE >= 512
+
+
+/*****************************************************************************
+*
+*          Vec32hb: Vector of 32 Booleans for use with Vec32h
+*
+*****************************************************************************/
+
+typedef Vec32b Vec32hb;  // compact boolean vector
+
+
+/*****************************************************************************
+*
+*          Vec32h: Vector of 32 half precision floating point values
+*
+*****************************************************************************/
+
+class Vec32h {
+protected:
+    __m512h zmm; // Float vector
+public:
+    // Default constructor:
+    Vec32h() = default;
+    // Constructor to broadcast the same value into all elements:
+    Vec32h(_Float16 f) {
+        zmm = _mm512_set1_ph (f);
+    }
+    // Constructor to build from all elements:
+    Vec32h(_Float16 f0, _Float16 f1, _Float16 f2, _Float16 f3, _Float16 f4, _Float16 f5, _Float16 f6, _Float16 f7,
+    _Float16 f8, _Float16 f9, _Float16 f10, _Float16 f11, _Float16 f12, _Float16 f13, _Float16 f14, _Float16 f15,
+    _Float16 f16, _Float16 f17, _Float16 f18, _Float16 f19, _Float16 f20, _Float16 f21, _Float16 f22, _Float16 f23,
+    _Float16 f24, _Float16 f25, _Float16 f26, _Float16 f27, _Float16 f28, _Float16 f29, _Float16 f30, _Float16 f31) {
+        zmm = _mm512_setr_ph (f0, f1, f2, f3, f4, f5, f6, f7, f8, f9, f10, f11, f12, f13, f14, f15,
+        f16, f17, f18, f19, f20, f21, f22, f23, f24, f25, f26, f27, f28, f29, f30, f31);
+    }
+    // Constructor to build from two Vec16h:
+    Vec32h(Vec16h const a0, Vec16h const a1) {     
+        zmm = _mm512_castps_ph(_mm512_insertf32x8(_mm512_castps256_ps512(_mm256_castph_ps(a0)),_mm256_castph_ps(a1),1));
+    }
+    // Constructor to convert from type __m512h used in intrinsics:
+    Vec32h(__m512h const x) {
+        zmm = x;
+    }
+    // Assignment operator to convert from type __m512h used in intrinsics:
+    Vec32h & operator = (__m512h const x) {
+        zmm = x;
+        return *this;
+    }
+    // Type cast operator to convert to __m512h used in intrinsics
+    operator __m512h() const {
+        return zmm;
+    }
+    // Member function to load from array (unaligned)
+    Vec32h & load(void const * p) {
+        zmm = _mm512_loadu_ph (p);
+        return *this;
+    }
+    // Member function to load from array, aligned by 64
+    // You may use load_a instead of load if you are certain that p points to an address
+    // divisible by 64. In most cases there is no difference in speed between load and load_a
+    Vec32h & load_a(void const * p) {
+        zmm = _mm512_load_ph (p);
+        return *this;
+    }
+    // Member function to store into array (unaligned)
+    void store(void * p) const {
+        _mm512_storeu_ph (p, zmm);
+    }
+    // Member function storing into array, aligned by 64
+    // You may use store_a instead of store if you are certain that p points to an address
+    // divisible by 64.
+    void store_a(void * p) const {
+        _mm512_store_ph (p, zmm);
+    }
+    // Member function storing to aligned uncached memory (non-temporal store).
+    // This may be more efficient than store_a when storing large blocks of memory if it 
+    // is unlikely that the data will stay in the cache until it is read again.
+    // Note: Will generate runtime error if p is not aligned by 64
+    void store_nt(void * p) const {
+        _mm512_stream_ps((float*)p, _mm512_castph_ps(zmm));
+    }
+    // Partial load. Load n elements and set the rest to 0
+    Vec32h & load_partial(int n, void const * p) {
+        zmm = _mm512_castsi512_ph(_mm512_maskz_loadu_epi16(__mmask32((1u << n) - 1), p));
+        return *this;
+    }
+    // Partial store. Store n elements
+    void store_partial(int n, void * p) const {
+        _mm512_mask_storeu_epi16(p, __mmask32((1u << n) - 1), _mm512_castph_si512(zmm));
+    }
+    // cut off vector to n elements. The last 8-n elements are set to zero
+    Vec32h & cutoff(int n) {
+        zmm = _mm512_castsi512_ph(_mm512_maskz_mov_epi16(__mmask32((1u << n) - 1), _mm512_castph_si512(zmm)));
+        return *this;
+    }
+    // Member function to change a single element in vector
+    Vec32h const insert(int index, _Float16 a) {
+        __m512h aa = _mm512_set1_ph (a);
+        zmm = _mm512_castsi512_ph(_mm512_mask_mov_epi16(_mm512_castph_si512(zmm), __mmask32(1u << index), _mm512_castph_si512(aa)));
+        return *this;
+    }
+    // Member function extract a single element from vector
+    _Float16 extract(int index) const {
+#if INSTRSET >= 10 && defined (__AVX512VBMI2__)
+        __m512i x = _mm512_maskz_compress_epi16(__mmask32(1u << index), _mm512_castph_si512(zmm));
+        return _mm512_cvtsh_h(_mm512_castsi512_ph(x));
+#elif 0
+        union {
+            __m512h v;
+            _Float16 f[32];
+        } y;
+        y.v = zmm;
+        return y.f[index & 31];
+#else
+        Vec16ui x = _mm512_maskz_compress_epi32(__mmask16(1u << (index >> 1)), _mm512_castph_si512(zmm));  // extract int32_t
+        x >>= uint32_t((index & 1) << 4);  // get upper 16 bits if index odd
+        return _mm512_cvtsh_h(_mm512_castsi512_ph(x));
+#endif
+    }
+    // Extract a single element. Use store function if extracting more than one element.
+    // Operator [] can only read an element, not write.
+    _Float16 operator [] (int index) const {
+        return extract(index);
+    }
+    Vec16h get_low() const {
+        return _mm512_castph512_ph256(zmm);
+    }
+    Vec16h get_high() const {
+        return _mm256_castps_ph(_mm512_extractf32x8_ps(_mm512_castph_ps(zmm),1));
+    }
+    static constexpr int size() {
+        return 32;
+    }
+    static constexpr int elementtype() {
+        return 15;
+    }
+    typedef __m512h registertype;
+};
+
+
+/*****************************************************************************
+*
+*          Operators for Vec32h
+*
+*****************************************************************************/
+
+// vector operator + : add element by element
+static inline Vec32h operator + (Vec32h const a, Vec32h const b) {
+    return _mm512_add_ph(a, b);
+}
+
+// vector operator + : add vector and scalar
+static inline Vec32h operator + (Vec32h const a, _Float16 b) {
+    return a + Vec32h(b);
+}
+static inline Vec32h operator + (_Float16 a, Vec32h const b) {
+    return Vec32h(a) + b;
+}
+
+// vector operator += : add
+static inline Vec32h & operator += (Vec32h & a, Vec32h const b) {
+    a = a + b;
+    return a;
+}
+
+// postfix operator ++
+static inline Vec32h operator ++ (Vec32h & a, int) {
+    Vec32h a0 = a;
+    a = a +  _Float16(1);
+    return a0;
+}
+
+// prefix operator ++
+static inline Vec32h & operator ++ (Vec32h & a) {
+    a = a +  _Float16(1);
+    return a;
+}
+
+// vector operator - : subtract element by element
+static inline Vec32h operator - (Vec32h const a, Vec32h const b) {
+    return _mm512_sub_ph(a, b);
+}
+
+// vector operator - : subtract vector and scalar
+static inline Vec32h operator - (Vec32h const a, _Float16 b) {
+    return a - Vec32h(b);
+}
+static inline Vec32h operator - (_Float16 a, Vec32h const b) {
+    return Vec32h(a) - b;
+}
+
+// vector operator - : unary minus
+// Change sign bit, even for 0, INF and NAN
+static inline Vec32h operator - (Vec32h const a) {
+    return _mm512_castps_ph(_mm512_xor_ps(_mm512_castph_ps(a), _mm512_castsi512_ps(_mm512_set1_epi32(0x80008000))));
+}
+
+// vector operator -= : subtract
+static inline Vec32h & operator -= (Vec32h & a, Vec32h const b) {
+    a = a - b;
+    return a;
+}
+
+// postfix operator --
+static inline Vec32h operator -- (Vec32h & a, int) {
+    Vec32h a0 = a;
+    a = a -  _Float16(1);
+    return a0;
+}
+
+// prefix operator --
+static inline Vec32h & operator -- (Vec32h & a) {
+    a = a -  _Float16(1);
+    return a;
+}
+
+// vector operator * : multiply element by element
+static inline Vec32h operator * (Vec32h const a, Vec32h const b) {
+    return _mm512_mul_ph(a, b);
+}
+
+// vector operator * : multiply vector and scalar
+static inline Vec32h operator * (Vec32h const a, _Float16 b) {
+    return a * Vec32h(b);
+}
+static inline Vec32h operator * (_Float16 a, Vec32h const b) {
+    return Vec32h(a) * b;
+}
+
+// vector operator *= : multiply
+static inline Vec32h & operator *= (Vec32h & a, Vec32h const b) {
+    a = a * b;
+    return a;
+}
+
+// vector operator / : divide all elements by same integer
+static inline Vec32h operator / (Vec32h const a, Vec32h const b) {
+    return _mm512_div_ph(a, b);
+}
+
+// vector operator / : divide vector and scalar
+static inline Vec32h operator / (Vec32h const a, _Float16 b) {
+    return a / Vec32h(b);
+}
+static inline Vec32h operator / (_Float16 a, Vec32h const b) {
+    return Vec32h(a) / b;
+}
+
+// vector operator /= : divide
+static inline Vec32h & operator /= (Vec32h & a, Vec32h const b) {
+    a = a / b;
+    return a;
+}
+
+// vector operator == : returns true for elements for which a == b
+static inline Vec32hb operator == (Vec32h const a, Vec32h const b) {
+    return _mm512_cmp_ph_mask(a, b, 0);
+}
+
+// vector operator != : returns true for elements for which a != b
+static inline Vec32hb operator != (Vec32h const a, Vec32h const b) {
+    return _mm512_cmp_ph_mask(a, b, 4);
+}
+
+// vector operator < : returns true for elements for which a < b
+static inline Vec32hb operator < (Vec32h const a, Vec32h const b) {
+    return _mm512_cmp_ph_mask(a, b, 1);
+}
+
+// vector operator <= : returns true for elements for which a <= b
+static inline Vec32hb operator <= (Vec32h const a, Vec32h const b) {
+    return _mm512_cmp_ph_mask(a, b, 2);
+}
+
+// vector operator > : returns true for elements for which a > b
+static inline Vec32hb operator > (Vec32h const a, Vec32h const b) {
+    return _mm512_cmp_ph_mask(a, b, 6+8);
+}
+
+// vector operator >= : returns true for elements for which a >= b
+static inline Vec32hb operator >= (Vec32h const a, Vec32h const b) {
+    return _mm512_cmp_ph_mask(a, b, 5+8);
+}
+
+// Bitwise logical operators
+
+// vector operator & : bitwise and
+static inline Vec32h operator & (Vec32h const a, Vec32h const b) {
+    return _mm512_castps_ph(_mm512_and_ps(_mm512_castph_ps(a), _mm512_castph_ps(b)));
+}
+
+// vector operator &= : bitwise and
+static inline Vec32h & operator &= (Vec32h & a, Vec32h const b) {
+    a = a & b;
+    return a;
+}
+
+// vector operator & : bitwise and of Vec32h and Vec32hb
+static inline Vec32h operator & (Vec32h const a, Vec32hb const b) {
+    return _mm512_castsi512_ph(_mm512_maskz_mov_epi16(b, _mm512_castph_si512(a)));
+}
+static inline Vec32h operator & (Vec32hb const a, Vec32h const b) {
+    return b & a;
+}
+
+// vector operator | : bitwise or
+static inline Vec32h operator | (Vec32h const a, Vec32h const b) {
+    return _mm512_castps_ph(_mm512_or_ps(_mm512_castph_ps(a), _mm512_castph_ps(b)));
+}
+
+// vector operator |= : bitwise or
+static inline Vec32h & operator |= (Vec32h & a, Vec32h const b) {
+    a = a | b;
+    return a;
+}
+
+// vector operator ^ : bitwise xor
+static inline Vec32h operator ^ (Vec32h const a, Vec32h const b) {
+    return _mm512_castps_ph(_mm512_xor_ps(_mm512_castph_ps(a), _mm512_castph_ps(b)));
+}
+
+// vector operator ^= : bitwise xor
+static inline Vec32h & operator ^= (Vec32h & a, Vec32h const b) {
+    a = a ^ b;
+    return a;
+}
+
+// vector operator ! : logical not. Returns Boolean vector
+static inline Vec32hb operator ! (Vec32h const a) {
+    return a == Vec32h(0.0);
+}
+
+
+/*****************************************************************************
+*
+*          Functions for Vec32h
+*
+*****************************************************************************/
+
+// Select between two operands. Corresponds to this pseudocode:
+// for (int i = 0; i < 4; i++) result[i] = s[i] ? a[i] : b[i];
+static inline Vec32h select(Vec32hb const s, Vec32h const a, Vec32h const b) {
+    return _mm512_castsi512_ph(_mm512_mask_mov_epi16(_mm512_castph_si512(b), s, _mm512_castph_si512(a)));
+}
+
+// Conditional add: For all vector elements i: result[i] = f[i] ? (a[i] + b[i]) : a[i]
+static inline Vec32h if_add(Vec32hb const f, Vec32h const a, Vec32h const b) {
+    return _mm512_mask_add_ph (a, f, a, b);
+}
+
+// Conditional subtract: For all vector elements i: result[i] = f[i] ? (a[i] - b[i]) : a[i]
+static inline Vec32h if_sub(Vec32hb const f, Vec32h const a, Vec32h const b) {
+    return _mm512_mask_sub_ph (a, f, a, b);
+}
+
+// Conditional multiply: For all vector elements i: result[i] = f[i] ? (a[i] * b[i]) : a[i]
+static inline Vec32h if_mul(Vec32hb const f, Vec32h const a, Vec32h const b) {
+    return _mm512_mask_mul_ph (a, f, a, b);
+}
+
+// Conditional divide: For all vector elements i: result[i] = f[i] ? (a[i] / b[i]) : a[i]
+static inline Vec32h if_div(Vec32hb const f, Vec32h const a, Vec32h const b) {
+    return _mm512_mask_div_ph (a, f, a, b);
+}
+
+// Sign functions
+
+// Function sign_bit: gives true for elements that have the sign bit set
+// even for -0.0f, -INF and -NAN
+// Note that sign_bit(Vec32h(-0.0f16)) gives true, while Vec32h(-0.0f16) < Vec32h(0.0f16) gives false
+// (the underscore in the name avoids a conflict with a macro in Intel's mathimf.h)
+static inline Vec32hb sign_bit(Vec32h const a) {
+    Vec32s t1 = _mm512_castph_si512(a);    // reinterpret as 16-bit integer
+    Vec32s t2 = t1 >> 15;                  // extend sign bit
+    return t2 != 0;
+}
+
+// Function sign_combine: changes the sign of a when b has the sign bit set
+// same as select(sign_bit(b), -a, a)
+static inline Vec32h sign_combine(Vec32h const a, Vec32h const b) {
+    return a ^ (b & Vec32h(_Float16(-0.0)));
+}
+
+// Categorization functions
+
+// Function is_finite: gives true for elements that are normal, subnormal or zero,
+// false for INF and NAN
+// (the underscore in the name avoids a conflict with a macro in Intel's mathimf.h)
+static inline Vec32hb is_finite(Vec32h const a) {
+    return __mmask32(~ _mm512_fpclass_ph_mask(a, 0x99));
+}
+
+// Function is_inf: gives true for elements that are +INF or -INF
+// false for finite numbers and NAN
+// (the underscore in the name avoids a conflict with a macro in Intel's mathimf.h)
+static inline Vec32hb is_inf(Vec32h const a) {
+    return __mmask32(_mm512_fpclass_ph_mask(a, 0x18));
+}
+
+// Function is_nan: gives true for elements that are +NAN or -NAN
+// false for finite numbers and +/-INF
+// (the underscore in the name avoids a conflict with a macro in Intel's mathimf.h)
+static inline Vec32hb is_nan(Vec32h const a) {
+    // assume that compiler does not optimize this away with -ffinite-math-only:
+    return Vec32sb(_mm512_fpclass_ph_mask(a, 0x81));
+}
+
+// Function is_subnormal: gives true for elements that are subnormal
+// false for finite numbers, zero, NAN and INF
+static inline Vec32hb is_subnormal(Vec32h const a) {
+    return Vec32hb(_mm512_fpclass_ph_mask(a, 0x20));
+}
+
+// Function is_zero_or_subnormal: gives true for elements that are zero or subnormal
+// false for finite numbers, NAN and INF
+static inline Vec32hb is_zero_or_subnormal(Vec32h const a) {
+    return Vec32hb(_mm512_fpclass_ph_mask(a, 0x26));
+}
+
+// Function infinite32h: returns a vector where all elements are +INF
+static inline Vec32h infinite32h() {
+    return _mm512_castsi512_ph(_mm512_set1_epi16(0x7C00));
+}
+
+// template for producing quiet NAN
+template <>
+Vec32h nan_vec<Vec32h>(uint32_t payload) {
+    if constexpr (Vec32h::elementtype() == 15) {  // _Float16
+        union {
+            uint16_t i;
+            _Float16 f;
+        } uf;
+        uf.i = 0x7E00 | (payload & 0x01FF);
+        return Vec32h(uf.f);
+    }
+} 
+
+// Function nan32h: returns a vector where all elements are NAN (quiet)
+static inline Vec32h nan32h(uint32_t n = 0x10) {
+    return nan_vec<Vec32h>(n);
+}
+
+// This function returns the code hidden in a NAN. The sign bit is ignored
+static inline Vec32us nan_code(Vec32h const x) {
+    Vec32us a = Vec32us(_mm512_castph_si512(x));
+    Vec32us const n = 0x3FF;
+    return select(is_nan(x), a & n, Vec32us(0));
+} 
+
+
+// General arithmetic functions, etc.
+
+// Horizontal add: Calculates the sum of all vector elements.
+static inline _Float16 horizontal_add(Vec32h const a) {
+    return horizontal_add(a.get_low()+a.get_high());
+}
+// same, with high precision
+static inline float horizontal_add_x(Vec32h const a) {
+    Vec16f b1 = _mm512_cvtph_ps(_mm256_castph_si256(a.get_low()));   //_mm512_cvtph_ps(a.get_low());
+    Vec16f b2 = _mm512_cvtph_ps(_mm256_castph_si256(a.get_high()));
+    return horizontal_add(b1 + b2);
+}
+
+// function max: a > b ? a : b
+static inline Vec32h max(Vec32h const a, Vec32h const b) {
+    return _mm512_max_ph(a, b);
+}
+
+// function min: a < b ? a : b
+static inline Vec32h min(Vec32h const a, Vec32h const b) {
+    return _mm512_min_ph(a, b);
+}
+// NAN-safe versions of maximum and minimum are in vector_convert.h
+
+// function abs: absolute value
+static inline Vec32h abs(Vec32h const a) {
+    return _mm512_abs_ph(a);
+}
+
+// function sqrt: square root
+static inline Vec32h sqrt(Vec32h const a) {
+    return _mm512_sqrt_ph(a);
+}
+
+// function square: a * a
+static inline Vec32h square(Vec32h const a) {
+    return a * a;
+}
+
+// The purpose of this template is to prevent implicit conversion of a float
+// exponent to int when calling pow(vector, float) and vectormath_exp.h is not included
+template <typename TT> static Vec32h pow(Vec32h const a, TT const n);  // = delete
+
+// Raise floating point numbers to integer power n
+template <>
+inline Vec32h pow<int>(Vec32h const x0, int const n) {
+    return pow_template_i<Vec32h>(x0, n);
+}
+
+// allow conversion from unsigned int
+template <>
+inline Vec32h pow<uint32_t>(Vec32h const x0, uint32_t const n) {
+    return pow_template_i<Vec32h>(x0, (int)n);
+}
+
+// Raise floating point numbers to integer power n, where n is a compile-time constant:
+// Template in vectorf28.h is used
+//template <typename V, int n>
+//static inline V pow_n(V const a);
+
+// implement as function pow(vector, const_int)
+template <int n>
+Vec32h pow(Vec32h const a, Const_int_t<n>) {
+    return pow_n<Vec32h, n>(a);
+}
+
+static inline Vec32h round(Vec32h const a) {
+    return _mm512_roundscale_ph (a, 8);
+}
+
+// function truncate: round towards zero. (result as float vector)
+static inline Vec32h truncate(Vec32h const a) {
+    return _mm512_roundscale_ph(a, 3 + 8);
+}
+
+// function floor: round towards minus infinity. (result as float vector)
+static inline Vec32h floor(Vec32h const a) {
+    return _mm512_roundscale_ph(a, 1 + 8);
+}
+
+// function ceil: round towards plus infinity. (result as float vector)
+static inline Vec32h ceil(Vec32h const a) {
+    return _mm512_roundscale_ph(a, 2 + 8);
+}
+
+// function roundi: round to nearest integer (even). (result as integer vector)
+static inline Vec32s roundi(Vec32h const a) {
+    // Note: assume MXCSR control register is set to rounding
+    return _mm512_cvtph_epi16(a);
+}
+
+// function truncatei: round towards zero. (result as integer vector)
+static inline Vec32s truncatei(Vec32h const a) {
+    return _mm512_cvttph_epi16(a);
+}
+
+// function to_float: convert integer vector to float vector
+static inline Vec32h to_float16(Vec32s const a) {
+    return _mm512_cvtepi16_ph(a);
+}
+
+// function to_float: convert unsigned integer vector to float vector
+static inline Vec32h to_float16(Vec32us const a) {
+    return _mm512_cvtepu16_ph(a);
+}
+
+// Approximate math functions
+
+// reciprocal (almost exact)
+static inline Vec32h approx_recipr(Vec32h const a) {
+    return _mm512_rcp_ph(a);
+}
+
+// reciprocal squareroot (almost exact)
+static inline Vec32h approx_rsqrt(Vec32h const a) {
+    return _mm512_rsqrt_ph(a);
+}
+
+// Fused multiply and add functions
+
+// Multiply and add. a*b+c
+static inline Vec32h mul_add(Vec32h const a, Vec32h const b, Vec32h const c) {
+    return _mm512_fmadd_ph(a, b, c);
+}
+
+// Multiply and subtract. a*b-c
+static inline Vec32h mul_sub(Vec32h const a, Vec32h const b, Vec32h const c) {
+    return _mm512_fmsub_ph(a, b, c);
+}
+
+// Multiply and inverse subtract
+static inline Vec32h nmul_add(Vec32h const a, Vec32h const b, Vec32h const c) {
+    return _mm512_fnmadd_ph(a, b, c);
+}
+
+// Math functions using fast bit manipulation
+
+// Extract the exponent as an integer
+// exponent(a) = floor(log2(abs(a)));
+// exponent(1.0f) = 0, exponent(0.0f) = -127, exponent(INF) = +128, exponent(NAN) = +128
+static inline Vec32s exponent(Vec32h const a) {
+    Vec32us t1 = _mm512_castph_si512(a);   // reinterpret as 16-bit integer
+    Vec32us t2 = t1 << 1;                  // shift out sign bit
+    Vec32us t3 = t2 >> 11;                 // shift down logical to position 0
+    Vec32s  t4 = Vec32s(t3) - 0x0F;        // subtract bias from exponent
+    return t4;
+}
+
+// Extract the fraction part of a floating point number
+// a = 2^exponent(a) * fraction(a), except for a = 0
+// fraction(1.0f) = 1.0f, fraction(5.0f) = 1.25f
+// NOTE: The name fraction clashes with an ENUM in MAC XCode CarbonCore script.h !
+static inline Vec32h fraction(Vec32h const a) {
+    return _mm512_getmant_ph(a, _MM_MANT_NORM_1_2, _MM_MANT_SIGN_zero);
+}
+
+// Fast calculation of pow(2,n) with n integer
+// n  =    0 gives 1.0f
+// n >=  16 gives +INF
+// n <= -15 gives 0.0f
+// This function will never produce subnormals, and never raise exceptions
+static inline Vec32h exp2(Vec32s const n) {
+    Vec32s t1 = max(n, -15);            // limit to allowed range
+    Vec32s t2 = min(t1, 16);
+    Vec32s t3 = t2 + 15;                // add bias
+    Vec32s t4 = t3 << 10;               // put exponent into position 10
+    return _mm512_castsi512_ph(t4);     // reinterpret as float
+}
+//static Vec32h exp2(Vec32h const x);    // defined in vectormath_exp.h ??
+
+
+// change signs on vectors Vec32h
+// Each index i0 - i31 is 1 for changing sign on the corresponding element, 0 for no change
+template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7, 
+int i8, int i9, int i10, int i11, int i12, int i13, int i14, int i15,
+int i16, int i17, int i18, int i19, int i20, int i21, int i22, int i23,
+int i24, int i25, int i26, int i27, int i28, int i29, int i30, int i31 >
+Vec32h change_sign(Vec32h const a) {
+    if constexpr ((i0 | i1 | i2 | i3 | i4 | i5 | i6 | i7 | i8 | i9 | i10 | i11 | i12 | i13 | i14 | i15 |
+    i16 | i17 | i18 | i19 | i20 | i21 | i22 | i23 | i24 | i25 | i26 | i27 | i28 | i29 | i30 | i31)
+    == 0) return a;
+    __m512i mask = constant16ui<
+        (i0  ? 0x8000 : 0) | (i1  ? 0x80000000 : 0), 
+        (i2  ? 0x8000 : 0) | (i3  ? 0x80000000 : 0), 
+        (i4  ? 0x8000 : 0) | (i5  ? 0x80000000 : 0), 
+        (i6  ? 0x8000 : 0) | (i7  ? 0x80000000 : 0), 
+        (i8  ? 0x8000 : 0) | (i9  ? 0x80000000 : 0), 
+        (i10 ? 0x8000 : 0) | (i11 ? 0x80000000 : 0), 
+        (i12 ? 0x8000 : 0) | (i13 ? 0x80000000 : 0), 
+        (i14 ? 0x8000 : 0) | (i15 ? 0x80000000 : 0),        
+        (i16 ? 0x8000 : 0) | (i17 ? 0x80000000 : 0), 
+        (i18 ? 0x8000 : 0) | (i19 ? 0x80000000 : 0), 
+        (i20 ? 0x8000 : 0) | (i21 ? 0x80000000 : 0), 
+        (i22 ? 0x8000 : 0) | (i23 ? 0x80000000 : 0), 
+        (i24 ? 0x8000 : 0) | (i25 ? 0x80000000 : 0), 
+        (i26 ? 0x8000 : 0) | (i27 ? 0x80000000 : 0), 
+        (i28 ? 0x8000 : 0) | (i29 ? 0x80000000 : 0), 
+        (i30 ? 0x8000 : 0) | (i31 ? 0x80000000 : 0) >();
+    return  _mm512_castps_ph(_mm512_xor_ps(_mm512_castph_ps(a), _mm512_castsi512_ps(mask)));     // flip sign bits
+}
+
+
+/*****************************************************************************
+*
+*          Functions for reinterpretation between vector types
+*
+*****************************************************************************/
+
+static inline __m512i reinterpret_i(__m512h const x) {
+    return _mm512_castph_si512(x);
+}
+
+static inline __m512h reinterpret_h(__m512i const x) {
+    return _mm512_castsi512_ph(x);
+}
+
+static inline __m512  reinterpret_f(__m512h const x) {
+    return _mm512_castph_ps(x);
+}
+
+static inline __m512d reinterpret_d(__m512h const x) {
+    return _mm512_castph_pd(x);
+}
+
+static inline Vec32h extend_z(Vec16h a) {
+    //return _mm512_zextsi256_si512(a);
+    return _mm512_zextph256_ph512(a);
+}
+
+/*****************************************************************************
+*
+*          Vector permute and blend functions
+*
+******************************************************************************
+*
+* The permute function can reorder the elements of a vector and optionally
+* set some elements to zero.
+*
+* See vectori128.h for details
+*
+*****************************************************************************/
+// permute vector Vec32h
+template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7, 
+int i8, int i9, int i10, int i11, int i12, int i13, int i14, int i15,
+int i16, int i17, int i18, int i19, int i20, int i21, int i22, int i23,
+int i24, int i25, int i26, int i27, int i28, int i29, int i30, int i31 >
+Vec32h permute32(Vec32h const a) {
+    return _mm512_castsi512_ph (
+    permute32<i0, i1, i2, i3, i4, i5, i6, i7, i8, i9, i10, i11, i12, i13, i14, i15,
+    i16, i17, i18, i19, i20, i21, i22, i23, i24, i25, i26, i27, i28, i29, i30, i31 > (
+    Vec32s(_mm512_castph_si512(a))));
+}
+
+
+/*****************************************************************************
+*
+*          Vector blend functions
+*
+*****************************************************************************/
+
+// permute and blend Vec32h
+template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7, 
+int i8, int i9, int i10, int i11, int i12, int i13, int i14, int i15,
+int i16, int i17, int i18, int i19, int i20, int i21, int i22, int i23,
+int i24, int i25, int i26, int i27, int i28, int i29, int i30, int i31 >
+Vec32h blend32(Vec32h const a, Vec32h const b) {
+    return _mm512_castsi512_ph (
+    blend32<i0, i1, i2, i3, i4, i5, i6, i7, i8, i9, i10, i11, i12, i13, i14, i15,
+    i16, i17, i18, i19, i20, i21, i22, i23, i24, i25, i26, i27, i28, i29, i30, i31 > (
+    Vec32s(_mm512_castph_si512(a)), Vec32s(_mm512_castph_si512(b))));
+}
+
+
+/*****************************************************************************
+*
+*          Vector lookup functions
+*
+******************************************************************************
+*
+* These functions use vector elements as indexes into a table.
+* The table is given as one or more vectors or as an array.
+*
+*****************************************************************************/
+
+static inline Vec32h lookup32 (Vec32s const index, Vec32h const table) {
+    return _mm512_castsi512_ph(lookup32(index, Vec32s(_mm512_castph_si512(table))));
+}
+
+template <int n>
+static inline Vec32h lookup(Vec32s const index, void const * table) {
+    return _mm512_castsi512_ph(lookup<n>(index, (void const *)(table)));
+}
+
+#endif // MAX_VECTOR_SIZE >= 512
+
+
+/***************************************************************************************
+*
+*                       Mathematical functions
+*
+* This code is designed to be independent of whether the vectormath files are included
+*
+***************************************************************************************/
+
+// pow(2,n)
+template <typename V>
+V vh_pow2n (V const n) {           
+    typedef decltype(roundi(n)) VI;              // corresponding integer vector type
+    const _Float16 pow2_10 = _Float16(1024);     // 2^10
+    const _Float16 bias = _Float16(15);          // bias in exponent
+    V  a = n + (bias + pow2_10);                 // put n + bias in least significant bits
+    VI b = reinterpret_i(a);                     // bit-cast to integer
+    VI c = b << 10;                              // shift left 10 places to get into exponent field
+    V  d = reinterpret_h(c);                     // bit-cast back to float16
+    return d;
+}
+
+// generate INF vector
+template <typename VTYPE>
+static inline VTYPE infinite_vech();
+
+template <>
+inline Vec8h infinite_vech<Vec8h>() {
+    return infinite8h();
+}
+#if MAX_VECTOR_SIZE >= 256
+template <>
+inline Vec16h infinite_vech<Vec16h>() {
+    return infinite16h();
+}
+#endif
+#if MAX_VECTOR_SIZE >= 512
+template <>
+inline Vec32h infinite_vech<Vec32h>() {
+    return infinite32h();
+}
+#endif
+
+
+// Template for exp function, half precision
+// The limit of abs(x) is defined by max_x below
+// Note on accuracy:
+// This function does not produce subnormal results
+// Max error is 7 ULP
+// The input range is slightly reduced. Inputs > 10.75 give INF. INputs < -10.75 give 0.
+// The emulated version without __AVX512FP16__ can produce subnormals, has full input range,
+// and a precision of 1 ULP
+
+// Template parameters:
+// VTYPE:  float vector type
+// M1: 0 for exp, 1 for expm1
+// BA: 0 for exp, 1 for 0.5*exp, 2 for pow(2,x), 10 for pow(10,x)
+
+template<typename VTYPE, int M1, int BA>
+VTYPE exp_h(VTYPE const initial_x) {
+
+    // Taylor coefficients
+    const _Float16 P0expf   =  _Float16(1.f/2.f);
+    const _Float16 P1expf   =  _Float16(1.f/6.f);
+    const _Float16 P2expf   =  _Float16(1.f/24.f);
+
+    VTYPE  x, r, x2, z, n2;                      // data vectors
+
+    // maximum abs(x), value depends on BA, defined below
+    // The lower limit of x is slightly more restrictive than the upper limit.
+    // We are specifying the lower limit, except for BA = 1 because it is not used for negative x
+    _Float16 max_x;
+
+    if constexpr (BA <= 1) {                     // exp(x)
+        const _Float16 ln2f  = _Float16(0.69314718f);   // ln(2)
+        const _Float16 log2e = _Float16(1.44269504089f); // log2(e)
+        x = initial_x;
+        r = round(initial_x*log2e);
+        x = nmul_add(r, VTYPE(ln2f), x);         //  x -= r * ln2f;
+        max_x = _Float16(10.75f);                // overflow limit
+    }
+    else if constexpr (BA == 2) {                // pow(2,x)
+        const _Float16 ln2 = _Float16(0.69314718f); // ln(2)
+        max_x = _Float16(15.5f);
+        r = round(initial_x);
+        x = initial_x - r;
+        x = x * ln2;
+    }
+    else if constexpr (BA == 10) {               // pow(10,x)
+        max_x = 4.667f;
+        const _Float16 log10_2 = _Float16(0.30102999566f); // log10(2)
+        x = initial_x;
+        r = round(initial_x*_Float16(3.32192809489f)); // VM_LOG2E*VM_LN10
+        x = nmul_add(r, VTYPE(log10_2), x);      //  x -= r * log10_2
+        x = x * _Float16(2.30258509299f);        // (float)VM_LN10;
+    }
+    else  {  // undefined value of BA
+        return 0.;
+    }
+    x2 = x * x;
+    // z = polynomial_2(x,P0expf,P1expf,P2expf);
+    z = mul_add(x2, P2expf, mul_add(x, P1expf, P0expf));
+    z = mul_add(z, x2, x);                       // z *= x2;  z += x;
+    if constexpr (BA == 1) r--;                  // 0.5 * exp(x)
+    n2 = vh_pow2n(r);                            // multiply by power of 2
+    if constexpr (M1 == 0) {                     // exp
+        z = (z + _Float16(1)) * n2;
+    }
+    else {                                       // expm1
+        z = mul_add(z, n2, n2 - _Float16(1));    //  z = z * n2 + (n2 - 1.0f);
+#ifdef SIGNED_ZERO                               // pedantic preservation of signed zero
+        z = select(initial_x == _Float16(0), initial_x, z);
+#endif
+    }
+    // check for overflow
+    auto inrange  = abs(initial_x) < VTYPE(max_x);// boolean vector
+    // check for INF and NAN
+    inrange &= is_finite(initial_x);
+    if (horizontal_and(inrange)) {               // fast normal path
+        return z;
+    }
+    else {                                       // overflow, underflow and NAN
+        VTYPE const inf = infinite_vech<VTYPE>();          // infinite
+        r = select(sign_bit(initial_x), _Float16(0.f-(M1&1)), inf);  // value in case of +/- overflow or INF
+        z = select(inrange, z, r);                         // +/- underflow
+        z = select(is_nan(initial_x), initial_x, z);       // NAN goes through
+        return z;
+    }
+}
+
+// dummy functions used for type definition in template sincos_h:
+static inline Vec8us  unsigned_int_type(Vec8h)  { return 0; }
+#if MAX_VECTOR_SIZE >= 256
+static inline Vec16us unsigned_int_type(Vec16h) { return 0; }
+#endif
+#if MAX_VECTOR_SIZE >= 512
+static inline Vec32us unsigned_int_type(Vec32h) { return 0; }
+#endif
+
+
+// Template for trigonometric functions.
+// Template parameters:
+// VTYPE:  vector type
+// SC:     1 = sin, 2 = cos, 3 = sincos, 4 = tan, 8 = multiply by pi
+// Parameters:
+// xx = input x (radians)
+// cosret = return pointer (only if SC = 3)
+template<typename VTYPE, int SC>
+VTYPE sincos_h(VTYPE * cosret, VTYPE const xx) {
+
+    // define constants
+    const _Float16 dp1h = _Float16(1.57031250f);           // pi/2 with lower bits of mantissa removed
+    const _Float16 dp2h = _Float16(1.57079632679489661923 - dp1h); // remaining bits
+
+    const _Float16 P0sinf = _Float16(-1.6666654611E-1f);   // Taylor coefficients
+    const _Float16 P1sinf = _Float16(8.3321608736E-3f);
+
+    const _Float16 P0cosf = _Float16(4.166664568298827E-2f);
+    const _Float16 P1cosf = _Float16(-1.388731625493765E-3f);
+
+    const float pi     = 3.14159265358979323846f;// pi
+    const _Float16 c2_pi  = _Float16(2./3.14159265358979323846);  // 2/pi
+
+    typedef decltype(roundi(xx)) ITYPE;          // integer vector type
+    typedef decltype(unsigned_int_type(xx)) UITYPE;// unsigned integer vector type
+    typedef decltype(xx < xx) BVTYPE;            // boolean vector type
+
+    VTYPE  xa, x, y, x2, s, c, sin1, cos1;       // data vectors
+    ITYPE  signsin, signcos;                     // integer vectors
+    UITYPE q;                                    // unsigned integer vector for quadrant
+    BVTYPE swap, overflow;                       // boolean vectors
+
+    xa = abs(xx);
+
+    // Find quadrant
+    if constexpr ((SC & 8) != 0) {               // sinpi
+        xa = select(xa > VTYPE(32000.f), VTYPE(0.f), xa); // avoid overflow when multiplying by 2
+        y = round(xa * VTYPE(2.0f)); 
+    }
+    else {                                       // sin
+        xa = select(xa > VTYPE(314.25f), VTYPE(0.f), xa); // avoid meaningless results for high x
+        y = round(xa * c2_pi);                   // quadrant, as float
+    }
+
+    q = UITYPE(roundi(y));                       // quadrant, as unsigned integer
+    //      0 -   pi/4 => 0
+    //   pi/4 - 3*pi/4 => 1
+    // 3*pi/4 - 5*pi/4 => 2
+    // 5*pi/4 - 7*pi/4 => 3
+    // 7*pi/4 - 8*pi/4 => 4
+
+    if constexpr ((SC & 8) != 0) {               // sinpi
+        // modulo 2: subtract 0.5*y
+        x = nmul_add(y, VTYPE(0.5f), xa) * VTYPE(pi);
+    }
+    else {                                       // sin
+        // Reduce by extended precision modular arithmetic    
+        x = nmul_add(y, dp2h, nmul_add(y, dp1h, xa)); 
+    }
+
+    // Taylor expansion of sin and cos, valid for -pi/4 <= x <= pi/4
+    x2 = x * x;
+
+    //x2 = select(is_inf(xx), reinterpret_h(UITYPE(0x7F00)), x2);  // return NAN rather than INF if INF input
+
+    s = mul_add(x2, P1sinf, P0sinf) * (x*x2) + x;
+    c = mul_add(x2, P1cosf, P0cosf) * (x2*x2) + nmul_add(_Float16(0.5), x2, _Float16(1.0f));
+    // s = P0sinf * (x*x2) + x;  // 2 ULP error
+    // c = P0cosf * (x2*x2) + nmul_add(0.5f, x2, 1.0f);  // 2 ULP error
+
+    // swap sin and cos if odd quadrant
+    swap = BVTYPE((q & 1) != 0);
+
+    if constexpr ((SC & 5) != 0) {               // get sin
+        sin1 = select(swap, c, s);
+        signsin = ((q << 14) ^ ITYPE(reinterpret_i(xx))); // sign
+        sin1 = sign_combine(sin1, VTYPE(reinterpret_h(signsin)));
+    }
+
+    if constexpr ((SC & 6) != 0) {               // get cos
+        cos1 = select(swap, s, c);
+        signcos = ((q + 1) & 2) << 14;           // sign
+        cos1 ^= reinterpret_h(signcos);
+    }
+    if      constexpr ((SC & 7) == 1) return sin1;
+    else if constexpr ((SC & 7) == 2) return cos1;
+    else if constexpr ((SC & 7) == 3) {          // both sin and cos. cos returned through pointer
+        *cosret = cos1;
+        return sin1;
+    }
+    else {                                       // (SC & 7) == 4. tan
+        if constexpr (SC == 12) {
+            // tanpi can give INF result, tan cannot. Get the right sign of INF result according to IEEE 754-2019
+            cos1 = select(cos1 == VTYPE(0.f), VTYPE(0.f), cos1); // remove sign of 0
+            // the sign of zero output is arbitrary. fixing it would be a waste of code
+        }
+        return sin1 / cos1;
+    }
+}
+
+// instantiations of math function templates
+
+static inline Vec8h exp(Vec8h const x) {
+    return exp_h<Vec8h, 0, 0>(x);
+} 
+static inline Vec8h exp2(Vec8h const x) {
+    return exp_h<Vec8h, 0, 2>(x);
+}
+static inline Vec8h exp10(Vec8h const x) {
+    return exp_h<Vec8h, 0, 10>(x);
+}
+static inline Vec8h expm1(Vec8h const x) {
+    return exp_h<Vec8h, 1, 0>(x);
+}
+static inline Vec8h sin(Vec8h const x) {
+    return sincos_h<Vec8h, 1>(0, x);
+}
+static inline Vec8h cos(Vec8h const x) {
+    return sincos_h<Vec8h, 2>(0, x);
+}
+static inline Vec8h sincos(Vec8h * cosret, Vec8h const x) {
+    return sincos_h<Vec8h, 3>(cosret, x);
+} 
+static inline Vec8h tan(Vec8h const x) {
+    return sincos_h<Vec8h, 4>(0, x);
+}
+
+static inline Vec8h sinpi(Vec8h const x) {
+    return sincos_h<Vec8h, 9>(0, x);
+}
+static inline Vec8h cospi(Vec8h const x) {
+    return sincos_h<Vec8h, 10>(0, x);
+}
+static inline Vec8h sincospi(Vec8h * cosret, Vec8h const x) {
+    return sincos_h<Vec8h, 11>(cosret, x);
+}
+static inline Vec8h tanpi(Vec8h const x) {
+    return sincos_h<Vec8h, 12>(0, x);
+}
+
+#if MAX_VECTOR_SIZE >= 256
+
+static inline Vec16h exp(Vec16h const x) {
+    return exp_h<Vec16h, 0, 0>(x);
+}
+static inline Vec16h exp2(Vec16h const x) {
+    return exp_h<Vec16h, 0, 2>(x);
+}
+static inline Vec16h exp10(Vec16h const x) {
+    return exp_h<Vec16h, 0, 10>(x);
+}
+static inline Vec16h expm1(Vec16h const x) {
+    return exp_h<Vec16h, 1, 0>(x);
+} 
+static inline Vec16h sin(Vec16h const x) {
+    return sincos_h<Vec16h, 1>(0, x);
+}
+static inline Vec16h cos(Vec16h const x) {
+    return sincos_h<Vec16h, 2>(0, x);
+}
+static inline Vec16h sincos(Vec16h * cosret, Vec16h const x) {
+    return sincos_h<Vec16h, 3>(cosret, x);
+} 
+static inline Vec16h tan(Vec16h const x) {
+    return sincos_h<Vec16h, 4>(0, x);
+}
+static inline Vec16h sinpi(Vec16h const x) {
+    return sincos_h<Vec16h, 9>(0, x);
+}
+static inline Vec16h cospi(Vec16h const x) {
+    return sincos_h<Vec16h, 10>(0, x);
+}
+static inline Vec16h sincospi(Vec16h * cosret, Vec16h const x) {
+    return sincos_h<Vec16h, 11>(cosret, x);
+} 
+static inline Vec16h tanpi(Vec16h const x) {
+    return sincos_h<Vec16h, 12>(0, x);
+}
+
+#endif  // MAX_VECTOR_SIZE >= 256
+
+#if MAX_VECTOR_SIZE >= 512
+
+static inline Vec32h exp(Vec32h const x) {
+    return exp_h<Vec32h, 0, 0>(x);
+}
+static inline Vec32h exp2(Vec32h const x) {
+    return exp_h<Vec32h, 0, 2>(x);
+}
+static inline Vec32h exp10(Vec32h const x) {
+    return exp_h<Vec32h, 0, 10>(x);
+}
+static inline Vec32h expm1(Vec32h const x) {
+    return exp_h<Vec32h, 1, 0>(x);
+}
+static inline Vec32h sin(Vec32h const x) {
+    return sincos_h<Vec32h, 1>(0, x);
+}
+static inline Vec32h cos(Vec32h const x) {
+    return sincos_h<Vec32h, 2>(0, x);
+}
+static inline Vec32h sincos(Vec32h * cosret, Vec32h const x) {
+    return sincos_h<Vec32h, 3>(cosret, x);
+} 
+static inline Vec32h tan(Vec32h const x) {
+    return sincos_h<Vec32h, 4>(0, x);
+}
+static inline Vec32h sinpi(Vec32h const x) {
+    return sincos_h<Vec32h, 9>(0, x);
+}
+static inline Vec32h cospi(Vec32h const x) {
+    return sincos_h<Vec32h, 10>(0, x);
+}
+static inline Vec32h sincospi(Vec32h * cosret, Vec32h const x) {
+    return sincos_h<Vec32h, 11>(cosret, x);
+} 
+static inline Vec32h tanpi(Vec32h const x) {
+    return sincos_h<Vec32h, 12>(0, x);
+}
+
+#endif  // MAX_VECTOR_SIZE >= 512
+
+
+#ifdef VCL_NAMESPACE
+}
+#endif
+
+#endif // defined(__AVX512FP16__)
+
+#endif // VECTORFP16_H
diff --git a/EEDI3/vectorclass/vectorfp16e.h b/EEDI3/vectorclass/vectorfp16e.h
new file mode 100644
index 0000000..5664017
--- /dev/null
+++ b/EEDI3/vectorclass/vectorfp16e.h
@@ -0,0 +1,3248 @@
+/****************************  vectorfp16e.h   *******************************
+* Author:        Agner Fog
+* Date created:  2022-05-03
+* Last modified: 2023-10-19
+* Version:       2.02.01
+* Project:       vector class library
+* Description:
+* Header file emulating half precision floating point vector classes
+* when instruction set AVX512_FP16 is not defined
+*
+* Instructions: see vcl_manual.pdf
+*
+* The following vector classes are defined here:
+* Vec8h     Vector of 8 half precision floating point numbers in 128 bit vector
+* Vec16h    Vector of 16 half precision floating point numbers in 256 bit vector
+* Vec32h    Vector of 32 half precision floating point numbers in 512 bit vector
+*
+* This header file defines operators and functions for these vectors.
+*
+* (c) Copyright 2012-2023 Agner Fog.
+* Apache License version 2.0 or later.
+*****************************************************************************/
+
+#ifndef VECTORFP16E_H
+#define VECTORFP16E_H
+
+#ifndef VECTORCLASS_H
+#include "vectorclass.h"
+#endif
+
+#if VECTORCLASS_H < 20200
+#error Incompatible versions of vector class library mixed
+#endif
+
+#if MAX_VECTOR_SIZE < 256
+#error Emulation of half precision floating point not supported for MAX_VECTOR_SIZE < 256
+#endif
+
+#ifdef VCL_NAMESPACE
+namespace VCL_NAMESPACE {
+#endif
+
+
+/*****************************************************************************
+*
+*        Float16: Use _Float16 if it is defined, or emulate it if not
+*
+*****************************************************************************/
+
+
+// test if _Float16 is defined
+#if defined(FLT16_MAX) || defined(__FLT16_MAX__)
+    // _Float16 is defined. 
+    typedef _Float16 Float16;
+    
+    // Define bit-casting between uint16_t <-> Float16
+    static inline uint16_t castfp162s(Float16 x) {
+        union {
+            Float16 f;
+            uint16_t i;
+        } u;
+        u.f = x;
+        return u.i;
+    }
+    static inline Float16 casts2fp16(uint16_t x) {
+        union {
+            uint16_t i;
+            Float16 f;
+        } u;
+        u.i = x;
+        return u.f;
+    }
+#else
+    // _Float16 is not defined
+    // define Float16 as a class with constructor, operators, etc. to avoid operators like + from treating Float16 like integer
+    class Float16 {
+    protected:
+        uint16_t x;
+    public:
+    // Default constructor:
+        Float16() = default;
+#ifdef __F16C__   // F16C instruction set includes conversion instructions
+    Float16(float f) { // Constructor to convert float to fp16
+        //x = uint16_t(_mm_cvtsi128_si32(_mm_cvtps_ph(_mm_set1_ps(f), _MM_FROUND_NO_EXC))); // requires __AVX512FP16__
+        x = uint16_t(_mm_cvtsi128_si32(_mm_cvtps_ph(_mm_set1_ps(f), 0)));
+    }
+    operator float() const {                     // Type cast operator to convert fp16 to float
+        return _mm_cvtss_f32(_mm_cvtph_ps(_mm_set1_epi32(x)));
+    }
+
+#else  // F16C instruction set not supported. Make conversion functions
+    Float16(float f) {                           // Constructor to convert float to fp16
+        union {                                  // single precision float as bitfield
+            float f;
+            struct {
+                uint32_t mant : 23;
+                uint32_t expo : 8;
+                uint32_t sign : 1;
+            };
+        } u;
+        union {                                  // half precision float as bitfield
+            uint16_t h;
+            struct {
+                uint16_t mant : 10;
+                uint16_t expo : 5;
+                uint16_t sign : 1;
+            };
+        } v;
+        u.f = f;
+        v.expo = u.expo - 0x70;                  // convert exponent
+        v.mant = u.mant >> 13;                   // get upper part of mantissa
+        if (u.mant & (1 << 12)) {                // round to nearest or even
+            if ((u.mant & ((1 << 12) - 1)) || (v.mant & 1)) { // round up if odd or remaining bits are nonzero
+                v.h++;                           // overflow here will give infinity
+            }
+        }
+        v.sign = u.sign;                         // copy sign bit
+        if (u.expo == 0xFF) {                    // infinity or nan
+            v.expo = 0x1F;
+            if (u.mant != 0) {                   // Nan
+                v.mant = u.mant >> 13;           // NAN payload is left-justified
+            }
+        }
+        else if (u.expo > 0x8E) {
+            v.expo = 0x1F;  v.mant = 0;          // overflow -> inf
+        }
+        else if (u.expo < 0x71) {
+            v.expo = 0;                          // subnormals are always supported
+            u.expo += 24;
+            u.sign = 0;
+            //v.mant = int(u.f) & 0x3FF;
+            int mants = _mm_cvt_ss2si(_mm_load_ss(&u.f));
+            v.mant = mants & 0x3FF; // proper rounding of subnormal
+            if (mants == 0x400) v.expo = 1;
+        }
+        x = v.h;                                 // store result
+    }    
+    operator float() const {                     // Type cast operator to convert fp16 to float
+        union {
+            uint32_t hhh;
+            float fff;
+            struct {
+                uint32_t mant : 23;
+                uint32_t expo : 8;
+                uint32_t sign : 1;
+            };
+        } u;
+        u.hhh = (x & 0x7fff) << 13;              // Exponent and mantissa
+        u.hhh += 0x38000000;                     // Adjust exponent bias
+        if ((x & 0x7C00) == 0) {                 // Subnormal or zero
+            u.hhh = 0x3F800000 - (24 << 23);     // 2^-24
+            u.fff *= int(x & 0x3FF);             // subnormal value = mantissa * 2^-24
+        }
+        if ((x & 0x7C00) == 0x7C00) {            // infinity or nan
+            u.expo = 0xFF;
+            if (x & 0x3FF) {                     // nan
+                u.mant = (x & 0x3FF) << 13;      // NAN payload is left-justified
+            }
+        }
+        u.hhh |= (x & 0x8000) << 16;             // copy sign bit
+        return u.fff;
+    } 
+#endif  // F16C supported
+
+    void setBits(uint16_t a) {
+        x = a;
+    }
+    uint16_t getBits() const {
+        return x;
+    }
+    };
+
+    static inline int16_t castfp162s(Float16 a) {
+        return a.getBits();
+    }
+    static inline Float16 casts2fp16(int16_t a) {
+        Float16 f;
+        f.setBits(a);
+        return f;
+    }
+
+    // Define operators for Float16 emulation class
+
+    static inline Float16 operator + (Float16 const a, Float16 const b) {
+        return Float16(float(a) + float(b));
+    }
+    static inline Float16 operator - (Float16 const a, Float16 const b) {
+        return Float16(float(a) - float(b));
+    }
+    static inline Float16 operator * (Float16 const a, Float16 const b) {
+        return Float16(float(a) * float(b));
+    }
+    static inline Float16 operator / (Float16 const a, Float16 const b) {
+        return Float16(float(a) / float(b));
+    }
+    static inline Float16 operator - (Float16 const a) {
+        return casts2fp16(castfp162s(a) ^ 0x8000);
+    }
+    static inline bool operator == (Float16 const a, Float16 const b) {
+        return float(a) == float(b);
+    }
+    static inline bool operator != (Float16 const a, Float16 const b) {
+        return float(a) != float(b);
+    }
+    static inline bool operator < (Float16 const a, Float16 const b) {
+        return float(a) < float(b);
+    }
+    static inline bool operator <= (Float16 const a, Float16 const b) {
+        return float(a) <= float(b);
+    }
+    static inline bool operator > (Float16 const a, Float16 const b) {
+        return float(a) > float(b);
+    }
+    static inline bool operator >= (Float16 const a, Float16 const b) {
+        return float(a) >= float(b);
+    }
+
+#endif  // Float16 defined
+
+
+/*****************************************************************************
+*
+*          Vec8hb: Vector of 8 Booleans for use with Vec8h
+*
+*****************************************************************************/
+
+#if INSTRSET >= 10
+typedef Vec8b Vec8hb;   // compact boolean vector
+static inline Vec8hb Vec8fb2hb (Vec8fb const a) {
+    return a;
+}
+#else
+typedef Vec8sb Vec8hb;  // broad boolean vector
+static inline Vec8hb Vec8fb2hb (Vec8fb const a) {
+    // boolean vector needs compression from 32 bits to 16 bits per element
+    Vec4ib lo = reinterpret_i(a.get_low());
+    Vec4ib hi = reinterpret_i(a.get_high());
+    return _mm_packs_epi32(lo, hi);
+}
+#endif
+
+
+/*****************************************************************************
+*
+*          Vec8h: Vector of 8 half precision floating point values
+*
+*****************************************************************************/
+
+class Vec8h {
+protected:
+    __m128i xmm; // Float vector
+public:
+    // Default constructor:
+    Vec8h() = default;
+    // Constructor to broadcast the same value into all elements:
+    Vec8h(Float16 f) {
+        xmm = _mm_set1_epi16 (castfp162s(f));
+    }
+    // Constructor to build from all elements:
+    Vec8h(Float16 f0, Float16 f1, Float16 f2, Float16 f3, Float16 f4, Float16 f5, Float16 f6, Float16 f7) {
+        xmm = _mm_setr_epi16 (castfp162s(f0), castfp162s(f1), castfp162s(f2), castfp162s(f3), castfp162s(f4), castfp162s(f5), castfp162s(f6), castfp162s(f7));
+    }
+    // Constructor to convert from type __m128i used in intrinsics:
+    Vec8h(__m128i const x) {
+        xmm = x;
+    }
+    // Assignment operator to convert from type __m128i used in intrinsics:
+    Vec8h & operator = (__m128i const x) {
+        xmm = x;
+        return *this;
+    }
+    // Type cast operator to convert to __m128i used in intrinsics
+    operator __m128i() const {
+        return xmm;
+    }
+    // Member function to load from array (unaligned)
+    Vec8h & load(void const * p) {
+        xmm = _mm_loadu_si128 ((const __m128i *)p);
+        return *this;
+    }
+    // Member function to load from array, aligned by 16
+    // You may use load_a instead of load if you are certain that p points to an address
+    // divisible by 16. In most cases there is no difference in speed between load and load_a
+    Vec8h & load_a(void const * p) {
+        xmm = _mm_load_si128 ((const __m128i *)p);
+        return *this;
+    }
+    // Member function to store into array (unaligned)
+    void store(void * p) const {
+        _mm_storeu_si128 ((__m128i *)p, xmm);
+    }
+    // Member function storing into array, aligned by 16
+    // You may use store_a instead of store if you are certain that p points to an address
+    // divisible by 16.
+    void store_a(void * p) const {
+        _mm_store_si128 ((__m128i *)p, xmm);
+    }
+    // Member function storing to aligned uncached memory (non-temporal store).
+    // This may be more efficient than store_a when storing large blocks of memory if it 
+    // is unlikely that the data will stay in the cache until it is read again.
+    // Note: Will generate runtime error if p is not aligned by 16
+    void store_nt(void * p) const {
+        _mm_stream_si128((__m128i*)p, xmm);
+    }
+    // Partial load. Load n elements and set the rest to 0
+    Vec8h & load_partial(int n, void const * p) {
+        xmm = Vec8s().load_partial(n, p);
+        return *this;
+    }
+    // Partial store. Store n elements
+    void store_partial(int n, void * p) const {
+        Vec8s(xmm).store_partial(n, p);
+    }
+    // cut off vector to n elements. The last 8-n elements are set to zero
+    Vec8h & cutoff(int n) {
+        xmm = Vec8s(xmm).cutoff(n);
+        return *this;
+    }
+    // Member function to change a single element in vector
+    Vec8h const insert(int index, Float16 a) {
+        xmm = Vec8s(xmm).insert(index, castfp162s(a));
+        return *this;
+    }
+    // Member function extract a single element from vector
+    Float16 extract(int index) const {
+        Float16 y;
+        y = casts2fp16(Vec8s(xmm).extract(index));
+        return y;
+    }
+    // Extract a single element. Use store function if extracting more than one element.
+    // Operator [] can only read an element, not write.
+    Float16 operator [] (int index) const {
+        return extract(index);
+    }
+    static constexpr int size() {
+        return 8;
+    }
+    static constexpr int elementtype() {
+        return 15;
+    }
+    typedef __m128i registertype;
+};
+
+/*****************************************************************************
+*
+*          conversions Vec8h <-> Vec4f
+*
+*****************************************************************************/
+
+#ifdef __F16C__    // F16C instruction set has conversion instructions
+
+// extend precision: Vec8h -> Vec4f. upper half ignored
+static inline Vec4f convert8h_4f (Vec8h h) {
+    return _mm_cvtph_ps(h);
+}
+
+// reduce precision: Vec4f -> Vec8h. upper half zero
+static inline Vec8h convert4f_8h (Vec4f f) {
+    return _mm_cvtps_ph(f, 0);
+}
+
+#else
+
+// extend precision: Vec8h -> Vec4f. upper half ignored
+static Vec4f convert8h_4f (Vec8h x) {
+    // __m128i a = _mm_cvtepu16_epi32(x);                            // SSE4.1
+    __m128i a = _mm_unpacklo_epi16(x, _mm_setzero_si128 ());         // zero extend
+    __m128i b = _mm_slli_epi32(a, 16);                               // left-justify
+    __m128i c = _mm_and_si128(b, _mm_set1_epi32(0x80000000));        // isolate sign bit
+    __m128i d = _mm_andnot_si128(_mm_set1_epi32(0x80000000),b);      // remove sign bit
+    __m128i e = _mm_srli_epi32(d, 3);                                // put exponent and mantissa in place
+    __m128i f = _mm_add_epi32(e, _mm_set1_epi32(0x38000000));        // adjust exponent bias
+    // check for subnormal, INF, and NAN
+    __m128i xx = _mm_set1_epi32(0x7C00);                             // exponent field in fp16
+    __m128i g  = _mm_and_si128(a, xx);                               // isolate exponent (low position)
+    __m128i zd = _mm_cmpeq_epi32(g, _mm_setzero_si128());            // -1 if x is zero or subnormal
+    __m128i in = _mm_cmpeq_epi32(g, xx);                             // -1 if x is INF or NAN
+    __m128i ma = _mm_and_si128(a, _mm_set1_epi32(0x3FF));            // isolate mantissa
+    __m128  sn = _mm_mul_ps(_mm_cvtepi32_ps(ma), _mm_set1_ps(1.f/16777216.f)); // converted subnormal = mantissa * 2^-24
+    __m128i snm = _mm_and_si128(_mm_castps_si128(sn), zd);           // converted subnormal, masked
+    __m128i inm = _mm_and_si128(in,_mm_set1_epi32(0x7F800000));      // INF or NAN exponent field, masked off if not INF or NAN
+    __m128i fm = _mm_andnot_si128(zd, f);                            // normal result, masked off if zero or subnormal
+    __m128i r = _mm_or_si128(fm, c);                                 // insert sign bit
+    __m128i s = _mm_or_si128(snm, inm);                              // combine branches
+    __m128i t = _mm_or_si128(r, s);                                  // combine branches
+    return _mm_castsi128_ps(t);                                      // cast result to float
+}
+
+// reduce precision: Vec4f -> Vec8h. upper half zero
+static Vec8h convert4f_8h (Vec4f x) {
+    __m128i a = _mm_castps_si128(x);                                 // bit-cast to integer
+    // 23 bit mantissa rounded to 10 bits - nearest or even
+    __m128i r = _mm_srli_epi32(a, 12);                               // get first discarded mantissa bit
+    __m128i s = _mm_and_si128(a, _mm_set1_epi32(0x2FFF));            // 0x2000 indicates if odd, 0x0FFF if remaining bits are nonzero
+    __m128i u = _mm_cmpeq_epi32(s, _mm_setzero_si128());             // false if odd or remaining bits nonzero
+    __m128i v = _mm_andnot_si128(u, r);                              // bit 0 = 1 if we have to round up
+    __m128i w = _mm_and_si128(v, _mm_set1_epi32(1));                 // = 1 if we need to round up
+    __m128i m = _mm_srli_epi32(a, 13);                               // get mantissa in place
+    __m128i n = _mm_and_si128(m, _mm_set1_epi32(0x3FF));             // mantissa isolated
+    __m128i e = _mm_and_si128(a, _mm_set1_epi32(0x7FFFFFFF));        // remove sign bit
+    __m128i f = _mm_sub_epi32(e, _mm_set1_epi32(0x70 << 23));        // adjust exponent bias (underflow will be caught by uu below)
+    __m128i g = _mm_srli_epi32(f, 13);                               // shift exponent into new place
+    __m128i h = _mm_and_si128(g, _mm_set1_epi32(0x3FC00));           // isolate exponent 
+    __m128i i = _mm_or_si128(n, h);                                  // combine exponent and mantissa
+    Vec4i   j = _mm_add_epi32(i, w);                                 // round mantissa. Overflow will carry into exponent
+    // check for overflow and underflow
+    Vec4ib  k  = j > 0x7BFF;                                         // overflow
+    Vec4i   ee = _mm_srli_epi32(e, 23);                              // exponent at position 0
+    Vec4ib  ii = ee == 0xFF;                                         // check for INF and NAN
+    Vec4ib  uu = ee < 0x71;                                          // check for exponent underflow
+    __m128i pp = _mm_or_si128(j, _mm_set1_epi32(0x7C00));            // insert exponent if INF or NAN
+    // compute potential subnormal result
+    __m128i ss = _mm_add_epi32(e, _mm_set1_epi32(24 << 23));         // add 24 to exponent
+    __m128i tt = _mm_cvtps_epi32(_mm_castsi128_ps(ss));              // convert float to int with rounding
+    __m128i vv = _mm_and_si128(tt, _mm_set1_epi32(0x3FF));           // mantissa of subnormal number
+    // combine results   
+    Vec4i  bb = select(k, 0x7C00, j);                                // select INF if overflow
+    Vec4i  dd = select(ii, pp, bb);                                  // select INF or NAN    
+    Vec4i  cc = select(uu, vv, dd);                                  // select if subnormal or zero or exponent underflow
+    // get sign bit
+    Vec4i  sa = Vec4i(a) >> 16;                                      // extend sign bit to avoid saturation in pack instruction below
+    Vec4i  const smask = 0xFFFF8000;                                 // extended sign mask
+    Vec4i  sb = sa & smask;                                          // isolate sign
+    Vec4i  sc = _mm_andnot_si128(smask, cc);                         // isolate exponent and mantissa
+    Vec4i  rr = sb | sc;                                             // combine with sign
+    Vec4i  rc  = _mm_packs_epi32(rr, _mm_setzero_si128());           // pack into 16-bit words (words are sign extended so they will not saturate)
+    return (__m128i)rc;                                              // return as Vec8h
+} 
+
+#endif
+
+/*****************************************************************************
+*
+*          conversions Vec8h <-> Vec8f
+*
+*****************************************************************************/
+#if defined (__F16C__) && INSTRSET >= 8  // F16C instruction set has conversion instructions
+
+// extend precision: Vec8h -> Vec8f
+static inline Vec8f to_float (Vec8h h) {
+    return _mm256_cvtph_ps(h);
+}
+
+// reduce precision: Vec8f -> Vec8h
+static inline Vec8h to_float16 (Vec8f f) {
+    return _mm256_cvtps_ph(f, 0);
+}
+
+#elif INSTRSET >= 8 // __F16C__ not defined, AVX2 supported
+
+// extend precision: Vec8h -> Vec8f
+static Vec8f to_float (Vec8h x) {
+    __m256i a = _mm256_cvtepu16_epi32(x);                            // zero-extend each element to 32 bits
+    __m256i b = _mm256_slli_epi32(a, 16);                            // left-justify
+    __m256i c = _mm256_and_si256(b, _mm256_set1_epi32(0x80000000));  // isolate sign bit
+    __m256i d = _mm256_andnot_si256(_mm256_set1_epi32(0x80000000),b);// remove sign bit
+    __m256i e = _mm256_srli_epi32(d, 3);                             // put exponent and mantissa in place
+    __m256i f = _mm256_add_epi32(e, _mm256_set1_epi32(0x38000000));  // adjust exponent bias
+    // check for subnormal, INF, and NAN
+    __m256i xx = _mm256_set1_epi32(0x7C00);                          // exponent field in fp16
+    __m256i g  = _mm256_and_si256(a, xx);                            // isolate exponent (low position)
+    __m256i zd = _mm256_cmpeq_epi32(g, _mm256_setzero_si256());      // -1 if x is zero or subnormal
+    __m256i in = _mm256_cmpeq_epi32(g, xx);                          // -1 if x is INF or NAN
+    __m256i ma = _mm256_and_si256(a, _mm256_set1_epi32(0x3FF));      // isolate mantissa
+    __m256  sn = _mm256_mul_ps(_mm256_cvtepi32_ps(ma), _mm256_set1_ps(1.f/16777216.f)); // converted subnormal = mantissa * 2^-24
+    __m256i snm = _mm256_and_si256(_mm256_castps_si256(sn), zd);     // converted subnormal, masked
+    __m256i inm = _mm256_and_si256(in,_mm256_set1_epi32(0x7F800000));// INF or NAN exponent field, masked off if not INF or NAN
+    __m256i fm = _mm256_andnot_si256(zd, f);                         // normal result, masked off if zero or subnormal
+    __m256i r = _mm256_or_si256(fm, c);                              // insert sign bit
+    __m256i s = _mm256_or_si256(snm, inm);                           // combine branches
+    __m256i t = _mm256_or_si256(r, s);                               // combine branches
+    return _mm256_castsi256_ps(t);                                   // cast result to float
+}
+
+// reduce precision: Vec8f -> Vec8h
+static Vec8h to_float16 (Vec8f x) {
+    __m256i a = _mm256_castps_si256(x);                              // bit-cast to integer
+    // 23 bit mantissa rounded to 10 bits - nearest or even
+    __m256i r = _mm256_srli_epi32(a, 12);                            // get first discarded mantissa bit
+    __m256i s = _mm256_and_si256(a, _mm256_set1_epi32(0x2FFF));      // 0x2000 indicates if odd, 0x0FFF if remaining bits are nonzero
+    __m256i u = _mm256_cmpeq_epi32(s, _mm256_setzero_si256());       // false if odd or remaining bits nonzero
+    __m256i v = _mm256_andnot_si256(u, r);                           // bit 0 = 1 if we have to round up
+    __m256i w = _mm256_and_si256(v, _mm256_set1_epi32(1));           // = 1 if we need to round up
+    __m256i m = _mm256_srli_epi32(a, 13);                            // get mantissa in place
+    __m256i n = _mm256_and_si256(m, _mm256_set1_epi32(0x3FF));       // mantissa isolated
+    __m256i e = _mm256_and_si256(a, _mm256_set1_epi32(0x7FFFFFFF));  // remove sign bit
+    __m256i f = _mm256_sub_epi32(e, _mm256_set1_epi32(0x70 << 23));  // adjust exponent bias (underflow will be caught by uu below)
+    __m256i g = _mm256_srli_epi32(f, 13);                            // shift exponent into new place
+    __m256i h = _mm256_and_si256(g, _mm256_set1_epi32(0x3FC00));     // isolate exponent 
+    __m256i i = _mm256_or_si256(n, h);                               // combine exponent and mantissa
+    __m256i j = _mm256_add_epi32(i, w);                              // round mantissa. Overflow will carry into exponent
+    // check for overflow and underflow
+    __m256i k = _mm256_cmpgt_epi32(j, _mm256_set1_epi32(0x7BFF));    // overflow
+    __m256i ee = _mm256_srli_epi32(e, 23);                           // exponent at position 0
+    __m256i ii = _mm256_cmpeq_epi32(ee, _mm256_set1_epi32(0xFF));    // check for INF and NAN
+    __m256i uu = _mm256_cmpgt_epi32(_mm256_set1_epi32(0x71), ee);    // check for exponent underflow
+    __m256i pp = _mm256_or_si256(j, _mm256_set1_epi32(0x7C00));      // insert exponent if INF or NAN
+    // compute potential subnormal result
+    __m256i ss = _mm256_add_epi32(e, _mm256_set1_epi32(24 << 23));   // add 24 to exponent
+    __m256i tt = _mm256_cvtps_epi32(_mm256_castsi256_ps(ss));        // convert float to int with rounding
+    __m256i vv = _mm256_and_si256(tt, _mm256_set1_epi32(0x7FF));     // mantissa of subnormal number (possible overflow to normal)
+    // combine results
+    __m256i bb = _mm256_blendv_epi8(j, _mm256_set1_epi32(0x7C00), k);// select INF if overflow
+    __m256i dd = _mm256_blendv_epi8(bb, pp, ii);                     // select INF or NAN    
+    __m256i cc = _mm256_blendv_epi8(dd, vv, uu);                     // select if subnormal or zero or exponent underflow
+    __m256i sa = _mm256_srai_epi32(a, 16);                           // extend sign bit to avoid saturation in pack instruction below
+    __m256i sb = _mm256_and_si256(sa, _mm256_set1_epi32(0xFFFF8000));// isolate sign
+    __m256i sc = _mm256_andnot_si256(_mm256_set1_epi32(0xFFFF8000), cc);// isolate exponent and mantissa
+    __m256i rr = _mm256_or_si256(sb, sc);                            // combine with sign
+    __m128i rl = _mm256_castsi256_si128(rr);                         // low half of results
+    __m128i rh = _mm256_extractf128_si256(rr, 1);                    // high half of results
+    __m128i rc = _mm_packs_epi32(rl, rh);                            // pack into 16-bit words (words are sign extended so they will not saturate)
+    return  rc;                                                      // return as Vec8h
+} 
+
+#else // __F16C__ not defined, AVX2 not supported 
+
+// extend precision: Vec8h -> Vec8f
+static Vec8f to_float (Vec8h x) {
+    Vec8s  xx = __m128i(x);
+    Vec4ui a1 = _mm_unpacklo_epi16(xx, _mm_setzero_si128 ());
+    Vec4ui a2 = _mm_unpackhi_epi16(xx, _mm_setzero_si128 ());
+    Vec4ui b1 = a1 << 16;                        // left-justify
+    Vec4ui b2 = a2 << 16;
+    Vec4ui c1 = b1 & 0x80000000;                 // isolate sign bit
+    Vec4ui c2 = b2 & 0x80000000;
+    Vec4ui d1 = _mm_andnot_si128(Vec4ui(0x80000000), b1); // remove sign bit
+    Vec4ui d2 = _mm_andnot_si128(Vec4ui(0x80000000), b2);
+    Vec4ui e1 = d1 >> 3;                         // put exponent and mantissa in place
+    Vec4ui e2 = d2 >> 3;
+    Vec4ui f1 = e1 + 0x38000000;                 // adjust exponent bias
+    Vec4ui f2 = e2 + 0x38000000;
+    Vec4ui g1 = a1 & 0x7C00;                     // isolate exponent (low position)
+    Vec4ui g2 = a2 & 0x7C00;
+    Vec4ib z1 = g1 == 0;                         // true if x is zero or subnormal (broad boolean vector)
+    Vec4ib z2 = g2 == 0;
+    Vec4ib i1 = g1 == 0x7C00;                    // true if x is INF or NAN
+    Vec4ib i2 = g2 == 0x7C00;
+    Vec4ui m1 = a1 & 0x3FF;                      // isolate mantissa (low position)
+    Vec4ui m2 = a2 & 0x3FF;
+    Vec4f  s1 = to_float(m1) * (1.f/16777216.f); // converted subnormal = mantissa * 2^-24
+    Vec4f  s2 = to_float(m2) * (1.f/16777216.f);
+    Vec4ui sm1 = Vec4ui(reinterpret_i(s1)) & Vec4ui(z1); // converted subnormal, masked
+    Vec4ui sm2 = Vec4ui(reinterpret_i(s2)) & Vec4ui(z2);
+    Vec4ui inm1 = Vec4ui(i1) & Vec4ui(0x7F800000); // INF or NAN exponent field, masked off if not INF or NAN 
+    Vec4ui inm2 = Vec4ui(i2) & Vec4ui(0x7F800000);
+    Vec4ui fm1 = _mm_andnot_si128(Vec4ui(z1), f1); // normal result, masked off if zero or subnormal
+    Vec4ui fm2 = _mm_andnot_si128(Vec4ui(z2), f2);
+    Vec4ui r1 = fm1 | c1;                        // insert sign bit
+    Vec4ui r2 = fm2 | c2;
+    Vec4ui q1 = sm1 | inm1;                      // combine branches
+    Vec4ui q2 = sm2 | inm2;
+    Vec4ui t1 = r1  | q1;                        // combine branches
+    Vec4ui t2 = r2  | q2;
+    Vec4f  u1 = reinterpret_f(t1);               // bit-cast to float
+    Vec4f  u2 = reinterpret_f(t2);
+    return Vec8f(u1, u2);                        // combine low and high part
+} 
+
+// reduce precision: Vec8f -> Vec8h
+static Vec8h to_float16 (Vec8f x) {              
+    Vec4ui a1 = _mm_castps_si128(x.get_low());             // low half
+    Vec4ui a2 = _mm_castps_si128(x.get_high());            // high half
+    Vec4ui r1 = a1 >> 12;                                  // get first discarded mantissa bit
+    Vec4ui r2 = a2 >> 12;
+    Vec4ui s1 = a1 & 0x2FFF;                               // 0x2000 indicates if odd, 0x0FFF if remaining bits are nonzero
+    Vec4ui s2 = a2 & 0x2FFF;
+    Vec4ib u1 = s1 == 0;                                   // false if odd or remaining bits nonzero
+    Vec4ib u2 = s2 == 0;
+    Vec4ui v1 = _mm_andnot_si128(u1, r1);                  // bit 0 = 1 if we have to round up
+    Vec4ui v2 = _mm_andnot_si128(u2, r2);
+    Vec4ui w1 = v1 & 1;                                    // = 1 if we need to round up
+    Vec4ui w2 = v2 & 1;
+    Vec4ui m1 = a1 >> 13;                                  // get mantissa in place
+    Vec4ui m2 = a2 >> 13;
+    Vec4ui n1 = m1 & 0x3FF;                                // mantissa isolated
+    Vec4ui n2 = m2 & 0x3FF;
+    Vec4ui e1 = a1 & 0x7FFFFFFF;                           // remove sign bit
+    Vec4ui e2 = a2 & 0x7FFFFFFF;
+    Vec4ui f1 = e1 - (0x70 << 23);                         // adjust exponent bias
+    Vec4ui f2 = e2 - (0x70 << 23);
+    Vec4ui g1 = f1 >> 13;                                  // shift exponent into new place
+    Vec4ui g2 = f2 >> 13;
+    Vec4ui h1 = g1 & 0x3FC00;                              // isolate exponent 
+    Vec4ui h2 = g2 & 0x3FC00;
+    Vec4ui i1 = n1 | h1;                                   // combine exponent and mantissa
+    Vec4ui i2 = n2 | h2;
+    Vec4ui j1 = i1 + w1;                                   // round mantissa. Overflow will carry into exponent
+    Vec4ui j2 = i2 + w2;
+    // check for overflow and underflow
+    Vec4ib k1 = j1 > 0x7BFF;                               // overflow
+    Vec4ib k2 = j2 > 0x7BFF;
+    Vec4ui ee1 = e1 >> 23;                                 // exponent at position 0
+    Vec4ui ee2 = e2 >> 23;
+    Vec4ib ii1 = ee1 == 0xFF;                              // check for INF and NAN
+    Vec4ib ii2 = ee2 == 0xFF;
+    Vec4ib uu1 = ee1 < 0x71;                               // exponent underflow
+    Vec4ib uu2 = ee2 < 0x71;
+    Vec4i  pp1 = Vec4i(0x7C00) | j1;                       // insert exponent if INF or NAN
+    Vec4i  pp2 = Vec4i(0x7C00) | j2;
+    // compute potential subnormal result
+    Vec4ui ss1 = e1 + (24 << 23);                          // add 24 to exponent
+    Vec4ui ss2 = e2 + (24 << 23);
+    Vec4ui tt1 = _mm_cvtps_epi32(_mm_castsi128_ps(ss1));   // convert float to int with rounding
+    Vec4ui tt2 = _mm_cvtps_epi32(_mm_castsi128_ps(ss2));
+    Vec4ui vv1 = tt1 & 0x7FF;                              // mantissa of subnormal number (possible overflow to normal)
+    Vec4ui vv2 = tt2 & 0x7FF;
+    // combine results
+    Vec4i  bb1 = select(k1, 0x7C00, j1);                   // select INF if overflow
+    Vec4i  bb2 = select(k2, 0x7C00, j2);
+    Vec4i  dd1 = select(ii1, pp1, bb1);                    // select INF or NAN    
+    Vec4i  dd2 = select(ii2, pp2, bb2);
+    Vec4i  cc1 = select(uu1, vv1, dd1);                    // select if subnormal or zero or exponent underflow
+    Vec4i  cc2 = select(uu2, vv2, dd2);
+    // get sign bit
+    Vec4i  sa1 = Vec4i(a1) >> 16;                          // extend sign bit to avoid saturation in pack instruction below
+    Vec4i  sa2 = Vec4i(a2) >> 16;
+    Vec4i  const smask = 0xFFFF8000;                       // extended sign mask
+    Vec4i  sb1 = sa1 & smask;                              // isolate sign
+    Vec4i  sb2 = sa2 & smask;
+    Vec4i  sc1 = _mm_andnot_si128(smask, cc1);             // isolate exponent and mantissa
+    Vec4i  sc2 = _mm_andnot_si128(smask, cc2);
+    Vec4i  rr1 = sb1 | sc1;                                // combine with sign
+    Vec4i  rr2 = sb2 | sc2;
+    Vec4i  rc  = _mm_packs_epi32(rr1, rr2);                // pack into 16-bit words (words are sign extended so they will not saturate)
+    return (__m128i)rc;                                    // return as Vec8h
+}
+
+#endif  // __F16C__
+
+
+/*****************************************************************************
+*
+*          Operators for Vec8h
+*
+*****************************************************************************/
+
+// vector operator + : add element by element
+static inline Vec8h operator + (Vec8h const a, Vec8h const b) {
+    return to_float16(to_float(a) + to_float(b));
+}
+
+// vector operator + : add vector and scalar
+static inline Vec8h operator + (Vec8h const a, Float16 b) {
+    return a + Vec8h(b);
+}
+static inline Vec8h operator + (Float16 a, Vec8h const b) {
+    return Vec8h(a) + b;
+}
+
+// vector operator += : add
+static inline Vec8h & operator += (Vec8h & a, Vec8h const b) {
+    a = a + b;
+    return a;
+}
+
+// postfix operator ++
+static inline Vec8h operator ++ (Vec8h & a, int) {
+    Vec8h a0 = a;
+    a = a + Float16(1.f); // 1.0f16 not supported by g++ version 12.1
+    return a0;
+}
+
+// prefix operator ++
+static inline Vec8h & operator ++ (Vec8h & a) {
+    a = a +  Float16(1.f);
+    return a;
+}
+
+// vector operator - : subtract element by element
+static inline Vec8h operator - (Vec8h const a, Vec8h const b) {
+    return to_float16(to_float(a) - to_float(b));
+}
+
+// vector operator - : subtract vector and scalar
+static inline Vec8h operator - (Vec8h const a, Float16 b) {
+    return a - Vec8h(b);
+}
+static inline Vec8h operator - (Float16 a, Vec8h const b) {
+    return Vec8h(a) - b;
+}
+
+// vector operator - : unary minus
+// Change sign bit, even for 0, INF and NAN
+static inline Vec8h operator - (Vec8h const a) {
+    return _mm_xor_si128(__m128i(a), _mm_set1_epi32(0x80008000));
+}
+
+// vector operator -= : subtract
+static inline Vec8h & operator -= (Vec8h & a, Vec8h const b) {
+    a = a - b;
+    return a;
+}
+
+// postfix operator --
+static inline Vec8h operator -- (Vec8h & a, int) {
+    Vec8h a0 = a;
+    a = a - Vec8h(Float16(1.f));
+    return a0;
+}
+
+// prefix operator --
+static inline Vec8h & operator -- (Vec8h & a) {
+    a = a - Vec8h(Float16(1.f));
+    return a;
+}
+
+// vector operator * : multiply element by element
+static inline Vec8h operator * (Vec8h const a, Vec8h const b) {
+    return to_float16(to_float(a) * to_float(b));
+}
+
+// vector operator * : multiply vector and scalar
+static inline Vec8h operator * (Vec8h const a, Float16 b) {
+    return a * Vec8h(b);
+}
+static inline Vec8h operator * (Float16 a, Vec8h const b) {
+    return Vec8h(a) * b;
+}
+
+// vector operator *= : multiply
+static inline Vec8h & operator *= (Vec8h & a, Vec8h const b) {
+    a = a * b;
+    return a;
+}
+
+// vector operator / : divide all elements by same integer
+static inline Vec8h operator / (Vec8h const a, Vec8h const b) {
+    return to_float16(to_float(a) / to_float(b));
+}
+
+// vector operator / : divide vector and scalar
+static inline Vec8h operator / (Vec8h const a, Float16 b) {
+    return a / Vec8h(b);
+}
+static inline Vec8h operator / (Float16 a, Vec8h const b) {
+    return Vec8h(a) / b;
+}
+
+// vector operator /= : divide
+static inline Vec8h & operator /= (Vec8h & a, Vec8h const b) {
+    a = a / b;
+    return a;
+}
+
+// vector operator == : returns true for elements for which a == b
+static inline Vec8hb operator == (Vec8h const a, Vec8h const b) {
+    return Vec8fb2hb(to_float(a) == to_float(b));
+}
+
+// vector operator != : returns true for elements for which a != b
+static inline Vec8hb operator != (Vec8h const a, Vec8h const b) {
+    return Vec8fb2hb(to_float(a) != to_float(b));
+}
+
+// vector operator < : returns true for elements for which a < b
+static inline Vec8hb operator < (Vec8h const a, Vec8h const b) {
+    return Vec8fb2hb(to_float(a) < to_float(b));
+}
+
+// vector operator <= : returns true for elements for which a <= b
+static inline Vec8hb operator <= (Vec8h const a, Vec8h const b) {
+    return Vec8fb2hb(to_float(a) <= to_float(b));
+}
+
+// vector operator > : returns true for elements for which a > b
+static inline Vec8hb operator > (Vec8h const a, Vec8h const b) {
+    return Vec8fb2hb(to_float(a) > to_float(b));
+}
+
+// vector operator >= : returns true for elements for which a >= b
+static inline Vec8hb operator >= (Vec8h const a, Vec8h const b) {
+    return Vec8fb2hb(to_float(a) >= to_float(b));
+}
+
+// Bitwise logical operators
+
+// vector operator & : bitwise and
+static inline Vec8h operator & (Vec8h const a, Vec8h const b) {
+    return _mm_and_si128(__m128i(a), __m128i(b));
+}
+
+// vector operator &= : bitwise and
+static inline Vec8h & operator &= (Vec8h & a, Vec8h const b) {
+    a = a & b;
+    return a;
+}
+
+// vector operator & : bitwise and of Vec8h and Vec8hb
+static inline Vec8h operator & (Vec8h const a, Vec8hb const b) {
+#if INSTRSET >= 10  // compact boolean vector
+    return _mm_maskz_mov_epi16(b, a);
+#else               // broad boolean vector
+    return _mm_and_si128(__m128i(a), __m128i(b));
+#endif
+}
+static inline Vec8h operator & (Vec8hb const a, Vec8h const b) {
+    return b & a;
+}
+
+// vector operator | : bitwise or
+static inline Vec8h operator | (Vec8h const a, Vec8h const b) {
+    return _mm_or_si128(__m128i(a), __m128i(b));
+}
+
+// vector operator |= : bitwise or
+static inline Vec8h & operator |= (Vec8h & a, Vec8h const b) {
+    a = a | b;
+    return a;
+}
+
+// vector operator ^ : bitwise xor
+static inline Vec8h operator ^ (Vec8h const a, Vec8h const b) {
+    return _mm_xor_si128(__m128i(a), __m128i(b));
+}
+
+// vector operator ^= : bitwise xor
+static inline Vec8h & operator ^= (Vec8h & a, Vec8h const b) {
+    a = a ^ b;
+    return a;
+}
+
+// vector operator ! : logical not. Returns Boolean vector
+static inline Vec8hb operator ! (Vec8h const a) {
+    return a == Vec8h(0.0);
+}
+
+
+/*****************************************************************************
+*
+*          Functions for Vec8h
+*
+*****************************************************************************/
+
+// Select between two operands. Corresponds to this pseudocode:
+// for (int i = 0; i < 4; i++) result[i] = s[i] ? a[i] : b[i];
+static inline Vec8h select(Vec8hb const s, Vec8h const a, Vec8h const b) {
+    return __m128i(select(Vec8sb(s), Vec8s(__m128i(a)), Vec8s(__m128i(b))));
+}
+
+// Conditional add: For all vector elements i: result[i] = f[i] ? (a[i] + b[i]) : a[i]
+static inline Vec8h if_add(Vec8hb const f, Vec8h const a, Vec8h const b) {
+    return a + (b & f);
+}
+
+// Conditional subtract: For all vector elements i: result[i] = f[i] ? (a[i] - b[i]) : a[i]
+static inline Vec8h if_sub(Vec8hb const f, Vec8h const a, Vec8h const b) {
+    return a - (b & f);
+}
+
+// Conditional multiply: For all vector elements i: result[i] = f[i] ? (a[i] * b[i]) : a[i]
+static inline Vec8h if_mul(Vec8hb const f, Vec8h const a, Vec8h const b) {
+    return select(f, a*b, a);
+}
+
+// Conditional divide: For all vector elements i: result[i] = f[i] ? (a[i] / b[i]) : a[i]
+static inline Vec8h if_div(Vec8hb const f, Vec8h const a, Vec8h const b) {
+    return select(f, a/b, a);
+}
+
+// Sign functions
+
+// Function sign_bit: gives true for elements that have the sign bit set
+// even for -0.0f, -INF and -NAN
+// Note that sign_bit(Vec8h(-0.0f16)) gives true, while Vec8h(-0.0f16) < Vec8h(0.0f16) gives false
+// (the underscore in the name avoids a conflict with a macro in Intel's mathimf.h)
+static inline Vec8hb sign_bit(Vec8h const a) {
+    Vec8s t1 = __m128i(a);             // reinterpret as 16-bit integer
+    Vec8s t2 = t1 >> 15;               // extend sign bit
+    return t2 != 0;
+}
+
+// Function sign_combine: changes the sign of a when b has the sign bit set
+// same as select(sign_bit(b), -a, a)
+static inline Vec8h sign_combine(Vec8h const a, Vec8h const b) {
+    return a ^ (b & Vec8h(Float16(-0.0)));
+}
+
+// Categorization functions
+
+// Function is_finite: gives true for elements that are normal, subnormal or zero,
+// false for INF and NAN
+// (the underscore in the name avoids a conflict with a macro in Intel's mathimf.h)
+static inline Vec8hb is_finite(Vec8h const a) {
+    Vec8s b = __m128i(a);
+    return (b & 0x7C00) != 0x7C00;
+}
+
+// Function is_inf: gives true for elements that are +INF or -INF
+// false for finite numbers and NAN
+// (the underscore in the name avoids a conflict with a macro in Intel's mathimf.h)
+static inline Vec8hb is_inf(Vec8h const a) {
+    Vec8s b = __m128i(a);
+    return (b & 0x7FFF) == 0x7C00;
+}
+
+// Function is_nan: gives true for elements that are +NAN or -NAN
+// false for finite numbers and +/-INF
+// (the underscore in the name avoids a conflict with a macro in Intel's mathimf.h)
+static inline Vec8hb is_nan(Vec8h const a) {
+    Vec8s b = __m128i(a);
+    return (b & 0x7FFF) > 0x7C00;
+}
+
+// Function is_subnormal: gives true for elements that are subnormal
+// false for finite numbers, zero, NAN and INF
+static inline Vec8hb is_subnormal(Vec8h const a) {
+    Vec8s b = __m128i(a);
+    return (b & 0x7C00) == 0 && (b & 0x3FF) != 0;
+}
+
+// Function is_zero_or_subnormal: gives true for elements that are zero or subnormal
+// false for finite numbers, NAN and INF
+static inline Vec8hb is_zero_or_subnormal(Vec8h const a) {
+    Vec8s b = __m128i(a);
+    return (b & 0x7C00) == 0;
+}
+
+// Function infinite8h: returns a vector where all elements are +INF
+static inline Vec8h infinite8h() {
+    return Vec8h(_mm_set1_epi16(0x7C00));
+}
+
+// template for producing quiet NAN
+template <>
+Vec8h nan_vec<Vec8h>(uint32_t payload) {
+    if constexpr (Vec8h::elementtype() == 15) {  // Float16
+        return Vec8h(_mm_set1_epi16(0x7E00 | (payload & 0x01FF)));
+    }
+} 
+
+// Function nan8h: returns a vector where all elements are NAN (quiet)
+static inline Vec8h nan8h(int n = 0x10) {
+    return nan_vec<Vec8h>(n);
+}
+
+// This function returns the code hidden in a NAN. The sign bit is ignored
+static inline Vec8us nan_code(Vec8h const x) {
+    Vec8us a = Vec8us(reinterpret_i(x));
+    Vec8us const n = 0x3FF;
+    return select(is_nan(x), a & n, Vec8us(0));
+}
+
+// General arithmetic functions, etc.
+
+// Horizontal add: Calculates the sum of all vector elements.
+static inline Float16 horizontal_add(Vec8h const a) {
+    return Float16(horizontal_add(to_float(a)));
+}
+// same, with high precision
+static inline float horizontal_add_x(Vec8h const a) {
+    return horizontal_add(to_float(a));
+}
+
+// function max: a > b ? a : b
+static inline Vec8h max(Vec8h const a, Vec8h const b) {
+    return to_float16(max(to_float(a), to_float(b)));
+}
+
+// function min: a < b ? a : b
+static inline Vec8h min(Vec8h const a, Vec8h const b) {
+    return to_float16(min(to_float(a), to_float(b)));
+}
+// NAN-safe versions of maximum and minimum are in vector_convert.h
+
+// function abs: absolute value
+static inline Vec8h abs(Vec8h const a) {
+    return _mm_and_si128(a, _mm_set1_epi16(0x7FFF));
+}
+
+// function sqrt: square root
+static inline Vec8h sqrt(Vec8h const a) {
+    return to_float16(sqrt(to_float(a)));
+}
+
+// function square: a * a
+static inline Vec8h square(Vec8h const a) {
+    return a * a;
+}
+
+// The purpose of this template is to prevent implicit conversion of a float
+// exponent to int when calling pow(vector, float) and vectormath_exp.h is not included
+template <typename TT> static Vec8h pow(Vec8h const a, TT const n);  // = delete
+
+// Raise floating point numbers to integer power n
+// To do: Optimize pow<int>(Vec8h/Vec16h/Vec32h, n) to do calculations with float precision
+template <>
+inline Vec8h pow<int>(Vec8h const x0, int const n) {
+    return to_float16(pow_template_i<Vec8f>(to_float(x0), n));
+}
+
+// allow conversion from unsigned int
+template <>
+inline Vec8h pow<uint32_t>(Vec8h const x0, uint32_t const n) {
+    return to_float16(pow_template_i<Vec8f>(to_float(x0), (int)n));
+}
+
+// Raise floating point numbers to integer power n, where n is a compile-time constant:
+// Template in vectorf128.h is used
+//template <typename V, int n>
+//static inline V pow_n(V const a);
+
+// implement as function pow(vector, const_int)
+template <int n>
+static inline Vec8h pow(Vec8h const a, Const_int_t<n>) {
+    return to_float16(pow_n<Vec8f, n>(to_float(a)));
+}
+
+
+static inline Vec8h round(Vec8h const a) {
+    return to_float16(round(to_float(a)));
+}
+
+// function truncate: round towards zero. (result as float vector)
+static inline Vec8h truncate(Vec8h const a) {
+    return to_float16(truncate(to_float(a)));
+}
+
+// function floor: round towards minus infinity. (result as float vector)
+static inline Vec8h floor(Vec8h const a) {
+    return to_float16(floor(to_float(a)));
+}
+
+// function ceil: round towards plus infinity. (result as float vector)
+static inline Vec8h ceil(Vec8h const a) {
+    return to_float16(ceil(to_float(a)));
+}
+
+// function roundi: round to nearest integer (even). (result as integer vector)
+static inline Vec8s roundi(Vec8h const a) {
+    return compress_saturated(roundi(to_float(a)));
+}
+
+// function truncatei: round towards zero. (result as integer vector)
+static inline Vec8s truncatei(Vec8h const a) {
+    //return compress(truncatei(to_float(a)));
+    return compress_saturated(truncatei(to_float(a)));
+}
+
+// function to_float: convert integer vector to float vector
+static inline Vec8h to_float16(Vec8s const a) {
+    return to_float16(to_float(extend(a)));
+}
+
+// function to_float: convert unsigned integer vector to float vector
+static inline Vec8h to_float16(Vec8us const a) {
+    return to_float16(to_float(extend(a)));
+}
+
+// Approximate math functions
+
+// reciprocal (almost exact)
+static inline Vec8h approx_recipr(Vec8h const a) {
+    return to_float16(approx_recipr(to_float(a)));
+}
+
+// reciprocal squareroot (almost exact)
+static inline Vec8h approx_rsqrt(Vec8h const a) {
+    return to_float16(approx_rsqrt(to_float(a)));
+}
+
+// Fused multiply and add functions
+
+// Multiply and add. a*b+c
+static inline Vec8h mul_add(Vec8h const a, Vec8h const b, Vec8h const c) {
+    return to_float16(mul_add(to_float(a),to_float(b),to_float(c)));
+}
+
+// Multiply and subtract. a*b-c
+static inline Vec8h mul_sub(Vec8h const a, Vec8h const b, Vec8h const c) {
+    return to_float16(mul_sub(to_float(a),to_float(b),to_float(c)));
+}
+
+// Multiply and inverse subtract
+static inline Vec8h nmul_add(Vec8h const a, Vec8h const b, Vec8h const c) {
+    return to_float16(nmul_add(to_float(a),to_float(b),to_float(c)));
+}
+
+// Math functions using fast bit manipulation
+
+// Extract the exponent as an integer
+// exponent(a) = floor(log2(abs(a)));
+// exponent(1.0f) = 0, exponent(0.0f) = -127, exponent(INF) = +128, exponent(NAN) = +128
+static inline Vec8s exponent(Vec8h const a) {
+    Vec8us t1 = __m128i(a);            // reinterpret as 16-bit integer
+    Vec8us t2 = t1 << 1;               // shift out sign bit
+    Vec8us t3 = t2 >> 11;              // shift down logical to position 0
+    Vec8s  t4 = Vec8s(t3) - 0x0F;      // subtract bias from exponent
+    return t4;
+}
+
+// Extract the fraction part of a floating point number
+// a = 2^exponent(a) * fraction(a), except for a = 0
+// fraction(1.0f) = 1.0f, fraction(5.0f) = 1.25f
+// NOTE: The name fraction clashes with an ENUM in MAC XCode CarbonCore script.h !
+static inline Vec8h fraction(Vec8h const a) {
+    Vec8us t1 = __m128i(a);   // reinterpret as 16-bit integer
+    Vec8us t2 = Vec8us((t1 & 0x3FF) | 0x3C00); // set exponent to 0 + bias
+    return __m128i(t2);
+}
+
+// Fast calculation of pow(2,n) with n integer
+// n  =    0 gives 1.0f
+// n >=  16 gives +INF
+// n <= -15 gives 0.0f
+// This function will never produce subnormals, and never raise exceptions
+static inline Vec8h exp2(Vec8s const n) {
+    Vec8s t1 = max(n, -15);            // limit to allowed range
+    Vec8s t2 = min(t1, 16);
+    Vec8s t3 = t2 + 15;                // add bias
+    Vec8s t4 = t3 << 10;               // put exponent into position 10
+    return __m128i(t4);                // bit-cast to float
+}
+
+// change signs on vectors Vec8h
+// Each index i0 - i7 is 1 for changing sign on the corresponding element, 0 for no change
+template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7>
+Vec8h change_sign(Vec8h const a) {
+    if constexpr ((i0 | i1 | i2 | i3 | i4 | i5 | i6 | i7) == 0) return a;
+    __m128i mask = constant4ui<
+        (i0 ? 0x8000 : 0) | (i1 ? 0x80000000 : 0), 
+        (i2 ? 0x8000 : 0) | (i3 ? 0x80000000 : 0), 
+        (i4 ? 0x8000 : 0) | (i5 ? 0x80000000 : 0), 
+        (i6 ? 0x8000 : 0) | (i7 ? 0x80000000 : 0) >();
+    return _mm_xor_si128(a, mask);
+}
+
+
+/*****************************************************************************
+*
+*          Functions for reinterpretation between vector types
+*
+*****************************************************************************/
+static inline __m128i reinterpret_h(__m128i const x) {
+    return x;
+}
+/* Defined in vectorf128.h:
+ __m128i reinterpret_i(__m128i const x)
+ __m128  reinterpret_f(__m128i const x)
+ __m128d reinterpret_d(__m128i const x)
+*/
+
+
+/*****************************************************************************
+*
+*          Vector permute and blend functions
+*
+******************************************************************************
+*
+* The permute function can reorder the elements of a vector and optionally
+* set some elements to zero.
+*
+* See vectori128.h for details
+*
+*****************************************************************************/
+// permute vector Vec8h
+template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7>
+Vec8h permute8(Vec8h const a) {
+    return __m128i(permute8<i0, i1, i2, i3, i4, i5, i6, i7>(Vec8s(__m128i(a))));
+}
+
+
+/*****************************************************************************
+*
+*          Vector blend functions
+*
+*****************************************************************************/
+
+// permute and blend Vec8h
+template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7>
+static inline Vec8h blend8(Vec8h const a, Vec8h const b) {
+    return __m128i (blend8<i0, i1, i2, i3, i4, i5, i6, i7>(Vec8s(__m128i(a)), Vec8s(__m128i(b))));
+}
+
+
+/*****************************************************************************
+*
+*          Vector lookup functions
+*
+******************************************************************************
+*
+* These functions use vector elements as indexes into a table.
+* The table is given as one or more vectors or as an array.
+*
+*****************************************************************************/
+
+static inline Vec8h lookup8 (Vec8s const index, Vec8h const table) {
+    return __m128i(lookup8(index, Vec8s(__m128i(table))));
+}
+
+static inline Vec8h lookup16(Vec8s const index, Vec8h const table0, Vec8h const table1) {
+    return __m128i(lookup16(index, Vec8s(__m128i(table0)), Vec8s(__m128i(table1))));
+}
+
+template <int n>
+Vec8h lookup(Vec8s const index, void const * table) {
+    return __m128i(lookup<n>(index, (void const *)(table)));
+}
+
+
+
+/*****************************************************************************
+*
+*          256 bit vectors
+*
+*****************************************************************************/
+
+#if MAX_VECTOR_SIZE >= 512
+
+
+/*****************************************************************************
+*
+*          Vec16hb: Vector of 16 Booleans for use with Vec16h
+*
+*****************************************************************************/
+
+#if INSTRSET >= 10
+typedef Vec16b Vec16hb;   // compact boolean vector
+
+#if MAX_VECTOR_SIZE >= 512
+static inline Vec16hb Vec16fb2hb (Vec16fb const a) {
+    return a;
+}
+#endif
+
+#else
+
+typedef Vec16sb Vec16hb;  // broad boolean vector
+
+static inline Vec16hb Vec16fb2hb (Vec16fb const a) {
+    // boolean vector needs compression from 32 bits to 16 bits per element
+    Vec8fb lo = a.get_low();           // (cannot use _mm256_packs_epi32)
+    Vec8fb hi = a.get_high();
+    return Vec16hb(Vec8fb2hb(lo), Vec8fb2hb(hi));
+}
+
+#endif
+
+
+/*****************************************************************************
+*
+*          Vec16h: Vector of 16 single precision floating point values
+*
+*****************************************************************************/
+
+class Vec16h : public Vec16s {
+public:
+    // Default constructor:
+    Vec16h() = default;
+    // Constructor to broadcast the same value into all elements:
+    Vec16h(Float16 f) : Vec16s(castfp162s(f)) {}
+    Vec16h(float f) : Vec16s(castfp162s(Float16(f))) {}
+
+    // Constructor to build from all elements:
+    Vec16h(Float16 f0, Float16 f1, Float16 f2, Float16 f3, Float16 f4, Float16 f5, Float16 f6, Float16 f7,
+    Float16 f8, Float16 f9, Float16 f10, Float16 f11, Float16 f12, Float16 f13, Float16 f14, Float16 f15) :
+        Vec16s(castfp162s(f0), castfp162s(f1), castfp162s(f2), castfp162s(f3), castfp162s(f4), castfp162s(f5), castfp162s(f6), castfp162s(f7), 
+            castfp162s(f8), castfp162s(f9), castfp162s(f10), castfp162s(f11), castfp162s(f12), castfp162s(f13), castfp162s(f14), castfp162s(f15)) {}
+
+    // Constructor to build from two Vec8h:
+    Vec16h(Vec8h const a0, Vec8h const a1) : Vec16s(Vec8s(a0), Vec8s(a1)) {};
+
+#if INSTRSET >= 8
+    // Constructor to convert from type __m256i used in intrinsics:
+    Vec16h(__m256i const x) {
+        ymm = x;
+    }
+    // Assignment operator to convert from type __m256i used in intrinsics:
+    Vec16h & operator = (__m256i const x) {
+        ymm = x;
+        return *this;
+    }
+    // Type cast operator to convert to __m256i used in intrinsics
+    operator __m256i() const {
+        return ymm;
+    }
+#else
+    // Constructor to convert from type Vec16s. This may cause undesired implicit conversions and ambiguities
+    //Vec16h(Vec16s const x) : Vec16s(x) {}
+#endif
+    // Member function to load from array (unaligned)
+    Vec16h & load(void const * p) {
+        Vec16s::load(p);
+        return *this;
+    }
+    // Member function to load from array, aligned by 32
+    // You may use load_a instead of load if you are certain that p points to an address
+    // divisible by 32. In most cases there is no difference in speed between load and load_a
+    Vec16h & load_a(void const * p) {
+        Vec16s::load_a(p);
+        return *this;
+    }
+    // Member function to store into array (unaligned)
+    // void store(void * p) const // inherited from Vec16s
+
+    // Member function storing into array, aligned by 32
+    // You may use store_a instead of store if you are certain that p points to an address
+    // divisible by 32.
+    // void store_a(void * p) const // inherited from Vec16s 
+
+    // Member function storing to aligned uncached memory (non-temporal store).
+    // This may be more efficient than store_a when storing large blocks of memory if it 
+    // is unlikely that the data will stay in the cache until it is read again.
+    // Note: Will generate runtime error if p is not aligned by 32
+    // void store_nt(void * p) const // inherited from Vec16s 
+
+    // Partial load. Load n elements and set the rest to 0
+    Vec16h & load_partial(int n, void const * p) {
+        Vec16s::load_partial(n, p);
+        return *this;
+    }
+    // Partial store. Store n elements
+    // void store_partial(int n, void * p) const // inherited from Vec16s 
+
+    // cut off vector to n elements. The last 8-n elements are set to zero
+    Vec16h & cutoff(int n) {
+        Vec16s::cutoff(n);
+        return *this;
+    }
+    // Member function to change a single element in vector
+    Vec16h const insert(int index, Float16 a) {
+        Vec16s::insert(index, castfp162s(a));
+        return *this;
+    }
+    // Member function extract a single element from vector
+    Float16 extract(int index) const {
+        return casts2fp16(Vec16s::extract(index));
+    }
+    // Extract a single element. Use store function if extracting more than one element.
+    // Operator [] can only read an element, not write.
+    Float16 operator [] (int index) const {
+        return extract(index);
+    }
+    Vec8h get_low() const {
+        return __m128i(Vec16s::get_low());
+    }
+    Vec8h get_high() const {
+        return __m128i(Vec16s::get_high());
+    }
+    static constexpr int size() {
+        return 16;
+    }
+    static constexpr int elementtype() {
+        return 15;
+    }
+};
+
+/*****************************************************************************
+*
+*          conversions Vec16h <-> Vec16f
+*
+*****************************************************************************/
+#if INSTRSET >= 9    // AVX512F instruction set has conversion instructions
+
+// extend precision: Vec16h -> Vec16f
+Vec16f to_float (Vec16h h) {
+    return _mm512_cvtph_ps(h);
+}
+
+// reduce precision: Vec16f -> Vec16h
+Vec16h to_float16 (Vec16f f) {
+    return _mm512_cvtps_ph(f, 0);
+}
+
+#else
+
+// extend precision: Vec16h -> Vec16f
+Vec16f to_float (Vec16h h) {
+    return Vec16f(to_float(h.get_low()), to_float(h.get_high()));
+}
+
+// reduce precision: Vec16f -> Vec16h
+Vec16h to_float16 (Vec16f f) {
+    return Vec16h(to_float16(f.get_low()), to_float16(f.get_high()));
+}
+
+#endif
+
+/*****************************************************************************
+*
+*          Operators for Vec16h
+*
+*****************************************************************************/
+
+// vector operator + : add element by element
+static inline Vec16h operator + (Vec16h const a, Vec16h const b) {
+    return to_float16(to_float(a) + to_float(b));
+}
+
+
+static inline Vec16h operator + (Float16 a, Vec16h const b) {
+    return Vec16h(a) + b;
+}
+
+// vector operator += : add
+static inline Vec16h & operator += (Vec16h & a, Vec16h const b) {
+    a = a + b;
+    return a;
+}
+
+// postfix operator ++
+static inline Vec16h operator ++ (Vec16h & a, int) {
+    Vec16h a0 = a;
+    a = a + Float16(1.f);
+    return a0;
+}
+
+// prefix operator ++
+static inline Vec16h & operator ++ (Vec16h & a) {
+    a = a + Float16(1.f);
+    return a;
+}
+
+// vector operator - : subtract element by element
+static inline Vec16h operator - (Vec16h const a, Vec16h const b) {
+    return to_float16(to_float(a) - to_float(b));
+}
+
+// vector operator - : subtract vector and scalar
+static inline Vec16h operator - (Vec16h const a, Float16 b) {
+    return a - Vec16h(b);
+}
+static inline Vec16h operator - (Float16 a, Vec16h const b) {
+    return Vec16h(a) - b;
+}
+
+// vector operator - : unary minus
+// Change sign bit, even for 0, INF and NAN
+static inline Vec16h operator - (Vec16h const a) {
+#if INSTRSET >= 8  // AVX2
+    return _mm256_xor_si256(a, _mm256_set1_epi32(0x80008000));
+#else
+    return Vec16h(-a.get_low(), -a.get_high());
+#endif
+}
+
+// vector operator -= : subtract
+static inline Vec16h & operator -= (Vec16h & a, Vec16h const b) {
+    a = a - b;
+    return a;
+}
+
+// postfix operator --
+static inline Vec16h operator -- (Vec16h & a, int) {
+    Vec16h a0 = a;
+    a = a - Vec16h(Float16(1.f));
+    return a0;
+}
+
+// prefix operator --
+static inline Vec16h & operator -- (Vec16h & a) {
+    a = a - Vec16h(Float16(1.f));
+    return a;
+}
+
+// vector operator * : multiply element by element
+static inline Vec16h operator * (Vec16h const a, Vec16h const b) {
+    return to_float16(to_float(a) * to_float(b));
+}
+
+// vector operator * : multiply vector and scalar
+static inline Vec16h operator * (Vec16h const a, Float16 b) {
+    return a * Vec16h(b);
+}
+static inline Vec16h operator * (Float16 a, Vec16h const b) {
+    return Vec16h(a) * b;
+}
+
+// vector operator *= : multiply
+static inline Vec16h & operator *= (Vec16h & a, Vec16h const b) {
+    a = a * b;
+    return a;
+}
+
+// vector operator / : divide all elements by same integer
+static inline Vec16h operator / (Vec16h const a, Vec16h const b) {
+    return to_float16(to_float(a) / to_float(b));
+}
+
+// vector operator / : divide vector and scalar
+static inline Vec16h operator / (Vec16h const a, Float16 b) {
+    return a / Vec16h(b);
+}
+static inline Vec16h operator / (Float16 a, Vec16h const b) {
+    return Vec16h(a) / b;
+}
+
+// vector operator /= : divide
+static inline Vec16h & operator /= (Vec16h & a, Vec16h const b) {
+    a = a / b;
+    return a;
+}
+
+// vector operator == : returns true for elements for which a == b
+static inline Vec16hb operator == (Vec16h const a, Vec16h const b) {
+    return Vec16fb2hb(to_float(a) == to_float(b));
+}
+
+// vector operator != : returns true for elements for which a != b
+static inline Vec16hb operator != (Vec16h const a, Vec16h const b) {
+    return Vec16fb2hb(to_float(a) != to_float(b));
+}
+
+// vector operator < : returns true for elements for which a < b
+static inline Vec16hb operator < (Vec16h const a, Vec16h const b) {
+    return Vec16fb2hb(to_float(a) < to_float(b));
+}
+
+// vector operator <= : returns true for elements for which a <= b
+static inline Vec16hb operator <= (Vec16h const a, Vec16h const b) {
+    return Vec16fb2hb(to_float(a) <= to_float(b));
+}
+
+// vector operator > : returns true for elements for which a > b
+static inline Vec16hb operator > (Vec16h const a, Vec16h const b) {
+    return Vec16fb2hb(to_float(a) > to_float(b));
+}
+
+// vector operator >= : returns true for elements for which a >= b
+static inline Vec16hb operator >= (Vec16h const a, Vec16h const b) {
+    return Vec16fb2hb(to_float(a) >= to_float(b));
+}
+
+
+// Bitwise logical operators
+
+// vector operator & : bitwise and
+static inline Vec16h operator & (Vec16h const a, Vec16h const b) {
+#if INSTRSET >= 8         
+    return _mm256_and_si256(__m256i(a), __m256i(b));
+#else
+    return Vec16h(a.get_low() & b.get_low(), a.get_high() & b.get_high());
+#endif
+}
+
+// vector operator &= : bitwise and
+static inline Vec16h & operator &= (Vec16h & a, Vec16h const b) {
+    a = a & b;
+    return a;
+}
+
+// vector operator & : bitwise and of Vec16h and Vec16hb
+static inline Vec16h operator & (Vec16h const a, Vec16hb const b) {
+#if INSTRSET >= 10         
+    return __m256i(_mm256_maskz_mov_epi16(b, __m256i(a)));
+#elif INSTRSET >= 8
+    return _mm256_and_si256(__m256i(a), __m256i(b));
+#else
+    return Vec16h(a.get_low() & b.get_low(), a.get_high() & b.get_high());
+#endif
+}
+static inline Vec16h operator & (Vec16hb const a, Vec16h const b) {
+    return b & a;
+}
+
+// vector operator | : bitwise or
+static inline Vec16h operator | (Vec16h const a, Vec16h const b) {
+#if INSTRSET >= 8         
+    return _mm256_or_si256(__m256i(a), __m256i(b));
+#else
+    return Vec16h(a.get_low() | b.get_low(), a.get_high() | b.get_high());
+#endif
+}
+
+// vector operator |= : bitwise or
+static inline Vec16h & operator |= (Vec16h & a, Vec16h const b) {
+    a = a | b;
+    return a;
+}
+
+// vector operator ^ : bitwise xor
+static inline Vec16h operator ^ (Vec16h const a, Vec16h const b) {
+#if INSTRSET >= 8         
+    return _mm256_xor_si256(__m256i(a), __m256i(b));
+#else
+    return Vec16h(a.get_low() ^ b.get_low(), a.get_high() ^ b.get_high());
+#endif
+}
+
+// vector operator ^= : bitwise xor
+static inline Vec16h & operator ^= (Vec16h & a, Vec16h const b) {
+    a = a ^ b;
+    return a;
+}
+
+// vector operator ! : logical not. Returns Boolean vector
+static inline Vec16hb operator ! (Vec16h const a) {
+    return a == Vec16h(Float16(0.0f));
+}
+
+/*****************************************************************************
+*
+*          Functions for reinterpretation between vector types
+*
+*****************************************************************************/
+#if INSTRSET >= 8
+static inline __m256i reinterpret_h(__m256i const x) {
+    return x;
+}
+
+#if defined(__GNUC__) && __GNUC__ <= 9 // GCC v. 9 is missing the _mm256_zextsi128_si256 intrinsic
+static inline Vec16h extend_z(Vec8h a) {
+    return Vec16h(a, Vec8h(Float16(0.f)));
+}
+
+#else
+static inline Vec16h extend_z(Vec8h a) {
+    return _mm256_zextsi128_si256(a);
+}
+#endif
+
+#else // INSTRSET
+
+static inline Vec16h reinterpret_h(Vec16s const x) {
+    return Vec16h(Vec8h(x.get_low()), Vec8h(x.get_high()));
+}
+
+static inline Vec16s reinterpret_i(Vec16h const x) {
+    return Vec16s(Vec8s(x.get_low()), Vec8s(x.get_high()));
+}
+
+static inline Vec16h extend_z(Vec8h a) {
+    return Vec16h(a, Vec8h(0));
+}
+
+#endif  // INSTRSET
+
+
+/*****************************************************************************
+*
+*          Functions for Vec16h
+*
+*****************************************************************************/
+
+// Select between two operands. Corresponds to this pseudocode:
+// for (int i = 0; i < 4; i++) result[i] = s[i] ? a[i] : b[i];
+static inline Vec16h select(Vec16hb const s, Vec16h const a, Vec16h const b) {
+#if INSTRSET >= 10
+    return __m256i(_mm256_mask_mov_epi16(__m256i(b), s, __m256i(a)));
+#elif INSTRSET >= 8
+    return __m256i(select(Vec16sb(s), Vec16s(__m256i(a)), Vec16s(__m256i(b))));
+#else
+    return Vec16h(select(s.get_low(), a.get_low(), b.get_low()), select(s.get_high(), a.get_high(), b.get_high()));
+#endif
+}
+
+// Conditional add: For all vector elements i: result[i] = f[i] ? (a[i] + b[i]) : a[i]
+static inline Vec16h if_add(Vec16hb const f, Vec16h const a, Vec16h const b) {
+#if INSTRSET >= 8
+    return a + (b & f);
+#else
+    return select(f, a+b, a);
+#endif
+}
+
+// Conditional subtract: For all vector elements i: result[i] = f[i] ? (a[i] - b[i]) : a[i]
+static inline Vec16h if_sub(Vec16hb const f, Vec16h const a, Vec16h const b) {
+#if INSTRSET >= 8
+    return a - (b & f);
+#else
+    return select(f, a-b, a);
+#endif
+}
+
+// Conditional multiply: For all vector elements i: result[i] = f[i] ? (a[i] * b[i]) : a[i]
+static inline Vec16h if_mul(Vec16hb const f, Vec16h const a, Vec16h const b) {
+    return select(f, a*b, a);
+}
+
+// Conditional divide: For all vector elements i: result[i] = f[i] ? (a[i] / b[i]) : a[i]
+static inline Vec16h if_div(Vec16hb const f, Vec16h const a, Vec16h const b) {
+    return select(f, a/b, a);
+}
+
+// Sign functions
+
+// Function sign_bit: gives true for elements that have the sign bit set
+// even for -0.0f, -INF and -NAN
+// Note that sign_bit(Vec16h(-0.0f16)) gives true, while Vec16h(-0.0f16) < Vec16h(0.0f16) gives false
+// (the underscore in the name avoids a conflict with a macro in Intel's mathimf.h)
+static inline Vec16hb sign_bit(Vec16h const a) {
+    Vec16s t1 = reinterpret_i(a);                // reinterpret as 16-bit integer
+    Vec16s t2 = t1 >> 15;                        // extend sign bit
+    return t2 != 0;
+}
+
+// Function sign_combine: changes the sign of a when b has the sign bit set
+// same as select(sign_bit(b), -a, a)
+static inline Vec16h sign_combine(Vec16h const a, Vec16h const b) {
+    return a ^ (b & Vec16h(Float16(-0.0)));
+}
+
+// Categorization functions
+
+// Function is_finite: gives true for elements that are normal, subnormal or zero,
+// false for INF and NAN
+// (the underscore in the name avoids a conflict with a macro in Intel's mathimf.h)
+static inline Vec16hb is_finite(Vec16h const a) {
+    return (Vec16s(reinterpret_i(a)) & 0x7C00) != 0x7C00;
+}
+
+// Function is_inf: gives true for elements that are +INF or -INF
+// false for finite numbers and NAN
+// (the underscore in the name avoids a conflict with a macro in Intel's mathimf.h)
+static inline Vec16hb is_inf(Vec16h const a) {
+    return (Vec16s(reinterpret_i(a)) & 0x7FFF) == 0x7C00;
+}
+
+// Function is_nan: gives true for elements that are +NAN or -NAN
+// false for finite numbers and +/-INF
+// (the underscore in the name avoids a conflict with a macro in Intel's mathimf.h)
+static inline Vec16hb is_nan(Vec16h const a) {
+    return (Vec16s(reinterpret_i(a)) & 0x7FFF) > 0x7C00;
+}
+
+// Function is_subnormal: gives true for elements that are subnormal
+// false for finite numbers, zero, NAN and INF
+static inline Vec16hb is_subnormal(Vec16h const a) {
+    return (Vec16s(reinterpret_i(a)) & 0x7C00) == 0 && (Vec16s(reinterpret_i(a)) & 0x03FF) != 0;
+}
+
+// Function is_zero_or_subnormal: gives true for elements that are zero or subnormal
+// false for finite numbers, NAN and INF
+static inline Vec16hb is_zero_or_subnormal(Vec16h const a) {
+    return (Vec16s(reinterpret_i(a)) & 0x7C00) == 0;
+} 
+
+// Function infinite16h: returns a vector where all elements are +INF
+static inline Vec16h infinite16h() {
+    return reinterpret_h(Vec16s(0x7C00));
+}
+
+// template for producing quiet NAN
+template <>
+Vec16h nan_vec<Vec16h>(uint32_t payload) {
+    if constexpr (Vec16h::elementtype() == 15) {  // Float16
+        return reinterpret_h(Vec16s(0x7E00 | (payload & 0x01FF)));
+    }
+} 
+
+// Function nan16h: returns a vector where all elements are NAN (quiet)
+static inline Vec16h nan16h(int n = 0x10) {
+    return nan_vec<Vec16h>(n);
+}
+
+// This function returns the code hidden in a NAN. The sign bit is ignored
+static inline Vec16us nan_code(Vec16h const x) {
+    Vec16us a = Vec16us(reinterpret_i(x));
+    Vec16us const n = 0x3FF;
+    return select(is_nan(x), a & n, Vec16us(0));
+}
+
+
+// General arithmetic functions, etc.
+
+// Horizontal add: Calculates the sum of all vector elements.
+static inline Float16 horizontal_add(Vec16h const a) {
+    return horizontal_add(a.get_low()+a.get_high());
+}
+// same, with high precision
+static inline float horizontal_add_x(Vec16h const a) {
+    return horizontal_add(to_float(a));
+}
+
+// function max: a > b ? a : b
+static inline Vec16h max(Vec16h const a, Vec16h const b) {
+    return to_float16(max(to_float(a), to_float(b)));
+}
+
+// function min: a < b ? a : b
+static inline Vec16h min(Vec16h const a, Vec16h const b) {
+    return to_float16(min(to_float(a), to_float(b)));
+}
+// NAN-safe versions of maximum and minimum are in vector_convert.h
+
+// function abs: absolute value
+static inline Vec16h abs(Vec16h const a) {
+    return reinterpret_h(Vec16s(reinterpret_i(a)) & 0x7FFF);
+}
+
+// function sqrt: square root
+static inline Vec16h sqrt(Vec16h const a) {
+    return to_float16(sqrt(to_float(a)));
+}
+
+// function square: a * a
+static inline Vec16h square(Vec16h const a) {
+    return a * a;
+}
+
+// The purpose of this template is to prevent implicit conversion of a float
+// exponent to int when calling pow(vector, float) and vectormath_exp.h is not included
+template <typename TT> static Vec16h pow(Vec16h const a, TT const n);  // = delete
+
+// Raise floating point numbers to integer power n
+template <>
+inline Vec16h pow<int>(Vec16h const x0, int const n) {
+    return pow_template_i<Vec16h>(x0, n);
+}
+
+// allow conversion from unsigned int
+template <>
+inline Vec16h pow<uint32_t>(Vec16h const x0, uint32_t const n) {
+    return pow_template_i<Vec16h>(x0, (int)n);
+}
+
+// Raise floating point numbers to integer power n, where n is a compile-time constant:
+// Template in vectorf28.h is used
+//template <typename V, int n>
+//static inline V pow_n(V const a);
+
+// implement as function pow(vector, const_int)
+template <int n>
+static inline Vec16h pow(Vec16h const a, Const_int_t<n>) {
+    return pow_n<Vec16h, n>(a);
+} 
+
+
+static inline Vec16h round(Vec16h const a) {
+    return to_float16(round(to_float(a)));
+}
+
+// function truncate: round towards zero. (result as float vector)
+static inline Vec16h truncate(Vec16h const a) {
+    return to_float16(truncate(to_float(a)));
+}
+
+// function floor: round towards minus infinity. (result as float vector)
+static inline Vec16h floor(Vec16h const a) {
+    return to_float16(floor(to_float(a)));
+}
+
+// function ceil: round towards plus infinity. (result as float vector)
+static inline Vec16h ceil(Vec16h const a) {
+    return to_float16(ceil(to_float(a)));
+}
+
+
+// function roundi: round to nearest integer (even). (result as integer vector)
+static inline Vec16s roundi(Vec16h const a) {
+    // Note: assume MXCSR control register is set to rounding
+    return compress_saturated(roundi(to_float(a)));
+}
+
+// function truncatei: round towards zero. (result as integer vector)
+static inline Vec16s truncatei(Vec16h const a) {
+    return compress_saturated(truncatei(to_float(a)));
+}
+
+// function to_float: convert integer vector to float vector
+static inline Vec16h to_float16(Vec16s const a) {
+    return to_float16(to_float(extend(a)));
+}
+
+// function to_float: convert unsigned integer vector to float vector
+static inline Vec16h to_float16(Vec16us const a) {
+    return to_float16(to_float(extend(a)));
+}
+
+
+// Approximate math functions
+
+// reciprocal (almost exact)
+static inline Vec16h approx_recipr(Vec16h const a) {
+    return to_float16(approx_recipr(to_float(a)));
+}
+
+// reciprocal squareroot (almost exact)
+static inline Vec16h approx_rsqrt(Vec16h const a) {
+    return to_float16(approx_rsqrt(to_float(a)));
+}
+
+// Fused multiply and add functions
+
+// Multiply and add. a*b+c
+static inline Vec16h mul_add(Vec16h const a, Vec16h const b, Vec16h const c) {
+    return to_float16(mul_add(to_float(a),to_float(b),to_float(c)));
+}
+
+// Multiply and subtract. a*b-c
+static inline Vec16h mul_sub(Vec16h const a, Vec16h const b, Vec16h const c) {
+    return to_float16(mul_sub(to_float(a),to_float(b),to_float(c)));
+}
+
+// Multiply and inverse subtract
+static inline Vec16h nmul_add(Vec16h const a, Vec16h const b, Vec16h const c) {
+    return to_float16(nmul_add(to_float(a),to_float(b),to_float(c)));
+}
+
+// Math functions using fast bit manipulation
+
+// Extract the exponent as an integer
+// exponent(a) = floor(log2(abs(a)));
+// exponent(1.0f) = 0, exponent(0.0f) = -127, exponent(INF) = +128, exponent(NAN) = +128
+static inline Vec16s exponent(Vec16h const a) {
+    Vec16us t1 = reinterpret_i(a);         // reinterpret as 16-bit integer
+    Vec16us t2 = t1 << 1;                  // shift out sign bit
+    Vec16us t3 = t2 >> 11;                 // shift down logical to position 0
+    Vec16s  t4 = Vec16s(t3) - 0x0F;        // subtract bias from exponent
+    return t4;
+}
+
+// Extract the fraction part of a floating point number
+// a = 2^exponent(a) * fraction(a), except for a = 0
+// fraction(1.0f) = 1.0f, fraction(5.0f) = 1.25f
+// NOTE: The name fraction clashes with an ENUM in MAC XCode CarbonCore script.h !
+static inline Vec16h fraction(Vec16h const a) {
+    Vec16us t1 = reinterpret_i(a);   // reinterpret as 16-bit integer
+    Vec16us t2 = Vec16us((t1 & 0x3FF) | 0x3C00); // set exponent to 0 + bias
+    return reinterpret_h(t2);
+}
+
+// Fast calculation of pow(2,n) with n integer
+// n  =    0 gives 1.0f
+// n >=  16 gives +INF
+// n <= -15 gives 0.0f
+// This function will never produce subnormals, and never raise exceptions
+static inline Vec16h exp2(Vec16s const n) {
+    Vec16s t1 = max(n, -15);            // limit to allowed range
+    Vec16s t2 = min(t1, 16);
+    Vec16s t3 = t2 + 15;                // add bias
+    Vec16s t4 = t3 << 10;               // put exponent into position 10
+    return reinterpret_h(t4);           // reinterpret as float
+}
+
+// change signs on vectors Vec16h
+// Each index i0 - i15 is 1 for changing sign on the corresponding element, 0 for no change
+template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7, 
+int i8, int i9, int i10, int i11, int i12, int i13, int i14, int i15>
+Vec16h change_sign(Vec16h const a) {
+#if INSTRSET >= 8
+    if constexpr ((i0 | i1 | i2 | i3 | i4 | i5 | i6 | i7 | i8 | i9 | i10 | i11 | i12 | i13 | i14 | i15) == 0) return a;
+    __m256i mask = constant8ui<
+        (i0  ? 0x8000 : 0) | (i1  ? 0x80000000 : 0), 
+        (i2  ? 0x8000 : 0) | (i3  ? 0x80000000 : 0), 
+        (i4  ? 0x8000 : 0) | (i5  ? 0x80000000 : 0), 
+        (i6  ? 0x8000 : 0) | (i7  ? 0x80000000 : 0), 
+        (i8  ? 0x8000 : 0) | (i9  ? 0x80000000 : 0), 
+        (i10 ? 0x8000 : 0) | (i11 ? 0x80000000 : 0), 
+        (i12 ? 0x8000 : 0) | (i13 ? 0x80000000 : 0), 
+        (i14 ? 0x8000 : 0) | (i15 ? 0x80000000 : 0) >();
+    return Vec16h(_mm256_xor_si256(a, mask));     // flip sign bits
+#else
+    return Vec16h(change_sign<i0,i1,i2,i3,i4,i5,i6,i7>(a.get_low()), change_sign<i8,i9,i10,i11,i12,i13,i14,i15>(a.get_high()));
+#endif
+}
+
+
+/*****************************************************************************
+*
+*          Vector permute and blend functions
+*
+******************************************************************************
+*
+* The permute function can reorder the elements of a vector and optionally
+* set some elements to zero.
+*
+* See vectori128.h for details
+*
+*****************************************************************************/
+// permute vector Vec16h
+template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7, 
+int i8, int i9, int i10, int i11, int i12, int i13, int i14, int i15>
+Vec16h permute16(Vec16h const a) {
+    return reinterpret_h (
+    permute16<i0, i1, i2, i3, i4, i5, i6, i7, i8, i9, i10, i11, i12, i13, i14, i15> (
+    Vec16s(reinterpret_i(a))));
+}
+
+/*****************************************************************************
+*
+*          Vector blend functions
+*
+*****************************************************************************/
+
+// permute and blend Vec16h
+template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7, 
+int i8, int i9, int i10, int i11, int i12, int i13, int i14, int i15>
+static inline Vec16h blend16(Vec16h const a, Vec16h const b) {
+    return reinterpret_h (
+    blend16<i0, i1, i2, i3, i4, i5, i6, i7, i8, i9, i10, i11, i12, i13, i14, i15> (
+    Vec16s(reinterpret_i(a)), Vec16s(reinterpret_i(b))));
+}
+
+
+/*****************************************************************************
+*
+*          Vector lookup functions
+*
+******************************************************************************
+*
+* These functions use vector elements as indexes into a table.
+* The table is given as one or more vectors or as an array.
+*
+*****************************************************************************/
+
+static inline Vec16h lookup16 (Vec16s const index, Vec16h const table) {
+    return reinterpret_h(lookup16(index, Vec16s(reinterpret_i(table))));
+}
+
+template <int n>
+static inline Vec16h lookup(Vec16s const index, void const * table) {
+    return reinterpret_h(lookup<n>(index, (void const *)(table)));
+}
+
+// prevent implicit type conversions
+bool horizontal_and(Vec16h x) = delete;
+bool horizontal_or(Vec16h x) = delete;
+
+#endif // MAX_VECTOR_SIZE >= 256
+
+
+
+/*****************************************************************************
+*
+*          512 bit vectors
+*
+*****************************************************************************/
+
+#if MAX_VECTOR_SIZE >= 512
+
+
+/*****************************************************************************
+*
+*          Vec32hb: Vector of 32 Booleans for use with Vec32h
+*
+*****************************************************************************/
+
+#if INSTRSET >= 10
+typedef Vec32b Vec32hb;   // compact boolean vector
+
+#else
+
+typedef Vec32sb Vec32hb;  // broad boolean vector
+
+#endif
+
+ 
+/*****************************************************************************
+*
+*          Vec32h: Vector of 4 single precision floating point values
+*
+*****************************************************************************/
+
+class Vec32h : public Vec32s {
+public:
+    // Default constructor:
+    Vec32h() = default;
+    // Constructor to broadcast the same value into all elements:
+    Vec32h(Float16 f) : Vec32s(castfp162s(f)) {}   
+    Vec32h(float f) : Vec32s(castfp162s(Float16(f))) {} 
+
+    // Copy constructor
+    Vec32h (Vec32h const &x) = default;
+
+    // Copy assignment operator
+    Vec32h & operator = (Vec32h const& x) = default;
+
+    // Constructor to build from all elements:
+    Vec32h(Float16 f0, Float16 f1, Float16 f2, Float16 f3, Float16 f4, Float16 f5, Float16 f6, Float16 f7,
+    Float16 f8, Float16 f9, Float16 f10, Float16 f11, Float16 f12, Float16 f13, Float16 f14, Float16 f15,
+    Float16 f16, Float16 f17, Float16 f18, Float16 f19, Float16 f20, Float16 f21, Float16 f22, Float16 f23,
+    Float16 f24, Float16 f25, Float16 f26, Float16 f27, Float16 f28, Float16 f29, Float16 f30, Float16 f31) :
+        Vec32s (castfp162s(f0), castfp162s(f1), castfp162s(f2), castfp162s(f3), castfp162s(f4), castfp162s(f5), castfp162s(f6), castfp162s(f7), 
+            castfp162s(f8), castfp162s(f9), castfp162s(f10), castfp162s(f11), castfp162s(f12), castfp162s(f13), castfp162s(f14), castfp162s(f15),
+            castfp162s(f16), castfp162s(f17), castfp162s(f18), castfp162s(f19), castfp162s(f20), castfp162s(f21), castfp162s(f22), castfp162s(f23),
+            castfp162s(f24), castfp162s(f25), castfp162s(f26), castfp162s(f27), castfp162s(f28), castfp162s(f29), castfp162s(f30), castfp162s(f31))
+    {}
+    // Constructor to build from two Vec16h:
+    Vec32h(Vec16h const a0, Vec16h const a1) : Vec32s(Vec16h(a0), Vec16h(a1)) {}
+
+    // Constructor to convert from type __m512i used in intrinsics:
+#if INSTRSET >= 10
+    Vec32h(__m512i const x) {
+        zmm = x;
+    }
+    // Assignment operator to convert from type __m256i used in intrinsics:
+    Vec32h & operator = (__m512i const x) {
+        zmm = x;
+        return *this;
+    }
+    // Type cast operator to convert to __m256i used in intrinsics
+    operator __m512i() const {
+        return zmm;
+    }
+#else
+    // Constructor to convert from type Vec32s. This may cause undesired implicit conversions and ambiguities
+    // Vec32h(Vec32s const x) : Vec32s(x) {  }
+#endif
+    // Member function to load from array (unaligned)
+    Vec32h & load(void const * p) {
+        Vec32s::load(p);
+        return *this;
+    }
+    // Member function to load from array, aligned by 64
+    // You may use load_a instead of load if you are certain that p points to an address
+    // divisible by 64. In most cases there is no difference in speed between load and load_a
+    Vec32h & load_a(void const * p) {
+        Vec32s::load_a(p);
+        return *this;
+    }
+    // Member function to store into array (unaligned)
+    // void store(void * p) const // inherited from Vec32s
+
+    // Member function storing into array, aligned by 64
+    // You may use store_a instead of store if you are certain that p points to an address
+    // divisible by 64.
+    //void store_a(void * p) const // inherited from Vec32s
+
+    // Member function storing to aligned uncached memory (non-temporal store).
+    // This may be more efficient than store_a when storing large blocks of memory if it 
+    // is unlikely that the data will stay in the cache until it is read again.
+    // Note: Will generate runtime error if p is not aligned by 64
+    // void store_nt(void * p) const // inherited from Vec32s
+
+    // Partial load. Load n elements and set the rest to 0
+    Vec32h & load_partial(int n, void const * p) {
+        Vec32s::load_partial(n, p);
+        return *this;
+    }
+    // Partial store. Store n elements
+    // void store_partial(int n, void * p) const // inherited from Vec32s
+
+    // cut off vector to n elements. The last 8-n elements are set to zero
+    Vec32h & cutoff(int n) {
+        Vec32s::cutoff(n);
+        return *this;
+    }
+    // Member function to change a single element in vector
+    Vec32h const insert(int index, Float16 a) {
+        Vec32s::insert(index, castfp162s(a));
+        return *this;
+    }
+    // Member function extract a single element from vector
+    Float16 extract(int index) const {
+        return casts2fp16(Vec32s::extract(index));
+    }
+    // Extract a single element. Use store function if extracting more than one element.
+    // Operator [] can only read an element, not write.
+    Float16 operator [] (int index) const {
+        return extract(index);
+    }
+    Vec16h get_low() const {
+#if INSTRSET >= 8
+        return __m256i(Vec32s::get_low());
+#else
+        return reinterpret_h(Vec32s::get_low());
+#endif
+    }
+
+    Vec16h get_high() const {
+#if INSTRSET >= 8
+        return __m256i(Vec32s::get_high());
+#else
+        return reinterpret_h(Vec32s::get_high());
+#endif
+    }
+    static constexpr int size() {
+        return 32;
+    }
+    static constexpr int elementtype() {
+        return 15;
+    }
+};
+
+
+/*****************************************************************************
+*
+*          Operators for Vec32h
+*
+*****************************************************************************/
+
+// vector operator + : add element by element
+static inline Vec32h operator + (Vec32h const a, Vec32h const b) {
+    return Vec32h(a.get_low() + b.get_low(), a.get_high() + b.get_high());
+}
+
+// vector operator + : add vector and scalar
+static inline Vec32h operator + (Vec32h const a, Float16 b) {
+    return a + Vec32h(b);
+}
+static inline Vec32h operator + (Float16 a, Vec32h const b) {
+    return Vec32h(a) + b;
+}
+
+// vector operator += : add
+static inline Vec32h & operator += (Vec32h & a, Vec32h const b) {
+    a = a + b;
+    return a;
+}
+
+// postfix operator ++
+static inline Vec32h operator ++ (Vec32h & a, int) {
+    Vec32h a0 = a;
+    a = a + Float16(1.f);
+    return a0;
+}
+
+// prefix operator ++
+static inline Vec32h & operator ++ (Vec32h & a) {
+    a = a + Float16(1.f);
+    return a;
+}
+
+// vector operator - : subtract element by element
+static inline Vec32h operator - (Vec32h const a, Vec32h const b) {
+    return Vec32h(a.get_low() - b.get_low(), a.get_high() - b.get_high());
+}
+
+// vector operator - : subtract vector and scalar
+static inline Vec32h operator - (Vec32h const a, Float16 b) {
+    return a - Vec32h(b);
+}
+static inline Vec32h operator - (Float16 a, Vec32h const b) {
+    return Vec32h(a) - b;
+}
+
+// vector operator - : unary minus
+// Change sign bit, even for 0, INF and NAN
+static inline Vec32h operator - (Vec32h const a) {
+#if INSTRSET >= 10  // AVX2
+    return _mm512_xor_si512(a, _mm512_set1_epi32(0x80008000));
+#else
+    return Vec32h(-a.get_low(), -a.get_high());
+#endif
+}
+
+// vector operator -= : subtract
+static inline Vec32h & operator -= (Vec32h & a, Vec32h const b) {
+    a = a - b;
+    return a;
+}
+
+// postfix operator --
+static inline Vec32h operator -- (Vec32h & a, int) {
+    Vec32h a0 = a;
+    a = a - Vec32h(Float16(1.f));
+    return a0;
+}
+
+// prefix operator --
+static inline Vec32h & operator -- (Vec32h & a) {
+    a = a - Vec32h(Float16(1.f));
+    return a;
+}
+
+// vector operator * : multiply element by element
+static inline Vec32h operator * (Vec32h const a, Vec32h const b) {
+    return Vec32h(a.get_low() * b.get_low(), a.get_high() * b.get_high());
+}
+
+// vector operator * : multiply vector and scalar
+static inline Vec32h operator * (Vec32h const a, Float16 b) {
+    return a * Vec32h(b);
+}
+static inline Vec32h operator * (Float16 a, Vec32h const b) {
+    return Vec32h(a) * b;
+}
+
+// vector operator *= : multiply
+static inline Vec32h & operator *= (Vec32h & a, Vec32h const b) {
+    a = a * b;
+    return a;
+}
+
+// vector operator / : divide all elements by same integer
+static inline Vec32h operator / (Vec32h const a, Vec32h const b) {
+    return Vec32h(a.get_low() / b.get_low(), a.get_high() / b.get_high());
+}
+
+// vector operator / : divide vector and scalar
+static inline Vec32h operator / (Vec32h const a, Float16 b) {
+    return a / Vec32h(b);
+}
+static inline Vec32h operator / (Float16 a, Vec32h const b) {
+    return Vec32h(a) / b;
+}
+
+// vector operator /= : divide
+static inline Vec32h & operator /= (Vec32h & a, Vec32h const b) {
+    a = a / b;
+    return a;
+}
+
+// vector operator == : returns true for elements for which a == b
+static inline Vec32hb operator == (Vec32h const a, Vec32h const b) {
+    return Vec32hb(a.get_low() == b.get_low(), a.get_high() == b.get_high());
+}
+
+// vector operator != : returns true for elements for which a != b
+static inline Vec32hb operator != (Vec32h const a, Vec32h const b) {
+    return Vec32hb(a.get_low() != b.get_low(), a.get_high() != b.get_high());
+}
+
+// vector operator < : returns true for elements for which a < b
+static inline Vec32hb operator < (Vec32h const a, Vec32h const b) {
+    return Vec32hb(a.get_low() < b.get_low(), a.get_high() < b.get_high());
+}
+
+// vector operator <= : returns true for elements for which a <= b
+static inline Vec32hb operator <= (Vec32h const a, Vec32h const b) {
+    return Vec32hb(a.get_low() <= b.get_low(), a.get_high() <= b.get_high());
+}
+
+// vector operator > : returns true for elements for which a > b
+static inline Vec32hb operator > (Vec32h const a, Vec32h const b) {
+    return Vec32hb(a.get_low() > b.get_low(), a.get_high() > b.get_high());
+}
+
+// vector operator >= : returns true for elements for which a >= b
+static inline Vec32hb operator >= (Vec32h const a, Vec32h const b) {
+    return Vec32hb(a.get_low() >= b.get_low(), a.get_high() >= b.get_high());
+}
+
+
+// Bitwise logical operators
+
+// vector operator & : bitwise and
+static inline Vec32h operator & (Vec32h const a, Vec32h const b) {
+#if INSTRSET >= 10         
+    return _mm512_and_si512(__m512i(a), __m512i(b));
+#else
+    return Vec32h(a.get_low() & b.get_low(), a.get_high() & b.get_high());
+#endif
+}
+
+// vector operator &= : bitwise and
+static inline Vec32h & operator &= (Vec32h & a, Vec32h const b) {
+    a = a & b;
+    return a;
+}
+
+// vector operator & : bitwise and of Vec32h and Vec32hb
+static inline Vec32h operator & (Vec32h const a, Vec32hb const b) {
+#if INSTRSET >= 10         
+    return _mm512_maskz_mov_epi16(b, a);
+#else
+    return Vec32h(a.get_low() & b.get_low(), a.get_high() & b.get_high());
+#endif
+}
+static inline Vec32h operator & (Vec32hb const a, Vec32h const b) {
+    return b & a;
+}
+
+// vector operator | : bitwise or
+static inline Vec32h operator | (Vec32h const a, Vec32h const b) {
+#if INSTRSET >= 10         
+    return _mm512_or_si512(__m512i(a), __m512i(b));
+#else
+    return Vec32h(a.get_low() | b.get_low(), a.get_high() | b.get_high());
+#endif
+}
+
+// vector operator |= : bitwise or
+static inline Vec32h & operator |= (Vec32h & a, Vec32h const b) {
+    a = a | b;
+    return a;
+}
+
+// vector operator ^ : bitwise xor
+static inline Vec32h operator ^ (Vec32h const a, Vec32h const b) {
+#if INSTRSET >= 10         
+    return _mm512_xor_si512(__m512i(a), __m512i(b));
+#else
+    return Vec32h(a.get_low() ^ b.get_low(), a.get_high() ^ b.get_high());
+#endif
+}
+
+// vector operator ^= : bitwise xor
+static inline Vec32h & operator ^= (Vec32h & a, Vec32h const b) {
+    a = a ^ b;
+    return a;
+}
+
+// vector operator ! : logical not. Returns Boolean vector
+static inline Vec32hb operator ! (Vec32h const a) {
+    return a == Vec32h(Float16(0.f));
+}
+
+
+/*****************************************************************************
+*
+*          Functions for reinterpretation between vector types
+*
+*****************************************************************************/
+#if INSTRSET >= 10
+static inline __m512i reinterpret_h(__m512i const x) {
+    return x;
+}
+
+#if defined(__GNUC__) && __GNUC__ <= 9 // GCC v. 9 is missing the _mm512_zextsi256_si512 intrinsic
+static inline Vec32h extend_z(Vec16h a) {
+    return Vec32h(a, Vec16h(0));
+}
+#else
+static inline Vec32h extend_z(Vec16h a) {
+    return _mm512_zextsi256_si512(a);
+}
+#endif
+#else
+
+static inline Vec32h reinterpret_h(Vec32s const x) {
+    return Vec32h(Vec16h(reinterpret_h(x.get_low())), Vec16h(reinterpret_h(x.get_high())));
+}
+
+static inline Vec32s reinterpret_i(Vec32h const x) {
+    return Vec32s(Vec16s(x.get_low()), Vec16s(x.get_high()));
+}
+
+static inline Vec32h extend_z(Vec16h a) {
+    return Vec32h(a, Vec16h(Float16(0.f)));
+}
+
+#endif
+
+
+/*****************************************************************************
+*
+*          Functions for Vec32h
+*
+*****************************************************************************/
+
+// Select between two operands. Corresponds to this pseudocode:
+// for (int i = 0; i < 4; i++) result[i] = s[i] ? a[i] : b[i];
+static inline Vec32h select(Vec32hb const s, Vec32h const a, Vec32h const b) {
+#if INSTRSET >= 10
+    return __m512i(_mm512_mask_mov_epi16(__m512i(b), s, __m512i(a)));
+#else
+    return Vec32h(select(s.get_low(), a.get_low(), b.get_low()), select(s.get_high(), a.get_high(), b.get_high()));
+#endif
+}
+
+// Conditional add: For all vector elements i: result[i] = f[i] ? (a[i] + b[i]) : a[i]
+static inline Vec32h if_add(Vec32hb const f, Vec32h const a, Vec32h const b) {
+    return a + (b & f);
+}
+
+// Conditional subtract: For all vector elements i: result[i] = f[i] ? (a[i] - b[i]) : a[i]
+static inline Vec32h if_sub(Vec32hb const f, Vec32h const a, Vec32h const b) {
+    return a - (b & f);
+}
+
+// Conditional multiply: For all vector elements i: result[i] = f[i] ? (a[i] * b[i]) : a[i]
+static inline Vec32h if_mul(Vec32hb const f, Vec32h const a, Vec32h const b) {
+    return select(f, a*b, a);
+}
+
+// Conditional divide: For all vector elements i: result[i] = f[i] ? (a[i] / b[i]) : a[i]
+static inline Vec32h if_div(Vec32hb const f, Vec32h const a, Vec32h const b) {
+    return select(f, a/b, a);
+}
+
+// Sign functions
+
+// Function sign_bit: gives true for elements that have the sign bit set
+// even for -0.0f, -INF and -NAN
+// Note that sign_bit(Vec32h(-0.0f16)) gives true, while Vec32h(-0.0f16) < Vec32h(0.0f16) gives false
+// (the underscore in the name avoids a conflict with a macro in Intel's mathimf.h)
+static inline Vec32hb sign_bit(Vec32h const a) {
+    Vec32s t1 = reinterpret_i(a);          // reinterpret as 16-bit integer
+    Vec32s t2 = t1 >> 15;                  // extend sign bit
+    return t2 != 0;
+}
+
+// Function sign_combine: changes the sign of a when b has the sign bit set
+// same as select(sign_bit(b), -a, a)
+static inline Vec32h sign_combine(Vec32h const a, Vec32h const b) {
+    return a ^ (b & Vec32h(Float16(-0.0)));
+}
+
+// Categorization functions
+
+// Function is_finite: gives true for elements that are normal, subnormal or zero,
+// false for INF and NAN
+// (the underscore in the name avoids a conflict with a macro in Intel's mathimf.h)
+static inline Vec32hb is_finite(Vec32h const a) {
+    return (Vec32s(reinterpret_i(a)) & 0x7C00) != 0x7C00;
+}
+
+// Function is_inf: gives true for elements that are +INF or -INF
+// false for finite numbers and NAN
+// (the underscore in the name avoids a conflict with a macro in Intel's mathimf.h)
+static inline Vec32hb is_inf(Vec32h const a) {
+    return (Vec32s(reinterpret_i(a)) & 0x7FFF) == 0x7C00;
+}
+
+// Function is_nan: gives true for elements that are +NAN or -NAN
+// false for finite numbers and +/-INF
+// (the underscore in the name avoids a conflict with a macro in Intel's mathimf.h)
+static inline Vec32hb is_nan(Vec32h const a) {
+    return (Vec32s(reinterpret_i(a)) & 0x7FFF) > 0x7C00;
+}
+
+// Function is_subnormal: gives true for elements that are subnormal
+// false for finite numbers, zero, NAN and INF
+static inline Vec32hb is_subnormal(Vec32h const a) {
+    return (Vec32s(reinterpret_i(a)) & 0x7C00) == 0 && (Vec32s(reinterpret_i(a)) & 0x03FF) != 0;
+}
+
+// Function is_zero_or_subnormal: gives true for elements that are zero or subnormal
+// false for finite numbers, NAN and INF
+static inline Vec32hb is_zero_or_subnormal(Vec32h const a) {
+    return (Vec32s(reinterpret_i(a)) & 0x7C00) == 0;
+}
+
+// Function infinite32h: returns a vector where all elements are +INF
+static inline Vec32h infinite32h() {
+    return reinterpret_h(Vec32s(0x7C00));
+}
+
+// template for producing quiet NAN
+template <>
+Vec32h nan_vec<Vec32h>(uint32_t payload) {
+    if constexpr (Vec32h::elementtype() == 15) {  // Float16
+        return reinterpret_h(Vec32s(0x7E00 | (payload & 0x01FF)));
+    }
+} 
+
+// Function nan32h: returns a vector where all elements are NAN (quiet)
+static inline Vec32h nan32h(int n = 0x10) {
+    return nan_vec<Vec32h>(n);
+}
+
+// This function returns the code hidden in a NAN. The sign bit is ignored
+static inline Vec32us nan_code(Vec32h const x) {
+    Vec32us a = Vec32us(reinterpret_i(x));
+    Vec32us const n = 0x3FF;
+    return select(is_nan(x), a & n, Vec32us(0));
+}
+
+
+// General arithmetic functions, etc.
+
+// Horizontal add: Calculates the sum of all vector elements.
+static inline Float16 horizontal_add(Vec32h const a) {
+    return horizontal_add(a.get_low()+a.get_high());
+}
+// same, with high precision
+static inline float horizontal_add_x(Vec32h const a) {
+    return horizontal_add_x(a.get_low()) + horizontal_add_x(a.get_high());
+}
+
+// function max: a > b ? a : b
+static inline Vec32h max(Vec32h const a, Vec32h const b) {
+        return Vec32h(max(a.get_low(), b.get_low()), max(a.get_high(), b.get_high()));
+} 
+// function min: a < b ? a : b
+static inline Vec32h min(Vec32h const a, Vec32h const b) {
+        return Vec32h(min(a.get_low(), b.get_low()), min(a.get_high(), b.get_high()));
+}
+// NAN-safe versions of maximum and minimum are in vector_convert.h
+
+// function abs: absolute value
+static inline Vec32h abs(Vec32h const a) {
+    return reinterpret_h(Vec32s(reinterpret_i(a)) & 0x7FFF);
+}
+
+// function sqrt: square root
+static inline Vec32h sqrt(Vec32h const a) {
+    return Vec32h(sqrt(a.get_low()), sqrt(a.get_high()));
+}
+
+// function square: a * a
+static inline Vec32h square(Vec32h const a) {
+    return a * a;
+}
+
+// The purpose of this template is to prevent implicit conversion of a float
+// exponent to int when calling pow(vector, float) and vectormath_exp.h is not included
+template <typename TT> static Vec32h pow(Vec32h const a, TT const n);  // = delete
+
+// Raise floating point numbers to integer power n
+template <>
+inline Vec32h pow<int>(Vec32h const x0, int const n) {
+    return pow_template_i<Vec32h>(x0, n);
+}
+
+// allow conversion from unsigned int
+template <>
+inline Vec32h pow<uint32_t>(Vec32h const x0, uint32_t const n) {
+    return pow_template_i<Vec32h>(x0, (int)n);
+}
+
+// Raise floating point numbers to integer power n, where n is a compile-time constant:
+// Template in vectorf28.h is used
+//template <typename V, int n>
+//static inline V pow_n(V const a);
+
+// implement as function pow(vector, const_int)
+template <int n>
+static inline Vec32h pow(Vec32h const a, Const_int_t<n>) {
+    return pow_n<Vec32h, n>(a);
+}
+
+static inline Vec32h round(Vec32h const a) {
+    return Vec32h(round(a.get_low()), round(a.get_high()));
+}
+
+// function truncate: round towards zero. (result as float vector)
+static inline Vec32h truncate(Vec32h const a) {
+    return Vec32h(truncate(a.get_low()), truncate(a.get_high()));
+}
+
+// function floor: round towards minus infinity. (result as float vector)
+static inline Vec32h floor(Vec32h const a) {
+    return Vec32h(floor(a.get_low()), floor(a.get_high()));
+}
+
+// function ceil: round towards plus infinity. (result as float vector)
+static inline Vec32h ceil(Vec32h const a) {
+    return Vec32h(ceil(a.get_low()), ceil(a.get_high()));
+}
+
+// function roundi: round to nearest integer (even). (result as integer vector)
+static inline Vec32s roundi(Vec32h const a) {
+    return Vec32s(roundi(a.get_low()), roundi(a.get_high()));
+}
+
+// function truncatei: round towards zero. (result as integer vector)
+static inline Vec32s truncatei(Vec32h const a) {
+    return Vec32s(truncatei(a.get_low()), truncatei(a.get_high()));
+}
+
+// function to_float: convert integer vector to float vector
+static inline Vec32h to_float16(Vec32s const a) {
+    return Vec32h(to_float16(a.get_low()), to_float16(a.get_high()));
+}
+
+// function to_float: convert unsigned integer vector to float vector
+static inline Vec32h to_float16(Vec32us const a) {
+    return Vec32h(to_float16(a.get_low()), to_float16(a.get_high()));
+}
+
+// Approximate math functions
+
+// reciprocal (almost exact)
+static inline Vec32h approx_recipr(Vec32h const a) {
+    return Vec32h(approx_recipr(a.get_low()), approx_recipr(a.get_high()));
+}
+
+// reciprocal squareroot (almost exact)
+static inline Vec32h approx_rsqrt(Vec32h const a) {
+    return Vec32h(approx_rsqrt(a.get_low()), approx_rsqrt(a.get_high()));
+}
+
+// Fused multiply and add functions
+
+// Multiply and add. a*b+c
+static inline Vec32h mul_add(Vec32h const a, Vec32h const b, Vec32h const c) {
+    return Vec32h(mul_add(a.get_low(), b.get_low(), c.get_low()), mul_add(a.get_high(), b.get_high(), c.get_high()));
+}
+
+// Multiply and subtract. a*b-c
+static inline Vec32h mul_sub(Vec32h const a, Vec32h const b, Vec32h const c) {
+    return Vec32h(mul_sub(a.get_low(), b.get_low(), c.get_low()), mul_sub(a.get_high(), b.get_high(), c.get_high()));
+}
+
+// Multiply and inverse subtract
+static inline Vec32h nmul_add(Vec32h const a, Vec32h const b, Vec32h const c) {
+    return Vec32h(nmul_add(a.get_low(), b.get_low(), c.get_low()), nmul_add(a.get_high(), b.get_high(), c.get_high()));
+}
+
+// Math functions using fast bit manipulation
+
+// Extract the exponent as an integer
+// exponent(a) = floor(log2(abs(a)));
+// exponent(1.0f) = 0, exponent(0.0f) = -127, exponent(INF) = +128, exponent(NAN) = +128
+static inline Vec32s exponent(Vec32h const a) {
+    Vec32us t1 = reinterpret_i(a);         // reinterpret as 16-bit integer
+    Vec32us t2 = t1 << 1;                  // shift out sign bit
+    Vec32us t3 = t2 >> 11;                 // shift down logical to position 0
+    Vec32s  t4 = Vec32s(t3) - Vec32s(0x0F);// subtract bias from exponent
+    return t4;
+}
+
+// Extract the fraction part of a floating point number
+// a = 2^exponent(a) * fraction(a), except for a = 0
+// fraction(1.0f) = 1.0f, fraction(5.0f) = 1.25f
+// NOTE: The name fraction clashes with an ENUM in MAC XCode CarbonCore script.h !
+static inline Vec32h fraction(Vec32h const a) {
+    Vec32us t1 = reinterpret_i(a);   // reinterpret as 16-bit integer
+    Vec32us t2 = Vec32us((t1 & 0x3FF) | 0x3C00); // set exponent to 0 + bias
+    return reinterpret_h(t2);
+}
+
+// Fast calculation of pow(2,n) with n integer
+// n  =    0 gives 1.0f
+// n >=  16 gives +INF
+// n <= -15 gives 0.0f
+// This function will never produce subnormals, and never raise exceptions
+static inline Vec32h exp2(Vec32s const n) {
+    Vec32s t1 = max(n, -15);            // limit to allowed range
+    Vec32s t2 = min(t1, 16);
+    Vec32s t3 = t2 + Vec32s(15);        // add bias
+    Vec32s t4 = t3 << 10;               // put exponent into position 10
+    return reinterpret_h(t4);           // reinterpret as float
+}
+
+
+// change signs on vectors Vec32h
+// Each index i0 - i31 is 1 for changing sign on the corresponding element, 0 for no change
+template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7, 
+int i8, int i9, int i10, int i11, int i12, int i13, int i14, int i15,
+int i16, int i17, int i18, int i19, int i20, int i21, int i22, int i23,
+int i24, int i25, int i26, int i27, int i28, int i29, int i30, int i31 >
+static inline Vec32h change_sign(Vec32h const a) {
+    
+#if INSTRSET >= 10
+    if constexpr ((i0 | i1 | i2 | i3 | i4 | i5 | i6 | i7 | i8 | i9 | i10 | i11 | i12 | i13 | i14 | i15) == 0) return a;
+    __m512i mask = constant16ui<
+        (i0  ? 0x8000 : 0) | (i1  ? 0x80000000 : 0), 
+        (i2  ? 0x8000 : 0) | (i3  ? 0x80000000 : 0), 
+        (i4  ? 0x8000 : 0) | (i5  ? 0x80000000 : 0), 
+        (i6  ? 0x8000 : 0) | (i7  ? 0x80000000 : 0), 
+        (i8  ? 0x8000 : 0) | (i9  ? 0x80000000 : 0), 
+        (i10 ? 0x8000 : 0) | (i11 ? 0x80000000 : 0), 
+        (i12 ? 0x8000 : 0) | (i13 ? 0x80000000 : 0), 
+        (i14 ? 0x8000 : 0) | (i15 ? 0x80000000 : 0),        
+        (i16 ? 0x8000 : 0) | (i17 ? 0x80000000 : 0), 
+        (i18 ? 0x8000 : 0) | (i19 ? 0x80000000 : 0), 
+        (i20 ? 0x8000 : 0) | (i21 ? 0x80000000 : 0), 
+        (i22 ? 0x8000 : 0) | (i23 ? 0x80000000 : 0), 
+        (i24 ? 0x8000 : 0) | (i25 ? 0x80000000 : 0), 
+        (i26 ? 0x8000 : 0) | (i27 ? 0x80000000 : 0), 
+        (i28 ? 0x8000 : 0) | (i29 ? 0x80000000 : 0), 
+        (i30 ? 0x8000 : 0) | (i31 ? 0x80000000 : 0) >();
+    return  _mm512_xor_si512(a, mask);     // flip sign bits
+#else
+    return Vec32h(change_sign<i0,i1,i2,i3,i4,i5,i6,i7,i8,i9,i10,i11,i12,i13,i14,i15>(a.get_low()), 
+        change_sign<i16,i17,i18,i19,i20,i21,i22,i23,i24,i25,i26,i27,i28,i29,i30,i31>(a.get_high()));
+#endif
+}
+    
+/*****************************************************************************
+*
+*          Vector permute and blend functions
+*
+******************************************************************************
+*
+* The permute function can reorder the elements of a vector and optionally
+* set some elements to zero.
+*
+* See vectori128.h for details
+*
+*****************************************************************************/
+
+// permute vector Vec32h
+template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7, 
+int i8, int i9, int i10, int i11, int i12, int i13, int i14, int i15,
+int i16, int i17, int i18, int i19, int i20, int i21, int i22, int i23,
+int i24, int i25, int i26, int i27, int i28, int i29, int i30, int i31 >
+static inline Vec32h permute32(Vec32h const a) {
+    return reinterpret_h (
+    permute32<i0, i1, i2, i3, i4, i5, i6, i7, i8, i9, i10, i11, i12, i13, i14, i15,
+    i16, i17, i18, i19, i20, i21, i22, i23, i24, i25, i26, i27, i28, i29, i30, i31 > (
+    Vec32s(reinterpret_i(a))));
+}
+
+/*****************************************************************************
+*
+*          Vector blend functions
+*
+*****************************************************************************/
+
+// permute and blend Vec32h
+template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7, 
+int i8, int i9, int i10, int i11, int i12, int i13, int i14, int i15,
+int i16, int i17, int i18, int i19, int i20, int i21, int i22, int i23,
+int i24, int i25, int i26, int i27, int i28, int i29, int i30, int i31 >
+static inline Vec32h blend32(Vec32h const a, Vec32h const b) {
+    return reinterpret_h (
+    blend32<i0, i1, i2, i3, i4, i5, i6, i7, i8, i9, i10, i11, i12, i13, i14, i15,
+    i16, i17, i18, i19, i20, i21, i22, i23, i24, i25, i26, i27, i28, i29, i30, i31 > (
+    Vec32s(reinterpret_i(a)), Vec32s(reinterpret_i(b))));
+}
+
+
+/*****************************************************************************
+*
+*          Vector lookup functions
+*
+******************************************************************************
+*
+* These functions use vector elements as indexes into a table.
+* The table is given as one or more vectors or as an array.
+*
+*****************************************************************************/
+
+static inline Vec32h lookup32 (Vec32s const index, Vec32h const table) {
+    return reinterpret_h(lookup32(index, Vec32s(reinterpret_i(table))));
+}
+
+template <int n>
+static inline Vec32h lookup(Vec32s const index, void const * table) {
+    return reinterpret_h(lookup<n>(index, (void const *)(table)));
+}
+
+// prevent implicit type conversions
+bool horizontal_and(Vec32h x) = delete;
+bool horizontal_or(Vec32h x) = delete;
+
+
+#endif // MAX_VECTOR_SIZE >= 512
+
+
+/*****************************************************************************
+*
+*          Mathematical functions
+*
+*****************************************************************************/
+
+template <typename V>
+static inline V vf_pow2n (V const n) {
+    typedef decltype(roundi(n)) VI;
+    const float pow2_23 =  8388608.0;            // 2^23
+    const float bias = 127.0;                    // bias in exponent
+    V a = n + (bias + pow2_23);                  // put n + bias in least significant bits
+    VI b = reinterpret_i(a);                     // bit-cast to integer
+    VI c = b << 23;                              // shift left 23 places to get into exponent field
+    V d = reinterpret_f(c);                      // bit-cast back to float
+    return d;
+}
+
+// Template for exp function, half precision
+// The limit of abs(x) is defined by max_x below
+// This function does not produce denormals
+// Template parameters:
+// VTYPE:  float vector type
+// M1: 0 for exp, 1 for expm1
+// BA: 0 for exp, 1 for 0.5*exp, 2 for pow(2,x), 10 for pow(10,x)
+
+template<typename VTYPE, int M1, int BA>
+static inline VTYPE exp_h(VTYPE const initial_x) { 
+    // Taylor coefficients
+    const float P0expf   =  1.f/2.f;
+    const float P1expf   =  1.f/6.f;
+    const float P2expf   =  1.f/24.f;
+    VTYPE  x, r, x2, z, n2;                      // data vectors
+    // maximum abs(x), value depends on BA, defined below
+    // The lower limit of x is slightly more restrictive than the upper limit.
+    // We are specifying the lower limit, except for BA = 1 because it is not used for negative x
+    float max_x;
+    if constexpr (BA <= 1) { // exp(x)
+        //const float ln2f_hi  =  0.693359375f;
+        //const float ln2f_lo  = -2.12194440e-4f;
+        const float ln2f  =  0.69314718f;
+        max_x = (BA == 0) ? 87.3f : 89.0f;
+        x = initial_x;
+        r = round(initial_x*float(1.44269504089f)); //VM_LOG2E
+        x = nmul_add(r, VTYPE(ln2f), x);         //  x -= r * ln2f;
+    }
+    else if constexpr (BA == 2) {                // pow(2,x)
+        max_x = 126.f;
+        r = round(initial_x);
+        x = initial_x - r;
+        x = x * 0.69314718056f; // (float)VM_LN2;
+    }
+    else if constexpr (BA == 10) {               // pow(10,x)
+        max_x = 37.9f;
+        const float log10_2 = 0.30102999566f;   // log10(2)
+        x = initial_x;
+        r = round(initial_x*float(3.32192809489f)); // VM_LOG2E*VM_LN10
+        x = nmul_add(r, VTYPE(log10_2), x);      //  x -= r * log10_2
+        x = x * 2.30258509299f;  // (float)VM_LN10;
+    }
+    else  {  // undefined value of BA
+        return 0.;
+    }
+    x2 = x * x;
+    //z = polynomial_2(x,P0expf,P1expf,P2expf);
+    z = mul_add(x2, P2expf, mul_add(x, P1expf, P0expf));
+    z = mul_add(z, x2, x);                       // z *= x2;  z += x;
+    if constexpr (BA == 1) r--;                  // 0.5 * exp(x)
+    n2 = vf_pow2n(r);                            // multiply by power of 2
+    if constexpr (M1 == 0) {                     // exp        
+        z = (z + 1.0f) * n2;
+    }
+    else {                                       // expm1
+        z = mul_add(z, n2, n2 - 1.0f);           //  z = z * n2 + (n2 - 1.0f);
+#ifdef SIGNED_ZERO                               // pedantic preservation of signed zero
+        z = select(initial_x == 0.f, initial_x, z);
+#endif
+    }
+    // check for overflow
+    auto inrange  = abs(initial_x) < max_x;      // boolean vector
+    // check for INF and NAN
+    inrange &= is_finite(initial_x);
+    if (horizontal_and(inrange)) {               // fast normal path
+        return z;
+    }
+    else {
+        // overflow, underflow and NAN
+        VTYPE inf = 1.e20f;                                // will overflow to INF
+        r = select(sign_bit(initial_x), 0.f-(M1&1), inf);  // value in case of +/- overflow or INF
+        z = select(inrange, z, r);                         // +/- underflow
+        z = select(is_nan(initial_x), initial_x, z);       // NAN goes through
+        return z;
+    }
+}
+
+
+// Template for trigonometric functions
+// Template parameters:
+// VTYPE:  vector type
+// SC:     1 = sin, 2 = cos, 3 = sincos, 4 = tan, 8 = multiply by pi
+// Parameters:
+// xx = input x (radians)
+// cosret = return pointer (only if SC = 3)
+template<typename VTYPE, int SC>
+static inline VTYPE sincos_h(VTYPE * cosret, VTYPE const xx) {
+
+    // define constants
+    const float DP1F = 0.78515625f * 2.f;
+    const float DP2F = 2.4187564849853515625E-4f * 2.f;
+    const float DP3F = 3.77489497744594108E-8f * 2.f;
+
+    const float P0sinf = -1.6666654611E-1f;
+    const float P1sinf = 8.3321608736E-3f;
+
+    const float P0cosf = 4.166664568298827E-2f;
+    const float P1cosf = -1.388731625493765E-3f;
+
+    const float pi     = 3.14159265358979323846f;// pi
+    const float c2_pi  = float(2./3.14159265358979323846); // 2/pi
+
+    typedef decltype(roundi(xx)) ITYPE;          // integer vector type
+    typedef decltype(xx < xx) BVTYPE;            // boolean vector type
+
+    VTYPE  xa, x, y, x2, s, c, sin1, cos1;       // data vectors
+    ITYPE  q;                                    // integer vector
+    BVTYPE swap;                                 // boolean vector
+
+    xa = abs(xx);
+
+    // Find quadrant
+    if constexpr ((SC & 8) != 0) {
+        y = round(xa * VTYPE(2.0f));
+    }
+    else {
+        xa = select(xa > VTYPE(314.25f), VTYPE(0.f), xa); // avoid meaningless results for high x
+        y = round(xa * c2_pi);                   // quadrant, as float
+    }
+    q = roundi(y);                               // quadrant, as integer
+    //      0 -   pi/4 => 0
+    //   pi/4 - 3*pi/4 => 1
+    // 3*pi/4 - 5*pi/4 => 2
+    // 5*pi/4 - 7*pi/4 => 3
+    // 7*pi/4 - 8*pi/4 => 4
+
+    if constexpr ((SC & 8) != 0) {               // sinpi
+        // modulo 2: subtract 0.5*y
+        x = nmul_add(y, VTYPE(0.5f), xa) * VTYPE(pi);
+    }
+    else {                                       // sin
+        // Reduce by extended precision modular arithmetic
+#if INSTRSET < 8
+        x = ((xa - y * DP1F) - y * DP2F) - y * DP3F; // accuracy 2 ULP without FMA
+#else
+        x = nmul_add(y, DP2F + DP3F, nmul_add(y, DP1F, xa)); // accuracy 1 ULP with FMA
+#endif
+    }
+
+    // Taylor expansion of sin and cos, valid for -pi/4 <= x <= pi/4
+    x2 = x * x;
+    s = mul_add(x2, P1sinf, P0sinf) * (x*x2) + x;
+    c = mul_add(x2, P1cosf, P0cosf) * (x2*x2) + nmul_add(0.5f, x2, 1.0f); 
+    // s = P0sinf * (x*x2) + x;  // 2 ULP error
+    // c = P0cosf * (x2*x2) + nmul_add(0.5f, x2, 1.0f);  // 2 ULP error
+
+    // swap sin and cos if odd quadrant
+    swap = BVTYPE((q & 1) != 0);
+
+    if constexpr ((SC & 5) != 0) {               // get sin
+        sin1 = select(swap, c, s);
+        ITYPE signsin = ((q << 30) ^ ITYPE(reinterpret_i(xx))); // sign
+        sin1 = sign_combine(sin1, reinterpret_f(signsin));
+    }
+    if constexpr ((SC & 6) != 0) {               // get cos
+        cos1 = select(swap, s, c);               // sign
+        ITYPE signcos = ((q + 1) & 2) << 30;
+        cos1 ^= reinterpret_f(signcos);
+    }
+    // select return
+    if      constexpr ((SC & 7) == 1) return sin1;
+    else if constexpr ((SC & 7) == 2) return cos1;
+    else if constexpr ((SC & 7) == 3) {          // both sin and cos. cos returned through pointer
+        *cosret = cos1;
+        return sin1;
+    }
+    else {                                       // (SC & 7) == 4. tan
+        if constexpr (SC == 12) {
+            // tanpi can give INF result, tan cannot. Get the right sign of INF result according to IEEE 754-2019
+            cos1 = select(cos1 == VTYPE(0.f), VTYPE(0.f), cos1); // remove sign of 0
+            // the sign of zero output is arbitrary. fixing it would be a waste of code
+        }
+        return sin1 / cos1;
+    }
+}
+
+// Instantiations of templates
+
+static inline Vec8h exp(Vec8h const x) {
+    Vec8f xf = to_float(x);
+    Vec8f yf = exp_h<Vec8f, 0, 0>(xf);
+    return to_float16(yf);
+}
+
+static inline Vec8h exp2(Vec8h const x) {
+    Vec8f xf = to_float(x);
+    Vec8f yf = exp_h<Vec8f, 0, 2>(xf);
+    return to_float16(yf);
+}
+
+static inline Vec8h exp10(Vec8h const x) {
+    Vec8f xf = to_float(x);
+    Vec8f yf = exp_h<Vec8f, 0, 10>(xf);
+    return to_float16(yf);
+}
+
+static inline Vec8h expm1(Vec8h const x) {
+    Vec8f xf = to_float(x);
+    Vec8f yf = exp_h<Vec8f, 1, 0>(xf);
+    return to_float16(yf);
+}
+
+static inline Vec8h sin(Vec8h const x) {
+    Vec8f xf = to_float(x);
+    Vec8f yf = sincos_h<Vec8f, 1>(0, xf);
+    return to_float16(yf);
+}
+static inline Vec8h cos(Vec8h const x) {
+    Vec8f xf = to_float(x);
+    Vec8f yf = sincos_h<Vec8f, 2>(0, xf);
+    return to_float16(yf);
+}
+static inline Vec8h sincos(Vec8h * cosret, Vec8h const x) {
+    Vec8f xf = to_float(x);
+    Vec8f cf;  // cos return
+    Vec8f yf = sincos_h<Vec8f, 3>(&cf, xf);
+    if (cosret) *cosret = to_float16(cf);
+    return to_float16(yf);
+}
+static inline Vec8h tan(Vec8h const x) {
+    Vec8f xf = to_float(x);
+    Vec8f yf = sincos_h<Vec8f, 4>(0, xf);
+    return to_float16(yf);
+}
+
+static inline Vec8h sinpi(Vec8h const x) {
+    Vec8f xf = to_float(x);
+    Vec8f yf = sincos_h<Vec8f, 9>(0, xf);
+    return to_float16(yf);
+}
+static inline Vec8h cospi(Vec8h const x) {
+    Vec8f xf = to_float(x);
+    Vec8f yf = sincos_h<Vec8f, 10>(0, xf);
+    return to_float16(yf);
+}
+static inline Vec8h sincospi(Vec8h * cosret, Vec8h const x) {
+    Vec8f xf = to_float(x);
+    Vec8f cf;  // cos return
+    Vec8f yf = sincos_h<Vec8f, 11>(&cf, xf);
+    if (cosret) *cosret = to_float16(cf);
+    return to_float16(yf);
+}
+static inline Vec8h tanpi(Vec8h const x) {
+    Vec8f xf = to_float(x);
+    Vec8f yf = sincos_h<Vec8f, 12>(0, xf);
+    return to_float16(yf);
+} 
+
+#if MAX_VECTOR_SIZE >= 512
+
+static inline Vec16h exp(Vec16h const x) {
+    Vec16f xf = to_float(x);
+    Vec16f yf = exp_h<Vec16f, 0, 0>(xf);
+    return to_float16(yf);
+}
+
+static inline Vec16h exp2(Vec16h const x) {
+    Vec16f xf = to_float(x);
+    Vec16f yf = exp_h<Vec16f, 0, 2>(xf);
+    return to_float16(yf);
+}
+
+static inline Vec16h exp10(Vec16h const x) {
+    Vec16f xf = to_float(x);
+    Vec16f yf = exp_h<Vec16f, 0, 10>(xf);
+    return to_float16(yf);
+}
+
+static inline Vec16h expm1(Vec16h const x) {
+    Vec16f xf = to_float(x);
+    Vec16f yf = exp_h<Vec16f, 1, 0>(xf);
+    return to_float16(yf);
+}
+
+static inline Vec16h sin(Vec16h const x) {
+    Vec16f xf = to_float(x);
+    Vec16f yf = sincos_h<Vec16f, 1>(0, xf);
+    return to_float16(yf);
+}
+static inline Vec16h cos(Vec16h const x) {
+    Vec16f xf = to_float(x);
+    Vec16f yf = sincos_h<Vec16f, 2>(0, xf);
+    return to_float16(yf);
+}
+static inline Vec16h sincos(Vec16h * cosret, Vec16h const x) {
+    Vec16f xf = to_float(x);
+    Vec16f cf;  // cos return
+    Vec16f yf = sincos_h<Vec16f, 3>(&cf, xf);
+    if (cosret) *cosret = to_float16(cf);
+    return to_float16(yf);
+}
+static inline Vec16h tan(Vec16h const x) {
+    Vec16f xf = to_float(x);
+    Vec16f yf = sincos_h<Vec16f, 4>(0, xf);
+    return to_float16(yf);
+} 
+
+static inline Vec16h sinpi(Vec16h const x) {
+    Vec16f xf = to_float(x);
+    Vec16f yf = sincos_h<Vec16f, 9>(0, xf);
+    return to_float16(yf);
+}
+static inline Vec16h cospi(Vec16h const x) {
+    Vec16f xf = to_float(x);
+    Vec16f yf = sincos_h<Vec16f, 10>(0, xf);
+    return to_float16(yf);
+}
+static inline Vec16h sincospi(Vec16h * cosret, Vec16h const x) {
+    Vec16f xf = to_float(x);
+    Vec16f cf;  // cos return
+    Vec16f yf = sincos_h<Vec16f, 11>(&cf, xf);
+    if (cosret) *cosret = to_float16(cf);
+    return to_float16(yf);
+}
+static inline Vec16h tanpi(Vec16h const x) {
+    Vec16f xf = to_float(x);
+    Vec16f yf = sincos_h<Vec16f, 12>(0, xf);
+    return to_float16(yf);
+} 
+
+#endif  // MAX_VECTOR_SIZE >= 256 
+
+#if MAX_VECTOR_SIZE >= 512
+
+static inline Vec32h exp(Vec32h const x) {
+    Vec16f xf_lo = to_float(x.get_low());
+    Vec16f xf_hi = to_float(x.get_high());
+    Vec16f yf_lo = exp_h<Vec16f, 0, 0>(xf_lo);
+    Vec16f yf_hi = exp_h<Vec16f, 0, 0>(xf_hi);
+    return Vec32h(to_float16(yf_lo), to_float16(yf_hi));
+}
+
+static inline Vec32h exp2(Vec32h const x) {
+    Vec16f xf_lo = to_float(x.get_low());
+    Vec16f xf_hi = to_float(x.get_high());
+    Vec16f yf_lo = exp_h<Vec16f, 0, 2>(xf_lo);
+    Vec16f yf_hi = exp_h<Vec16f, 0, 2>(xf_hi);
+    return Vec32h(to_float16(yf_lo), to_float16(yf_hi));
+}
+
+static inline Vec32h exp10(Vec32h const x) {
+    Vec16f xf_lo = to_float(x.get_low());
+    Vec16f xf_hi = to_float(x.get_high());
+    Vec16f yf_lo = exp_h<Vec16f, 0, 10>(xf_lo);
+    Vec16f yf_hi = exp_h<Vec16f, 0, 10>(xf_hi);
+    return Vec32h(to_float16(yf_lo), to_float16(yf_hi));
+}
+
+static inline Vec32h expm1(Vec32h const x) {
+    Vec16f xf_lo = to_float(x.get_low());
+    Vec16f xf_hi = to_float(x.get_high());
+    Vec16f yf_lo = exp_h<Vec16f, 1, 0>(xf_lo);
+    Vec16f yf_hi = exp_h<Vec16f, 1, 0>(xf_hi);
+    return Vec32h(to_float16(yf_lo), to_float16(yf_hi));
+}
+
+static inline Vec32h sin(Vec32h const x) {
+    Vec16f xf_lo = to_float(x.get_low());
+    Vec16f xf_hi = to_float(x.get_high());
+    Vec16f yf_lo = sincos_h<Vec16f, 1>(0, xf_lo);
+    Vec16f yf_hi = sincos_h<Vec16f, 1>(0, xf_hi);
+    return Vec32h(to_float16(yf_lo), to_float16(yf_hi));
+}
+static inline Vec32h cos(Vec32h const x) {
+    Vec16f xf_lo = to_float(x.get_low());
+    Vec16f xf_hi = to_float(x.get_high());
+    Vec16f yf_lo = sincos_h<Vec16f, 2>(0, xf_lo);
+    Vec16f yf_hi = sincos_h<Vec16f, 2>(0, xf_hi);
+    return Vec32h(to_float16(yf_lo), to_float16(yf_hi));
+}
+static inline Vec32h sincos(Vec32h * cosret, Vec32h const x) {
+    Vec16f xf_lo = to_float(x.get_low());
+    Vec16f xf_hi = to_float(x.get_high());
+    Vec16f cf_lo, cf_hi;
+    Vec16f yf_lo = sincos_h<Vec16f, 3>(&cf_lo, xf_lo);
+    Vec16f yf_hi = sincos_h<Vec16f, 3>(&cf_hi, xf_hi);
+    if (cosret) * cosret = Vec32h(to_float16(cf_lo), to_float16(cf_hi));
+    return Vec32h(to_float16(yf_lo), to_float16(yf_hi));
+}
+static inline Vec32h tan(Vec32h const x) {
+    Vec16f xf_lo = to_float(x.get_low());
+    Vec16f xf_hi = to_float(x.get_high());
+    Vec16f yf_lo = sincos_h<Vec16f, 4>(0, xf_lo);
+    Vec16f yf_hi = sincos_h<Vec16f, 4>(0, xf_hi);
+    return Vec32h(to_float16(yf_lo), to_float16(yf_hi));
+}
+
+static inline Vec32h sinpi(Vec32h const x) {
+    Vec16f xf_lo = to_float(x.get_low());
+    Vec16f xf_hi = to_float(x.get_high());
+    Vec16f yf_lo = sincos_h<Vec16f, 9>(0, xf_lo);
+    Vec16f yf_hi = sincos_h<Vec16f, 9>(0, xf_hi);
+    return Vec32h(to_float16(yf_lo), to_float16(yf_hi));
+}
+static inline Vec32h cospi(Vec32h const x) {
+    Vec16f xf_lo = to_float(x.get_low());
+    Vec16f xf_hi = to_float(x.get_high());
+    Vec16f yf_lo = sincos_h<Vec16f, 10>(0, xf_lo);
+    Vec16f yf_hi = sincos_h<Vec16f, 10>(0, xf_hi);
+    return Vec32h(to_float16(yf_lo), to_float16(yf_hi));
+}
+static inline Vec32h sincospi(Vec32h * cosret, Vec32h const x) {
+    Vec16f xf_lo = to_float(x.get_low());
+    Vec16f xf_hi = to_float(x.get_high());
+    Vec16f cf_lo, cf_hi;
+    Vec16f yf_lo = sincos_h<Vec16f, 11>(&cf_lo, xf_lo);
+    Vec16f yf_hi = sincos_h<Vec16f, 11>(&cf_hi, xf_hi);
+    if (cosret) * cosret = Vec32h(to_float16(cf_lo), to_float16(cf_hi));
+    return Vec32h(to_float16(yf_lo), to_float16(yf_hi));
+}
+static inline Vec32h tanpi(Vec32h const x) {
+    Vec16f xf_lo = to_float(x.get_low());
+    Vec16f xf_hi = to_float(x.get_high());
+    Vec16f yf_lo = sincos_h<Vec16f, 12>(0, xf_lo);
+    Vec16f yf_hi = sincos_h<Vec16f, 12>(0, xf_hi);
+    return Vec32h(to_float16(yf_lo), to_float16(yf_hi));
+} 
+
+#endif  // MAX_VECTOR_SIZE >= 512
+
+#ifdef VCL_NAMESPACE
+}
+#endif
+
+#endif // VECTORFP16_H
diff --git a/EEDI3/vectorclass/vectori128.h b/EEDI3/vectorclass/vectori128.h
index 17f5746..f7d70b6 100644
--- a/EEDI3/vectorclass/vectori128.h
+++ b/EEDI3/vectorclass/vectori128.h
@@ -41,6 +41,40 @@
 *
 * (c) Copyright 2012-2017 GNU General Public License http://www.gnu.org/licenses
 *****************************************************************************/
+
+/*
+ARM compatible include of the vectorclass
+
+on ARM/MAC the sse2neon lib will be imported
+and some parameters for the vectorclass are prepared.
+
+#IMPORTANT in vectorclass.h->instrset.h the cpuid function must be
+hidden, since it is not compatible with ARM-compilers.
+
+if missing, add the header-include check #if !defined(SSE2NEON_H)
+to the function to hide it when compiling on ARM
+
+remember that a dispatcher is not possible in this case.
+
+*/
+
+#if __arm64
+#include "sse2neon.h"
+
+// limit to 128byte, since we want to use ARM-neon
+#define MAX_VECTOR_SIZE 128
+
+//limit to sse4.2, sse2neon does not have any AVX instructions ( so far )
+#define INSTRSET 6
+
+//define unknown function
+#define _mm_getcsr() 1
+
+//simulate header included
+#define __X86INTRIN_H
+#endif
+// finally include vectorclass
+
 #ifndef VECTORI128_H
 #define VECTORI128_H
 
diff --git a/EEDI3/vectorclass/vectori256.h b/EEDI3/vectorclass/vectori256.h
index 61ac5da..3242778 100644
--- a/EEDI3/vectorclass/vectori256.h
+++ b/EEDI3/vectorclass/vectori256.h
@@ -1,19 +1,17 @@
 /****************************  vectori256.h   *******************************
 * Author:        Agner Fog
 * Date created:  2012-05-30
-* Last modified: 2017-02-19
-* Version:       1.27
-* Project:       vector classes
+* Last modified: 2023-07-04
+* Version:       2.02.02
+* Project:       vector class library
 * Description:
-* Header file defining integer vector classes as interface to intrinsic 
+* Header file defining integer vector classes as interface to intrinsic
 * functions in x86 microprocessors with AVX2 and later instruction sets.
 *
-* Instructions:
-* Use Gnu, Intel or Microsoft C++ compiler. Compile for the desired 
-* instruction set, which must be at least AVX2. 
+* Instructions: see vcl_manual.pdf
 *
 * The following vector classes are defined here:
-* Vec256b   Vector of 256  1-bit unsigned  integers or Booleans
+* Vec256b   Vector of 256  bits. Used internally as base class
 * Vec32c    Vector of  32  8-bit signed    integers
 * Vec32uc   Vector of  32  8-bit unsigned  integers
 * Vec32cb   Vector of  32  Booleans for use with Vec32c and Vec32uc
@@ -30,73 +28,171 @@
 * Each vector object is represented internally in the CPU as a 256-bit register.
 * This header file defines operators and functions for these vectors.
 *
-* For example:
-* Vec8i a(1,2,3,4,5,6,7,8), b(9,10,11,12,13,14,15,16), c;
-* c = a + b;     // now c contains (10,12,14,16,18,20,22,24)
-*
-* For detailed instructions, see VectorClass.pdf
-*
-* (c) Copyright 2012-2017 GNU General Public License http://www.gnu.org/licenses
+* (c) Copyright 2012-2023 Agner Fog.
+* Apache License version 2.0 or later.
 *****************************************************************************/
 
-// check combination of header files
-#if defined (VECTORI256_H)
-#if    VECTORI256_H != 2
-#error Two different versions of vectori256.h included
-#endif
-#else
-#define VECTORI256_H  2
+#ifndef VECTORI256_H
+#define VECTORI256_H 1
 
-#ifdef VECTORF256_H
-#error Please put header file vectori256.h before vectorf256.h
+#ifndef VECTORCLASS_H
+#include "vectorclass.h"
 #endif
 
+#if VECTORCLASS_H < 20200
+#error Incompatible versions of vector class library mixed
+#endif
 
-#if INSTRSET < 8   // AVX2 required
-#error Wrong instruction set for vectori256.h, AVX2 required or use vectori256e.h
+// check combination of header files
+#if defined (VECTORI256E_H)
+#error Two different versions of vectori256.h included
 #endif
 
-#include "vectori128.h"
 
 #ifdef VCL_NAMESPACE
 namespace VCL_NAMESPACE {
 #endif
 
+// Generate a constant vector of 8 integers stored in memory.
+template <uint32_t i0, uint32_t i1, uint32_t i2, uint32_t i3, uint32_t i4, uint32_t i5, uint32_t i6, uint32_t i7 >
+    static inline constexpr __m256i constant8ui() {
+    /*
+    const union {
+        uint32_t i[8];
+        __m256i ymm;
+    } u = { {i0,i1,i2,i3,i4,i5,i6,i7} };
+    return u.ymm;
+    */
+    return _mm256_setr_epi32(i0,i1,i2,i3,i4,i5,i6,i7);
+}
+
+
+// Join two 128-bit vectors
+#define set_m128ir(lo,hi) _mm256_inserti128_si256(_mm256_castsi128_si256(lo),(hi),1)
+
 /*****************************************************************************
 *
-*         Join two 128-bit vectors
+*         Compact boolean vectors
 *
 *****************************************************************************/
-#define set_m128ir(lo,hi) _mm256_inserti128_si256(_mm256_castsi128_si256(lo),(hi),1)
+
+#if INSTRSET >= 10  // 32-bit and 64-bit masks require AVX512BW
+
+// Compact vector of 32 booleans
+class Vec32b {
+protected:
+    __mmask32  mm; // Boolean mask register
+public:
+    // Default constructor:
+    Vec32b() = default;
+    // Constructor to convert from type __mmask32 used in intrinsics
+    // Made explicit to prevent implicit conversion from int
+    Vec32b(__mmask32 x) {
+        mm = x;
+    }
+    /*
+    // Constructor to build from all elements:
+    Vec32b(bool b0, bool b1, bool b2, bool b3, bool b4, bool b5, bool b6, bool b7,
+        bool b8, bool b9, bool b10, bool b11, bool b12, bool b13, bool b14, bool b15,
+        bool b16, bool b17, bool b18, bool b19, bool b20, bool b21, bool b22, bool b23,
+        bool b24, bool b25, bool b26, bool b27, bool b28, bool b29, bool b30, bool b31) {
+        mm = uint32_t(
+            (uint32_t)b0 | (uint32_t)b1 << 1 | (uint32_t)b2 << 2 | (uint32_t)b3 << 3 |
+            (uint32_t)b4 << 4 | (uint32_t)b5 << 5 | (uint32_t)b6 << 6 | (uint32_t)b7 << 7 |
+            (uint32_t)b8 << 8 | (uint32_t)b9 << 9 | (uint32_t)b10 << 10 | (uint32_t)b11 << 11 |
+            (uint32_t)b12 << 12 | (uint32_t)b13 << 13 | (uint32_t)b14 << 14 | (uint32_t)b15 << 15 |
+            (uint32_t)b16 << 16 | (uint32_t)b17 << 17 | (uint32_t)b18 << 18 | (uint32_t)b19 << 19 |
+            (uint32_t)b20 << 20 | (uint32_t)b21 << 21 | (uint32_t)b22 << 22 | (uint32_t)b23 << 23 |
+            (uint32_t)b24 << 24 | (uint32_t)b25 << 25 | (uint32_t)b26 << 26 | (uint32_t)b27 << 27 |
+            (uint32_t)b28 << 28 | (uint32_t)b29 << 29 | (uint32_t)b30 << 30 | (uint32_t)b31 << 31);
+    } */
+    // Constructor to broadcast single value:
+    Vec32b(bool b) {
+        mm = __mmask32(-int32_t(b));
+    }
+    // Constructor to make from two halves
+    Vec32b(Vec16b const x0, Vec16b const x1) {
+        mm = uint16_t(__mmask16(x0)) | uint32_t(__mmask16(x1)) << 16;
+    }
+    // Assignment operator to convert from type __mmask32 used in intrinsics:
+    Vec32b & operator = (__mmask32 x) {
+        mm = x;
+        return *this;
+    }
+    // Assignment operator to broadcast scalar value:
+    Vec32b & operator = (bool b) {
+        mm = Vec32b(b);
+        return *this;
+    }
+    // Type cast operator to convert to __mmask32 used in intrinsics
+    operator __mmask32() const {
+        return mm;
+    }
+    // split into two halves
+    Vec16b get_low() const {
+        return Vec16b(__mmask16(mm));
+    }
+    Vec16b get_high() const {
+        return Vec16b(__mmask16(mm >> 16));
+    }
+    // Member function to change a single element in vector
+    Vec32b const insert(int index, bool value) {
+        mm = __mmask32(((uint32_t)mm & ~(1 << index)) | (uint32_t)value << index);
+        return *this;
+    }
+    // Member function extract a single element from vector
+    bool extract(int index) const {
+        return ((uint32_t)mm >> index) & 1;
+    }
+    // Extract a single element. Operator [] can only read an element, not write.
+    bool operator [] (int index) const {
+        return extract(index);
+    }
+    // Member function to change a bitfield to a boolean vector
+    Vec32b & load_bits(uint32_t a) {
+        mm = __mmask32(a);
+        return *this;
+    }
+    // Number of elements
+    static constexpr int size() {
+        return 32;
+    }
+    // Type of elements
+    static constexpr int elementtype() {
+        return 2;
+    }
+};
+
+#endif
 
 
 /*****************************************************************************
 *
-*          Vector of 256 1-bit unsigned integers or Booleans
+*          Vector of 256 bits. Used as base class
 *
 *****************************************************************************/
+
 class Vec256b {
 protected:
     __m256i ymm; // Integer vector
 public:
     // Default constructor:
-    Vec256b() {
-    }
+    Vec256b() = default;
+
     // Constructor to broadcast the same value into all elements
-    // Removed because of undesired implicit conversions
-    //Vec256b(int i) {
-    //    ymm = _mm256_set1_epi32(-(i & 1));}
+    // Removed because of undesired implicit conversions:
+    //Vec256b(int i) {ymm = _mm256_set1_epi32(-(i & 1));}
 
     // Constructor to build from two Vec128b:
-    Vec256b(Vec128b const & a0, Vec128b const & a1) {
+    Vec256b(Vec128b const a0, Vec128b const a1) {
         ymm = set_m128ir(a0, a1);
     }
     // Constructor to convert from type __m256i used in intrinsics:
-    Vec256b(__m256i const & x) {
+    Vec256b(__m256i const x) {
         ymm = x;
     }
     // Assignment operator to convert from type __m256i used in intrinsics:
-    Vec256b & operator = (__m256i const & x) {
+    Vec256b & operator = (__m256i const x) {
         ymm = x;
         return *this;
     }
@@ -120,142 +216,88 @@ public:
     void store(void * p) const {
         _mm256_storeu_si256((__m256i*)p, ymm);
     }
-    // Member function to store into array, aligned by 32
+    // Member function storing into array, aligned by 32
     // You may use store_a instead of store if you are certain that p points to an address
     // divisible by 32, but there is hardly any speed advantage of load_a on modern processors
     void store_a(void * p) const {
         _mm256_store_si256((__m256i*)p, ymm);
     }
-    // Member function to store into array using a non-temporal memory hint, aligned by 32
-    void stream(void * p) const {
+    // Member function storing to aligned uncached memory (non-temporal store).
+    // This may be more efficient than store_a when storing large blocks of memory if it 
+    // is unlikely that the data will stay in the cache until it is read again.
+    // Note: Will generate runtime error if p is not aligned by 32
+    void store_nt(void * p) const {
         _mm256_stream_si256((__m256i*)p, ymm);
     }
-    // Member function to change a single bit
-    // Note: This function is inefficient. Use load function if changing more than one bit
-    Vec256b const & set_bit(uint32_t index, int value) {
-        static uint64_t m[8] = {0,0,0,0,1,0,0,0};
-        int wi = (index >> 6) & 3;               // qword index
-        int bi = index & 0x3F;                   // bit index within qword w
-
-        __m256i mask = Vec256b().load(m+4-wi);   // 1 in qword number wi
-        mask = _mm256_sll_epi64(mask,_mm_cvtsi32_si128(bi)); // mask with bit number b set
-        if (value & 1) {
-            ymm = _mm256_or_si256(mask,ymm);
-        }
-        else {
-            ymm = _mm256_andnot_si256(mask,ymm);
-        }
-        return *this;
-    }
-    // Member function to get a single bit
-    // Note: This function is inefficient. Use store function if reading more than one bit
-    int get_bit(uint32_t index) const {
-        union {
-            __m256i x;
-            uint8_t i[32];
-        } u;
-        u.x = ymm; 
-        int wi = (index >> 3) & 0x1F;            // byte index
-        int bi = index & 7;                      // bit index within byte w
-        return (u.i[wi] >> bi) & 1;
-    }
-    // Extract a single element. Use store function if extracting more than one element.
-    // Operator [] can only read an element, not write.
-    bool operator [] (uint32_t index) const {
-        return get_bit(index) != 0;
-    }
     // Member functions to split into two Vec128b:
     Vec128b get_low() const {
         return _mm256_castsi256_si128(ymm);
     }
     Vec128b get_high() const {
-        return _mm256_extracti128_si256(ymm,1);
+        return _mm256_extractf128_si256(ymm,1);
     }
-    static int size() {
+    static constexpr int size() {
         return 256;
     }
+    static constexpr int elementtype() {
+        return 1;
+    }
+    typedef __m256i registertype;
 };
 
 
-// Define operators for this class
+// Define operators and functions for this class
 
 // vector operator & : bitwise and
-static inline Vec256b operator & (Vec256b const & a, Vec256b const & b) {
+static inline Vec256b operator & (Vec256b const a, Vec256b const b) {
     return _mm256_and_si256(a, b);
 }
-static inline Vec256b operator && (Vec256b const & a, Vec256b const & b) {
+static inline Vec256b operator && (Vec256b const a, Vec256b const b) {
     return a & b;
 }
 
 // vector operator | : bitwise or
-static inline Vec256b operator | (Vec256b const & a, Vec256b const & b) {
+static inline Vec256b operator | (Vec256b const a, Vec256b const b) {
     return _mm256_or_si256(a, b);
 }
-static inline Vec256b operator || (Vec256b const & a, Vec256b const & b) {
+static inline Vec256b operator || (Vec256b const a, Vec256b const b) {
     return a | b;
 }
 
 // vector operator ^ : bitwise xor
-static inline Vec256b operator ^ (Vec256b const & a, Vec256b const & b) {
+static inline Vec256b operator ^ (Vec256b const a, Vec256b const b) {
     return _mm256_xor_si256(a, b);
 }
 
 // vector operator ~ : bitwise not
-static inline Vec256b operator ~ (Vec256b const & a) {
+static inline Vec256b operator ~ (Vec256b const a) {
     return _mm256_xor_si256(a, _mm256_set1_epi32(-1));
 }
 
 // vector operator &= : bitwise and
-static inline Vec256b & operator &= (Vec256b & a, Vec256b const & b) {
+static inline Vec256b & operator &= (Vec256b & a, Vec256b const b) {
     a = a & b;
     return a;
 }
 
 // vector operator |= : bitwise or
-static inline Vec256b & operator |= (Vec256b & a, Vec256b const & b) {
+static inline Vec256b & operator |= (Vec256b & a, Vec256b const b) {
     a = a | b;
     return a;
 }
 
 // vector operator ^= : bitwise xor
-static inline Vec256b & operator ^= (Vec256b & a, Vec256b const & b) {
+static inline Vec256b & operator ^= (Vec256b & a, Vec256b const b) {
     a = a ^ b;
     return a;
 }
 
-// Define functions for this class
-
-static inline __m256i zero_256b() {
-    return _mm256_setzero_si256();
-}
-
 // function andnot: a & ~ b
-static inline Vec256b andnot (Vec256b const & a, Vec256b const & b) {
+static inline Vec256b andnot (Vec256b const a, Vec256b const b) {
     return _mm256_andnot_si256(b, a);
 }
 
 
-/*****************************************************************************
-*
-*          Generate compile-time constant vector
-*
-*****************************************************************************/
-// Generate a constant vector of 8 integers stored in memory.
-// Can be converted to any integer vector type
-template <int32_t i0, int32_t i1, int32_t i2, int32_t i3, int32_t i4, int32_t i5, int32_t i6, int32_t i7>
-static inline __m256i constant8i() {
-    static const union {
-        int32_t i[8];
-        __m256i ymm;
-    } u = {{i0,i1,i2,i3,i4,i5,i6,i7}};
-    return u.ymm;
-}
-
-template <uint32_t i0, uint32_t i1, uint32_t i2, uint32_t i3, uint32_t i4, uint32_t i5, uint32_t i6, uint32_t i7>
-static inline __m256i constant8ui() {
-    return constant8i<int32_t(i0), int32_t(i1), int32_t(i2), int32_t(i3), int32_t(i4), int32_t(i5), int32_t(i6), int32_t(i7)>();
-}
-
 /*****************************************************************************
 *
 *          selectb function
@@ -265,31 +307,22 @@ static inline __m256i constant8ui() {
 // Corresponds to this pseudocode:
 // for (int i = 0; i < 32; i++) result[i] = s[i] ? a[i] : b[i];
 // Each byte in s must be either 0 (false) or 0xFF (true). No other values are allowed.
-// Only bit 7 in each byte of s is checked, 
-static inline __m256i selectb (__m256i const & s, __m256i const & a, __m256i const & b) {
+// Only bit 7 in each byte of s is checked,
+static inline __m256i selectb (__m256i const s, __m256i const a, __m256i const b) {
     return _mm256_blendv_epi8 (b, a, s);
 }
 
-
-
-/*****************************************************************************
-*
-*          Horizontal Boolean functions
-*
-*****************************************************************************/
-
 // horizontal_and. Returns true if all bits are 1
-static inline bool horizontal_and (Vec256b const & a) {
-    return _mm256_testc_si256(a,constant8i<-1,-1,-1,-1,-1,-1,-1,-1>()) != 0;
+static inline bool horizontal_and (Vec256b const a) {
+    return _mm256_testc_si256(a,_mm256_set1_epi32(-1)) != 0;
 }
 
 // horizontal_or. Returns true if at least one bit is 1
-static inline bool horizontal_or (Vec256b const & a) {
+static inline bool horizontal_or (Vec256b const a) {
     return ! _mm256_testz_si256(a,a);
 }
 
 
-
 /*****************************************************************************
 *
 *          Vector of 32 8-bit signed integers
@@ -299,33 +332,36 @@ static inline bool horizontal_or (Vec256b const & a) {
 class Vec32c : public Vec256b {
 public:
     // Default constructor:
-    Vec32c(){
-    }
+    Vec32c() = default;
     // Constructor to broadcast the same value into all elements:
     Vec32c(int i) {
         ymm = _mm256_set1_epi8((char)i);
     }
     // Constructor to build from all elements:
     Vec32c(int8_t i0, int8_t i1, int8_t i2, int8_t i3, int8_t i4, int8_t i5, int8_t i6, int8_t i7,
-        int8_t i8, int8_t i9, int8_t i10, int8_t i11, int8_t i12, int8_t i13, int8_t i14, int8_t i15,        
+        int8_t i8, int8_t i9, int8_t i10, int8_t i11, int8_t i12, int8_t i13, int8_t i14, int8_t i15,
         int8_t i16, int8_t i17, int8_t i18, int8_t i19, int8_t i20, int8_t i21, int8_t i22, int8_t i23,
         int8_t i24, int8_t i25, int8_t i26, int8_t i27, int8_t i28, int8_t i29, int8_t i30, int8_t i31) {
         ymm = _mm256_setr_epi8(i0, i1, i2, i3, i4, i5, i6, i7, i8, i9, i10, i11, i12, i13, i14, i15,
             i16, i17, i18, i19, i20, i21, i22, i23, i24, i25, i26, i27, i28, i29, i30, i31);
     }
     // Constructor to build from two Vec16c:
-    Vec32c(Vec16c const & a0, Vec16c const & a1) {
+    Vec32c(Vec16c const a0, Vec16c const a1) {
         ymm = set_m128ir(a0, a1);
     }
     // Constructor to convert from type __m256i used in intrinsics:
-    Vec32c(__m256i const & x) {
+    Vec32c(__m256i const x) {
         ymm = x;
     }
     // Assignment operator to convert from type __m256i used in intrinsics:
-    Vec32c & operator = (__m256i const & x) {
+    Vec32c & operator = (__m256i const x) {
         ymm = x;
         return *this;
     }
+    // Constructor to convert from type Vec256b used in emulation
+    Vec32c(Vec256b const & x) {  // gcc requires const &
+        ymm = x;
+    }
     // Type cast operator to convert to __m256i used in intrinsics
     operator __m256i() const {
         return ymm;
@@ -342,6 +378,9 @@ public:
     }
     // Partial load. Load n elements and set the rest to 0
     Vec32c & load_partial(int n, void const * p) {
+#if INSTRSET >= 10  // AVX512VL
+        ymm = _mm256_maskz_loadu_epi8(__mmask32(((uint64_t)1 << n) - 1), p);
+#else
         if (n <= 0) {
             *this = 0;
         }
@@ -354,10 +393,14 @@ public:
         else {
             load(p);
         }
+#endif
         return *this;
     }
     // Partial store. Store n elements
     void store_partial(int n, void * p) const {
+#if INSTRSET >= 10  // AVX512VL + AVX512BW
+        _mm256_mask_storeu_epi8(p, __mmask32(((uint64_t)1 << n) - 1), ymm);
+#else
         if (n <= 0) {
             return;
         }
@@ -371,36 +414,49 @@ public:
         else {
             store(p);
         }
+#endif
     }
     // cut off vector to n elements. The last 32-n elements are set to zero
     Vec32c & cutoff(int n) {
+#if INSTRSET >= 10
+        ymm = _mm256_maskz_mov_epi8(__mmask32(((uint64_t)1 << n) - 1), ymm);
+#else
         if (uint32_t(n) >= 32) return *this;
-        static const union {
+        const union {
             int32_t i[16];
             char    c[64];
         } mask = {{-1,-1,-1,-1,-1,-1,-1,-1,0,0,0,0,0,0,0,0}};
         *this &= Vec32c().load(mask.c+32-n);
+#endif
         return *this;
     }
     // Member function to change a single element in vector
-    // Note: This function is inefficient. Use load function if changing more than one element
-    Vec32c const & insert(uint32_t index, int8_t value) {
-        static const int8_t maskl[64] = {0,0,0,0, 0,0,0,0, 0,0,0,0 ,0,0,0,0, 0,0,0,0, 0,0,0,0, 0,0,0,0, 0,0,0,0,
+    Vec32c const insert(int index, int8_t value) {
+#if INSTRSET >= 10
+        ymm = _mm256_mask_set1_epi8(ymm, __mmask32(1u << index), value);
+#else
+        const int8_t maskl[64] = {0,0,0,0, 0,0,0,0, 0,0,0,0 ,0,0,0,0, 0,0,0,0, 0,0,0,0, 0,0,0,0, 0,0,0,0,
             -1,0,0,0, 0,0,0,0, 0,0,0,0, 0,0,0,0, 0,0,0,0 ,0,0,0,0, 0,0,0,0, 0,0,0,0};
         __m256i broad = _mm256_set1_epi8(value);  // broadcast value into all elements
         __m256i mask  = _mm256_loadu_si256((__m256i const*)(maskl+32-(index & 0x1F))); // mask with FF at index position
         ymm = selectb(mask,broad,ymm);
+#endif
         return *this;
     }
     // Member function extract a single element from vector
-    int8_t extract(uint32_t index) const {
+    int8_t extract(int index) const {
+#if INSTRSET >= 10 && defined (__AVX512VBMI2__)
+        __m256i x = _mm256_maskz_compress_epi8(__mmask32(1u << index), ymm);
+        return (int8_t)_mm_cvtsi128_si32(_mm256_castsi256_si128(x));
+#else
         int8_t x[32];
         store(x);
         return x[index & 0x1F];
+#endif
     }
     // Extract a single element. Use store function if extracting more than one element.
     // Operator [] can only read an element, not write.
-    int8_t operator [] (uint32_t index) const {
+    int8_t operator [] (int index) const {
         return extract(index);
     }
     // Member functions to split into two Vec16c:
@@ -408,15 +464,14 @@ public:
         return _mm256_castsi256_si128(ymm);
     }
     Vec16c get_high() const {
-#if defined (_MSC_VER) && _MSC_VER <= 1700 && ! defined(__INTEL_COMPILER)
-        return _mm256_extractf128_si256(ymm,1);    // workaround bug in MS compiler VS 11
-#else
         return _mm256_extracti128_si256(ymm,1);
-#endif
     }
-    static int size() {
+    static constexpr int size() {
         return 32;
     }
+    static constexpr int elementtype() {
+        return 4;
+    }
 };
 
 
@@ -426,42 +481,46 @@ public:
 *
 *****************************************************************************/
 
+#if INSTRSET < 10  // broad boolean vectors
 class Vec32cb : public Vec32c {
 public:
     // Default constructor:
-    Vec32cb(){
-    }
+    Vec32cb() = default;
     // Constructor to build from all elements:
+    /*
     Vec32cb(bool x0, bool x1, bool x2, bool x3, bool x4, bool x5, bool x6, bool x7,
         bool x8, bool x9, bool x10, bool x11, bool x12, bool x13, bool x14, bool x15,
         bool x16, bool x17, bool x18, bool x19, bool x20, bool x21, bool x22, bool x23,
         bool x24, bool x25, bool x26, bool x27, bool x28, bool x29, bool x30, bool x31) :
-        Vec32c(-int8_t(x0), -int8_t(x1), -int8_t(x2), -int8_t(x3), -int8_t(x4), -int8_t(x5), -int8_t(x6), -int8_t(x7), 
+        Vec32c(-int8_t(x0), -int8_t(x1), -int8_t(x2), -int8_t(x3), -int8_t(x4), -int8_t(x5), -int8_t(x6), -int8_t(x7),
             -int8_t(x8), -int8_t(x9), -int8_t(x10), -int8_t(x11), -int8_t(x12), -int8_t(x13), -int8_t(x14), -int8_t(x15),
             -int8_t(x16), -int8_t(x17), -int8_t(x18), -int8_t(x19), -int8_t(x20), -int8_t(x21), -int8_t(x22), -int8_t(x23),
             -int8_t(x24), -int8_t(x25), -int8_t(x26), -int8_t(x27), -int8_t(x28), -int8_t(x29), -int8_t(x30), -int8_t(x31))
-        {}
+        {} */
     // Constructor to convert from type __m256i used in intrinsics:
-    Vec32cb(__m256i const & x) {
+    Vec32cb(__m256i const x) {
         ymm = x;
     }
     // Assignment operator to convert from type __m256i used in intrinsics:
-    Vec32cb & operator = (__m256i const & x) {
+    Vec32cb & operator = (__m256i const x) {
         ymm = x;
         return *this;
     }
     // Constructor to broadcast scalar value:
     Vec32cb(bool b) : Vec32c(-int8_t(b)) {
     }
+    // Constructor to convert from Vec32c
+    Vec32cb(Vec32c const a) {
+        ymm = a;
+    }
     // Assignment operator to broadcast scalar value:
     Vec32cb & operator = (bool b) {
         *this = Vec32cb(b);
         return *this;
     }
-private: // Prevent constructing from int, etc.
-    Vec32cb(int b);
-    Vec32cb & operator = (int x);
-public:
+    // Constructor to build from two Vec16cb:
+    Vec32cb(Vec16cb const a0, Vec16cb const a1) : Vec32c(Vec16c(a0), Vec16c(a1)) {
+    }
     // Member functions to split into two Vec16c:
     Vec16cb get_low() const {
         return Vec16cb(Vec32c::get_low());
@@ -470,78 +529,166 @@ public:
         return Vec16cb(Vec32c::get_high());
     }
     Vec32cb & insert (int index, bool a) {
-        Vec32c::insert(index, -(int)a);
+        Vec32c::insert(index, -(int8_t)a);
         return *this;
-    }    
+    }
     // Member function extract a single element from vector
-    bool extract(uint32_t index) const {
+    bool extract(int index) const {
         return Vec32c::extract(index) != 0;
     }
     // Extract a single element. Use store function if extracting more than one element.
     // Operator [] can only read an element, not write.
-    bool operator [] (uint32_t index) const {
+    bool operator [] (int index) const {
         return extract(index);
     }
+    // Member function to change a bitfield to a boolean vector
+    Vec32cb & load_bits(uint32_t a) {
+        __m256i b1 = _mm256_set1_epi32((int32_t)~a);       // broadcast a. Invert because we have no compare-not-equal
+        __m256i m1 = constant8ui<0,0,0x01010101,0x01010101,0x02020202,0x02020202,0x03030303,0x03030303>();
+        __m256i c1 = _mm256_shuffle_epi8(b1, m1);          // get right byte in each position
+        __m256i m2 = constant8ui<0x08040201,0x80402010,0x08040201,0x80402010,0x08040201,0x80402010,0x08040201,0x80402010>();
+        __m256i d1 = _mm256_and_si256(c1, m2);             // isolate one bit in each byte
+        ymm = _mm256_cmpeq_epi8(d1,_mm256_setzero_si256());// compare with 0
+        return *this;
+    }
+    static constexpr int elementtype() {
+        return 3;
+    }
+    // Prevent constructing from int, etc.
+    Vec32cb(int b) = delete;
+    Vec32cb & operator = (int x) = delete;
 };
+#else
+
+typedef Vec32b Vec32cb;  // compact boolean vector
+
+#endif
 
 
 /*****************************************************************************
 *
-*          Define operators for Vec32cb
+*          Define operators and functions for Vec32b or Vec32cb
 *
 *****************************************************************************/
 
 // vector operator & : bitwise and
-static inline Vec32cb operator & (Vec32cb const & a, Vec32cb const & b) {
-    return Vec32cb(Vec256b(a) & Vec256b(b));
+static inline Vec32cb operator & (Vec32cb const a, Vec32cb const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return __mmask32(__mmask32(a) & __mmask32(b)); // _kand_mask32 not defined in all compilers
+#else
+    return Vec32c(Vec256b(a) & Vec256b(b));
+#endif
 }
-static inline Vec32cb operator && (Vec32cb const & a, Vec32cb const & b) {
+static inline Vec32cb operator && (Vec32cb const a, Vec32cb const b) {
     return a & b;
 }
 // vector operator &= : bitwise and
-static inline Vec32cb & operator &= (Vec32cb & a, Vec32cb const & b) {
+static inline Vec32cb & operator &= (Vec32cb & a, Vec32cb const b) {
     a = a & b;
     return a;
 }
 
 // vector operator | : bitwise or
-static inline Vec32cb operator | (Vec32cb const & a, Vec32cb const & b) {
-    return Vec32cb(Vec256b(a) | Vec256b(b));
+static inline Vec32cb operator | (Vec32cb const a, Vec32cb const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return __mmask32(__mmask32(a) | __mmask32(b)); // _kor_mask32
+#else
+    return Vec32c(Vec256b(a) | Vec256b(b));
+#endif
 }
-static inline Vec32cb operator || (Vec32cb const & a, Vec32cb const & b) {
+static inline Vec32cb operator || (Vec32cb const a, Vec32cb const b) {
     return a | b;
 }
 // vector operator |= : bitwise or
-static inline Vec32cb & operator |= (Vec32cb & a, Vec32cb const & b) {
+static inline Vec32cb & operator |= (Vec32cb & a, Vec32cb const b) {
     a = a | b;
     return a;
 }
 
 // vector operator ^ : bitwise xor
-static inline Vec32cb operator ^ (Vec32cb const & a, Vec32cb const & b) {
-    return Vec32cb(Vec256b(a) ^ Vec256b(b));
+static inline Vec32cb operator ^ (Vec32cb const a, Vec32cb const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return __mmask32(__mmask32(a) ^ __mmask32(b)); // _kxor_mask32
+#else
+    return Vec32c(Vec256b(a) ^ Vec256b(b));
+#endif
 }
 // vector operator ^= : bitwise xor
-static inline Vec32cb & operator ^= (Vec32cb & a, Vec32cb const & b) {
+static inline Vec32cb & operator ^= (Vec32cb & a, Vec32cb const b) {
     a = a ^ b;
     return a;
 }
 
+// vector operator == : xnor
+static inline Vec32cb operator == (Vec32cb const a, Vec32cb const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return __mmask32(__mmask32(a) ^ ~__mmask32(b)); // _kxnor_mask32
+#else
+    return Vec32c(a ^ (~b));
+#endif
+}
+
+// vector operator != : xor
+static inline Vec32cb operator != (Vec32cb const a, Vec32cb const b) {
+    return Vec32cb(a ^ b);
+}
+
 // vector operator ~ : bitwise not
-static inline Vec32cb operator ~ (Vec32cb const & a) {
-    return Vec32cb( ~ Vec256b(a));
+static inline Vec32cb operator ~ (Vec32cb const a) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return __mmask32(~ __mmask32(a)); // _knot_mask32
+#else
+    return Vec32c( ~ Vec256b(a));
+#endif
 }
 
 // vector operator ! : element not
-static inline Vec32cb operator ! (Vec32cb const & a) {
+static inline Vec32cb operator ! (Vec32cb const a) {
     return ~ a;
 }
 
 // vector function andnot
-static inline Vec32cb andnot (Vec32cb const & a, Vec32cb const & b) {
-    return Vec32cb(andnot(Vec256b(a), Vec256b(b)));
+static inline Vec32cb andnot (Vec32cb const a, Vec32cb const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return __mmask32(~__mmask32(b) & __mmask32(a)); // _kandn_mask32
+#else
+    return Vec32c(andnot(Vec256b(a), Vec256b(b)));
+#endif
+}
+
+#if INSTRSET >= 10  // compact boolean vectors
+
+// horizontal_and. Returns true if all elements are true
+static inline bool horizontal_and(Vec32b const a) {
+    return __mmask32(a) == 0xFFFFFFFF;
+}
+
+// horizontal_or. Returns true if at least one element is true
+static inline bool horizontal_or(Vec32b const a) {
+    return __mmask32(a) != 0;
+}
+
+// fix bug in gcc version 70400 header file: _mm256_cmp_epi8_mask returns 16 bit mask, should be 32 bit
+template <int i>
+static inline __mmask32 _mm256_cmp_epi8_mask_fix(__m256i a, __m256i b) {
+#if defined (GCC_VERSION) && GCC_VERSION < 70900 &&  ! defined (__INTEL_COMPILER)
+    return (__mmask32) __builtin_ia32_cmpb256_mask ((__v32qi)a, (__v32qi)b, i, (__mmask32)(-1));
+#else
+    return _mm256_cmp_epi8_mask(a, b, i);
+#endif
 }
 
+template <int i>
+static inline __mmask32 _mm256_cmp_epu8_mask_fix(__m256i a, __m256i b) {
+#if defined (GCC_VERSION) && GCC_VERSION < 70900 &&  ! defined (__INTEL_COMPILER)
+    return (__mmask32) __builtin_ia32_ucmpb256_mask ((__v32qi)a, (__v32qi)b, i, (__mmask32)(-1));
+#else
+    return _mm256_cmp_epu8_mask(a, b, i);
+#endif
+}
+
+#endif
+
 
 /*****************************************************************************
 *
@@ -550,12 +697,11 @@ static inline Vec32cb andnot (Vec32cb const & a, Vec32cb const & b) {
 *****************************************************************************/
 
 // vector operator + : add element by element
-static inline Vec32c operator + (Vec32c const & a, Vec32c const & b) {
+static inline Vec32c operator + (Vec32c const a, Vec32c const b) {
     return _mm256_add_epi8(a, b);
 }
-
 // vector operator += : add
-static inline Vec32c & operator += (Vec32c & a, Vec32c const & b) {
+static inline Vec32c & operator += (Vec32c & a, Vec32c const b) {
     a = a + b;
     return a;
 }
@@ -566,7 +712,6 @@ static inline Vec32c operator ++ (Vec32c & a, int) {
     a = a + 1;
     return a0;
 }
-
 // prefix operator ++
 static inline Vec32c & operator ++ (Vec32c & a) {
     a = a + 1;
@@ -574,17 +719,15 @@ static inline Vec32c & operator ++ (Vec32c & a) {
 }
 
 // vector operator - : subtract element by element
-static inline Vec32c operator - (Vec32c const & a, Vec32c const & b) {
+static inline Vec32c operator - (Vec32c const a, Vec32c const b) {
     return _mm256_sub_epi8(a, b);
 }
-
 // vector operator - : unary minus
-static inline Vec32c operator - (Vec32c const & a) {
+static inline Vec32c operator - (Vec32c const a) {
     return _mm256_sub_epi8(_mm256_setzero_si256(), a);
 }
-
 // vector operator -= : add
-static inline Vec32c & operator -= (Vec32c & a, Vec32c const & b) {
+static inline Vec32c & operator -= (Vec32c & a, Vec32c const b) {
     a = a - b;
     return a;
 }
@@ -595,7 +738,6 @@ static inline Vec32c operator -- (Vec32c & a, int) {
     a = a - 1;
     return a0;
 }
-
 // prefix operator --
 static inline Vec32c & operator -- (Vec32c & a) {
     a = a - 1;
@@ -603,29 +745,33 @@ static inline Vec32c & operator -- (Vec32c & a) {
 }
 
 // vector operator * : multiply element by element
-static inline Vec32c operator * (Vec32c const & a, Vec32c const & b) {
-    // There is no 8-bit multiply in SSE2. Split into two 16-bit multiplies
-    __m256i aodd    = _mm256_srli_epi16(a,8);                 // odd numbered elements of a
-    __m256i bodd    = _mm256_srli_epi16(b,8);                 // odd numbered elements of b
-    __m256i muleven = _mm256_mullo_epi16(a,b);                // product of even numbered elements
-    __m256i mulodd  = _mm256_mullo_epi16(aodd,bodd);          // product of odd  numbered elements
-            mulodd  = _mm256_slli_epi16(mulodd,8);            // put odd numbered elements back in place
-    __m256i mask    = _mm256_set1_epi32(0x00FF00FF);          // mask for even positions
-    __m256i product = selectb(mask,muleven,mulodd);           // interleave even and odd
+static inline Vec32c operator * (Vec32c const a, Vec32c const b) {
+    // There is no 8-bit multiply in AVX2. Split into two 16-bit multiplications
+    __m256i aodd    = _mm256_srli_epi16(a,8);              // odd numbered elements of a
+    __m256i bodd    = _mm256_srli_epi16(b,8);              // odd numbered elements of b
+    __m256i muleven = _mm256_mullo_epi16(a,b);             // product of even numbered elements
+    __m256i mulodd  = _mm256_mullo_epi16(aodd,bodd);       // product of odd  numbered elements
+            mulodd  = _mm256_slli_epi16(mulodd,8);         // put odd numbered elements back in place
+#if INSTRSET >= 10   // AVX512VL + AVX512BW
+    return _mm256_mask_mov_epi8(mulodd, 0x55555555, muleven);
+#else
+    __m256i mask    = _mm256_set1_epi32(0x00FF00FF);       // mask for even positions
+    __m256i product = selectb(mask,muleven,mulodd);        // interleave even and odd
     return product;
+#endif
 }
 
 // vector operator *= : multiply
-static inline Vec32c & operator *= (Vec32c & a, Vec32c const & b) {
+static inline Vec32c & operator *= (Vec32c & a, Vec32c const b) {
     a = a * b;
     return a;
 }
 
 // vector operator << : shift left all elements
-static inline Vec32c operator << (Vec32c const & a, int b) {
-    uint32_t mask = (uint32_t)0xFF >> (uint32_t)b;                // mask to remove bits that are shifted out
+static inline Vec32c operator << (Vec32c const a, int b) {
+    uint32_t mask = (uint32_t)0xFF >> (uint32_t)b;                   // mask to remove bits that are shifted out
     __m256i am    = _mm256_and_si256(a,_mm256_set1_epi8((char)mask));// remove bits that will overflow
-    __m256i res   = _mm256_sll_epi16(am,_mm_cvtsi32_si128(b));   // 16-bit shifts
+    __m256i res   = _mm256_sll_epi16(am,_mm_cvtsi32_si128(b));       // 16-bit shifts
     return res;
 }
 
@@ -636,13 +782,17 @@ static inline Vec32c & operator <<= (Vec32c & a, int b) {
 }
 
 // vector operator >> : shift right arithmetic all elements
-static inline Vec32c operator >> (Vec32c const & a, int b) {
-    __m256i aeven = _mm256_slli_epi16(a,8);                            // even numbered elements of a. get sign bit in position
-            aeven = _mm256_sra_epi16(aeven,_mm_cvtsi32_si128(b+8));    // shift arithmetic, back to position
-    __m256i aodd  = _mm256_sra_epi16(a,_mm_cvtsi32_si128(b));          // shift odd numbered elements arithmetic
-    __m256i mask  = _mm256_set1_epi32(0x00FF00FF);                     // mask for even positions
-    __m256i res   = selectb(mask,aeven,aodd);                          // interleave even and odd
+static inline Vec32c operator >> (Vec32c const a, int b) {
+    __m256i aeven = _mm256_slli_epi16(a,8);                          // even numbered elements of a. get sign bit in position
+            aeven = _mm256_sra_epi16(aeven,_mm_cvtsi32_si128(b+8));  // shift arithmetic, back to position
+    __m256i aodd  = _mm256_sra_epi16(a,_mm_cvtsi32_si128(b));        // shift odd numbered elements arithmetic
+#if INSTRSET >= 10   // AVX512VL + AVX512BW
+    return _mm256_mask_mov_epi8(aodd, 0x55555555, aeven);
+#else
+    __m256i mask  = _mm256_set1_epi32(0x00FF00FF);                   // mask for even positions
+    __m256i res   = selectb(mask,aeven,aodd);                        // interleave even and odd
     return res;
+#endif
 }
 
 // vector operator >>= : shift right artihmetic
@@ -652,106 +802,151 @@ static inline Vec32c & operator >>= (Vec32c & a, int b) {
 }
 
 // vector operator == : returns true for elements for which a == b
-static inline Vec32cb operator == (Vec32c const & a, Vec32c const & b) {
+static inline Vec32cb operator == (Vec32c const a, Vec32c const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    //return _mm256_cmp_epi8_mask (a, b, 0);
+    return _mm256_cmp_epi8_mask_fix<0> (a, b);
+#else
     return _mm256_cmpeq_epi8(a,b);
+#endif
 }
 
 // vector operator != : returns true for elements for which a != b
-static inline Vec32cb operator != (Vec32c const & a, Vec32c const & b) {
+static inline Vec32cb operator != (Vec32c const a, Vec32c const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_cmp_epi8_mask_fix<4> (a, b);
+#else
     return Vec32cb(Vec32c(~(a == b)));
+#endif
 }
 
 // vector operator > : returns true for elements for which a > b (signed)
-static inline Vec32cb operator > (Vec32c const & a, Vec32c const & b) {
+static inline Vec32cb operator > (Vec32c const a, Vec32c const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_cmp_epi8_mask_fix<6> (a, b);
+#else
     return _mm256_cmpgt_epi8(a,b);
+#endif
 }
 
 // vector operator < : returns true for elements for which a < b (signed)
-static inline Vec32cb operator < (Vec32c const & a, Vec32c const & b) {
+static inline Vec32cb operator < (Vec32c const a, Vec32c const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_cmp_epi8_mask_fix<1> (a, b);
+#else
     return b > a;
+#endif
 }
 
 // vector operator >= : returns true for elements for which a >= b (signed)
-static inline Vec32cb operator >= (Vec32c const & a, Vec32c const & b) {
+static inline Vec32cb operator >= (Vec32c const a, Vec32c const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_cmp_epi8_mask_fix<5> (a, b);
+#else
     return Vec32cb(Vec32c(~(b > a)));
+#endif
 }
 
 // vector operator <= : returns true for elements for which a <= b (signed)
-static inline Vec32cb operator <= (Vec32c const & a, Vec32c const & b) {
+static inline Vec32cb operator <= (Vec32c const a, Vec32c const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_cmp_epi8_mask_fix<2> (a, b);
+#else
     return b >= a;
+#endif
 }
 
 // vector operator & : bitwise and
-static inline Vec32c operator & (Vec32c const & a, Vec32c const & b) {
+static inline Vec32c operator & (Vec32c const a, Vec32c const b) {
     return Vec32c(Vec256b(a) & Vec256b(b));
 }
-static inline Vec32c operator && (Vec32c const & a, Vec32c const & b) {
+static inline Vec32c operator && (Vec32c const a, Vec32c const b) {
     return a & b;
 }
 // vector operator &= : bitwise and
-static inline Vec32c & operator &= (Vec32c & a, Vec32c const & b) {
+static inline Vec32c & operator &= (Vec32c & a, Vec32c const b) {
     a = a & b;
     return a;
 }
 
 // vector operator | : bitwise or
-static inline Vec32c operator | (Vec32c const & a, Vec32c const & b) {
+static inline Vec32c operator | (Vec32c const a, Vec32c const b) {
     return Vec32c(Vec256b(a) | Vec256b(b));
 }
-static inline Vec32c operator || (Vec32c const & a, Vec32c const & b) {
+static inline Vec32c operator || (Vec32c const a, Vec32c const b) {
     return a | b;
 }
 // vector operator |= : bitwise or
-static inline Vec32c & operator |= (Vec32c & a, Vec32c const & b) {
+static inline Vec32c & operator |= (Vec32c & a, Vec32c const b) {
     a = a | b;
     return a;
 }
 
 // vector operator ^ : bitwise xor
-static inline Vec32c operator ^ (Vec32c const & a, Vec32c const & b) {
+static inline Vec32c operator ^ (Vec32c const a, Vec32c const b) {
     return Vec32c(Vec256b(a) ^ Vec256b(b));
 }
 // vector operator ^= : bitwise xor
-static inline Vec32c & operator ^= (Vec32c & a, Vec32c const & b) {
+static inline Vec32c & operator ^= (Vec32c & a, Vec32c const b) {
     a = a ^ b;
     return a;
 }
 
 // vector operator ~ : bitwise not
-static inline Vec32c operator ~ (Vec32c const & a) {
+static inline Vec32c operator ~ (Vec32c const a) {
     return Vec32c( ~ Vec256b(a));
 }
 
 // vector operator ! : logical not, returns true for elements == 0
-static inline Vec32cb operator ! (Vec32c const & a) {
+static inline Vec32cb operator ! (Vec32c const a) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_cmp_epi8_mask_fix<0> (a, _mm256_setzero_si256());
+#else
     return _mm256_cmpeq_epi8(a,_mm256_setzero_si256());
+#endif
 }
 
 // Functions for this class
 
 // Select between two operands. Corresponds to this pseudocode:
 // for (int i = 0; i < 16; i++) result[i] = s[i] ? a[i] : b[i];
-// Each byte in s must be either 0 (false) or -1 (true). No other values are allowed.
-static inline Vec32c select (Vec32cb const & s, Vec32c const & a, Vec32c const & b) {
+static inline Vec32c select (Vec32cb const s, Vec32c const a, Vec32c const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_mask_mov_epi8(b, s, a);
+#else
     return selectb(s,a,b);
+#endif
 }
 
 // Conditional add: For all vector elements i: result[i] = f[i] ? (a[i] + b[i]) : a[i]
-static inline Vec32c if_add (Vec32cb const & f, Vec32c const & a, Vec32c const & b) {
+static inline Vec32c if_add (Vec32cb const f, Vec32c const a, Vec32c const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_mask_add_epi8 (a, f, a, b);
+#else
     return a + (Vec32c(f) & b);
+#endif
+}
+
+// Conditional subtract
+static inline Vec32c if_sub (Vec32cb const f, Vec32c const a, Vec32c const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_mask_sub_epi8 (a, f, a, b);
+#else
+    return a - (Vec32c(f) & b);
+#endif
+}
+
+// Conditional multiply
+static inline Vec32c if_mul (Vec32cb const f, Vec32c const a, Vec32c const b) {
+    return select(f, a*b, a);
 }
 
-// Horizontal add: Calculates the sum of all vector elements.
-// Overflow will wrap around
-static inline int32_t horizontal_add (Vec32c const & a) {
+// Horizontal add: Calculates the sum of all vector elements. Overflow will wrap around
+static inline int8_t horizontal_add (Vec32c const a) {
     __m256i sum1 = _mm256_sad_epu8(a,_mm256_setzero_si256());
     __m256i sum2 = _mm256_shuffle_epi32(sum1,2);
     __m256i sum3 = _mm256_add_epi16(sum1,sum2);
-#if defined (_MSC_VER) && _MSC_VER <= 1700 && ! defined(__INTEL_COMPILER)
-    __m128i sum4 = _mm256_extractf128_si256(sum3,1);                // bug in MS VS 11
-#else
     __m128i sum4 = _mm256_extracti128_si256(sum3,1);
-#endif
     __m128i sum5 = _mm_add_epi16(_mm256_castsi256_si128(sum3),sum4);
     int8_t  sum6 = (int8_t)_mm_cvtsi128_si32(sum5);                  // truncate to 8 bits
     return  sum6;                                                    // sign extend to 32 bits
@@ -759,76 +954,77 @@ static inline int32_t horizontal_add (Vec32c const & a) {
 
 // Horizontal add extended: Calculates the sum of all vector elements.
 // Each element is sign-extended before addition to avoid overflow
-static inline int32_t horizontal_add_x (Vec32c const & a) {
+static inline int32_t horizontal_add_x (Vec32c const a) {
     __m256i aeven = _mm256_slli_epi16(a,8);                          // even numbered elements of a. get sign bit in position
             aeven = _mm256_srai_epi16(aeven,8);                      // sign extend even numbered elements
     __m256i aodd  = _mm256_srai_epi16(a,8);                          // sign extend odd  numbered elements
     __m256i sum1  = _mm256_add_epi16(aeven,aodd);                    // add even and odd elements
-    __m256i sum2  = _mm256_hadd_epi16(sum1,sum1);                    // horizontally add 2x8 elements in 3 steps
-    __m256i sum3  = _mm256_hadd_epi16(sum2,sum2);
-    __m256i sum4  = _mm256_hadd_epi16(sum3,sum3);
-#if defined (_MSC_VER) && _MSC_VER <= 1700 && ! defined(__INTEL_COMPILER)
-    __m128i sum5  = _mm256_extractf128_si256(sum4,1);                // bug in MS VS 11
+    __m128i sum2  = _mm_add_epi16(_mm256_extracti128_si256(sum1,1),_mm256_castsi256_si128(sum1));
+    // The hadd instruction is inefficient, and may be split into two instructions for faster decoding
+#if false
+    __m128i sum3  = _mm_hadd_epi16(sum2,sum2);
+    __m128i sum4  = _mm_hadd_epi16(sum3,sum3);
+    __m128i sum5  = _mm_hadd_epi16(sum4,sum4);
 #else
-    __m128i sum5  = _mm256_extracti128_si256(sum4,1);                // get high sum
+    __m128i sum3  = _mm_add_epi16(sum2,_mm_unpackhi_epi64(sum2,sum2));
+    __m128i sum4  = _mm_add_epi16(sum3,_mm_shuffle_epi32(sum3,1));
+    __m128i sum5  = _mm_add_epi16(sum4,_mm_shufflelo_epi16(sum4,1));
 #endif
-    __m128i sum6  = _mm_add_epi16(_mm256_castsi256_si128(sum4),sum5);// add high and low sum
-    int16_t sum7  = (int16_t)_mm_cvtsi128_si32(sum6);                // 16 bit sum
-    return  sum7;                                                    // sign extend to 32 bits
+    int16_t sum6  = (int16_t)_mm_cvtsi128_si32(sum5);                // 16 bit sum
+    return  sum6;                                                    // sign extend to 32 bits
 }
 
 // function add_saturated: add element by element, signed with saturation
-static inline Vec32c add_saturated(Vec32c const & a, Vec32c const & b) {
+static inline Vec32c add_saturated(Vec32c const a, Vec32c const b) {
     return _mm256_adds_epi8(a, b);
 }
 
 // function sub_saturated: subtract element by element, signed with saturation
-static inline Vec32c sub_saturated(Vec32c const & a, Vec32c const & b) {
+static inline Vec32c sub_saturated(Vec32c const a, Vec32c const b) {
     return _mm256_subs_epi8(a, b);
 }
 
 // function max: a > b ? a : b
-static inline Vec32c max(Vec32c const & a, Vec32c const & b) {
+static inline Vec32c max(Vec32c const a, Vec32c const b) {
     return _mm256_max_epi8(a,b);
 }
 
 // function min: a < b ? a : b
-static inline Vec32c min(Vec32c const & a, Vec32c const & b) {
+static inline Vec32c min(Vec32c const a, Vec32c const b) {
     return _mm256_min_epi8(a,b);
 }
 
 // function abs: a >= 0 ? a : -a
-static inline Vec32c abs(Vec32c const & a) {
-    return _mm256_sign_epi8(a,a);
+static inline Vec32c abs(Vec32c const a) {
+    return _mm256_abs_epi8(a);
 }
 
 // function abs_saturated: same as abs, saturate if overflow
-static inline Vec32c abs_saturated(Vec32c const & a) {
-    __m256i absa   = abs(a);                                         // abs(a)
-    __m256i overfl = _mm256_cmpgt_epi8(_mm256_setzero_si256(),absa); // 0 > a
-    return           _mm256_add_epi8(absa,overfl);                   // subtract 1 if 0x80
+static inline Vec32c abs_saturated(Vec32c const a) {
+    __m256i absa = abs(a);                                 // abs(a)
+#if INSTRSET >= 10
+    return _mm256_min_epu8(absa, Vec32c(0x7F));
+#else
+    __m256i overfl = _mm256_cmpgt_epi8(_mm256_setzero_si256(), absa);// 0 > a
+    return           _mm256_add_epi8(absa, overfl);        // subtract 1 if 0x80
+#endif
 }
 
 // function rotate_left all elements
 // Use negative count to rotate right
-static inline Vec32c rotate_left(Vec32c const & a, int b) {
-    __m128i bb        = _mm_cvtsi32_si128(b & 7);             // b modulo 8
-    __m128i mbb       = _mm_cvtsi32_si128((8-b) & 7);         // 8-b modulo 8
-    __m256i maskeven  = _mm256_set1_epi32(0x00FF00FF);        // mask for even numbered bytes
-    __m256i even      = _mm256_and_si256(a,maskeven);         // even numbered bytes of a
-    __m256i odd       = _mm256_andnot_si256(maskeven,a);      // odd numbered bytes of a
-    __m256i evenleft  = _mm256_sll_epi16(even,bb);            // even bytes of a << b
-    __m256i oddleft   = _mm256_sll_epi16(odd,bb);             // odd  bytes of a << b
-    __m256i evenright = _mm256_srl_epi16(even,mbb);           // even bytes of a >> 8-b
-    __m256i oddright  = _mm256_srl_epi16(odd,mbb);            // odd  bytes of a >> 8-b
-    __m256i evenrot   = _mm256_or_si256(evenleft,evenright);  // even bytes of a rotated
-    __m256i oddrot    = _mm256_or_si256(oddleft,oddright);    // odd  bytes of a rotated
-    __m256i allrot    = selectb(maskeven,evenrot,oddrot);     // all  bytes rotated
-    return  allrot;
+static inline Vec32c rotate_left(Vec32c const a, int b) {
+    int8_t  mask  = int8_t(0xFFu << b);                    // mask off overflow bits
+    __m256i m     = _mm256_set1_epi8(mask);
+    __m128i bb    = _mm_cvtsi32_si128(b & 7);              // b modulo 8
+    __m128i mbb   = _mm_cvtsi32_si128((- b) & 7);          // 8-b modulo 8
+    __m256i left  = _mm256_sll_epi16(a, bb);               // a << b
+    __m256i right = _mm256_srl_epi16(a, mbb);              // a >> 8-b
+            left  = _mm256_and_si256(m, left);             // mask off overflow bits
+            right = _mm256_andnot_si256(m, right);
+    return  _mm256_or_si256(left, right);                  // combine left and right shifted bits
 }
 
 
-
 /*****************************************************************************
 *
 *          Vector of 16 8-bit unsigned integers
@@ -838,30 +1034,29 @@ static inline Vec32c rotate_left(Vec32c const & a, int b) {
 class Vec32uc : public Vec32c {
 public:
     // Default constructor:
-    Vec32uc(){
-    }
+    Vec32uc() = default;
     // Constructor to broadcast the same value into all elements:
     Vec32uc(uint32_t i) {
         ymm = _mm256_set1_epi8((char)i);
     }
     // Constructor to build from all elements:
     Vec32uc(uint8_t i0, uint8_t i1, uint8_t i2, uint8_t i3, uint8_t i4, uint8_t i5, uint8_t i6, uint8_t i7,
-        uint8_t i8, uint8_t i9, uint8_t i10, uint8_t i11, uint8_t i12, uint8_t i13, uint8_t i14, uint8_t i15,        
+        uint8_t i8, uint8_t i9, uint8_t i10, uint8_t i11, uint8_t i12, uint8_t i13, uint8_t i14, uint8_t i15,
         uint8_t i16, uint8_t i17, uint8_t i18, uint8_t i19, uint8_t i20, uint8_t i21, uint8_t i22, uint8_t i23,
         uint8_t i24, uint8_t i25, uint8_t i26, uint8_t i27, uint8_t i28, uint8_t i29, uint8_t i30, uint8_t i31) {
-        ymm = _mm256_setr_epi8(i0, i1, i2, i3, i4, i5, i6, i7, i8, i9, i10, i11, i12, i13, i14, i15,
-            i16, i17, i18, i19, i20, i21, i22, i23, i24, i25, i26, i27, i28, i29, i30, i31);
+        ymm = _mm256_setr_epi8((int8_t)i0, (int8_t)i1, (int8_t)i2, (int8_t)i3, (int8_t)i4, (int8_t)i5, (int8_t)i6, (int8_t)i7, (int8_t)i8, (int8_t)i9, (int8_t)i10, (int8_t)i11, (int8_t)i12, (int8_t)i13, (int8_t)i14, (int8_t)i15,
+            (int8_t)i16, (int8_t)i17, (int8_t)i18, (int8_t)i19, (int8_t)i20, (int8_t)i21, (int8_t)i22, (int8_t)i23, (int8_t)i24, (int8_t)i25, (int8_t)i26, (int8_t)i27, (int8_t)i28, (int8_t)i29, (int8_t)i30, (int8_t)i31);
     }
     // Constructor to build from two Vec16uc:
-    Vec32uc(Vec16uc const & a0, Vec16uc const & a1) {
+    Vec32uc(Vec16uc const a0, Vec16uc const a1) {
         ymm = set_m128ir(a0, a1);
     }
     // Constructor to convert from type __m256i used in intrinsics:
-    Vec32uc(__m256i const & x) {
+    Vec32uc(__m256i const x) {
         ymm = x;
     }
     // Assignment operator to convert from type __m256i used in intrinsics:
-    Vec32uc & operator = (__m256i const & x) {
+    Vec32uc & operator = (__m256i const x) {
         ymm = x;
         return *this;
     }
@@ -876,18 +1071,17 @@ public:
         return *this;
     }
     // Member function to change a single element in vector
-    // Note: This function is inefficient. Use load function if changing more than one element
-    Vec32uc const & insert(uint32_t index, uint8_t value) {
-        Vec32c::insert(index, value);
+    Vec32uc const insert(int index, uint8_t value) {
+        Vec32c::insert(index, (int8_t)value);
         return *this;
     }
     // Member function extract a single element from vector
-    uint8_t extract(uint32_t index) const {
-        return Vec32c::extract(index);
+    uint8_t extract(int index) const {
+        return (uint8_t)Vec32c::extract(index);
     }
     // Extract a single element. Use store function if extracting more than one element.
     // Operator [] can only read an element, not write.
-    uint8_t operator [] (uint32_t index) const {
+    uint8_t operator [] (int index) const {
         return extract(index);
     }
     // Member functions to split into two Vec16uc:
@@ -895,50 +1089,53 @@ public:
         return _mm256_castsi256_si128(ymm);
     }
     Vec16uc get_high() const {
-        return _mm256_extracti128_si256(ymm,1);
+        return _mm256_extractf128_si256(ymm,1);
+    }
+    static constexpr int elementtype() {
+        return 5;
     }
 };
 
 // Define operators for this class
 
 // vector operator + : add
-static inline Vec32uc operator + (Vec32uc const & a, Vec32uc const & b) {
+static inline Vec32uc operator + (Vec32uc const a, Vec32uc const b) {
     return Vec32uc (Vec32c(a) + Vec32c(b));
 }
 
 // vector operator - : subtract
-static inline Vec32uc operator - (Vec32uc const & a, Vec32uc const & b) {
+static inline Vec32uc operator - (Vec32uc const a, Vec32uc const b) {
     return Vec32uc (Vec32c(a) - Vec32c(b));
 }
 
 // vector operator * : multiply
-static inline Vec32uc operator * (Vec32uc const & a, Vec32uc const & b) {
+static inline Vec32uc operator * (Vec32uc const a, Vec32uc const b) {
     return Vec32uc (Vec32c(a) * Vec32c(b));
 }
 
 // vector operator << : shift left all elements
-static inline Vec32uc operator << (Vec32uc const & a, uint32_t b) {
+static inline Vec32uc operator << (Vec32uc const a, uint32_t b) {
     uint32_t mask = (uint32_t)0xFF >> (uint32_t)b;                // mask to remove bits that are shifted out
     __m256i am    = _mm256_and_si256(a,_mm256_set1_epi8((char)mask));// remove bits that will overflow
-    __m256i res   = _mm256_sll_epi16(am,_mm_cvtsi32_si128(b));    // 16-bit shifts
+    __m256i res   = _mm256_sll_epi16(am,_mm_cvtsi32_si128((int)b));    // 16-bit shifts
     return res;
 }
 
 // vector operator << : shift left all elements
-static inline Vec32uc operator << (Vec32uc const & a, int32_t b) {
+static inline Vec32uc operator << (Vec32uc const a, int32_t b) {
     return a << (uint32_t)b;
 }
 
 // vector operator >> : shift right logical all elements
-static inline Vec32uc operator >> (Vec32uc const & a, uint32_t b) {
+static inline Vec32uc operator >> (Vec32uc const a, uint32_t b) {
     uint32_t mask = (uint32_t)0xFF << (uint32_t)b;                // mask to remove bits that are shifted out
     __m256i am    = _mm256_and_si256(a,_mm256_set1_epi8((char)mask));// remove bits that will overflow
-    __m256i res   = _mm256_srl_epi16(am,_mm_cvtsi32_si128(b));    // 16-bit shifts
+    __m256i res   = _mm256_srl_epi16(am,_mm_cvtsi32_si128((int)b));    // 16-bit shifts
     return res;
 }
 
 // vector operator >> : shift right logical all elements
-static inline Vec32uc operator >> (Vec32uc const & a, int32_t b) {
+static inline Vec32uc operator >> (Vec32uc const a, int32_t b) {
     return a >> (uint32_t)b;
 }
 
@@ -949,48 +1146,65 @@ static inline Vec32uc & operator >>= (Vec32uc & a, uint32_t b) {
 }
 
 // vector operator >= : returns true for elements for which a >= b (unsigned)
-static inline Vec32cb operator >= (Vec32uc const & a, Vec32uc const & b) {
+static inline Vec32cb operator >= (Vec32uc const a, Vec32uc const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    //return _mm256_cmp_epu8_mask (a, b, 5);
+    return _mm256_cmp_epu8_mask_fix<5> (a, b);
+#else
     return _mm256_cmpeq_epi8(_mm256_max_epu8(a,b), a); // a == max(a,b)
+#endif
 }
 
 // vector operator <= : returns true for elements for which a <= b (unsigned)
-static inline Vec32cb operator <= (Vec32uc const & a, Vec32uc const & b) {
+static inline Vec32cb operator <= (Vec32uc const a, Vec32uc const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_cmp_epu8_mask_fix<2> (a, b);
+#else
     return b >= a;
+#endif
 }
 
 // vector operator > : returns true for elements for which a > b (unsigned)
-static inline Vec32cb operator > (Vec32uc const & a, Vec32uc const & b) {
+static inline Vec32cb operator > (Vec32uc const a, Vec32uc const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_cmp_epu8_mask_fix<6> (a, b);
+#else
     return Vec32cb(Vec32c(~(b >= a)));
+#endif
 }
 
 // vector operator < : returns true for elements for which a < b (unsigned)
-static inline Vec32cb operator < (Vec32uc const & a, Vec32uc const & b) {
+static inline Vec32cb operator < (Vec32uc const a, Vec32uc const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_cmp_epu8_mask_fix<1> (a, b);
+#else
     return b > a;
+#endif
 }
 
 // vector operator & : bitwise and
-static inline Vec32uc operator & (Vec32uc const & a, Vec32uc const & b) {
+static inline Vec32uc operator & (Vec32uc const a, Vec32uc const b) {
     return Vec32uc(Vec256b(a) & Vec256b(b));
 }
-static inline Vec32uc operator && (Vec32uc const & a, Vec32uc const & b) {
+static inline Vec32uc operator && (Vec32uc const a, Vec32uc const b) {
     return a & b;
 }
 
 // vector operator | : bitwise or
-static inline Vec32uc operator | (Vec32uc const & a, Vec32uc const & b) {
+static inline Vec32uc operator | (Vec32uc const a, Vec32uc const b) {
     return Vec32uc(Vec256b(a) | Vec256b(b));
 }
-static inline Vec32uc operator || (Vec32uc const & a, Vec32uc const & b) {
+static inline Vec32uc operator || (Vec32uc const a, Vec32uc const b) {
     return a | b;
 }
 
 // vector operator ^ : bitwise xor
-static inline Vec32uc operator ^ (Vec32uc const & a, Vec32uc const & b) {
+static inline Vec32uc operator ^ (Vec32uc const a, Vec32uc const b) {
     return Vec32uc(Vec256b(a) ^ Vec256b(b));
 }
 
 // vector operator ~ : bitwise not
-static inline Vec32uc operator ~ (Vec32uc const & a) {
+static inline Vec32uc operator ~ (Vec32uc const a) {
     return Vec32uc( ~ Vec256b(a));
 }
 
@@ -998,76 +1212,81 @@ static inline Vec32uc operator ~ (Vec32uc const & a) {
 
 // Select between two operands. Corresponds to this pseudocode:
 // for (int i = 0; i < 32; i++) result[i] = s[i] ? a[i] : b[i];
-// Each byte in s must be either 0 (false) or -1 (true). No other values are allowed.
-// (s is signed)
-static inline Vec32uc select (Vec32cb const & s, Vec32uc const & a, Vec32uc const & b) {
+static inline Vec32uc select (Vec32cb const s, Vec32uc const a, Vec32uc const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_mask_mov_epi8(b, s, a);
+#else
     return selectb(s,a,b);
+#endif
 }
 
 // Conditional add: For all vector elements i: result[i] = f[i] ? (a[i] + b[i]) : a[i]
-static inline Vec32uc if_add (Vec32cb const & f, Vec32uc const & a, Vec32uc const & b) {
+static inline Vec32uc if_add (Vec32cb const f, Vec32uc const a, Vec32uc const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_mask_add_epi8 (a, f, a, b);
+#else
     return a + (Vec32uc(f) & b);
+#endif
 }
 
-// Horizontal add: Calculates the sum of all vector elements.
-// Overflow will wrap around
+// Conditional subtract
+static inline Vec32uc if_sub (Vec32cb const f, Vec32uc const a, Vec32uc const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_mask_sub_epi8 (a, f, a, b);
+#else
+    return a - (Vec32uc(f) & b);
+#endif
+}
+
+// Conditional multiply
+static inline Vec32uc if_mul (Vec32cb const f, Vec32uc const a, Vec32uc const b) {
+    return select(f, a*b, a);
+}
+
+// Horizontal add: Calculates the sum of all vector elements. Overflow will wrap around
 // (Note: horizontal_add_x(Vec32uc) is slightly faster)
-static inline uint32_t horizontal_add (Vec32uc const & a) {
+static inline uint8_t horizontal_add (Vec32uc const a) {
     __m256i  sum1 = _mm256_sad_epu8(a,_mm256_setzero_si256());
     __m256i  sum2 = _mm256_shuffle_epi32(sum1,2);
     __m256i  sum3 = _mm256_add_epi16(sum1,sum2);
-#if defined (_MSC_VER) && _MSC_VER <= 1700 && ! defined(__INTEL_COMPILER)
-    __m128i  sum4 = _mm256_extractf128_si256(sum3,1); // bug in MS compiler VS 11
-#else
     __m128i  sum4 = _mm256_extracti128_si256(sum3,1);
-#endif
     __m128i  sum5 = _mm_add_epi16(_mm256_castsi256_si128(sum3),sum4);
-    uint8_t  sum6 = (uint8_t)_mm_cvtsi128_si32(sum5); // truncate to 8 bits
-    return   sum6;                                    // zero extend to 32 bits
+    uint8_t  sum6 = (uint8_t)_mm_cvtsi128_si32(sum5);      // truncate to 8 bits
+    return   sum6;                                         // zero extend to 32 bits
 }
 
 // Horizontal add extended: Calculates the sum of all vector elements.
 // Each element is zero-extended before addition to avoid overflow
-static inline uint32_t horizontal_add_x (Vec32uc const & a) {
+static inline uint32_t horizontal_add_x (Vec32uc const a) {
     __m256i sum1 = _mm256_sad_epu8(a,_mm256_setzero_si256());
     __m256i sum2 = _mm256_shuffle_epi32(sum1,2);
     __m256i sum3 = _mm256_add_epi16(sum1,sum2);
-#if defined (_MSC_VER) && _MSC_VER <= 1700 && ! defined(__INTEL_COMPILER)
-    __m128i sum4 = _mm256_extractf128_si256(sum3,1); // bug in MS compiler VS 11
-#else
     __m128i sum4 = _mm256_extracti128_si256(sum3,1);
-#endif
     __m128i sum5 = _mm_add_epi16(_mm256_castsi256_si128(sum3),sum4);
-    return         _mm_cvtsi128_si32(sum5);
+    return         (uint32_t)_mm_cvtsi128_si32(sum5);
 }
 
 // function add_saturated: add element by element, unsigned with saturation
-static inline Vec32uc add_saturated(Vec32uc const & a, Vec32uc const & b) {
+static inline Vec32uc add_saturated(Vec32uc const a, Vec32uc const b) {
     return _mm256_adds_epu8(a, b);
 }
 
 // function sub_saturated: subtract element by element, unsigned with saturation
-static inline Vec32uc sub_saturated(Vec32uc const & a, Vec32uc const & b) {
+static inline Vec32uc sub_saturated(Vec32uc const a, Vec32uc const b) {
     return _mm256_subs_epu8(a, b);
 }
 
 // function max: a > b ? a : b
-static inline Vec32uc max(Vec32uc const & a, Vec32uc const & b) {
+static inline Vec32uc max(Vec32uc const a, Vec32uc const b) {
     return _mm256_max_epu8(a,b);
 }
 
 // function min: a < b ? a : b
-static inline Vec32uc min(Vec32uc const & a, Vec32uc const & b) {
+static inline Vec32uc min(Vec32uc const a, Vec32uc const b) {
     return _mm256_min_epu8(a,b);
 }
 
-// function avg: (a + b + 1) >> 1
-static inline Vec32uc avg(Vec32uc const & a, Vec32uc const & b) {
-    return _mm256_avg_epu8(a,b);
-}
-
 
-    
 /*****************************************************************************
 *
 *          Vector of 16 16-bit signed integers
@@ -1077,8 +1296,7 @@ static inline Vec32uc avg(Vec32uc const & a, Vec32uc const & b) {
 class Vec16s : public Vec256b {
 public:
     // Default constructor:
-    Vec16s() {
-    }
+    Vec16s() = default;
     // Constructor to broadcast the same value into all elements:
     Vec16s(int i) {
         ymm = _mm256_set1_epi16((int16_t)i);
@@ -1089,18 +1307,22 @@ public:
         ymm = _mm256_setr_epi16(i0, i1, i2, i3, i4, i5, i6, i7, i8, i9, i10, i11, i12, i13, i14, i15 );
     }
     // Constructor to build from two Vec8s:
-    Vec16s(Vec8s const & a0, Vec8s const & a1) {
+    Vec16s(Vec8s const a0, Vec8s const a1) {
         ymm = set_m128ir(a0, a1);
     }
     // Constructor to convert from type __m256i used in intrinsics:
-    Vec16s(__m256i const & x) {
+    Vec16s(__m256i const x) {
         ymm = x;
     }
     // Assignment operator to convert from type __m256i used in intrinsics:
-    Vec16s & operator = (__m256i const & x) {
+    Vec16s & operator = (__m256i const x) {
         ymm = x;
         return *this;
     }
+    // Constructor to convert from type Vec256b used in emulation:
+    Vec16s(Vec256b const & x) {
+        ymm = x;
+    }
     // Type cast operator to convert to __m256i used in intrinsics
     operator __m256i() const {
         return ymm;
@@ -1115,13 +1337,11 @@ public:
         ymm = _mm256_load_si256((__m256i const*)p);
         return *this;
     }
-    // Member function to load 16 8-bit unsigned integers from array
-    Vec16s & load_16uc(void const * p) {
-        ymm = _mm256_cvtepu8_epi16(Vec16uc().load(p));
-        return *this;
-    }
     // Partial load. Load n elements and set the rest to 0
     Vec16s & load_partial(int n, void const * p) {
+#if INSTRSET >= 10  // AVX512VL
+        ymm = _mm256_maskz_loadu_epi16(__mmask16((1u << n) - 1), p);
+#else
         if (n <= 0) {
             *this = 0;
         }
@@ -1134,10 +1354,14 @@ public:
         else {
             load(p);
         }
+#endif
         return *this;
     }
     // Partial store. Store n elements
     void store_partial(int n, void * p) const {
+#if INSTRSET >= 10  // AVX512VL + AVX512BW
+        _mm256_mask_storeu_epi16(p, __mmask16((1u << n) - 1), ymm);
+#else
         if (n <= 0) {
             return;
         }
@@ -1151,30 +1375,43 @@ public:
         else {
             store(p);
         }
+#endif
     }
     // cut off vector to n elements. The last 16-n elements are set to zero
     Vec16s & cutoff(int n) {
-        *this = Vec32c(*this).cutoff(n * 2);
+#if INSTRSET >= 10
+        ymm = _mm256_maskz_mov_epi16(__mmask16((1u << n) - 1), ymm);
+#else
+        *this = Vec16s(Vec32c(*this).cutoff(n * 2));
+#endif
         return *this;
     }
     // Member function to change a single element in vector
-    // Note: This function is inefficient. Use load function if changing more than one element
-    Vec16s const & insert(uint32_t index, int16_t value) {
-        static const int16_t m[32] = {0,0,0,0, 0,0,0,0, 0,0,0,0, 0,0,0,0, -1,0,0,0, 0,0,0,0, 0,0,0,0, 0,0,0,0};
+    Vec16s const insert(int index, int16_t value) {
+#if INSTRSET >= 10
+        ymm = _mm256_mask_set1_epi16(ymm, __mmask16(1u << index), value);
+#else
+        const int16_t m[32] = {0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, -1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0};
         __m256i mask  = Vec256b().load(m + 16 - (index & 0x0F));
         __m256i broad = _mm256_set1_epi16(value);
         ymm = selectb(mask, broad, ymm);
+#endif
         return *this;
     }
     // Member function extract a single element from vector
-    int16_t extract(uint32_t index) const {
-        int16_t x[16];
+    int16_t extract(int index) const {
+#if INSTRSET >= 10 && defined (__AVX512VBMI2__)
+        __m256i x = _mm256_maskz_compress_epi16(__mmask16(1u << index), ymm);
+        return (int16_t)_mm_cvtsi128_si32(_mm256_castsi256_si128(x));
+#else
+        int16_t x[16];  // find faster version
         store(x);
         return x[index & 0x0F];
+#endif
     }
     // Extract a single element. Use store function if extracting more than one element.
     // Operator [] can only read an element, not write.
-    int16_t operator [] (uint32_t index) const {
+    int16_t operator [] (int index) const {
         return extract(index);
     }
     // Member functions to split into two Vec8s:
@@ -1182,11 +1419,14 @@ public:
         return _mm256_castsi256_si128(ymm);
     }
     Vec8s get_high() const {
-        return _mm256_extracti128_si256(ymm,1);
+        return _mm256_extractf128_si256(ymm,1);
     }
-    static int size() {
+    static constexpr int size() {
         return 16;
     }
+    static constexpr int elementtype() {
+        return 6;
+    }
 };
 
 
@@ -1195,59 +1435,86 @@ public:
 *          Vec16sb: Vector of 16 Booleans for use with Vec16s and Vec16us
 *
 *****************************************************************************/
+
+#if INSTRSET < 10  // broad boolean vectors
+
 class Vec16sb : public Vec16s {
 public:
     // Default constructor:
-    Vec16sb() {
-    }
+    Vec16sb() = default;
     // Constructor to build from all elements:
+    /*
     Vec16sb(bool x0, bool x1, bool x2, bool x3, bool x4, bool x5, bool x6, bool x7,
         bool x8, bool x9, bool x10, bool x11, bool x12, bool x13, bool x14, bool x15) :
-        Vec16s(-int16_t(x0), -int16_t(x1), -int16_t(x2), -int16_t(x3), -int16_t(x4), -int16_t(x5), -int16_t(x6), -int16_t(x7), 
+        Vec16s(-int16_t(x0), -int16_t(x1), -int16_t(x2), -int16_t(x3), -int16_t(x4), -int16_t(x5), -int16_t(x6), -int16_t(x7),
             -int16_t(x8), -int16_t(x9), -int16_t(x10), -int16_t(x11), -int16_t(x12), -int16_t(x13), -int16_t(x14), -int16_t(x15))
-        {}
+        {} */
     // Constructor to convert from type __m256i used in intrinsics:
-    Vec16sb(__m256i const & x) {
+    Vec16sb(__m256i const x) {
         ymm = x;
     }
     // Assignment operator to convert from type __m256i used in intrinsics:
-    Vec16sb & operator = (__m256i const & x) {
+    Vec16sb & operator = (__m256i const x) {
         ymm = x;
         return *this;
     }
     // Constructor to broadcast scalar value:
     Vec16sb(bool b) : Vec16s(-int16_t(b)) {
     }
+    // Constructor to convert from type Vec256b used in emulation:
+    Vec16sb(Vec256b const & x) : Vec16s(x) {
+    }
     // Assignment operator to broadcast scalar value:
     Vec16sb & operator = (bool b) {
         *this = Vec16sb(b);
         return *this;
     }
-private: // Prevent constructing from int, etc.
-    Vec16sb(int b);
-    Vec16sb & operator = (int x);
-public:
+    // Constructor to build from two Vec8sb:
+    Vec16sb(Vec8sb const a0, Vec8sb const a1) : Vec16s(Vec8s(a0), Vec8s(a1)) {
+    }
     Vec8sb get_low() const {
         return Vec8sb(Vec16s::get_low());
     }
     Vec8sb get_high() const {
         return Vec8sb(Vec16s::get_high());
     }
-    Vec16sb & insert (int index, bool a) {
+    Vec16sb & insert(int index, bool a) {
         Vec16s::insert(index, -(int)a);
         return *this;
-    }    
+    }
     // Member function extract a single element from vector
-    bool extract(uint32_t index) const {
+    bool extract(int index) const {
         return Vec16s::extract(index) != 0;
     }
     // Extract a single element. Use store function if extracting more than one element.
     // Operator [] can only read an element, not write.
-    bool operator [] (uint32_t index) const {
+    bool operator [] (int index) const {
         return extract(index);
     }
+    // Member function to change a bitfield to a boolean vector
+    Vec16sb & load_bits(uint16_t a) {
+        __m256i b1 = _mm256_set1_epi16((int16_t)a);  // broadcast a
+        __m256i m1 = constant8ui<0,0,0,0,0x00010001,0x00010001,0x00010001,0x00010001>();
+        __m256i c1 = _mm256_shuffle_epi8(b1, m1);  // get right byte in each position
+        __m256i m2 = constant8ui<0x00020001,0x00080004,0x00200010,0x00800040,0x00020001,0x00080004,0x00200010,0x00800040>();
+        __m256i d1 = _mm256_and_si256(c1, m2); // isolate one bit in each byte
+        ymm = _mm256_cmpgt_epi16(d1, _mm256_setzero_si256());  // compare with 0
+        return *this;
+    }
+    static constexpr int elementtype() {
+        return 3;
+    }
+    // Prevent constructing from int, etc.
+    Vec16sb(int b) = delete;
+    Vec16sb & operator = (int x) = delete;
 };
 
+#else
+
+typedef Vec16b Vec16sb;  // compact boolean vector
+
+#endif
+
 
 /*****************************************************************************
 *
@@ -1255,57 +1522,70 @@ public:
 *
 *****************************************************************************/
 
+#if INSTRSET < 10  // broad boolean vectors
+
 // vector operator & : bitwise and
-static inline Vec16sb operator & (Vec16sb const & a, Vec16sb const & b) {
+static inline Vec16sb operator & (Vec16sb const a, Vec16sb const b) {
     return Vec16sb(Vec256b(a) & Vec256b(b));
 }
-static inline Vec16sb operator && (Vec16sb const & a, Vec16sb const & b) {
+static inline Vec16sb operator && (Vec16sb const a, Vec16sb const b) {
     return a & b;
 }
 // vector operator &= : bitwise and
-static inline Vec16sb & operator &= (Vec16sb & a, Vec16sb const & b) {
+static inline Vec16sb & operator &= (Vec16sb & a, Vec16sb const b) {
     a = a & b;
     return a;
 }
 
 // vector operator | : bitwise or
-static inline Vec16sb operator | (Vec16sb const & a, Vec16sb const & b) {
+static inline Vec16sb operator | (Vec16sb const a, Vec16sb const b) {
     return Vec16sb(Vec256b(a) | Vec256b(b));
 }
-static inline Vec16sb operator || (Vec16sb const & a, Vec16sb const & b) {
+static inline Vec16sb operator || (Vec16sb const a, Vec16sb const b) {
     return a | b;
 }
 // vector operator |= : bitwise or
-static inline Vec16sb & operator |= (Vec16sb & a, Vec16sb const & b) {
+static inline Vec16sb & operator |= (Vec16sb & a, Vec16sb const b) {
     a = a | b;
     return a;
 }
 
 // vector operator ^ : bitwise xor
-static inline Vec16sb operator ^ (Vec16sb const & a, Vec16sb const & b) {
+static inline Vec16sb operator ^ (Vec16sb const a, Vec16sb const b) {
     return Vec16sb(Vec256b(a) ^ Vec256b(b));
 }
 // vector operator ^= : bitwise xor
-static inline Vec16sb & operator ^= (Vec16sb & a, Vec16sb const & b) {
+static inline Vec16sb & operator ^= (Vec16sb & a, Vec16sb const b) {
     a = a ^ b;
     return a;
 }
 
-// vector operator ~ : bitwise not
-static inline Vec16sb operator ~ (Vec16sb const & a) {
-    return Vec16sb( ~ Vec256b(a));
+// vector operator == : xnor
+static inline Vec16sb operator == (Vec16sb const a, Vec16sb const b) {
+    return Vec16sb(a ^ Vec16sb(~b));
+}
+
+// vector operator != : xor
+static inline Vec16sb operator != (Vec16sb const a, Vec16sb const b) {
+    return Vec16sb(a ^ b);
+}
+
+// vector operator ~ : bitwise not
+static inline Vec16sb operator ~ (Vec16sb const a) {
+    return Vec16sb( ~ Vec256b(a));
 }
 
 // vector operator ! : element not
-static inline Vec16sb operator ! (Vec16sb const & a) {
+static inline Vec16sb operator ! (Vec16sb const a) {
     return ~ a;
 }
 
 // vector function andnot
-static inline Vec16sb andnot (Vec16sb const & a, Vec16sb const & b) {
+static inline Vec16sb andnot (Vec16sb const a, Vec16sb const b) {
     return Vec16sb(andnot(Vec256b(a), Vec256b(b)));
 }
 
+#endif
 
 /*****************************************************************************
 *
@@ -1314,12 +1594,11 @@ static inline Vec16sb andnot (Vec16sb const & a, Vec16sb const & b) {
 *****************************************************************************/
 
 // vector operator + : add element by element
-static inline Vec16s operator + (Vec16s const & a, Vec16s const & b) {
+static inline Vec16s operator + (Vec16s const a, Vec16s const b) {
     return _mm256_add_epi16(a, b);
 }
-
 // vector operator += : add
-static inline Vec16s & operator += (Vec16s & a, Vec16s const & b) {
+static inline Vec16s & operator += (Vec16s & a, Vec16s const b) {
     a = a + b;
     return a;
 }
@@ -1330,7 +1609,6 @@ static inline Vec16s operator ++ (Vec16s & a, int) {
     a = a + 1;
     return a0;
 }
-
 // prefix operator ++
 static inline Vec16s & operator ++ (Vec16s & a) {
     a = a + 1;
@@ -1338,17 +1616,15 @@ static inline Vec16s & operator ++ (Vec16s & a) {
 }
 
 // vector operator - : subtract element by element
-static inline Vec16s operator - (Vec16s const & a, Vec16s const & b) {
+static inline Vec16s operator - (Vec16s const a, Vec16s const b) {
     return _mm256_sub_epi16(a, b);
 }
-
 // vector operator - : unary minus
-static inline Vec16s operator - (Vec16s const & a) {
+static inline Vec16s operator - (Vec16s const a) {
     return _mm256_sub_epi16(_mm256_setzero_si256(), a);
 }
-
 // vector operator -= : subtract
-static inline Vec16s & operator -= (Vec16s & a, Vec16s const & b) {
+static inline Vec16s & operator -= (Vec16s & a, Vec16s const b) {
     a = a - b;
     return a;
 }
@@ -1359,7 +1635,6 @@ static inline Vec16s operator -- (Vec16s & a, int) {
     a = a - 1;
     return a0;
 }
-
 // prefix operator --
 static inline Vec16s & operator -- (Vec16s & a) {
     a = a - 1;
@@ -1367,25 +1642,22 @@ static inline Vec16s & operator -- (Vec16s & a) {
 }
 
 // vector operator * : multiply element by element
-static inline Vec16s operator * (Vec16s const & a, Vec16s const & b) {
+static inline Vec16s operator * (Vec16s const a, Vec16s const b) {
     return _mm256_mullo_epi16(a, b);
 }
-
 // vector operator *= : multiply
-static inline Vec16s & operator *= (Vec16s & a, Vec16s const & b) {
+static inline Vec16s & operator *= (Vec16s & a, Vec16s const b) {
     a = a * b;
     return a;
 }
 
-// vector operator / : divide all elements by same integer
-// See bottom of file
+// vector operator / : divide all elements by same integer. See bottom of file
 
 
 // vector operator << : shift left
-static inline Vec16s operator << (Vec16s const & a, int b) {
+static inline Vec16s operator << (Vec16s const a, int b) {
     return _mm256_sll_epi16(a,_mm_cvtsi32_si128(b));
 }
-
 // vector operator <<= : shift left
 static inline Vec16s & operator <<= (Vec16s & a, int b) {
     a = a << b;
@@ -1393,10 +1665,9 @@ static inline Vec16s & operator <<= (Vec16s & a, int b) {
 }
 
 // vector operator >> : shift right arithmetic
-static inline Vec16s operator >> (Vec16s const & a, int b) {
+static inline Vec16s operator >> (Vec16s const a, int b) {
     return _mm256_sra_epi16(a,_mm_cvtsi32_si128(b));
 }
-
 // vector operator >>= : shift right arithmetic
 static inline Vec16s & operator >>= (Vec16s & a, int b) {
     a = a >> b;
@@ -1404,169 +1675,213 @@ static inline Vec16s & operator >>= (Vec16s & a, int b) {
 }
 
 // vector operator == : returns true for elements for which a == b
-static inline Vec16sb operator == (Vec16s const & a, Vec16s const & b) {
+static inline Vec16sb operator == (Vec16s const a, Vec16s const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_cmp_epi16_mask (a, b, 0);
+#else
     return _mm256_cmpeq_epi16(a, b);
+#endif
 }
 
 // vector operator != : returns true for elements for which a != b
-static inline Vec16sb operator != (Vec16s const & a, Vec16s const & b) {
+static inline Vec16sb operator != (Vec16s const a, Vec16s const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_cmp_epi16_mask (a, b, 4);
+#else
     return Vec16sb(Vec16s(~(a == b)));
+#endif
 }
 
 // vector operator > : returns true for elements for which a > b
-static inline Vec16sb operator > (Vec16s const & a, Vec16s const & b) {
+static inline Vec16sb operator > (Vec16s const a, Vec16s const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_cmp_epi16_mask (a, b, 6);
+#else
     return _mm256_cmpgt_epi16(a, b);
+#endif
 }
 
 // vector operator < : returns true for elements for which a < b
-static inline Vec16sb operator < (Vec16s const & a, Vec16s const & b) {
+static inline Vec16sb operator < (Vec16s const a, Vec16s const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_cmp_epi16_mask (a, b, 1);
+#else
     return b > a;
+#endif
 }
 
 // vector operator >= : returns true for elements for which a >= b (signed)
-static inline Vec16sb operator >= (Vec16s const & a, Vec16s const & b) {
+static inline Vec16sb operator >= (Vec16s const a, Vec16s const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_cmp_epi16_mask (a, b, 5);
+#else
     return Vec16sb(Vec16s(~(b > a)));
+#endif
 }
 
 // vector operator <= : returns true for elements for which a <= b (signed)
-static inline Vec16sb operator <= (Vec16s const & a, Vec16s const & b) {
+static inline Vec16sb operator <= (Vec16s const a, Vec16s const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_cmp_epi16_mask (a, b, 2);
+#else
     return b >= a;
+#endif
 }
 
 // vector operator & : bitwise and
-static inline Vec16s operator & (Vec16s const & a, Vec16s const & b) {
+static inline Vec16s operator & (Vec16s const a, Vec16s const b) {
     return Vec16s(Vec256b(a) & Vec256b(b));
 }
-static inline Vec16s operator && (Vec16s const & a, Vec16s const & b) {
+static inline Vec16s operator && (Vec16s const a, Vec16s const b) {
     return a & b;
 }
 // vector operator &= : bitwise and
-static inline Vec16s & operator &= (Vec16s & a, Vec16s const & b) {
+static inline Vec16s & operator &= (Vec16s & a, Vec16s const b) {
     a = a & b;
     return a;
 }
 
 // vector operator | : bitwise or
-static inline Vec16s operator | (Vec16s const & a, Vec16s const & b) {
+static inline Vec16s operator | (Vec16s const a, Vec16s const b) {
     return Vec16s(Vec256b(a) | Vec256b(b));
 }
-static inline Vec16s operator || (Vec16s const & a, Vec16s const & b) {
+static inline Vec16s operator || (Vec16s const a, Vec16s const b) {
     return a | b;
 }
 // vector operator |= : bitwise or
-static inline Vec16s & operator |= (Vec16s & a, Vec16s const & b) {
+static inline Vec16s & operator |= (Vec16s & a, Vec16s const b) {
     a = a | b;
     return a;
 }
 
 // vector operator ^ : bitwise xor
-static inline Vec16s operator ^ (Vec16s const & a, Vec16s const & b) {
+static inline Vec16s operator ^ (Vec16s const a, Vec16s const b) {
     return Vec16s(Vec256b(a) ^ Vec256b(b));
 }
 // vector operator ^= : bitwise xor
-static inline Vec16s & operator ^= (Vec16s & a, Vec16s const & b) {
+static inline Vec16s & operator ^= (Vec16s & a, Vec16s const b) {
     a = a ^ b;
     return a;
 }
 
 // vector operator ~ : bitwise not
-static inline Vec16s operator ~ (Vec16s const & a) {
+static inline Vec16s operator ~ (Vec16s const a) {
     return Vec16s( ~ Vec256b(a));
 }
 
 // vector operator ! : logical not, returns true for elements == 0
-static inline Vec16sb operator ! (Vec16s const & a) {
+static inline Vec16sb operator ! (Vec16s const a) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_cmp_epi16_mask (a, _mm256_setzero_si256(), 0);
+#else
     return _mm256_cmpeq_epi16(a,_mm256_setzero_si256());
+#endif
 }
 
 // Functions for this class
 
 // Select between two operands. Corresponds to this pseudocode:
 // for (int i = 0; i < 16; i++) result[i] = s[i] ? a[i] : b[i];
-// Each byte in s must be either 0 (false) or -1 (true). No other values are allowed.
-// (s is signed)
-static inline Vec16s select (Vec16sb const & s, Vec16s const & a, Vec16s const & b) {
+static inline Vec16s select (Vec16sb const s, Vec16s const a, Vec16s const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_mask_mov_epi16(b, s, a);
+#else
     return selectb(s,a,b);
+#endif
 }
 
 // Conditional add: For all vector elements i: result[i] = f[i] ? (a[i] + b[i]) : a[i]
-static inline Vec16s if_add (Vec16sb const & f, Vec16s const & a, Vec16s const & b) {
+static inline Vec16s if_add (Vec16sb const f, Vec16s const a, Vec16s const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_mask_add_epi16 (a, f, a, b);
+#else
     return a + (Vec16s(f) & b);
+#endif
 }
 
-// Horizontal add: Calculates the sum of all vector elements.
-// Overflow will wrap around
-static inline int32_t horizontal_add (Vec16s const & a) {
-    __m256i sum1  = _mm256_hadd_epi16(a,a);                           // horizontally add 2x8 elements in 3 steps
-    __m256i sum2  = _mm256_hadd_epi16(sum1,sum1);
-    __m256i sum3  = _mm256_hadd_epi16(sum2,sum2); 
-#if defined (_MSC_VER) && _MSC_VER <= 1700 && ! defined(__INTEL_COMPILER)
-    __m128i sum4  = _mm256_extractf128_si256(sum3,1);                 // bug in MS compiler VS 11
+// Conditional subtract
+static inline Vec16s if_sub (Vec16sb const f, Vec16s const a, Vec16s const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_mask_sub_epi16 (a, f, a, b);
 #else
-    __m128i sum4  = _mm256_extracti128_si256(sum3,1);                 // get high part
+    return a - (Vec16s(f) & b);
 #endif
-    __m128i sum5  = _mm_add_epi16(_mm256_castsi256_si128(sum3),sum4); // add low and high parts
-    int16_t sum6  = (int16_t)_mm_cvtsi128_si32(sum5);                 // truncate to 16 bits
-    return  sum6;                                                     // sign extend to 32 bits
 }
 
-// Horizontal add extended: Calculates the sum of all vector elements.
-// Elements are sign extended before adding to avoid overflow
-static inline int32_t horizontal_add_x (Vec16s const & a) {
-    __m256i aeven = _mm256_slli_epi32(a,16);                  // even numbered elements of a. get sign bit in position
-            aeven = _mm256_srai_epi32(aeven,16);              // sign extend even numbered elements
-    __m256i aodd  = _mm256_srai_epi32(a,16);                  // sign extend odd  numbered elements
-    __m256i sum1  = _mm256_add_epi32(aeven,aodd);             // add even and odd elements
-    __m256i sum2  = _mm256_hadd_epi32(sum1,sum1);             // horizontally add 2x4 elements in 2 steps
-    __m256i sum3  = _mm256_hadd_epi32(sum2,sum2);
-#if defined (_MSC_VER) && _MSC_VER <= 1700 && ! defined(__INTEL_COMPILER)
-    __m128i sum4  = _mm256_extractf128_si256(sum3,1);         // bug in MS compiler VS 11
+// Conditional multiply
+static inline Vec16s if_mul (Vec16sb const f, Vec16s const a, Vec16s const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_mask_mullo_epi16 (a, f, a, b);
 #else
-    __m128i sum4  = _mm256_extracti128_si256(sum3,1);
+    return select(f, a*b, a);
 #endif
-    __m128i sum5  = _mm_add_epi32(_mm256_castsi256_si128(sum3),sum4);
-    return          _mm_cvtsi128_si32(sum5); 
+}
+
+// Horizontal add: Calculates the sum of all vector elements. Overflow will wrap around
+static inline int16_t horizontal_add (Vec16s const a) {
+    // The hadd instruction is inefficient, and may be split into two instructions for faster decoding
+    __m128i sum1  = _mm_add_epi16(_mm256_extracti128_si256(a,1),_mm256_castsi256_si128(a));
+    __m128i sum2  = _mm_add_epi16(sum1,_mm_unpackhi_epi64(sum1,sum1));
+    __m128i sum3  = _mm_add_epi16(sum2,_mm_shuffle_epi32(sum2,1));
+    __m128i sum4  = _mm_add_epi16(sum3,_mm_shufflelo_epi16(sum3,1));
+    return (int16_t)_mm_cvtsi128_si32(sum4);               // truncate to 16 bits
+}
+
+// Horizontal add extended: Calculates the sum of all vector elements.
+// Elements are sign extended before adding to avoid overflow
+static inline int32_t horizontal_add_x (Vec16s const a) {
+    __m256i aeven = _mm256_slli_epi32(a,16);               // even numbered elements of a. get sign bit in position
+            aeven = _mm256_srai_epi32(aeven,16);           // sign extend even numbered elements
+    __m256i aodd  = _mm256_srai_epi32(a,16);               // sign extend odd  numbered elements
+    __m256i sum1  = _mm256_add_epi32(aeven,aodd);          // add even and odd elements
+    __m128i sum2  = _mm_add_epi32(_mm256_extracti128_si256(sum1,1),_mm256_castsi256_si128(sum1));
+    __m128i sum3  = _mm_add_epi32(sum2,_mm_unpackhi_epi64(sum2,sum2));
+    __m128i sum4  = _mm_add_epi32(sum3,_mm_shuffle_epi32(sum3,1));
+    return (int16_t)_mm_cvtsi128_si32(sum4);               // truncate to 16 bits
 }
 
 // function add_saturated: add element by element, signed with saturation
-static inline Vec16s add_saturated(Vec16s const & a, Vec16s const & b) {
+static inline Vec16s add_saturated(Vec16s const a, Vec16s const b) {
     return _mm256_adds_epi16(a, b);
 }
 
 // function sub_saturated: subtract element by element, signed with saturation
-static inline Vec16s sub_saturated(Vec16s const & a, Vec16s const & b) {
+static inline Vec16s sub_saturated(Vec16s const a, Vec16s const b) {
     return _mm256_subs_epi16(a, b);
 }
 
 // function max: a > b ? a : b
-static inline Vec16s max(Vec16s const & a, Vec16s const & b) {
+static inline Vec16s max(Vec16s const a, Vec16s const b) {
     return _mm256_max_epi16(a,b);
 }
 
 // function min: a < b ? a : b
-static inline Vec16s min(Vec16s const & a, Vec16s const & b) {
+static inline Vec16s min(Vec16s const a, Vec16s const b) {
     return _mm256_min_epi16(a,b);
 }
 
 // function abs: a >= 0 ? a : -a
-static inline Vec16s abs(Vec16s const & a) {
-    return _mm256_sign_epi16(a,a);
+static inline Vec16s abs(Vec16s const a) {
+    return _mm256_abs_epi16(a);
 }
 
 // function abs_saturated: same as abs, saturate if overflow
-static inline Vec16s abs_saturated(Vec16s const & a) {
-    __m256i absa   = abs(a);                                  // abs(a)
-    __m256i overfl = _mm256_srai_epi16(absa,15);              // sign
-    return           _mm256_add_epi16(absa,overfl);           // subtract 1 if 0x8000
+static inline Vec16s abs_saturated(Vec16s const a) {
+#if INSTRSET >= 10
+    return _mm256_min_epu16(abs(a), Vec16s(0x7FFF));
+#else
+    __m256i absa   = abs(a);                               // abs(a)
+    __m256i overfl = _mm256_srai_epi16(absa,15);           // sign
+    return           _mm256_add_epi16(absa,overfl);        // subtract 1 if 0x8000
+#endif
 }
 
 // function rotate_left all elements
 // Use negative count to rotate right
-static inline Vec16s rotate_left(Vec16s const & a, int b) {
-    __m256i left  = _mm256_sll_epi16(a,_mm_cvtsi32_si128(b & 0x0F));      // a << b 
-    __m256i right = _mm256_srl_epi16(a,_mm_cvtsi32_si128((16-b) & 0x0F)); // a >> (16 - b)
-    __m256i rot   = _mm256_or_si256(left,right);                          // or
-    return  rot;
+static inline Vec16s rotate_left(Vec16s const a, int b) {
+    __m256i left  = _mm256_sll_epi16(a,_mm_cvtsi32_si128(b & 0x0F));    // a << b
+    __m256i right = _mm256_srl_epi16(a,_mm_cvtsi32_si128((-b) & 0x0F)); // a >> (16 - b)
+    return          _mm256_or_si256(left,right);                        // or
 }
 
 
@@ -1579,8 +1894,7 @@ static inline Vec16s rotate_left(Vec16s const & a, int b) {
 class Vec16us : public Vec16s {
 public:
     // Default constructor:
-    Vec16us(){
-    }
+    Vec16us() = default;
     // Constructor to broadcast the same value into all elements:
     Vec16us(uint32_t i) {
         ymm = _mm256_set1_epi16((int16_t)i);
@@ -1588,18 +1902,19 @@ public:
     // Constructor to build from all elements:
     Vec16us(uint16_t i0, uint16_t i1, uint16_t i2,  uint16_t i3,  uint16_t i4,  uint16_t i5,  uint16_t i6,  uint16_t i7,
             uint16_t i8, uint16_t i9, uint16_t i10, uint16_t i11, uint16_t i12, uint16_t i13, uint16_t i14, uint16_t i15) {
-        ymm = _mm256_setr_epi16(i0, i1, i2, i3, i4, i5, i6, i7, i8, i9, i10, i11, i12, i13, i14, i15 );
+        ymm = _mm256_setr_epi16((int16_t)i0, (int16_t)i1, (int16_t)i2, (int16_t)i3, (int16_t)i4, (int16_t)i5, (int16_t)i6, (int16_t)i7,
+            (int16_t)i8, (int16_t)i9, (int16_t)i10, (int16_t)i11, (int16_t)i12, (int16_t)i13, (int16_t)i14, (int16_t)i15);
     }
     // Constructor to build from two Vec8us:
-    Vec16us(Vec8us const & a0, Vec8us const & a1) {
+    Vec16us(Vec8us const a0, Vec8us const a1) {
         ymm = set_m128ir(a0, a1);
     }
     // Constructor to convert from type __m256i used in intrinsics:
-    Vec16us(__m256i const & x) {
+    Vec16us(__m256i const x) {
         ymm = x;
     }
     // Assignment operator to convert from type __m256i used in intrinsics:
-    Vec16us & operator = (__m256i const & x) {
+    Vec16us & operator = (__m256i const x) {
         ymm = x;
         return *this;
     }
@@ -1614,18 +1929,17 @@ public:
         return *this;
     }
     // Member function to change a single element in vector
-    // Note: This function is inefficient. Use load function if changing more than one element
-    Vec16us const & insert(uint32_t index, uint16_t value) {
-        Vec16s::insert(index, value);
+    Vec16us const insert(int index, uint16_t value) {
+        Vec16s::insert(index, (int16_t)value);
         return *this;
     }
     // Member function extract a single element from vector
-    uint16_t extract(uint32_t index) const {
-        return Vec16s::extract(index);
+    uint16_t extract(int index) const {
+        return (uint16_t)Vec16s::extract(index);
     }
     // Extract a single element. Use store function if extracting more than one element.
     // Operator [] can only read an element, not write.
-    uint16_t operator [] (uint32_t index) const {
+    uint16_t operator [] (int index) const {
         return extract(index);
     }
     // Member functions to split into two Vec8us:
@@ -1633,24 +1947,27 @@ public:
         return _mm256_castsi256_si128(ymm);
     }
     Vec8us get_high() const {
-        return _mm256_extracti128_si256(ymm,1);
+        return _mm256_extractf128_si256(ymm,1);
+    }
+    static constexpr int elementtype() {
+        return 7;
     }
 };
 
 // Define operators for this class
 
 // vector operator + : add
-static inline Vec16us operator + (Vec16us const & a, Vec16us const & b) {
+static inline Vec16us operator + (Vec16us const a, Vec16us const b) {
     return Vec16us (Vec16s(a) + Vec16s(b));
 }
 
 // vector operator - : subtract
-static inline Vec16us operator - (Vec16us const & a, Vec16us const & b) {
+static inline Vec16us operator - (Vec16us const a, Vec16us const b) {
     return Vec16us (Vec16s(a) - Vec16s(b));
 }
 
 // vector operator * : multiply
-static inline Vec16us operator * (Vec16us const & a, Vec16us const & b) {
+static inline Vec16us operator * (Vec16us const a, Vec16us const b) {
     return Vec16us (Vec16s(a) * Vec16s(b));
 }
 
@@ -1658,12 +1975,12 @@ static inline Vec16us operator * (Vec16us const & a, Vec16us const & b) {
 // See bottom of file
 
 // vector operator >> : shift right logical all elements
-static inline Vec16us operator >> (Vec16us const & a, uint32_t b) {
-    return _mm256_srl_epi16(a,_mm_cvtsi32_si128(b)); 
+static inline Vec16us operator >> (Vec16us const a, uint32_t b) {
+    return _mm256_srl_epi16(a,_mm_cvtsi32_si128((int)b));
 }
 
 // vector operator >> : shift right logical all elements
-static inline Vec16us operator >> (Vec16us const & a, int32_t b) {
+static inline Vec16us operator >> (Vec16us const a, int32_t b) {
     return a >> (uint32_t)b;
 }
 
@@ -1674,59 +1991,75 @@ static inline Vec16us & operator >>= (Vec16us & a, uint32_t b) {
 }
 
 // vector operator << : shift left all elements
-static inline Vec16us operator << (Vec16us const & a, uint32_t b) {
-    return _mm256_sll_epi16(a,_mm_cvtsi32_si128(b)); 
+static inline Vec16us operator << (Vec16us const a, uint32_t b) {
+    return _mm256_sll_epi16(a,_mm_cvtsi32_si128((int)b));
 }
 
 // vector operator << : shift left all elements
-static inline Vec16us operator << (Vec16us const & a, int32_t b) {
+static inline Vec16us operator << (Vec16us const a, int32_t b) {
     return a << (uint32_t)b;
 }
 
 // vector operator >= : returns true for elements for which a >= b (unsigned)
-static inline Vec16sb operator >= (Vec16us const & a, Vec16us const & b) {
-    __m256i max_ab = _mm256_max_epu16(a,b);                   // max(a,b), unsigned
-    return _mm256_cmpeq_epi16(a,max_ab);                      // a == max(a,b)
+static inline Vec16sb operator >= (Vec16us const a, Vec16us const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_cmp_epu16_mask (a, b, 5);
+#else
+    __m256i max_ab = _mm256_max_epu16(a,b);                // max(a,b), unsigned
+    return _mm256_cmpeq_epi16(a,max_ab);                   // a == max(a,b)
+#endif
 }
 
 // vector operator <= : returns true for elements for which a <= b (unsigned)
-static inline Vec16sb operator <= (Vec16us const & a, Vec16us const & b) {
+static inline Vec16sb operator <= (Vec16us const a, Vec16us const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_cmp_epu16_mask (a, b, 2);
+#else
     return b >= a;
+#endif
 }
 
 // vector operator > : returns true for elements for which a > b (unsigned)
-static inline Vec16sb operator > (Vec16us const & a, Vec16us const & b) {
+static inline Vec16sb operator > (Vec16us const a, Vec16us const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_cmp_epu16_mask (a, b, 6);
+#else
     return Vec16sb(Vec16s(~(b >= a)));
+#endif
 }
 
 // vector operator < : returns true for elements for which a < b (unsigned)
-static inline Vec16sb operator < (Vec16us const & a, Vec16us const & b) {
+static inline Vec16sb operator < (Vec16us const a, Vec16us const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_cmp_epu16_mask (a, b, 1);
+#else
     return b > a;
+#endif
 }
 
 // vector operator & : bitwise and
-static inline Vec16us operator & (Vec16us const & a, Vec16us const & b) {
+static inline Vec16us operator & (Vec16us const a, Vec16us const b) {
     return Vec16us(Vec256b(a) & Vec256b(b));
 }
-static inline Vec16us operator && (Vec16us const & a, Vec16us const & b) {
+static inline Vec16us operator && (Vec16us const a, Vec16us const b) {
     return a & b;
 }
 
 // vector operator | : bitwise or
-static inline Vec16us operator | (Vec16us const & a, Vec16us const & b) {
+static inline Vec16us operator | (Vec16us const a, Vec16us const b) {
     return Vec16us(Vec256b(a) | Vec256b(b));
 }
-static inline Vec16us operator || (Vec16us const & a, Vec16us const & b) {
+static inline Vec16us operator || (Vec16us const a, Vec16us const b) {
     return a | b;
 }
 
 // vector operator ^ : bitwise xor
-static inline Vec16us operator ^ (Vec16us const & a, Vec16us const & b) {
+static inline Vec16us operator ^ (Vec16us const a, Vec16us const b) {
     return Vec16us(Vec256b(a) ^ Vec256b(b));
 }
 
 // vector operator ~ : bitwise not
-static inline Vec16us operator ~ (Vec16us const & a) {
+static inline Vec16us operator ~ (Vec16us const a) {
     return Vec16us( ~ Vec256b(a));
 }
 
@@ -1734,75 +2067,83 @@ static inline Vec16us operator ~ (Vec16us const & a) {
 
 // Select between two operands. Corresponds to this pseudocode:
 // for (int i = 0; i < 8; i++) result[i] = s[i] ? a[i] : b[i];
-// Each word in s must be either 0 (false) or -1 (true). No other values are allowed.
-// (s is signed)
-static inline Vec16us select (Vec16sb const & s, Vec16us const & a, Vec16us const & b) {
+static inline Vec16us select (Vec16sb const s, Vec16us const a, Vec16us const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_mask_mov_epi16(b, s, a);
+#else
     return selectb(s,a,b);
+#endif
 }
 
 // Conditional add: For all vector elements i: result[i] = f[i] ? (a[i] + b[i]) : a[i]
-static inline Vec16us if_add (Vec16sb const & f, Vec16us const & a, Vec16us const & b) {
+static inline Vec16us if_add (Vec16sb const f, Vec16us const a, Vec16us const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_mask_add_epi16 (a, f, a, b);
+#else
     return a + (Vec16us(f) & b);
+#endif
+}
+
+// Conditional subtract
+static inline Vec16us if_sub (Vec16sb const f, Vec16us const a, Vec16us const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_mask_sub_epi16 (a, f, a, b);
+#else
+    return a - (Vec16us(f) & b);
+#endif
 }
 
-// Horizontal add: Calculates the sum of all vector elements.
-// Overflow will wrap around
-static inline uint32_t horizontal_add (Vec16us const & a) {
-    __m256i sum1  = _mm256_hadd_epi16(a,a);                           // horizontally add 2x8 elements in 3 steps
-    __m256i sum2  = _mm256_hadd_epi16(sum1,sum1);
-    __m256i sum3  = _mm256_hadd_epi16(sum2,sum2);
-#if defined (_MSC_VER) && _MSC_VER <= 1700 && ! defined(__INTEL_COMPILER)
-    __m128i sum4  = _mm256_extractf128_si256(sum3,1);                 // bug in MS compiler VS 11
+// Conditional multiply
+static inline Vec16us if_mul (Vec16sb const f, Vec16us const a, Vec16us const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_mask_mullo_epi16 (a, f, a, b);
 #else
-    __m128i sum4  = _mm256_extracti128_si256(sum3,1);                 // get high part
+    return select(f, a*b, a);
 #endif
-    __m128i sum5  = _mm_add_epi32(_mm256_castsi256_si128(sum3),sum4); // add low and high parts
-    return          _mm_cvtsi128_si32(sum5);  
+}
+
+// Horizontal add: Calculates the sum of all vector elements. Overflow will wrap around
+static inline uint16_t horizontal_add (Vec16us const a) {
+    return (uint16_t)horizontal_add(Vec16s(a));
 }
 
 // Horizontal add extended: Calculates the sum of all vector elements.
 // Each element is zero-extended before addition to avoid overflow
-static inline uint32_t horizontal_add_x (Vec16us const & a) {
-    __m256i mask  = _mm256_set1_epi32(0x0000FFFF);                    // mask for even positions
-    __m256i aeven = _mm256_and_si256(a,mask);                         // even numbered elements of a
-    __m256i aodd  = _mm256_srli_epi32(a,16);                          // zero extend odd numbered elements
-    __m256i sum1  = _mm256_add_epi32(aeven,aodd);                     // add even and odd elements
-    __m256i sum2  = _mm256_hadd_epi32(sum1,sum1);                     // horizontally add 2x4 elements in 2 steps
-    __m256i sum3  = _mm256_hadd_epi32(sum2,sum2);
-#if defined (_MSC_VER) && _MSC_VER <= 1700 && ! defined(__INTEL_COMPILER)
-    __m128i sum4  = _mm256_extractf128_si256(sum3,1);                 // bug in MS compiler VS 11
+static inline uint32_t horizontal_add_x (Vec16us const a) {
+#if INSTRSET >= 10
+    __m256i aeven = _mm256_maskz_mov_epi16 (__mmask16(0x5555), a);
 #else
-    __m128i sum4  = _mm256_extracti128_si256(sum3,1);                 // get high part
+    __m256i mask  = _mm256_set1_epi32(0x0000FFFF);         // mask for even positions
+    __m256i aeven = _mm256_and_si256(a,mask);              // even numbered elements of a
 #endif
-    __m128i sum5  = _mm_add_epi32(_mm256_castsi256_si128(sum3),sum4); // add low and high parts
-    return          _mm_cvtsi128_si32(sum5);  
+    __m256i aodd  = _mm256_srli_epi32(a,16);               // zero extend odd numbered elements
+    __m256i sum1  = _mm256_add_epi32(aeven,aodd);          // add even and odd elements
+    __m128i sum2  = _mm_add_epi32(_mm256_extracti128_si256(sum1,1),_mm256_castsi256_si128(sum1));
+    __m128i sum3  = _mm_add_epi32(sum2,_mm_unpackhi_epi64(sum2,sum2));
+    __m128i sum4  = _mm_add_epi32(sum3,_mm_shuffle_epi32(sum3,1));
+    return (uint32_t)(uint16_t)_mm_cvtsi128_si32(sum4);    // truncate to 16 bits
 }
 
 // function add_saturated: add element by element, unsigned with saturation
-static inline Vec16us add_saturated(Vec16us const & a, Vec16us const & b) {
+static inline Vec16us add_saturated(Vec16us const a, Vec16us const b) {
     return _mm256_adds_epu16(a, b);
 }
 
 // function sub_saturated: subtract element by element, unsigned with saturation
-static inline Vec16us sub_saturated(Vec16us const & a, Vec16us const & b) {
+static inline Vec16us sub_saturated(Vec16us const a, Vec16us const b) {
     return _mm256_subs_epu16(a, b);
 }
 
 // function max: a > b ? a : b
-static inline Vec16us max(Vec16us const & a, Vec16us const & b) {
+static inline Vec16us max(Vec16us const a, Vec16us const b) {
     return _mm256_max_epu16(a,b);
 }
 
 // function min: a < b ? a : b
-static inline Vec16us min(Vec16us const & a, Vec16us const & b) {
+static inline Vec16us min(Vec16us const a, Vec16us const b) {
     return _mm256_min_epu16(a,b);
 }
 
-// function avg: (a + b + 1) >> 1
-static inline Vec16us avg(Vec16us const & a, Vec16us const & b) {
-    return _mm256_avg_epu16(a,b);
-}
-
 
 /*****************************************************************************
 *
@@ -1813,8 +2154,7 @@ static inline Vec16us avg(Vec16us const & a, Vec16us const & b) {
 class Vec8i : public Vec256b {
 public:
     // Default constructor:
-    Vec8i() {
-    }
+    Vec8i() = default;
     // Constructor to broadcast the same value into all elements:
     Vec8i(int i) {
         ymm = _mm256_set1_epi32(i);
@@ -1824,15 +2164,15 @@ public:
         ymm = _mm256_setr_epi32(i0, i1, i2, i3, i4, i5, i6, i7);
     }
     // Constructor to build from two Vec4i:
-    Vec8i(Vec4i const & a0, Vec4i const & a1) {
+    Vec8i(Vec4i const a0, Vec4i const a1) {
         ymm = set_m128ir(a0, a1);
     }
     // Constructor to convert from type __m256i used in intrinsics:
-    Vec8i(__m256i const & x) {
+    Vec8i(__m256i const x) {
         ymm = x;
     }
     // Assignment operator to convert from type __m256i used in intrinsics:
-    Vec8i & operator = (__m256i const & x) {
+    Vec8i & operator = (__m256i const x) {
         ymm = x;
         return *this;
     }
@@ -1850,18 +2190,11 @@ public:
         ymm = _mm256_load_si256((__m256i const*)p);
         return *this;
     }
-    // Member function to load 8 8-bit unsigned integers from array
-    Vec8i & load_8uc(void const * p) {
-        ymm = _mm256_cvtepu8_epi32(Vec16uc().loadl(p));
-        return *this;
-    }
-    // Member function to load 8 16-bit unsigned integers from array
-    Vec8i & load_8us(void const * p) {
-        ymm = _mm256_cvtepu16_epi32(Vec8us().load(p));
-        return *this;
-    }
     // Partial load. Load n elements and set the rest to 0
     Vec8i & load_partial(int n, void const * p) {
+#if INSTRSET >= 10  // AVX512VL
+        ymm = _mm256_maskz_loadu_epi32(__mmask8((1u << n) - 1), p);
+#else
         if (n <= 0) {
             *this = 0;
         }
@@ -1874,10 +2207,14 @@ public:
         else {
             load(p);
         }
+#endif
         return *this;
     }
     // Partial store. Store n elements
     void store_partial(int n, void * p) const {
+#if INSTRSET >= 10  // AVX512VL
+        _mm256_mask_storeu_epi32(p, __mmask8((1u << n) - 1), ymm);
+#else
         if (n <= 0) {
             return;
         }
@@ -1891,30 +2228,43 @@ public:
         else {
             store(p);
         }
+#endif
     }
     // cut off vector to n elements. The last 8-n elements are set to zero
     Vec8i & cutoff(int n) {
+#if INSTRSET >= 10
+        ymm = _mm256_maskz_mov_epi32(__mmask8((1u << n) - 1), ymm);
+#else
         *this = Vec32c(*this).cutoff(n * 4);
+#endif
         return *this;
     }
     // Member function to change a single element in vector
-    // Note: This function is inefficient. Use load function if changing more than one element
-    Vec8i const & insert(uint32_t index, int32_t value) {
-        static const int32_t maskl[16] = {0,0,0,0,0,0,0,0, -1,0,0,0,0,0,0,0};
+    Vec8i const insert(int index, int32_t value) {
+#if INSTRSET >= 10
+        ymm = _mm256_mask_set1_epi32(ymm, __mmask8(1u << index), value);
+#else
         __m256i broad = _mm256_set1_epi32(value);  // broadcast value into all elements
+        const int32_t maskl[16] = {0,0,0,0,0,0,0,0, -1,0,0,0,0,0,0,0};
         __m256i mask  = Vec256b().load(maskl + 8 - (index & 7)); // mask with FFFFFFFF at index position
         ymm = selectb (mask, broad, ymm);
+#endif
         return *this;
     }
     // Member function extract a single element from vector
-    int32_t extract(uint32_t index) const {
+    int32_t extract(int index) const {
+#if INSTRSET >= 10
+        __m256i x = _mm256_maskz_compress_epi32(__mmask8(1u << index), ymm);
+        return _mm_cvtsi128_si32(_mm256_castsi256_si128(x));
+#else
         int32_t x[8];
         store(x);
         return x[index & 7];
+#endif
     }
     // Extract a single element. Use store function if extracting more than one element.
     // Operator [] can only read an element, not write.
-    int32_t operator [] (uint32_t index) const {
+    int32_t operator [] (int index) const {
         return extract(index);
     }
     // Member functions to split into two Vec4i:
@@ -1922,9 +2272,12 @@ public:
         return _mm256_castsi256_si128(ymm);
     }
     Vec4i get_high() const {
-        return _mm256_extracti128_si256(ymm,1);
+        return _mm256_extractf128_si256(ymm,1);
+    }
+    static constexpr int size() {
+        return 8;
     }
-    static int size() {
+    static constexpr int elementtype() {
         return 8;
     }
 };
@@ -1936,21 +2289,22 @@ public:
 *
 *****************************************************************************/
 
+#if INSTRSET < 10  // broad boolean vectors
+
 class Vec8ib : public Vec8i {
 public:
     // Default constructor:
-    Vec8ib() {
-    }
+    Vec8ib() = default;
     // Constructor to build from all elements:
     Vec8ib(bool x0, bool x1, bool x2, bool x3, bool x4, bool x5, bool x6, bool x7) :
         Vec8i(-int32_t(x0), -int32_t(x1), -int32_t(x2), -int32_t(x3), -int32_t(x4), -int32_t(x5), -int32_t(x6), -int32_t(x7))
         {}
     // Constructor to convert from type __m256i used in intrinsics:
-    Vec8ib(__m256i const & x) {
+    Vec8ib(__m256i const x) {
         ymm = x;
     }
     // Assignment operator to convert from type __m256i used in intrinsics:
-    Vec8ib & operator = (__m256i const & x) {
+    Vec8ib & operator = (__m256i const x) {
         ymm = x;
         return *this;
     }
@@ -1962,10 +2316,9 @@ public:
         *this = Vec8ib(b);
         return *this;
     }
-private: // Prevent constructing from int, etc.
-    Vec8ib(int b);
-    Vec8ib & operator = (int x);
-public:
+    // Constructor to build from two Vec4ib:
+    Vec8ib(Vec4ib const a0, Vec4ib const a1) : Vec8i(Vec4i(a0), Vec4i(a1)) {
+    }
     Vec4ib get_low() const {
         return Vec4ib(Vec8i::get_low());
     }
@@ -1977,16 +2330,36 @@ public:
         return *this;
     }
     // Member function extract a single element from vector
-    bool extract(uint32_t index) const {
+    bool extract(int index) const {
         return Vec8i::extract(index) != 0;
     }
     // Extract a single element. Use store function if extracting more than one element.
     // Operator [] can only read an element, not write.
-    bool operator [] (uint32_t index) const {
+    bool operator [] (int index) const {
         return extract(index);
     }
+    // Member function to change a bitfield to a boolean vector
+    Vec8ib & load_bits(uint8_t a) {
+        __m256i b1 = _mm256_set1_epi32((int32_t)a);  // broadcast a
+        __m256i m2 = constant8ui<1,2,4,8,0x10,0x20,0x40,0x80>();
+        __m256i d1 = _mm256_and_si256(b1, m2); // isolate one bit in each dword
+        ymm = _mm256_cmpgt_epi32(d1, _mm256_setzero_si256());  // compare with 0
+        return *this;
+    }
+    static constexpr int elementtype() {
+        return 3;
+    }
+    // Prevent constructing from int, etc.
+    Vec8ib(int b) = delete;
+    Vec8ib & operator = (int x) = delete;
 };
 
+#else
+
+typedef Vec8b Vec8ib;  // compact boolean vector
+
+#endif
+
 
 /*****************************************************************************
 *
@@ -1994,57 +2367,70 @@ public:
 *
 *****************************************************************************/
 
+#if INSTRSET < 10  // broad boolean vectors
+
 // vector operator & : bitwise and
-static inline Vec8ib operator & (Vec8ib const & a, Vec8ib const & b) {
+static inline Vec8ib operator & (Vec8ib const a, Vec8ib const b) {
     return Vec8ib(Vec256b(a) & Vec256b(b));
 }
-static inline Vec8ib operator && (Vec8ib const & a, Vec8ib const & b) {
+static inline Vec8ib operator && (Vec8ib const a, Vec8ib const b) {
     return a & b;
 }
 // vector operator &= : bitwise and
-static inline Vec8ib & operator &= (Vec8ib & a, Vec8ib const & b) {
+static inline Vec8ib & operator &= (Vec8ib & a, Vec8ib const b) {
     a = a & b;
     return a;
 }
 
 // vector operator | : bitwise or
-static inline Vec8ib operator | (Vec8ib const & a, Vec8ib const & b) {
+static inline Vec8ib operator | (Vec8ib const a, Vec8ib const b) {
     return Vec8ib(Vec256b(a) | Vec256b(b));
 }
-static inline Vec8ib operator || (Vec8ib const & a, Vec8ib const & b) {
+static inline Vec8ib operator || (Vec8ib const a, Vec8ib const b) {
     return a | b;
 }
 // vector operator |= : bitwise or
-static inline Vec8ib & operator |= (Vec8ib & a, Vec8ib const & b) {
+static inline Vec8ib & operator |= (Vec8ib & a, Vec8ib const b) {
     a = a | b;
     return a;
 }
 
 // vector operator ^ : bitwise xor
-static inline Vec8ib operator ^ (Vec8ib const & a, Vec8ib const & b) {
+static inline Vec8ib operator ^ (Vec8ib const a, Vec8ib const b) {
     return Vec8ib(Vec256b(a) ^ Vec256b(b));
 }
 // vector operator ^= : bitwise xor
-static inline Vec8ib & operator ^= (Vec8ib & a, Vec8ib const & b) {
+static inline Vec8ib & operator ^= (Vec8ib & a, Vec8ib const b) {
     a = a ^ b;
     return a;
 }
 
+// vector operator == : xnor
+static inline Vec8ib operator == (Vec8ib const a, Vec8ib const b) {
+    return Vec8ib(a ^ (~b));
+}
+
+// vector operator != : xor
+static inline Vec8ib operator != (Vec8ib const a, Vec8ib const b) {
+    return Vec8ib(a ^ b);
+}
+
 // vector operator ~ : bitwise not
-static inline Vec8ib operator ~ (Vec8ib const & a) {
+static inline Vec8ib operator ~ (Vec8ib const a) {
     return Vec8ib( ~ Vec256b(a));
 }
 
 // vector operator ! : element not
-static inline Vec8ib operator ! (Vec8ib const & a) {
+static inline Vec8ib operator ! (Vec8ib const a) {
     return ~ a;
 }
 
 // vector function andnot
-static inline Vec8ib andnot (Vec8ib const & a, Vec8ib const & b) {
+static inline Vec8ib andnot (Vec8ib const a, Vec8ib const b) {
     return Vec8ib(andnot(Vec256b(a), Vec256b(b)));
 }
 
+#endif
 
 /*****************************************************************************
 *
@@ -2053,12 +2439,11 @@ static inline Vec8ib andnot (Vec8ib const & a, Vec8ib const & b) {
 *****************************************************************************/
 
 // vector operator + : add element by element
-static inline Vec8i operator + (Vec8i const & a, Vec8i const & b) {
+static inline Vec8i operator + (Vec8i const a, Vec8i const b) {
     return _mm256_add_epi32(a, b);
 }
-
 // vector operator += : add
-static inline Vec8i & operator += (Vec8i & a, Vec8i const & b) {
+static inline Vec8i & operator += (Vec8i & a, Vec8i const b) {
     a = a + b;
     return a;
 }
@@ -2069,7 +2454,6 @@ static inline Vec8i operator ++ (Vec8i & a, int) {
     a = a + 1;
     return a0;
 }
-
 // prefix operator ++
 static inline Vec8i & operator ++ (Vec8i & a) {
     a = a + 1;
@@ -2077,17 +2461,15 @@ static inline Vec8i & operator ++ (Vec8i & a) {
 }
 
 // vector operator - : subtract element by element
-static inline Vec8i operator - (Vec8i const & a, Vec8i const & b) {
+static inline Vec8i operator - (Vec8i const a, Vec8i const b) {
     return _mm256_sub_epi32(a, b);
 }
-
 // vector operator - : unary minus
-static inline Vec8i operator - (Vec8i const & a) {
+static inline Vec8i operator - (Vec8i const a) {
     return _mm256_sub_epi32(_mm256_setzero_si256(), a);
 }
-
 // vector operator -= : subtract
-static inline Vec8i & operator -= (Vec8i & a, Vec8i const & b) {
+static inline Vec8i & operator -= (Vec8i & a, Vec8i const b) {
     a = a - b;
     return a;
 }
@@ -2098,7 +2480,6 @@ static inline Vec8i operator -- (Vec8i & a, int) {
     a = a - 1;
     return a0;
 }
-
 // prefix operator --
 static inline Vec8i & operator -- (Vec8i & a) {
     a = a - 1;
@@ -2106,25 +2487,21 @@ static inline Vec8i & operator -- (Vec8i & a) {
 }
 
 // vector operator * : multiply element by element
-static inline Vec8i operator * (Vec8i const & a, Vec8i const & b) {
+static inline Vec8i operator * (Vec8i const a, Vec8i const b) {
     return _mm256_mullo_epi32(a, b);
 }
-
 // vector operator *= : multiply
-static inline Vec8i & operator *= (Vec8i & a, Vec8i const & b) {
+static inline Vec8i & operator *= (Vec8i & a, Vec8i const b) {
     a = a * b;
     return a;
 }
 
-// vector operator / : divide all elements by same integer
-// See bottom of file
-
+// vector operator / : divide all elements by same integer. See bottom of file
 
 // vector operator << : shift left
-static inline Vec8i operator << (Vec8i const & a, int32_t b) {
+static inline Vec8i operator << (Vec8i const a, int32_t b) {
     return _mm256_sll_epi32(a, _mm_cvtsi32_si128(b));
 }
-
 // vector operator <<= : shift left
 static inline Vec8i & operator <<= (Vec8i & a, int32_t b) {
     a = a << b;
@@ -2132,10 +2509,9 @@ static inline Vec8i & operator <<= (Vec8i & a, int32_t b) {
 }
 
 // vector operator >> : shift right arithmetic
-static inline Vec8i operator >> (Vec8i const & a, int32_t b) {
+static inline Vec8i operator >> (Vec8i const a, int32_t b) {
     return _mm256_sra_epi32(a, _mm_cvtsi32_si128(b));
 }
-
 // vector operator >>= : shift right arithmetic
 static inline Vec8i & operator >>= (Vec8i & a, int32_t b) {
     a = a >> b;
@@ -2143,171 +2519,222 @@ static inline Vec8i & operator >>= (Vec8i & a, int32_t b) {
 }
 
 // vector operator == : returns true for elements for which a == b
-static inline Vec8ib operator == (Vec8i const & a, Vec8i const & b) {
+static inline Vec8ib operator == (Vec8i const a, Vec8i const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_cmp_epi32_mask (a, b, 0);
+#else
     return _mm256_cmpeq_epi32(a, b);
+#endif
 }
 
 // vector operator != : returns true for elements for which a != b
-static inline Vec8ib operator != (Vec8i const & a, Vec8i const & b) {
+static inline Vec8ib operator != (Vec8i const a, Vec8i const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_cmp_epi32_mask (a, b, 4);
+#else
     return Vec8ib(Vec8i(~(a == b)));
+#endif
 }
-  
+
 // vector operator > : returns true for elements for which a > b
-static inline Vec8ib operator > (Vec8i const & a, Vec8i const & b) {
+static inline Vec8ib operator > (Vec8i const a, Vec8i const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_cmp_epi32_mask (a, b, 6);
+#else
     return _mm256_cmpgt_epi32(a, b);
+#endif
 }
 
 // vector operator < : returns true for elements for which a < b
-static inline Vec8ib operator < (Vec8i const & a, Vec8i const & b) {
+static inline Vec8ib operator < (Vec8i const a, Vec8i const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_cmp_epi32_mask (a, b, 1);
+#else
     return b > a;
+#endif
 }
 
 // vector operator >= : returns true for elements for which a >= b (signed)
-static inline Vec8ib operator >= (Vec8i const & a, Vec8i const & b) {
+static inline Vec8ib operator >= (Vec8i const a, Vec8i const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_cmp_epi32_mask (a, b, 5);
+#else
     return Vec8ib(Vec8i(~(b > a)));
+#endif
 }
 
 // vector operator <= : returns true for elements for which a <= b (signed)
-static inline Vec8ib operator <= (Vec8i const & a, Vec8i const & b) {
+static inline Vec8ib operator <= (Vec8i const a, Vec8i const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_cmp_epi32_mask (a, b, 2);
+#else
     return b >= a;
+#endif
 }
 
 // vector operator & : bitwise and
-static inline Vec8i operator & (Vec8i const & a, Vec8i const & b) {
+static inline Vec8i operator & (Vec8i const a, Vec8i const b) {
     return Vec8i(Vec256b(a) & Vec256b(b));
 }
-static inline Vec8i operator && (Vec8i const & a, Vec8i const & b) {
+static inline Vec8i operator && (Vec8i const a, Vec8i const b) {
     return a & b;
 }
 // vector operator &= : bitwise and
-static inline Vec8i & operator &= (Vec8i & a, Vec8i const & b) {
+static inline Vec8i & operator &= (Vec8i & a, Vec8i const b) {
     a = a & b;
     return a;
 }
 
 // vector operator | : bitwise or
-static inline Vec8i operator | (Vec8i const & a, Vec8i const & b) {
+static inline Vec8i operator | (Vec8i const a, Vec8i const b) {
     return Vec8i(Vec256b(a) | Vec256b(b));
 }
-static inline Vec8i operator || (Vec8i const & a, Vec8i const & b) {
+static inline Vec8i operator || (Vec8i const a, Vec8i const b) {
     return a | b;
 }
 // vector operator |= : bitwise or
-static inline Vec8i & operator |= (Vec8i & a, Vec8i const & b) {
+static inline Vec8i & operator |= (Vec8i & a, Vec8i const b) {
     a = a | b;
     return a;
 }
 
 // vector operator ^ : bitwise xor
-static inline Vec8i operator ^ (Vec8i const & a, Vec8i const & b) {
+static inline Vec8i operator ^ (Vec8i const a, Vec8i const b) {
     return Vec8i(Vec256b(a) ^ Vec256b(b));
 }
 // vector operator ^= : bitwise xor
-static inline Vec8i & operator ^= (Vec8i & a, Vec8i const & b) {
+static inline Vec8i & operator ^= (Vec8i & a, Vec8i const b) {
     a = a ^ b;
     return a;
 }
 
 // vector operator ~ : bitwise not
-static inline Vec8i operator ~ (Vec8i const & a) {
+static inline Vec8i operator ~ (Vec8i const a) {
     return Vec8i( ~ Vec256b(a));
 }
 
 // vector operator ! : returns true for elements == 0
-static inline Vec8ib operator ! (Vec8i const & a) {
+static inline Vec8ib operator ! (Vec8i const a) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_cmp_epi32_mask (a, _mm256_setzero_si256(), 0);
+#else
     return _mm256_cmpeq_epi32(a, _mm256_setzero_si256());
+#endif
 }
 
 // Functions for this class
 
 // Select between two operands. Corresponds to this pseudocode:
 // for (int i = 0; i < 8; i++) result[i] = s[i] ? a[i] : b[i];
-// Each byte in s must be either 0 (false) or -1 (true). No other values are allowed.
-// (s is signed)
-static inline Vec8i select (Vec8ib const & s, Vec8i const & a, Vec8i const & b) {
+static inline Vec8i select (Vec8ib const s, Vec8i const a, Vec8i const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_mask_mov_epi32(b, s, a);
+#else
     return selectb(s,a,b);
+#endif
 }
 
 // Conditional add: For all vector elements i: result[i] = f[i] ? (a[i] + b[i]) : a[i]
-static inline Vec8i if_add (Vec8ib const & f, Vec8i const & a, Vec8i const & b) {
+static inline Vec8i if_add (Vec8ib const f, Vec8i const a, Vec8i const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_mask_add_epi32 (a, f, a, b);
+#else
     return a + (Vec8i(f) & b);
+#endif
+}
+
+// Conditional subtract
+static inline Vec8i if_sub (Vec8ib const f, Vec8i const a, Vec8i const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_mask_sub_epi32 (a, f, a, b);
+#else
+    return a - (Vec8i(f) & b);
+#endif
 }
 
-// Horizontal add: Calculates the sum of all vector elements.
-// Overflow will wrap around
-static inline int32_t horizontal_add (Vec8i const & a) {
-    __m256i sum1  = _mm256_hadd_epi32(a,a);                           // horizontally add 2x4 elements in 2 steps
-    __m256i sum2  = _mm256_hadd_epi32(sum1,sum1);
-#if defined (_MSC_VER) && _MSC_VER <= 1700 && ! defined(__INTEL_COMPILER)
-    __m128i sum3  = _mm256_extractf128_si256(sum2,1);                 // bug in MS VS 11
+// Conditional multiply
+static inline Vec8i if_mul (Vec8ib const f, Vec8i const a, Vec8i const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_mask_mullo_epi32 (a, f, a, b);
 #else
-    __m128i sum3  = _mm256_extracti128_si256(sum2,1);                 // get high part
+    return select(f, a*b, a);
 #endif
-    __m128i sum4  = _mm_add_epi32(_mm256_castsi256_si128(sum2),sum3); // add low and high parts
-    return          _mm_cvtsi128_si32(sum4);
+}
+
+// Horizontal add: Calculates the sum of all vector elements. Overflow will wrap around
+static inline int32_t horizontal_add (Vec8i const a) {
+    // The hadd instruction is inefficient, and may be split into two instructions for faster decoding
+    __m128i sum1  = _mm_add_epi32(_mm256_extracti128_si256(a,1),_mm256_castsi256_si128(a));
+    __m128i sum2  = _mm_add_epi32(sum1,_mm_unpackhi_epi64(sum1,sum1));
+    __m128i sum3  = _mm_add_epi32(sum2,_mm_shuffle_epi32(sum2,1));
+    return (int32_t)_mm_cvtsi128_si32(sum3);
 }
 
 // Horizontal add extended: Calculates the sum of all vector elements.
 // Elements are sign extended before adding to avoid overflow
-// static inline int64_t horizontal_add_x (Vec8i const & a); // defined below
+// static inline int64_t horizontal_add_x (Vec8i const a); // defined below
 
 // function add_saturated: add element by element, signed with saturation
-static inline Vec8i add_saturated(Vec8i const & a, Vec8i const & b) {
-    __m256i sum    = _mm256_add_epi32(a, b);                  // a + b
-    __m256i axb    = _mm256_xor_si256(a, b);                  // check if a and b have different sign
-    __m256i axs    = _mm256_xor_si256(a, sum);                // check if a and sum have different sign
-    __m256i overf1 = _mm256_andnot_si256(axb,axs);            // check if sum has wrong sign
-    __m256i overf2 = _mm256_srai_epi32(overf1,31);            // -1 if overflow
-    __m256i asign  = _mm256_srli_epi32(a,31);                 // 1  if a < 0
-    __m256i sat1   = _mm256_srli_epi32(overf2,1);             // 7FFFFFFF if overflow
-    __m256i sat2   = _mm256_add_epi32(sat1,asign);            // 7FFFFFFF if positive overflow 80000000 if negative overflow
-    return  selectb(overf2,sat2,sum);                         // sum if not overflow, else sat2
+static inline Vec8i add_saturated(Vec8i const a, Vec8i const b) {
+    __m256i sum    = _mm256_add_epi32(a, b);               // a + b
+    __m256i axb    = _mm256_xor_si256(a, b);               // check if a and b have different sign
+    __m256i axs    = _mm256_xor_si256(a, sum);             // check if a and sum have different sign
+    __m256i overf1 = _mm256_andnot_si256(axb,axs);         // check if sum has wrong sign
+    __m256i overf2 = _mm256_srai_epi32(overf1,31);         // -1 if overflow
+    __m256i asign  = _mm256_srli_epi32(a,31);              // 1  if a < 0
+    __m256i sat1   = _mm256_srli_epi32(overf2,1);          // 7FFFFFFF if overflow
+    __m256i sat2   = _mm256_add_epi32(sat1,asign);         // 7FFFFFFF if positive overflow 80000000 if negative overflow
+    return  selectb(overf2,sat2,sum);                      // sum if not overflow, else sat2
 }
 
 // function sub_saturated: subtract element by element, signed with saturation
-static inline Vec8i sub_saturated(Vec8i const & a, Vec8i const & b) {
-    __m256i diff   = _mm256_sub_epi32(a, b);                  // a + b
-    __m256i axb    = _mm256_xor_si256(a, b);                  // check if a and b have different sign
-    __m256i axs    = _mm256_xor_si256(a, diff);               // check if a and sum have different sign
-    __m256i overf1 = _mm256_and_si256(axb,axs);               // check if sum has wrong sign
-    __m256i overf2 = _mm256_srai_epi32(overf1,31);            // -1 if overflow
-    __m256i asign  = _mm256_srli_epi32(a,31);                 // 1  if a < 0
-    __m256i sat1   = _mm256_srli_epi32(overf2,1);             // 7FFFFFFF if overflow
-    __m256i sat2   = _mm256_add_epi32(sat1,asign);            // 7FFFFFFF if positive overflow 80000000 if negative overflow
-    return  selectb(overf2,sat2,diff);                        // diff if not overflow, else sat2
+static inline Vec8i sub_saturated(Vec8i const a, Vec8i const b) {
+    __m256i diff   = _mm256_sub_epi32(a, b);               // a + b
+    __m256i axb    = _mm256_xor_si256(a, b);               // check if a and b have different sign
+    __m256i axs    = _mm256_xor_si256(a, diff);            // check if a and sum have different sign
+    __m256i overf1 = _mm256_and_si256(axb,axs);            // check if sum has wrong sign
+    __m256i overf2 = _mm256_srai_epi32(overf1,31);         // -1 if overflow
+    __m256i asign  = _mm256_srli_epi32(a,31);              // 1  if a < 0
+    __m256i sat1   = _mm256_srli_epi32(overf2,1);          // 7FFFFFFF if overflow
+    __m256i sat2   = _mm256_add_epi32(sat1,asign);         // 7FFFFFFF if positive overflow 80000000 if negative overflow
+    return  selectb(overf2,sat2,diff);                     // diff if not overflow, else sat2
 }
 
 // function max: a > b ? a : b
-static inline Vec8i max(Vec8i const & a, Vec8i const & b) {
+static inline Vec8i max(Vec8i const a, Vec8i const b) {
     return _mm256_max_epi32(a,b);
 }
 
 // function min: a < b ? a : b
-static inline Vec8i min(Vec8i const & a, Vec8i const & b) {
+static inline Vec8i min(Vec8i const a, Vec8i const b) {
     return _mm256_min_epi32(a,b);
 }
 
 // function abs: a >= 0 ? a : -a
-static inline Vec8i abs(Vec8i const & a) {
-    return _mm256_sign_epi32(a,a);
+static inline Vec8i abs(Vec8i const a) {
+    return _mm256_abs_epi32(a);
 }
 
 // function abs_saturated: same as abs, saturate if overflow
-static inline Vec8i abs_saturated(Vec8i const & a) {
-    __m256i absa   = abs(a);                                  // abs(a)
-    __m256i overfl = _mm256_srai_epi32(absa,31);              // sign
-    return           _mm256_add_epi32(absa,overfl);           // subtract 1 if 0x80000000
+static inline Vec8i abs_saturated(Vec8i const a) {
+#if INSTRSET >= 10
+    return _mm256_min_epu32(abs(a), Vec8i(0x7FFFFFFF));
+#else
+    __m256i absa   = abs(a);                               // abs(a)
+    __m256i overfl = _mm256_srai_epi32(absa,31);           // sign
+    return           _mm256_add_epi32(absa,overfl);        // subtract 1 if 0x80000000
+#endif
 }
 
 // function rotate_left all elements
 // Use negative count to rotate right
-static inline Vec8i rotate_left(Vec8i const & a, int b) {
-#ifdef __AVX512VL__
+static inline Vec8i rotate_left(Vec8i const a, int b) {
+#if INSTRSET >= 10  // __AVX512VL__
     return _mm256_rolv_epi32(a, _mm256_set1_epi32(b));
 #else
-    __m256i left  = _mm256_sll_epi32(a,_mm_cvtsi32_si128(b & 0x1F));      // a << b 
-    __m256i right = _mm256_srl_epi32(a,_mm_cvtsi32_si128((32-b) & 0x1F)); // a >> (32 - b)
-    __m256i rot   = _mm256_or_si256(left,right);                          // or
+    __m256i left  = _mm256_sll_epi32(a,_mm_cvtsi32_si128(b & 0x1F));   // a << b
+    __m256i right = _mm256_srl_epi32(a,_mm_cvtsi32_si128((-b) & 0x1F));// a >> (32 - b)
+    __m256i rot   = _mm256_or_si256(left,right);                       // or
     return  rot;
 #endif
 }
@@ -2322,26 +2749,25 @@ static inline Vec8i rotate_left(Vec8i const & a, int b) {
 class Vec8ui : public Vec8i {
 public:
     // Default constructor:
-    Vec8ui() {
-    }
+    Vec8ui() = default;
     // Constructor to broadcast the same value into all elements:
     Vec8ui(uint32_t i) {
-        ymm = _mm256_set1_epi32(i);
+        ymm = _mm256_set1_epi32((int32_t)i);
     }
     // Constructor to build from all elements:
     Vec8ui(uint32_t i0, uint32_t i1, uint32_t i2, uint32_t i3, uint32_t i4, uint32_t i5, uint32_t i6, uint32_t i7) {
-        ymm = _mm256_setr_epi32(i0, i1, i2, i3, i4, i5, i6, i7);
+        ymm = _mm256_setr_epi32((int32_t)i0, (int32_t)i1, (int32_t)i2, (int32_t)i3, (int32_t)i4, (int32_t)i5, (int32_t)i6, (int32_t)i7);
     }
     // Constructor to build from two Vec4ui:
-    Vec8ui(Vec4ui const & a0, Vec4ui const & a1) {
+    Vec8ui(Vec4ui const a0, Vec4ui const a1) {
         ymm = set_m128ir(a0, a1);
     }
     // Constructor to convert from type __m256i used in intrinsics:
-    Vec8ui(__m256i const & x) {
+    Vec8ui(__m256i const x) {
         ymm = x;
     }
     // Assignment operator to convert from type __m256i used in intrinsics:
-    Vec8ui & operator = (__m256i const & x) {
+    Vec8ui & operator = (__m256i const x) {
         ymm = x;
         return *this;
     }
@@ -2356,18 +2782,17 @@ public:
         return *this;
     }
     // Member function to change a single element in vector
-    // Note: This function is inefficient. Use load function if changing more than one element
-    Vec8ui const & insert(uint32_t index, uint32_t value) {
-        Vec8i::insert(index, value);
+    Vec8ui const insert(int index, uint32_t value) {
+        Vec8i::insert(index, (int32_t)value);
         return *this;
     }
     // Member function extract a single element from vector
-    uint32_t extract(uint32_t index) const {
-        return Vec8i::extract(index);
+    uint32_t extract(int index) const {
+        return (uint32_t)Vec8i::extract(index);
     }
     // Extract a single element. Use store function if extracting more than one element.
     // Operator [] can only read an element, not write.
-    uint32_t operator [] (uint32_t index) const {
+    uint32_t operator [] (int index) const {
         return extract(index);
     }
     // Member functions to split into two Vec4ui:
@@ -2375,24 +2800,27 @@ public:
         return _mm256_castsi256_si128(ymm);
     }
     Vec4ui get_high() const {
-        return _mm256_extracti128_si256(ymm,1);
+        return _mm256_extractf128_si256(ymm,1);
+    }
+    static constexpr int elementtype() {
+        return 9;
     }
 };
 
 // Define operators for this class
 
 // vector operator + : add
-static inline Vec8ui operator + (Vec8ui const & a, Vec8ui const & b) {
+static inline Vec8ui operator + (Vec8ui const a, Vec8ui const b) {
     return Vec8ui (Vec8i(a) + Vec8i(b));
 }
 
 // vector operator - : subtract
-static inline Vec8ui operator - (Vec8ui const & a, Vec8ui const & b) {
+static inline Vec8ui operator - (Vec8ui const a, Vec8ui const b) {
     return Vec8ui (Vec8i(a) - Vec8i(b));
 }
 
 // vector operator * : multiply
-static inline Vec8ui operator * (Vec8ui const & a, Vec8ui const & b) {
+static inline Vec8ui operator * (Vec8ui const a, Vec8ui const b) {
     return Vec8ui (Vec8i(a) * Vec8i(b));
 }
 
@@ -2400,78 +2828,92 @@ static inline Vec8ui operator * (Vec8ui const & a, Vec8ui const & b) {
 // See bottom of file
 
 // vector operator >> : shift right logical all elements
-static inline Vec8ui operator >> (Vec8ui const & a, uint32_t b) {
-    return _mm256_srl_epi32(a,_mm_cvtsi32_si128(b)); 
+static inline Vec8ui operator >> (Vec8ui const a, uint32_t b) {
+    return _mm256_srl_epi32(a,_mm_cvtsi32_si128((int)b));
 }
 
 // vector operator >> : shift right logical all elements
-static inline Vec8ui operator >> (Vec8ui const & a, int32_t b) {
+static inline Vec8ui operator >> (Vec8ui const a, int32_t b) {
     return a >> (uint32_t)b;
 }
-
 // vector operator >>= : shift right logical
 static inline Vec8ui & operator >>= (Vec8ui & a, uint32_t b) {
     a = a >> b;
     return a;
-} 
+}
 
 // vector operator << : shift left all elements
-static inline Vec8ui operator << (Vec8ui const & a, uint32_t b) {
+static inline Vec8ui operator << (Vec8ui const a, uint32_t b) {
     return Vec8ui ((Vec8i)a << (int32_t)b);
 }
-
 // vector operator << : shift left all elements
-static inline Vec8ui operator << (Vec8ui const & a, int32_t b) {
+static inline Vec8ui operator << (Vec8ui const a, int32_t b) {
     return Vec8ui ((Vec8i)a << (int32_t)b);
 }
 
 // vector operator > : returns true for elements for which a > b (unsigned)
-static inline Vec8ib operator > (Vec8ui const & a, Vec8ui const & b) {
+static inline Vec8ib operator > (Vec8ui const a, Vec8ui const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_cmp_epu32_mask (a, b, 6);
+#else
     __m256i signbit = _mm256_set1_epi32(0x80000000);
     __m256i a1      = _mm256_xor_si256(a,signbit);
     __m256i b1      = _mm256_xor_si256(b,signbit);
-    return _mm256_cmpgt_epi32(a1,b1);                         // signed compare
+    return _mm256_cmpgt_epi32(a1,b1);                      // signed compare
+#endif
 }
 
 // vector operator < : returns true for elements for which a < b (unsigned)
-static inline Vec8ib operator < (Vec8ui const & a, Vec8ui const & b) {
+static inline Vec8ib operator < (Vec8ui const a, Vec8ui const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_cmp_epu32_mask (a, b, 1);
+#else
     return b > a;
+#endif
 }
 
 // vector operator >= : returns true for elements for which a >= b (unsigned)
-static inline Vec8ib operator >= (Vec8ui const & a, Vec8ui const & b) {
-    __m256i max_ab = _mm256_max_epu32(a,b);                   // max(a,b), unsigned
-    return _mm256_cmpeq_epi32(a,max_ab);                      // a == max(a,b)
+static inline Vec8ib operator >= (Vec8ui const a, Vec8ui const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_cmp_epu32_mask (a, b, 5);
+#else
+    __m256i max_ab = _mm256_max_epu32(a,b);                // max(a,b), unsigned
+    return _mm256_cmpeq_epi32(a,max_ab);                   // a == max(a,b)
+#endif
 }
 
 // vector operator <= : returns true for elements for which a <= b (unsigned)
-static inline Vec8ib operator <= (Vec8ui const & a, Vec8ui const & b) {
+static inline Vec8ib operator <= (Vec8ui const a, Vec8ui const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_cmp_epu32_mask (a, b, 2);
+#else
     return b >= a;
+#endif
 }
 
 // vector operator & : bitwise and
-static inline Vec8ui operator & (Vec8ui const & a, Vec8ui const & b) {
+static inline Vec8ui operator & (Vec8ui const a, Vec8ui const b) {
     return Vec8ui(Vec256b(a) & Vec256b(b));
 }
-static inline Vec8ui operator && (Vec8ui const & a, Vec8ui const & b) {
+static inline Vec8ui operator && (Vec8ui const a, Vec8ui const b) {
     return a & b;
 }
 
 // vector operator | : bitwise or
-static inline Vec8ui operator | (Vec8ui const & a, Vec8ui const & b) {
+static inline Vec8ui operator | (Vec8ui const a, Vec8ui const b) {
     return Vec8ui(Vec256b(a) | Vec256b(b));
 }
-static inline Vec8ui operator || (Vec8ui const & a, Vec8ui const & b) {
+static inline Vec8ui operator || (Vec8ui const a, Vec8ui const b) {
     return a | b;
 }
 
 // vector operator ^ : bitwise xor
-static inline Vec8ui operator ^ (Vec8ui const & a, Vec8ui const & b) {
+static inline Vec8ui operator ^ (Vec8ui const a, Vec8ui const b) {
     return Vec8ui(Vec256b(a) ^ Vec256b(b));
 }
 
 // vector operator ~ : bitwise not
-static inline Vec8ui operator ~ (Vec8ui const & a) {
+static inline Vec8ui operator ~ (Vec8ui const a) {
     return Vec8ui( ~ Vec256b(a));
 }
 
@@ -2479,49 +2921,82 @@ static inline Vec8ui operator ~ (Vec8ui const & a) {
 
 // Select between two operands. Corresponds to this pseudocode:
 // for (int i = 0; i < 16; i++) result[i] = s[i] ? a[i] : b[i];
-// Each word in s must be either 0 (false) or -1 (true). No other values are allowed.
-// (s is signed)
-static inline Vec8ui select (Vec8ib const & s, Vec8ui const & a, Vec8ui const & b) {
+static inline Vec8ui select (Vec8ib const s, Vec8ui const a, Vec8ui const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_mask_mov_epi32(b, s, a);
+#else
     return selectb(s,a,b);
+#endif
 }
 
 // Conditional add: For all vector elements i: result[i] = f[i] ? (a[i] + b[i]) : a[i]
-static inline Vec8ui if_add (Vec8ib const & f, Vec8ui const & a, Vec8ui const & b) {
+static inline Vec8ui if_add (Vec8ib const f, Vec8ui const a, Vec8ui const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_mask_add_epi32 (a, f, a, b);
+#else
     return a + (Vec8ui(f) & b);
+#endif
+}
+
+// Conditional subtract
+static inline Vec8ui if_sub (Vec8ib const f, Vec8ui const a, Vec8ui const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_mask_sub_epi32 (a, f, a, b);
+#else
+    return a - (Vec8ui(f) & b);
+#endif
+}
+
+// Conditional multiply
+static inline Vec8ui if_mul (Vec8ib const f, Vec8ui const a, Vec8ui const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_mask_mullo_epi32 (a, f, a, b);
+#else
+    return select(f, a*b, a);
+#endif
 }
 
-// Horizontal add: Calculates the sum of all vector elements.
-// Overflow will wrap around
-static inline uint32_t horizontal_add (Vec8ui const & a) {
-    return horizontal_add((Vec8i)a);
+// Horizontal add: Calculates the sum of all vector elements. Overflow will wrap around
+static inline uint32_t horizontal_add (Vec8ui const a) {
+    return (uint32_t)horizontal_add((Vec8i)a);
 }
 
 // Horizontal add extended: Calculates the sum of all vector elements.
 // Elements are zero extended before adding to avoid overflow
-// static inline uint64_t horizontal_add_x (Vec8ui const & a); // defined later
+// static inline uint64_t horizontal_add_x (Vec8ui const a); // defined later
 
 // function add_saturated: add element by element, unsigned with saturation
-static inline Vec8ui add_saturated(Vec8ui const & a, Vec8ui const & b) {
-    Vec8ui sum      = a + b;
-    Vec8ui aorb     = Vec8ui(a | b);
+static inline Vec8ui add_saturated(Vec8ui const a, Vec8ui const b) {
+    Vec8ui sum = a + b;
+    Vec8ui aorb = Vec8ui(a | b);
+#if INSTRSET >= 10
+    Vec8b  overflow = _mm256_cmp_epu32_mask(sum, aorb, 1);
+    return _mm256_mask_set1_epi32(sum, overflow, -1);
+#else
     Vec8ui overflow = Vec8ui(sum < aorb);                  // overflow if a + b < (a | b)
-    return Vec8ui (sum | overflow);                        // return 0xFFFFFFFF if overflow
+    return Vec8ui(sum | overflow);                         // return 0xFFFFFFFF if overflow
+#endif
 }
 
 // function sub_saturated: subtract element by element, unsigned with saturation
-static inline Vec8ui sub_saturated(Vec8ui const & a, Vec8ui const & b) {
-    Vec8ui diff      = a - b;
+static inline Vec8ui sub_saturated(Vec8ui const a, Vec8ui const b) {
+    Vec8ui diff = a - b;
+#if INSTRSET >= 10
+    Vec8b  nunderflow = _mm256_cmp_epu32_mask(diff, a, 2); // not underflow if a - b <= a
+    return _mm256_maskz_mov_epi32(nunderflow, diff);       // zero if underflow
+#else
     Vec8ui underflow = Vec8ui(diff > a);                   // underflow if a - b > a
-    return _mm256_andnot_si256(underflow,diff);            // return 0 if underflow
+    return _mm256_andnot_si256(underflow, diff);           // return 0 if underflow
+#endif
 }
 
 // function max: a > b ? a : b
-static inline Vec8ui max(Vec8ui const & a, Vec8ui const & b) {
+static inline Vec8ui max(Vec8ui const a, Vec8ui const b) {
     return _mm256_max_epu32(a,b);
 }
 
 // function min: a < b ? a : b
-static inline Vec8ui min(Vec8ui const & a, Vec8ui const & b) {
+static inline Vec8ui min(Vec8ui const a, Vec8ui const b) {
     return _mm256_min_epu32(a,b);
 }
 
@@ -2535,49 +3010,25 @@ static inline Vec8ui min(Vec8ui const & a, Vec8ui const & b) {
 class Vec4q : public Vec256b {
 public:
     // Default constructor:
-    Vec4q() {
-    }
+    Vec4q() = default;
     // Constructor to broadcast the same value into all elements:
     Vec4q(int64_t i) {
-#if defined (_MSC_VER) && _MSC_VER < 1900 && ! defined (__x86_64__) && ! defined(__INTEL_COMPILER)
-        // MS compiler cannot use _mm256_set1_epi64x in 32 bit mode, and  
-        // cannot put 64-bit values into xmm register without using
-        // mmx registers, and it makes no emms
-        union {
-            int64_t q[4];
-            int32_t r[8];
-        } u;
-        u.q[0] = u.q[1] = u.q[2] = u.q[3] = i;
-        ymm = _mm256_setr_epi32(u.r[0], u.r[1], u.r[2], u.r[3], u.r[4], u.r[5], u.r[6], u.r[7]);
-#else
         ymm = _mm256_set1_epi64x(i);
-#endif
     }
     // Constructor to build from all elements:
     Vec4q(int64_t i0, int64_t i1, int64_t i2, int64_t i3) {
-#if defined (_MSC_VER) && _MSC_VER < 1900 && ! defined (__x86_64__) && ! defined(__INTEL_COMPILER)
-        // MS compiler cannot put 64-bit values into xmm register without using
-        // mmx registers, and it makes no emms
-        union {
-            int64_t q[4];
-            int32_t r[8];
-        } u;
-        u.q[0] = i0;  u.q[1] = i1;  u.q[2] = i2;  u.q[3] = i3;
-        ymm = _mm256_setr_epi32(u.r[0], u.r[1], u.r[2], u.r[3], u.r[4], u.r[5], u.r[6], u.r[7]);
-#else
         ymm = _mm256_setr_epi64x(i0, i1, i2, i3);
-#endif
     }
     // Constructor to build from two Vec2q:
-    Vec4q(Vec2q const & a0, Vec2q const & a1) {
+    Vec4q(Vec2q const a0, Vec2q const a1) {
         ymm = set_m128ir(a0, a1);
     }
     // Constructor to convert from type __m256i used in intrinsics:
-    Vec4q(__m256i const & x) {
+    Vec4q(__m256i const x) {
         ymm = x;
     }
     // Assignment operator to convert from type __m256i used in intrinsics:
-    Vec4q & operator = (__m256i const & x) {
+    Vec4q & operator = (__m256i const x) {
         ymm = x;
         return *this;
     }
@@ -2597,6 +3048,9 @@ public:
     }
     // Partial load. Load n elements and set the rest to 0
     Vec4q & load_partial(int n, void const * p) {
+#if INSTRSET >= 10  // AVX512VL
+        ymm = _mm256_maskz_loadu_epi64(__mmask8((1u << n) - 1), p);
+#else
         if (n <= 0) {
             *this = 0;
         }
@@ -2609,10 +3063,14 @@ public:
         else {
             load(p);
         }
+#endif
         return *this;
     }
     // Partial store. Store n elements
     void store_partial(int n, void * p) const {
+#if INSTRSET >= 10  // AVX512VL
+        _mm256_mask_storeu_epi64(p, __mmask8((1u << n) - 1), ymm);
+#else
         if (n <= 0) {
             return;
         }
@@ -2626,18 +3084,25 @@ public:
         else {
             store(p);
         }
+#endif
     }
     // cut off vector to n elements. The last 8-n elements are set to zero
     Vec4q & cutoff(int n) {
+#if INSTRSET >= 10
+        ymm = _mm256_maskz_mov_epi64(__mmask8((1u << n) - 1), ymm);
+#else
         *this = Vec32c(*this).cutoff(n * 8);
+#endif
         return *this;
     }
     // Member function to change a single element in vector
-    // Note: This function is inefficient. Use load function if changing more than one element
-    Vec4q const & insert(uint32_t index, int64_t value) {
+    Vec4q const insert(int index, int64_t value) {
+#if INSTRSET >= 10
+        ymm = _mm256_mask_set1_epi64(ymm, __mmask8(1u << index), value);
+#else
         Vec4q x(value);
         switch (index) {
-        case 0:        
+        case 0:
             ymm = _mm256_blend_epi32(ymm,x,0x03);  break;
         case 1:
             ymm = _mm256_blend_epi32(ymm,x,0x0C);  break;
@@ -2646,17 +3111,23 @@ public:
         case 3:
             ymm = _mm256_blend_epi32(ymm,x,0xC0);  break;
         }
+#endif
         return *this;
     }
     // Member function extract a single element from vector
-    int64_t extract(uint32_t index) const {
+    int64_t extract(int index) const {
+#if INSTRSET >= 10
+        __m256i x = _mm256_maskz_compress_epi64(__mmask8(1u << index), ymm);
+        return _emulate_movq(_mm256_castsi256_si128(x));
+#else
         int64_t x[4];
         store(x);
         return x[index & 3];
+#endif
     }
     // Extract a single element. Use store function if extracting more than one element.
     // Operator [] can only read an element, not write.
-    int64_t operator [] (uint32_t index) const {
+    int64_t operator [] (int index) const {
         return extract(index);
     }
     // Member functions to split into two Vec2q:
@@ -2664,11 +3135,14 @@ public:
         return _mm256_castsi256_si128(ymm);
     }
     Vec2q get_high() const {
-        return _mm256_extracti128_si256(ymm,1);
+        return _mm256_extractf128_si256(ymm,1);
     }
-    static int size() {
+    static constexpr int size() {
         return 4;
     }
+    static constexpr int elementtype() {
+        return 10;
+    }
 };
 
 /*****************************************************************************
@@ -2677,21 +3151,22 @@ public:
 *
 *****************************************************************************/
 
+#if INSTRSET < 10  // broad boolean vectors
+
 class Vec4qb : public Vec4q {
 public:
     // Default constructor:
-    Vec4qb() {
-    }
+    Vec4qb() = default;
     // Constructor to build from all elements:
     Vec4qb(bool x0, bool x1, bool x2, bool x3) :
         Vec4q(-int64_t(x0), -int64_t(x1), -int64_t(x2), -int64_t(x3)) {
     }
     // Constructor to convert from type __m256i used in intrinsics:
-    Vec4qb(__m256i const & x) {
+    Vec4qb(__m256i const x) {
         ymm = x;
     }
     // Assignment operator to convert from type __m256i used in intrinsics:
-    Vec4qb & operator = (__m256i const & x) {
+    Vec4qb & operator = (__m256i const x) {
         ymm = x;
         return *this;
     }
@@ -2703,10 +3178,9 @@ public:
         *this = Vec4qb(b);
         return *this;
     }
-private: // Prevent constructing from int, etc.
-    Vec4qb(int b);
-    Vec4qb & operator = (int x);
-public:
+    // Constructor to build from two Vec2qb:
+    Vec4qb(Vec2qb const a0, Vec2qb const a1) : Vec4q(Vec2q(a0), Vec2q(a1)) {
+    }
     // Member functions to split into two Vec2qb:
     Vec2qb get_low() const {
         return Vec2qb(Vec4q::get_low());
@@ -2717,18 +3191,37 @@ public:
     Vec4qb & insert (int index, bool a) {
         Vec4q::insert(index, -(int64_t)a);
         return *this;
-    }    
+    }
     // Member function extract a single element from vector
-    bool extract(uint32_t index) const {
+    bool extract(int index) const {
         return Vec4q::extract(index) != 0;
     }
     // Extract a single element. Use store function if extracting more than one element.
     // Operator [] can only read an element, not write.
-    bool operator [] (uint32_t index) const {
+    bool operator [] (int index) const {
         return extract(index);
     }
+    // Member function to change a bitfield to a boolean vector
+    Vec4qb & load_bits(uint8_t a) {
+        __m256i b1 = _mm256_set1_epi32((int32_t)a);  // broadcast a
+        __m256i m2 = constant8ui<1,0,2,0,4,0,8,0>();
+        __m256i d1 = _mm256_and_si256(b1, m2); // isolate one bit in each dword
+        ymm = _mm256_cmpgt_epi64(d1, _mm256_setzero_si256());  // we can use signed compare here because no value is negative
+        return *this;
+    }
+    static constexpr int elementtype() {
+        return 3;
+    }
+    // Prevent constructing from int, etc.
+    Vec4qb(int b) = delete;
+    Vec4qb & operator = (int x) = delete;
 };
 
+#else
+
+typedef Vec4b Vec4qb;  // compact boolean vector
+
+#endif
 
 /*****************************************************************************
 *
@@ -2736,58 +3229,70 @@ public:
 *
 *****************************************************************************/
 
+#if INSTRSET < 10  // broad boolean vectors
+
 // vector operator & : bitwise and
-static inline Vec4qb operator & (Vec4qb const & a, Vec4qb const & b) {
+static inline Vec4qb operator & (Vec4qb const a, Vec4qb const b) {
     return Vec4qb(Vec256b(a) & Vec256b(b));
 }
-static inline Vec4qb operator && (Vec4qb const & a, Vec4qb const & b) {
+static inline Vec4qb operator && (Vec4qb const a, Vec4qb const b) {
     return a & b;
 }
 // vector operator &= : bitwise and
-static inline Vec4qb & operator &= (Vec4qb & a, Vec4qb const & b) {
+static inline Vec4qb & operator &= (Vec4qb & a, Vec4qb const b) {
     a = a & b;
     return a;
 }
 
 // vector operator | : bitwise or
-static inline Vec4qb operator | (Vec4qb const & a, Vec4qb const & b) {
+static inline Vec4qb operator | (Vec4qb const a, Vec4qb const b) {
     return Vec4qb(Vec256b(a) | Vec256b(b));
 }
-static inline Vec4qb operator || (Vec4qb const & a, Vec4qb const & b) {
+static inline Vec4qb operator || (Vec4qb const a, Vec4qb const b) {
     return a | b;
 }
 // vector operator |= : bitwise or
-static inline Vec4qb & operator |= (Vec4qb & a, Vec4qb const & b) {
+static inline Vec4qb & operator |= (Vec4qb & a, Vec4qb const b) {
     a = a | b;
     return a;
 }
 
 // vector operator ^ : bitwise xor
-static inline Vec4qb operator ^ (Vec4qb const & a, Vec4qb const & b) {
+static inline Vec4qb operator ^ (Vec4qb const a, Vec4qb const b) {
     return Vec4qb(Vec256b(a) ^ Vec256b(b));
 }
 // vector operator ^= : bitwise xor
-static inline Vec4qb & operator ^= (Vec4qb & a, Vec4qb const & b) {
+static inline Vec4qb & operator ^= (Vec4qb & a, Vec4qb const b) {
     a = a ^ b;
     return a;
 }
 
+// vector operator == : xnor
+static inline Vec4qb operator == (Vec4qb const a, Vec4qb const b) {
+    return Vec4qb(a ^ (~b));
+}
+
+// vector operator != : xor
+static inline Vec4qb operator != (Vec4qb const a, Vec4qb const b) {
+    return Vec4qb(a ^ b);
+}
+
 // vector operator ~ : bitwise not
-static inline Vec4qb operator ~ (Vec4qb const & a) {
+static inline Vec4qb operator ~ (Vec4qb const a) {
     return Vec4qb( ~ Vec256b(a));
 }
 
 // vector operator ! : element not
-static inline Vec4qb operator ! (Vec4qb const & a) {
+static inline Vec4qb operator ! (Vec4qb const a) {
     return ~ a;
 }
 
 // vector function andnot
-static inline Vec4qb andnot (Vec4qb const & a, Vec4qb const & b) {
+static inline Vec4qb andnot (Vec4qb const a, Vec4qb const b) {
     return Vec4qb(andnot(Vec256b(a), Vec256b(b)));
 }
 
-
+#endif
 
 
 /*****************************************************************************
@@ -2797,12 +3302,11 @@ static inline Vec4qb andnot (Vec4qb const & a, Vec4qb const & b) {
 *****************************************************************************/
 
 // vector operator + : add element by element
-static inline Vec4q operator + (Vec4q const & a, Vec4q const & b) {
+static inline Vec4q operator + (Vec4q const a, Vec4q const b) {
     return _mm256_add_epi64(a, b);
 }
-
 // vector operator += : add
-static inline Vec4q & operator += (Vec4q & a, Vec4q const & b) {
+static inline Vec4q & operator += (Vec4q & a, Vec4q const b) {
     a = a + b;
     return a;
 }
@@ -2813,7 +3317,6 @@ static inline Vec4q operator ++ (Vec4q & a, int) {
     a = a + 1;
     return a0;
 }
-
 // prefix operator ++
 static inline Vec4q & operator ++ (Vec4q & a) {
     a = a + 1;
@@ -2821,17 +3324,15 @@ static inline Vec4q & operator ++ (Vec4q & a) {
 }
 
 // vector operator - : subtract element by element
-static inline Vec4q operator - (Vec4q const & a, Vec4q const & b) {
+static inline Vec4q operator - (Vec4q const a, Vec4q const b) {
     return _mm256_sub_epi64(a, b);
 }
-
 // vector operator - : unary minus
-static inline Vec4q operator - (Vec4q const & a) {
+static inline Vec4q operator - (Vec4q const a) {
     return _mm256_sub_epi64(_mm256_setzero_si256(), a);
 }
-
 // vector operator -= : subtract
-static inline Vec4q & operator -= (Vec4q & a, Vec4q const & b) {
+static inline Vec4q & operator -= (Vec4q & a, Vec4q const b) {
     a = a - b;
     return a;
 }
@@ -2842,7 +3343,6 @@ static inline Vec4q operator -- (Vec4q & a, int) {
     a = a - 1;
     return a0;
 }
-
 // prefix operator --
 static inline Vec4q & operator -- (Vec4q & a) {
     a = a - 1;
@@ -2850,33 +3350,32 @@ static inline Vec4q & operator -- (Vec4q & a) {
 }
 
 // vector operator * : multiply element by element
-static inline Vec4q operator * (Vec4q const & a, Vec4q const & b) {
-#if defined (__AVX512DQ__) && defined (__AVX512VL__)
+static inline Vec4q operator * (Vec4q const a, Vec4q const b) {
+#if INSTRSET >= 10 // __AVX512DQ__ __AVX512VL__
     return _mm256_mullo_epi64(a, b);
 #else
-    // instruction does not exist. Split into 32-bit multiplies
-    __m256i bswap   = _mm256_shuffle_epi32(b,0xB1);           // swap H<->L
-    __m256i prodlh  = _mm256_mullo_epi32(a,bswap);            // 32 bit L*H products
-    __m256i zero    = _mm256_setzero_si256();                 // 0
-    __m256i prodlh2 = _mm256_hadd_epi32(prodlh,zero);         // a0Lb0H+a0Hb0L,a1Lb1H+a1Hb1L,0,0
-    __m256i prodlh3 = _mm256_shuffle_epi32(prodlh2,0x73);     // 0, a0Lb0H+a0Hb0L, 0, a1Lb1H+a1Hb1L
-    __m256i prodll  = _mm256_mul_epu32(a,b);                  // a0Lb0L,a1Lb1L, 64 bit unsigned products
-    __m256i prod    = _mm256_add_epi64(prodll,prodlh3);       // a0Lb0L+(a0Lb0H+a0Hb0L)<<32, a1Lb1L+(a1Lb1H+a1Hb1L)<<32
+    // Split into 32-bit multiplies
+    __m256i bswap   = _mm256_shuffle_epi32(b,0xB1);        // swap H<->L
+    __m256i prodlh  = _mm256_mullo_epi32(a,bswap);         // 32 bit L*H products
+    __m256i zero    = _mm256_setzero_si256();              // 0
+    __m256i prodlh2 = _mm256_hadd_epi32(prodlh,zero);      // a0Lb0H+a0Hb0L,a1Lb1H+a1Hb1L,0,0
+    __m256i prodlh3 = _mm256_shuffle_epi32(prodlh2,0x73);  // 0, a0Lb0H+a0Hb0L, 0, a1Lb1H+a1Hb1L
+    __m256i prodll  = _mm256_mul_epu32(a,b);               // a0Lb0L,a1Lb1L, 64 bit unsigned products
+    __m256i prod    = _mm256_add_epi64(prodll,prodlh3);    // a0Lb0L+(a0Lb0H+a0Hb0L)<<32, a1Lb1L+(a1Lb1H+a1Hb1L)<<32
     return  prod;
 #endif
 }
 
 // vector operator *= : multiply
-static inline Vec4q & operator *= (Vec4q & a, Vec4q const & b) {
+static inline Vec4q & operator *= (Vec4q & a, Vec4q const b) {
     a = a * b;
     return a;
 }
 
 // vector operator << : shift left
-static inline Vec4q operator << (Vec4q const & a, int32_t b) {
+static inline Vec4q operator << (Vec4q const a, int32_t b) {
     return _mm256_sll_epi64(a, _mm_cvtsi32_si128(b));
 }
-
 // vector operator <<= : shift left
 static inline Vec4q & operator <<= (Vec4q & a, int32_t b) {
     a = a << b;
@@ -2884,25 +3383,26 @@ static inline Vec4q & operator <<= (Vec4q & a, int32_t b) {
 }
 
 // vector operator >> : shift right arithmetic
-static inline Vec4q operator >> (Vec4q const & a, int32_t b) {
-    // instruction does not exist. Split into 32-bit shifts
+static inline Vec4q operator >> (Vec4q const a, int32_t b) {
+#if INSTRSET >= 10   // AVX512VL
+    return _mm256_sra_epi64(a, _mm_cvtsi32_si128(b));
+#else
+    __m128i bb;
+    __m256i shi, slo, sra2;
     if (b <= 32) {
-        __m128i bb   = _mm_cvtsi32_si128(b);                   // b
-        __m256i sra  = _mm256_sra_epi32(a,bb);                 // a >> b signed dwords
-        __m256i srl  = _mm256_srl_epi64(a,bb);                 // a >> b unsigned qwords
-        __m256i mask = constant8i<0,-1,0,-1,0,-1,0,-1>();      // mask for signed high part
-        return  selectb(mask, sra, srl);
+        bb   = _mm_cvtsi32_si128(b);             // b
+        shi  = _mm256_sra_epi32(a,bb);           // a >> b signed dwords
+        slo  = _mm256_srl_epi64(a,bb);           // a >> b unsigned qwords
     }
     else {  // b > 32
-        __m128i bm32 = _mm_cvtsi32_si128(b-32);                // b - 32
-        __m256i sign = _mm256_srai_epi32(a,31);                // sign of a
-        __m256i sra2 = _mm256_sra_epi32(a,bm32);               // a >> (b-32) signed dwords
-        __m256i sra3 = _mm256_srli_epi64(sra2,32);             // a >> (b-32) >> 32 (second shift unsigned qword)
-        __m256i mask = constant8i<0,-1,0,-1,0,-1,0,-1>();      // mask for high part containing only sign
-        return  selectb(mask, sign ,sra3);
+        bb   = _mm_cvtsi32_si128(b-32);          // b - 32
+        shi  = _mm256_srai_epi32(a,31);          // sign of a
+        sra2 = _mm256_sra_epi32(a,bb);           // a >> (b-32) signed dwords
+        slo  = _mm256_srli_epi64(sra2,32);       // a >> (b-32) >> 32 (second shift unsigned qword)
     }
+    return _mm256_blend_epi32(slo,shi,0xAA);
+#endif
 }
-
 // vector operator >>= : shift right arithmetic
 static inline Vec4q & operator >>= (Vec4q & a, int32_t b) {
     a = a >> b;
@@ -2910,152 +3410,198 @@ static inline Vec4q & operator >>= (Vec4q & a, int32_t b) {
 }
 
 // vector operator == : returns true for elements for which a == b
-static inline Vec4qb operator == (Vec4q const & a, Vec4q const & b) {
+static inline Vec4qb operator == (Vec4q const a, Vec4q const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_cmp_epi64_mask (a, b, 0);
+#else
     return _mm256_cmpeq_epi64(a, b);
+#endif
 }
 
 // vector operator != : returns true for elements for which a != b
-static inline Vec4qb operator != (Vec4q const & a, Vec4q const & b) {
+static inline Vec4qb operator != (Vec4q const a, Vec4q const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_cmp_epi64_mask (a, b, 4);
+#else
     return Vec4qb(Vec4q(~(a == b)));
+#endif
 }
-  
+
 // vector operator < : returns true for elements for which a < b
-static inline Vec4qb operator < (Vec4q const & a, Vec4q const & b) {
+static inline Vec4qb operator < (Vec4q const a, Vec4q const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_cmp_epi64_mask (a, b, 1);
+#else
     return _mm256_cmpgt_epi64(b, a);
+#endif
 }
 
 // vector operator > : returns true for elements for which a > b
-static inline Vec4qb operator > (Vec4q const & a, Vec4q const & b) {
+static inline Vec4qb operator > (Vec4q const a, Vec4q const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_cmp_epi64_mask (a, b, 6);
+#else
     return b < a;
+#endif
 }
 
 // vector operator >= : returns true for elements for which a >= b (signed)
-static inline Vec4qb operator >= (Vec4q const & a, Vec4q const & b) {
+static inline Vec4qb operator >= (Vec4q const a, Vec4q const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_cmp_epi64_mask (a, b, 5);
+#else
     return Vec4qb(Vec4q(~(a < b)));
+#endif
 }
 
 // vector operator <= : returns true for elements for which a <= b (signed)
-static inline Vec4qb operator <= (Vec4q const & a, Vec4q const & b) {
+static inline Vec4qb operator <= (Vec4q const a, Vec4q const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_cmp_epi64_mask (a, b, 2);
+#else
     return b >= a;
+#endif
 }
 
 // vector operator & : bitwise and
-static inline Vec4q operator & (Vec4q const & a, Vec4q const & b) {
+static inline Vec4q operator & (Vec4q const a, Vec4q const b) {
     return Vec4q(Vec256b(a) & Vec256b(b));
 }
-static inline Vec4q operator && (Vec4q const & a, Vec4q const & b) {
+static inline Vec4q operator && (Vec4q const a, Vec4q const b) {
     return a & b;
 }
 // vector operator &= : bitwise and
-static inline Vec4q & operator &= (Vec4q & a, Vec4q const & b) {
+static inline Vec4q & operator &= (Vec4q & a, Vec4q const b) {
     a = a & b;
     return a;
 }
 
 // vector operator | : bitwise or
-static inline Vec4q operator | (Vec4q const & a, Vec4q const & b) {
+static inline Vec4q operator | (Vec4q const a, Vec4q const b) {
     return Vec4q(Vec256b(a) | Vec256b(b));
 }
-static inline Vec4q operator || (Vec4q const & a, Vec4q const & b) {
+static inline Vec4q operator || (Vec4q const a, Vec4q const b) {
     return a | b;
 }
 // vector operator |= : bitwise or
-static inline Vec4q & operator |= (Vec4q & a, Vec4q const & b) {
+static inline Vec4q & operator |= (Vec4q & a, Vec4q const b) {
     a = a | b;
     return a;
 }
 
 // vector operator ^ : bitwise xor
-static inline Vec4q operator ^ (Vec4q const & a, Vec4q const & b) {
+static inline Vec4q operator ^ (Vec4q const a, Vec4q const b) {
     return Vec4q(Vec256b(a) ^ Vec256b(b));
 }
 // vector operator ^= : bitwise xor
-static inline Vec4q & operator ^= (Vec4q & a, Vec4q const & b) {
+static inline Vec4q & operator ^= (Vec4q & a, Vec4q const b) {
     a = a ^ b;
     return a;
 }
 
 // vector operator ~ : bitwise not
-static inline Vec4q operator ~ (Vec4q const & a) {
+static inline Vec4q operator ~ (Vec4q const a) {
     return Vec4q( ~ Vec256b(a));
 }
 
 // vector operator ! : logical not, returns true for elements == 0
-static inline Vec4qb operator ! (Vec4q const & a) {
+static inline Vec4qb operator ! (Vec4q const a) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_cmp_epi64_mask (a, _mm256_setzero_si256(), 0);
+#else
     return a == Vec4q(_mm256_setzero_si256());
+#endif
 }
 
 // Functions for this class
 
 // Select between two operands. Corresponds to this pseudocode:
 // for (int i = 0; i < 4; i++) result[i] = s[i] ? a[i] : b[i];
-// Each byte in s must be either 0 (false) or -1 (true). No other values are allowed.
-// (s is signed)
-static inline Vec4q select (Vec4qb const & s, Vec4q const & a, Vec4q const & b) {
+static inline Vec4q select (Vec4qb const s, Vec4q const a, Vec4q const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_mask_mov_epi64(b, s, a);
+#else
     return selectb(s,a,b);
+#endif
 }
 
 // Conditional add: For all vector elements i: result[i] = f[i] ? (a[i] + b[i]) : a[i]
-static inline Vec4q if_add (Vec4qb const & f, Vec4q const & a, Vec4q const & b) {
+static inline Vec4q if_add (Vec4qb const f, Vec4q const a, Vec4q const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_mask_add_epi64 (a, f, a, b);
+#else
     return a + (Vec4q(f) & b);
+#endif
 }
 
-// Horizontal add: Calculates the sum of all vector elements.
-// Overflow will wrap around
-static inline int64_t horizontal_add (Vec4q const & a) {
-    __m256i sum1  = _mm256_shuffle_epi32(a,0x0E);                     // high element
-    __m256i sum2  = _mm256_add_epi64(a,sum1);                         // sum
-#if defined (_MSC_VER) && _MSC_VER <= 1700 && ! defined(__INTEL_COMPILER)
-    __m128i sum3  = _mm256_extractf128_si256(sum2, 1);                // bug in MS compiler VS 11
+// Conditional subtract
+static inline Vec4q if_sub (Vec4qb const f, Vec4q const a, Vec4q const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_mask_sub_epi64 (a, f, a, b);
 #else
-    __m128i sum3  = _mm256_extracti128_si256(sum2, 1);                // get high part
+    return a - (Vec4q(f) & b);
 #endif
-    __m128i sum4  = _mm_add_epi64(_mm256_castsi256_si128(sum2),sum3); // add low and high parts
-#if defined(__x86_64__)
-    return          _mm_cvtsi128_si64(sum4);                          // 64 bit mode
+}
+
+// Conditional multiply
+static inline Vec4q if_mul (Vec4qb const f, Vec4q const a, Vec4q const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_mask_mullo_epi64 (a, f, a, b);
 #else
-    union {
-        __m128i x;  // silly definition of _mm256_storel_epi64 requires __m256i
-        uint64_t i;
-    } u;
-    _mm_storel_epi64(&u.x,sum4);
-    return u.i;
+    return select(f, a*b, a);
 #endif
 }
 
+// Horizontal add: Calculates the sum of all vector elements. Overflow will wrap around
+static inline int64_t horizontal_add (Vec4q const a) {
+    __m256i sum1  = _mm256_shuffle_epi32(a,0x0E);                     // high element
+    __m256i sum2  = _mm256_add_epi64(a,sum1);                         // sum
+    __m128i sum3  = _mm256_extracti128_si256(sum2, 1);                // get high part
+    __m128i sum4  = _mm_add_epi64(_mm256_castsi256_si128(sum2),sum3); // add low and high parts
+    return _emulate_movq(sum4);
+}
+
 // function max: a > b ? a : b
-static inline Vec4q max(Vec4q const & a, Vec4q const & b) {
+static inline Vec4q max(Vec4q const a, Vec4q const b) {
     return select(a > b, a, b);
 }
 
 // function min: a < b ? a : b
-static inline Vec4q min(Vec4q const & a, Vec4q const & b) {
+static inline Vec4q min(Vec4q const a, Vec4q const b) {
     return select(a < b, a, b);
 }
 
 // function abs: a >= 0 ? a : -a
-static inline Vec4q abs(Vec4q const & a) {
+static inline Vec4q abs(Vec4q const a) {
+#if INSTRSET >= 10     // AVX512VL
+    return _mm256_abs_epi64(a);
+#else
     __m256i sign  = _mm256_cmpgt_epi64(_mm256_setzero_si256(), a);// 0 > a
-    __m256i inv   = _mm256_xor_si256(a, sign);                    // invert bits if negative
-    return          _mm256_sub_epi64(inv, sign);                  // add 1
+    __m256i inv   = _mm256_xor_si256(a, sign);             // invert bits if negative
+    return          _mm256_sub_epi64(inv, sign);           // add 1
+#endif
 }
 
 // function abs_saturated: same as abs, saturate if overflow
-static inline Vec4q abs_saturated(Vec4q const & a) {
-    __m256i absa   = abs(a);                                        // abs(a)
+static inline Vec4q abs_saturated(Vec4q const a) {
+#if INSTRSET >= 10
+    return _mm256_min_epu64(abs(a), Vec4q(0x7FFFFFFFFFFFFFFF));
+#else
+    __m256i absa   = abs(a);                               // abs(a)
     __m256i overfl = _mm256_cmpgt_epi64(_mm256_setzero_si256(), absa); // 0 > a
-    return           _mm256_add_epi64(absa, overfl);                // subtract 1 if 0x8000000000000000
+    return           _mm256_add_epi64(absa, overfl);       // subtract 1 if 0x8000000000000000
+#endif
 }
 
 // function rotate_left all elements
 // Use negative count to rotate right
-static inline Vec4q rotate_left(Vec4q const & a, int b) {
-#ifdef __AVX512VL__
+static inline Vec4q rotate_left(Vec4q const a, int b) {
+#if INSTRSET >= 10  // __AVX512VL__
     return _mm256_rolv_epi64(a, _mm256_set1_epi64x(int64_t(b)));
 #else
-    __m256i left  = _mm256_sll_epi64(a,_mm_cvtsi32_si128(b & 0x3F));      // a << b 
-    __m256i right = _mm256_srl_epi64(a,_mm_cvtsi32_si128((64-b) & 0x3F)); // a >> (64 - b)
-    __m256i rot   = _mm256_or_si256(left, right);                         // or
+    __m256i left  = _mm256_sll_epi64(a,_mm_cvtsi32_si128(b & 0x3F));    // a << b
+    __m256i right = _mm256_srl_epi64(a,_mm_cvtsi32_si128((-b) & 0x3F)); // a >> (64 - b)
+    __m256i rot   = _mm256_or_si256(left, right);                       // or
     return  rot;
 #endif
 }
@@ -3070,26 +3616,25 @@ static inline Vec4q rotate_left(Vec4q const & a, int b) {
 class Vec4uq : public Vec4q {
 public:
     // Default constructor:
-    Vec4uq() {
-    }
+    Vec4uq() = default;
     // Constructor to broadcast the same value into all elements:
     Vec4uq(uint64_t i) {
-        ymm = Vec4q(i);
+        ymm = Vec4q((int64_t)i);
     }
     // Constructor to build from all elements:
     Vec4uq(uint64_t i0, uint64_t i1, uint64_t i2, uint64_t i3) {
-        ymm = Vec4q(i0, i1, i2, i3);
+        ymm = Vec4q((int64_t)i0, (int64_t)i1, (int64_t)i2, (int64_t)i3);
     }
     // Constructor to build from two Vec2uq:
-    Vec4uq(Vec2uq const & a0, Vec2uq const & a1) {
+    Vec4uq(Vec2uq const a0, Vec2uq const a1) {
         ymm = set_m128ir(a0, a1);
     }
     // Constructor to convert from type __m256i used in intrinsics:
-    Vec4uq(__m256i const & x) {
+    Vec4uq(__m256i const x) {
         ymm = x;
     }
     // Assignment operator to convert from type __m256i used in intrinsics:
-    Vec4uq & operator = (__m256i const & x) {
+    Vec4uq & operator = (__m256i const x) {
         ymm = x;
         return *this;
     }
@@ -3104,18 +3649,17 @@ public:
         return *this;
     }
     // Member function to change a single element in vector
-    // Note: This function is inefficient. Use load function if changing more than one element
-    Vec4uq const & insert(uint32_t index, uint64_t value) {
-        Vec4q::insert(index, value);
+    Vec4uq const insert(int index, uint64_t value) {
+        Vec4q::insert(index, (int64_t)value);
         return *this;
     }
     // Member function extract a single element from vector
-    uint64_t extract(uint32_t index) const {
-        return Vec4q::extract(index);
+    uint64_t extract(int index) const {
+        return (uint64_t)Vec4q::extract(index);
     }
     // Extract a single element. Use store function if extracting more than one element.
     // Operator [] can only read an element, not write.
-    uint64_t operator [] (uint32_t index) const {
+    uint64_t operator [] (int index) const {
         return extract(index);
     }
     // Member functions to split into two Vec2uq:
@@ -3123,96 +3667,112 @@ public:
         return _mm256_castsi256_si128(ymm);
     }
     Vec2uq get_high() const {
-        return _mm256_extracti128_si256(ymm,1);
+        return _mm256_extractf128_si256(ymm,1);
+    }
+    static constexpr int elementtype() {
+        return 11;
     }
 };
 
 // Define operators for this class
 
 // vector operator + : add
-static inline Vec4uq operator + (Vec4uq const & a, Vec4uq const & b) {
+static inline Vec4uq operator + (Vec4uq const a, Vec4uq const b) {
     return Vec4uq (Vec4q(a) + Vec4q(b));
 }
 
 // vector operator - : subtract
-static inline Vec4uq operator - (Vec4uq const & a, Vec4uq const & b) {
+static inline Vec4uq operator - (Vec4uq const a, Vec4uq const b) {
     return Vec4uq (Vec4q(a) - Vec4q(b));
 }
 
 // vector operator * : multiply element by element
-static inline Vec4uq operator * (Vec4uq const & a, Vec4uq const & b) {
+static inline Vec4uq operator * (Vec4uq const a, Vec4uq const b) {
     return Vec4uq (Vec4q(a) * Vec4q(b));
 }
 
 // vector operator >> : shift right logical all elements
-static inline Vec4uq operator >> (Vec4uq const & a, uint32_t b) {
-    return _mm256_srl_epi64(a,_mm_cvtsi32_si128(b)); 
+static inline Vec4uq operator >> (Vec4uq const a, uint32_t b) {
+    return _mm256_srl_epi64(a,_mm_cvtsi32_si128((int)b));
 }
 
 // vector operator >> : shift right logical all elements
-static inline Vec4uq operator >> (Vec4uq const & a, int32_t b) {
+static inline Vec4uq operator >> (Vec4uq const a, int32_t b) {
     return a >> (uint32_t)b;
 }
-
 // vector operator >>= : shift right artihmetic
 static inline Vec4uq & operator >>= (Vec4uq & a, uint32_t b) {
     a = a >> b;
     return a;
-} 
+}
 
 // vector operator << : shift left all elements
-static inline Vec4uq operator << (Vec4uq const & a, uint32_t b) {
+static inline Vec4uq operator << (Vec4uq const a, uint32_t b) {
     return Vec4uq ((Vec4q)a << (int32_t)b);
 }
-
 // vector operator << : shift left all elements
-static inline Vec4uq operator << (Vec4uq const & a, int32_t b) {
+static inline Vec4uq operator << (Vec4uq const a, int32_t b) {
     return Vec4uq ((Vec4q)a << b);
 }
 
 // vector operator > : returns true for elements for which a > b (unsigned)
-static inline Vec4qb operator > (Vec4uq const & a, Vec4uq const & b) {
-//#if defined ( __XOP__ ) // AMD XOP instruction set
+static inline Vec4qb operator > (Vec4uq const a, Vec4uq const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_cmp_epu64_mask (a, b, 6);
+#else
     __m256i sign64 = Vec4uq(0x8000000000000000);
     __m256i aflip  = _mm256_xor_si256(a, sign64);
     __m256i bflip  = _mm256_xor_si256(b, sign64);
     Vec4q   cmp    = _mm256_cmpgt_epi64(aflip,bflip);
     return Vec4qb(cmp);
+#endif
 }
 
 // vector operator < : returns true for elements for which a < b (unsigned)
-static inline Vec4qb operator < (Vec4uq const & a, Vec4uq const & b) {
+static inline Vec4qb operator < (Vec4uq const a, Vec4uq const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_cmp_epu64_mask (a, b, 1);
+#else
     return b > a;
+#endif
 }
 
 // vector operator >= : returns true for elements for which a >= b (unsigned)
-static inline Vec4qb operator >= (Vec4uq const & a, Vec4uq const & b) {
+static inline Vec4qb operator >= (Vec4uq const a, Vec4uq const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_cmp_epu64_mask (a, b, 5);
+#else
     return  Vec4qb(Vec4q(~(b > a)));
+#endif
 }
 
 // vector operator <= : returns true for elements for which a <= b (unsigned)
-static inline Vec4qb operator <= (Vec4uq const & a, Vec4uq const & b) {
+static inline Vec4qb operator <= (Vec4uq const a, Vec4uq const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_cmp_epu64_mask (a, b, 2);
+#else
     return b >= a;
+#endif
 }
 
 // vector operator & : bitwise and
-static inline Vec4uq operator & (Vec4uq const & a, Vec4uq const & b) {
+static inline Vec4uq operator & (Vec4uq const a, Vec4uq const b) {
     return Vec4uq(Vec256b(a) & Vec256b(b));
 }
-static inline Vec4uq operator && (Vec4uq const & a, Vec4uq const & b) {
+static inline Vec4uq operator && (Vec4uq const a, Vec4uq const b) {
     return a & b;
 }
 
 // vector operator | : bitwise or
-static inline Vec4uq operator | (Vec4uq const & a, Vec4uq const & b) {
+static inline Vec4uq operator | (Vec4uq const a, Vec4uq const b) {
     return Vec4uq(Vec256b(a) | Vec256b(b));
 }
-static inline Vec4uq operator || (Vec4uq const & a, Vec4uq const & b) {
+static inline Vec4uq operator || (Vec4uq const a, Vec4uq const b) {
     return a | b;
 }
 
 // vector operator ^ : bitwise xor
-static inline Vec4uq operator ^ (Vec4uq const & a, Vec4uq const & b) {
+static inline Vec4uq operator ^ (Vec4uq const a, Vec4uq const b) {
     return Vec4uq(Vec256b(a) ^ Vec256b(b));
 }
 
@@ -3220,47 +3780,78 @@ static inline Vec4uq operator ^ (Vec4uq const & a, Vec4uq const & b) {
 
 // Select between two operands. Corresponds to this pseudocode:
 // for (int i = 0; i < 4; i++) result[i] = s[i] ? a[i] : b[i];
-// Each word in s must be either 0 (false) or -1 (true). No other values are allowed.
-// (s is signed)
-static inline Vec4uq select (Vec4qb const & s, Vec4uq const & a, Vec4uq const & b) {
+static inline Vec4uq select (Vec4qb const s, Vec4uq const a, Vec4uq const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_mask_mov_epi64(b, s, a);
+#else
     return selectb(s,a,b);
+#endif
 }
 
 // Conditional add: For all vector elements i: result[i] = f[i] ? (a[i] + b[i]) : a[i]
-static inline Vec4uq if_add (Vec4qb const & f, Vec4uq const & a, Vec4uq const & b) {
+static inline Vec4uq if_add (Vec4qb const f, Vec4uq const a, Vec4uq const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_mask_add_epi64 (a, f, a, b);
+#else
     return a + (Vec4uq(f) & b);
+#endif
 }
 
-// Horizontal add: Calculates the sum of all vector elements.
-// Overflow will wrap around
-static inline uint64_t horizontal_add (Vec4uq const & a) {
-    return horizontal_add((Vec4q)a);
+// Conditional subtract
+static inline Vec4uq if_sub (Vec4qb const f, Vec4uq const a, Vec4uq const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_mask_sub_epi64 (a, f, a, b);
+#else
+    return a - (Vec4uq(f) & b);
+#endif
+}
+
+// Conditional multiply
+static inline Vec4uq if_mul (Vec4qb const f, Vec4uq const a, Vec4uq const b) {
+#if INSTRSET >= 10  // compact boolean vectors
+    return _mm256_mask_mullo_epi64 (a, f, a, b);
+#else
+    return select(f, a*b, a);
+#endif
+}
+
+// Horizontal add: Calculates the sum of all vector elements. Overflow will wrap around
+static inline uint64_t horizontal_add (Vec4uq const a) {
+    return (uint64_t)horizontal_add((Vec4q)a);
 }
 
 // Horizontal add extended: Calculates the sum of all vector elements.
-// Elements are sing/zero extended before adding to avoid overflow
-static inline int64_t horizontal_add_x (Vec8i const & a) {
-    __m256i signs = _mm256_srai_epi32(a,31);                          // sign of all elements
-    Vec4q   a01   = _mm256_unpacklo_epi32(a,signs);                   // sign-extended a0, a1, a4, a5
-    Vec4q   a23   = _mm256_unpackhi_epi32(a,signs);                   // sign-extended a2, a3, a6, a7
+// Elements are sign/zero extended before adding to avoid overflow
+static inline int64_t horizontal_add_x (Vec8i const a) {
+    __m256i signs = _mm256_srai_epi32(a,31);               // sign of all elements
+    Vec4q   a01   = _mm256_unpacklo_epi32(a,signs);        // sign-extended a0, a1, a4, a5
+    Vec4q   a23   = _mm256_unpackhi_epi32(a,signs);        // sign-extended a2, a3, a6, a7
     return  horizontal_add(a01 + a23);
 }
 
-static inline uint64_t horizontal_add_x (Vec8ui const & a) {
-    __m256i zero  = _mm256_setzero_si256();                           // 0
-    __m256i a01   = _mm256_unpacklo_epi32(a,zero);                    // zero-extended a0, a1
-    __m256i a23   = _mm256_unpackhi_epi32(a,zero);                    // zero-extended a2, a3
-    return horizontal_add(Vec4q(a01) + Vec4q(a23));
+static inline uint64_t horizontal_add_x (Vec8ui const a) {
+    __m256i zero  = _mm256_setzero_si256();                // 0
+    __m256i a01   = _mm256_unpacklo_epi32(a,zero);         // zero-extended a0, a1
+    __m256i a23   = _mm256_unpackhi_epi32(a,zero);         // zero-extended a2, a3
+    return (uint64_t)horizontal_add(Vec4q(a01) + Vec4q(a23));
 }
 
 // function max: a > b ? a : b
-static inline Vec4uq max(Vec4uq const & a, Vec4uq const & b) {
+static inline Vec4uq max(Vec4uq const a, Vec4uq const b) {
+#if INSTRSET >= 10  // AVX512VL
+    return _mm256_max_epu64 (a, b);
+#else
     return Vec4uq(select(a > b, a, b));
+#endif
 }
 
 // function min: a < b ? a : b
-static inline Vec4uq min(Vec4uq const & a, Vec4uq const & b) {
+static inline Vec4uq min(Vec4uq const a, Vec4uq const b) {
+#if INSTRSET >= 10  // AVX512VL
+    return _mm256_min_epu64 (a, b);
+#else
     return Vec4uq(select(a > b, b, a));
+#endif
 }
 
 
@@ -3271,618 +3862,421 @@ static inline Vec4uq min(Vec4uq const & a, Vec4uq const & b) {
 ******************************************************************************
 *
 * These permute functions can reorder the elements of a vector and optionally
-* set some elements to zero. 
-*
-* The indexes are inserted as template parameters in <>. These indexes must be
-* constants. Each template parameter is an index to the element you want to select.
-* An index of -1 will generate zero. An index of -256 means don't care.
-*
-* Example:
-* Vec8i a(10,11,12,13,14,15,16,17);      // a is (10,11,12,13,14,15,16,17)
-* Vec8i b;
-* b = permute8i<0,2,7,7,-1,-1,1,1>(a);   // b is (10,12,17,17, 0, 0,11,11)
+* set some elements to zero. See Vectori128.h for description
 *
-* A lot of the code here is metaprogramming aiming to find the instructions
-* that best fit the template parameters and instruction set. The metacode
-* will be reduced out to leave only a few vector instructions in release
-* mode with optimization on.
 *****************************************************************************/
 
 // Permute vector of 4 64-bit integers.
-// Index -1 gives 0, index -256 means don't care.
 template <int i0, int i1, int i2, int i3 >
-static inline Vec4q permute4q(Vec4q const & a) {
-
-    // Combine indexes into a single bitfield, with 8 bits for each
-    const int m1 = (i0 & 3) | (i1 & 3) << 8 | (i2 & 3) << 16 | (i3 & 3) << 24;
-
-    // Mask to zero out negative indexes
-    const int mz = (i0<0 ? 0 : 0xFF) | (i1<0 ? 0 : 0xFF) << 8 | (i2<0 ? 0 : 0xFF) << 16 | (i3<0 ? 0 : 0xFF) << 24;
-
-    // zeroing needed
-    const bool dozero = ((i0|i1|i2|i3) & 0x80) != 0;
-
-    if (((m1 ^ 0x03020100) & mz) == 0) {
-        // no shuffling
-        if (dozero) {
-            // zero some elements
-            const __m256i maskz = constant8i <
-                i0 < 0 ? 0 : -1, i0 < 0 ? 0 : -1, i1 < 0 ? 0 : -1, i1 < 0 ? 0 : -1, 
-                i2 < 0 ? 0 : -1, i2 < 0 ? 0 : -1, i3 < 0 ? 0 : -1, i3 < 0 ? 0 : -1 > ();                    
-            return _mm256_and_si256(a, maskz);
+static inline Vec4q permute4(Vec4q const a) {
+    int constexpr indexs[4] = { i0, i1, i2, i3 };          // indexes as array
+    __m256i y = a;                                         // result
+    // get flags for possibilities that fit the permutation pattern
+    constexpr uint64_t flags = perm_flags<Vec4q>(indexs);
+
+    static_assert((flags & perm_outofrange) == 0, "Index out of range in permute function");
+
+    if constexpr ((flags & perm_allzero) != 0) return _mm256_setzero_si256();  // just return zero
+
+    if constexpr ((flags & perm_largeblock) != 0) {        // permute 128-bit blocks
+        constexpr EList<int, 2> L = largeblock_perm<4>(indexs); // get 128-bit permute pattern
+        constexpr int j0 = L.a[0];
+        constexpr int j1 = L.a[1];
+#ifndef ZEXT_MISSING
+        if constexpr (j0 == 0 && j1 == -1 && !(flags & perm_addz)) { // zero extend
+            return _mm256_zextsi128_si256(_mm256_castsi256_si128(y));
+        }
+        if constexpr (j0 == 1 && j1 < 0 && !(flags & perm_addz)) {   // extract upper part, zero extend
+            return _mm256_zextsi128_si256(_mm256_extracti128_si256(y, 1));
+        }
+#endif
+        if constexpr ((flags & perm_perm) != 0  && !(flags & perm_zeroing)) {
+            return _mm256_permute2x128_si256(y, y, (j0 & 1) | (j1 & 1) << 4);
         }
-        return a;                                 // do nothing
     }
-
-    if (((m1 ^ 0x02020000) & 0x02020202 & mz) == 0) {
-        // no exchange of data between low and high half
-
-        if (((m1 ^ (m1 >> 16)) & 0x0101 & mz & (mz >> 16)) == 0 && !dozero) {
-            // same pattern in low and high half. use VPSHUFD
-            const int sd = (((i0>=0)?(i0&1):(i2&1)) * 10 + 4) | (((i1>=0)?(i1&1):(i3&1)) * 10 + 4) << 4;
-            return _mm256_shuffle_epi32(a, sd);
+    if constexpr ((flags & perm_perm) != 0) {              // permutation needed
+        if constexpr ((flags & perm_same_pattern) != 0) {  // same pattern in both lanes
+            // try to fit various instructions
+            if constexpr ((flags & perm_punpckh) != 0) {   // fits punpckhi
+                y = _mm256_unpackhi_epi64(y, y);
+            }
+            else if constexpr ((flags & perm_punpckl)!=0){ // fits punpcklo
+                y = _mm256_unpacklo_epi64(y, y);
+            }
+            else { // general permute
+                y = _mm256_shuffle_epi32(a, uint8_t(flags >> perm_ipattern));
+            }
+        }
+        else if constexpr ((flags & perm_broadcast) != 0 && (flags >> perm_rot_count) == 0) {
+            y = _mm256_broadcastq_epi64(_mm256_castsi256_si128(y)); // broadcast first element
+        }
+        else {  // different patterns in two lanes
+#if INSTRSET >= 10  // AVX512VL
+            if constexpr ((flags & perm_rotate_big) != 0) { // fits big rotate
+                constexpr uint8_t rot = uint8_t(flags >> perm_rot_count); // rotation count
+                return _mm256_maskz_alignr_epi64 (zero_mask<4>(indexs), y, y, rot);
+            }
+            else { // full permute
+                constexpr uint8_t mms = (i0 & 3) | (i1 & 3) << 2 | (i2 & 3) << 4 | (i3 & 3) << 6;
+                constexpr __mmask8 mmz = zero_mask<4>(indexs);//(i0 >= 0) | (i1 >= 0) << 1 | (i2 >= 0) << 2 | (i3 >= 0) << 3;
+                return _mm256_maskz_permutex_epi64(mmz, a, mms);
+            }
+#else
+            // full permute
+            constexpr int ms = (i0 & 3) | (i1 & 3) << 2 | (i2 & 3) << 4 | (i3 & 3) << 6;
+            y = _mm256_permute4x64_epi64(a, ms);
+#endif
         }
-
-        // use VPSHUFB
-        const __m256i mm = constant8i <
-            i0 < 0 ? -1 : (i0 & 1) * 0x08080808 + 0x03020100,
-            i0 < 0 ? -1 : (i0 & 1) * 0x08080808 + 0x07060504,
-            i1 < 0 ? -1 : (i1 & 1) * 0x08080808 + 0x03020100,
-            i1 < 0 ? -1 : (i1 & 1) * 0x08080808 + 0x07060504,
-            i2 < 0 ? -1 : (i2 & 1) * 0x08080808 + 0x03020100,
-            i2 < 0 ? -1 : (i2 & 1) * 0x08080808 + 0x07060504,
-            i3 < 0 ? -1 : (i3 & 1) * 0x08080808 + 0x03020100,
-            i3 < 0 ? -1 : (i3 & 1) * 0x08080808 + 0x07060504 > ();
-        return _mm256_shuffle_epi8(a, mm);
     }
-
-    // general case. Use VPERMQ
-    const int ms = (i0 & 3) | (i1 & 3) << 2 | (i2 & 3) << 4 | (i3 & 3) << 6;        
-    __m256i t1 = _mm256_permute4x64_epi64(a, ms);
-
-    if (dozero) {
-        // zero some elements
-        const __m256i maskz = constant8i <
-            i0 < 0 ? 0 : -1, i0 < 0 ? 0 : -1, i1 < 0 ? 0 : -1, i1 < 0 ? 0 : -1, 
-            i2 < 0 ? 0 : -1, i2 < 0 ? 0 : -1, i3 < 0 ? 0 : -1, i3 < 0 ? 0 : -1 > ();                    
-        return _mm256_and_si256(t1, maskz);
+    if constexpr ((flags & perm_zeroing) != 0) {
+        // additional zeroing needed
+#if INSTRSET >= 10  // use compact mask
+        y = _mm256_maskz_mov_epi64(zero_mask<4>(indexs), y);
+#else  // use broad mask
+        constexpr EList <int64_t, 4> bm = zero_mask_broad<Vec4q>(indexs);
+        y = _mm256_and_si256(Vec4q().load(bm.a), y);
+#endif
     }
-    return t1;
+    return y;
 }
 
 template <int i0, int i1, int i2, int i3>
-static inline Vec4uq permute4uq(Vec4uq const & a) {
-    return Vec4uq (permute4q<i0,i1,i2,i3> (a));
+static inline Vec4uq permute4(Vec4uq const a) {
+    return Vec4uq (permute4<i0,i1,i2,i3> (Vec4q(a)));
 }
 
+
 // Permute vector of 8 32-bit integers.
-// Index -1 gives 0, index -256 means don't care.
 template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7 >
-static inline Vec8i permute8i(Vec8i const & a) {
-
-    // Combine indexes into a single bitfield, with 4 bits for each
-    const int m1 = (i0&7) | (i1&7)<<4 | (i2&7)<<8 | (i3&7)<<12
-        | (i4&7)<<16 | (i5&7)<<20 | (i6&7)<<24 | (i7&7)<<28;
+static inline Vec8i permute8(Vec8i const a) {
+    int constexpr indexs[8] = { i0, i1, i2, i3, i4, i5, i6, i7 }; // indexes as array
+    __m256i y = a;                                         // result
+    // get flags for possibilities that fit the permutation pattern
+    constexpr uint64_t flags = perm_flags<Vec8i>(indexs);
 
-    // Mask to zero out negative indexes
-    const int mz = (i0<0?0:0xF) | (i1<0?0:0xF)<<4 | (i2<0?0:0xF)<<8 | (i3<0?0:0xF)<<12
-        | (i4<0?0:0xF)<<16 | (i5<0?0:0xF)<<20 | (i6<0?0:0xF)<<24 | (i7<0?0:0xF)<<28;
+    static_assert((flags & perm_outofrange) == 0, "Index out of range in permute function");
 
-    // zeroing needed
-    const bool dozero = ((i0|i1|i2|i3|i4|i5|i6|i7) & 0x80) != 0;
+    if constexpr ((flags & perm_allzero) != 0) return _mm256_setzero_si256();  // just return zero
 
-    __m256i t1, mask;
+    if constexpr ((flags & perm_perm) != 0) {              // permutation needed
 
-    if (((m1 ^ 0x76543210) & mz) == 0) {
-        // no shuffling
-        if (dozero) {
-            // zero some elements
-            mask = constant8i <
-                i0 < 0 ? 0 : -1, i1 < 0 ? 0 : -1, i2 < 0 ? 0 : -1, i3 < 0 ? 0 : -1, 
-                i4 < 0 ? 0 : -1, i5 < 0 ? 0 : -1, i6 < 0 ? 0 : -1, i7 < 0 ? 0 : -1 > ();                    
-            return _mm256_and_si256(a, mask);
+        if constexpr ((flags & perm_largeblock) != 0) {    // use larger permutation
+            constexpr EList<int, 4> L = largeblock_perm<8>(indexs); // permutation pattern
+            y = permute4 <L.a[0], L.a[1], L.a[2], L.a[3]> (Vec4q(a));
+            if (!(flags & perm_addz)) return y;            // no remaining zeroing
         }
-        return a;                                 // do nothing
-    }
-
-    // Check if we can use 64-bit permute. Even numbered indexes must be even and odd numbered
-    // indexes must be equal to the preceding index + 1, except for negative indexes.
-    if (((m1 ^ 0x10101010) & 0x11111111 & mz) == 0 && ((m1 ^ m1 >> 4) & 0x0E0E0E0E & mz & mz >> 4) == 0) {
-
-        const bool partialzero = int((i0^i1)|(i2^i3)|(i4^i5)|(i6^i7)) < 0; // part of a 64-bit block is zeroed
-        const int blank1 = partialzero ? -0x100 : -1;  // ignore or zero
-        const int n0 = i0 > 0 ? i0 /2 : i1 > 0 ? i1 /2 : blank1;  // indexes for 64 bit blend
-        const int n1 = i2 > 0 ? i2 /2 : i3 > 0 ? i3 /2 : blank1;
-        const int n2 = i4 > 0 ? i4 /2 : i5 > 0 ? i5 /2 : blank1;
-        const int n3 = i6 > 0 ? i6 /2 : i7 > 0 ? i7 /2 : blank1;
-        // do 64-bit permute
-        t1 = permute4q<n0,n1,n2,n3> (Vec4q(a));
-        if (blank1 == -1 || !dozero) {    
-            return  t1;
+        else if constexpr ((flags & perm_same_pattern) != 0) {  // same pattern in both lanes
+            // try to fit various instructions
+            if constexpr ((flags & perm_punpckh) != 0) {   // fits punpckhi
+                y = _mm256_unpackhi_epi32(y, y);
+            }
+            else if constexpr ((flags & perm_punpckl)!=0){ // fits punpcklo
+                y = _mm256_unpacklo_epi32(y, y);
+            }
+            else { // general permute
+                y = _mm256_shuffle_epi32(a, uint8_t(flags >> perm_ipattern));
+            }
         }
-        // need more zeroing
-        mask = constant8i <
-            i0 < 0 ? 0 : -1, i1 < 0 ? 0 : -1, i2 < 0 ? 0 : -1, i3 < 0 ? 0 : -1, 
-            i4 < 0 ? 0 : -1, i5 < 0 ? 0 : -1, i6 < 0 ? 0 : -1, i7 < 0 ? 0 : -1 > ();                    
-        return _mm256_and_si256(t1, mask);
-    }
-
-    if (((m1 ^ 0x44440000) & 0x44444444 & mz) == 0) {
-        // no exchange of data between low and high half
-
-        if (((m1 ^ (m1 >> 16)) & 0x3333 & mz & (mz >> 16)) == 0 && !dozero) {
-            // same pattern in low and high half. use VPSHUFD
-            const int sd = ((i0>=0)?(i0&3):(i4&3)) | ((i1>=0)?(i1&3):(i5&3)) << 2 |
-                ((i2>=0)?(i2&3):(i6&3)) << 4 | ((i3>=0)?(i3&3):(i7&3)) << 6;
-            return _mm256_shuffle_epi32(a, sd);
+#if INSTRSET >= 10
+        else if constexpr ((flags & perm_broadcast) != 0 && (flags & perm_zeroing) == 0) {
+            constexpr uint8_t e = flags >> perm_rot_count & 0xF; // broadcast one element
+            if constexpr (e > 0) {
+                y = _mm256_alignr_epi32(y, y, e);
+            }
+            return _mm256_broadcastd_epi32(_mm256_castsi256_si128(y));
+#else
+        else if constexpr ((flags & perm_broadcast) != 0 && (flags & perm_zeroing) == 0 && (flags >> perm_rot_count == 0)) {
+            return _mm256_broadcastd_epi32(_mm256_castsi256_si128(y)); // broadcast first element
+#endif
         }
-
-        // use VPSHUFB
-        mask = constant8i <
-            i0 < 0 ? -1 : (i0 & 3) * 0x04040404 + 0x03020100,
-            i1 < 0 ? -1 : (i1 & 3) * 0x04040404 + 0x03020100,
-            i2 < 0 ? -1 : (i2 & 3) * 0x04040404 + 0x03020100,
-            i3 < 0 ? -1 : (i3 & 3) * 0x04040404 + 0x03020100,
-            i4 < 0 ? -1 : (i4 & 3) * 0x04040404 + 0x03020100,
-            i5 < 0 ? -1 : (i5 & 3) * 0x04040404 + 0x03020100,
-            i6 < 0 ? -1 : (i6 & 3) * 0x04040404 + 0x03020100,
-            i7 < 0 ? -1 : (i7 & 3) * 0x04040404 + 0x03020100 > ();
-        return _mm256_shuffle_epi8(a, mask);
-    }
-
-    // general case. Use VPERMD
-    mask = constant8i <
-        i0 < 0 ? -1 : (i0 & 7), i1 < 0 ? -1 : (i1 & 7),
-        i2 < 0 ? -1 : (i2 & 7), i3 < 0 ? -1 : (i3 & 7),
-        i4 < 0 ? -1 : (i4 & 7), i5 < 0 ? -1 : (i5 & 7),
-        i6 < 0 ? -1 : (i6 & 7), i7 < 0 ? -1 : (i7 & 7) > ();
-#if defined (_MSC_VER) && _MSC_VER < 1700 && ! defined(__INTEL_COMPILER)
-    // bug in MS VS 11 beta: operands in wrong order. fixed in v. 11.0
-    t1 = _mm256_permutevar8x32_epi32(mask, a);   // ms
-#elif defined (GCC_VERSION) && GCC_VERSION <= 40700 && !defined(__INTEL_COMPILER) && !defined(__clang__)
-    // Gcc 4.7.0 also has operands in wrong order. fixed in version 4.7.1
-    t1 = _mm256_permutevar8x32_epi32(mask, a);   // GCC
-#else
-    t1 = _mm256_permutevar8x32_epi32(a, mask);   // no-bug version
-#endif
-
-    if (dozero) {
-        // zero some elements
-        mask = constant8i <
-            i0 < 0 ? 0 : -1, i1 < 0 ? 0 : -1, i2 < 0 ? 0 : -1, i3 < 0 ? 0 : -1, 
-            i4 < 0 ? 0 : -1, i5 < 0 ? 0 : -1, i6 < 0 ? 0 : -1, i7 < 0 ? 0 : -1 > ();                    
-        return _mm256_and_si256(t1, mask);
-    }
-    return t1;
+        else if constexpr ((flags & perm_zext) != 0) {
+            y = _mm256_cvtepu32_epi64(_mm256_castsi256_si128(y));  // zero extension
+            if constexpr ((flags & perm_addz2) == 0) return y;
+        }
+#if INSTRSET >= 10  // AVX512VL
+        else if constexpr ((flags & perm_compress) != 0) {
+            y = _mm256_maskz_compress_epi32(__mmask8(compress_mask(indexs)), y); // compress
+            if constexpr ((flags & perm_addz2) == 0) return y;
+        }
+        else if constexpr ((flags & perm_expand) != 0) {
+            y = _mm256_maskz_expand_epi32(__mmask8(expand_mask(indexs)), y); // expand
+            if constexpr ((flags & perm_addz2) == 0) return y;
+        }
+#endif
+        else {  // different patterns in two lanes
+#if INSTRSET >= 10  // AVX512VL
+            if constexpr ((flags & perm_rotate_big) != 0) { // fits big rotate
+                constexpr uint8_t rot = uint8_t(flags >> perm_rot_count); // rotation count
+                return _mm256_maskz_alignr_epi32(zero_mask<8>(indexs), y, y, rot);
+            }
+            else
+#endif
+            if constexpr ((flags & perm_cross_lane) == 0) {  // no lane crossing. Use pshufb
+                constexpr EList <int8_t, 32> bm = pshufb_mask<Vec8i>(indexs);
+                return _mm256_shuffle_epi8(a, Vec8i().load(bm.a));
+            }
+            // full permute needed
+            __m256i permmask = constant8ui <
+                i0 & 7, i1 & 7, i2 & 7, i3 & 7, i4 & 7, i5 & 7, i6 & 7, i7 & 7 > ();
+#if INSTRSET >= 10  // AVX512VL
+            return _mm256_maskz_permutexvar_epi32 (zero_mask<8>(indexs), permmask, y);
+#else
+            y =_mm256_permutevar8x32_epi32(y, permmask);
+#endif
+        }
+    }
+    if constexpr ((flags & perm_zeroing) != 0) {
+        // additional zeroing needed
+#if INSTRSET >= 10  // use compact mask
+        y = _mm256_maskz_mov_epi32(zero_mask<8>(indexs), y);
+#else  // use broad mask
+        constexpr EList <int32_t, 8> bm = zero_mask_broad<Vec8i>(indexs);
+        y = _mm256_and_si256(Vec8i().load(bm.a), y);
+#endif
+    }
+    return y;
 }
 
 template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7 >
-static inline Vec8ui permute8ui(Vec8ui const & a) {
-    return Vec8ui (permute8i<i0,i1,i2,i3,i4,i5,i6,i7> (a));
+static inline Vec8ui permute8(Vec8ui const a) {
+    return Vec8ui (permute8<i0,i1,i2,i3,i4,i5,i6,i7> (Vec8i(a)));
 }
 
+
 // Permute vector of 16 16-bit integers.
-// Index -1 gives 0, index -256 means don't care.
+// Index -1 gives 0, index V_DC means don't care.
 template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7,
     int i8, int i9, int i10, int i11, int i12, int i13, int i14, int i15 >
-static inline Vec16s permute16s(Vec16s const & a) {
-
-    // Combine indexes 0 - 7 into a single bitfield, with 4 bits for each
-    const int mlo = (i0&0xF) | (i1&0xF)<<4 | (i2&0xF)<<8 | (i3&0xF)<<12 
-        | (i4&0xF)<<16 | (i5&0xF)<<20 | (i6&0xF)<<24 | (i7&0xF)<<28; 
-
-    // Combine indexes 8 - 15 into a single bitfield, with 4 bits for each
-    const int mhi = (i8&0xF) | (i9&0xF)<<4 | (i10&0xF)<<8 | (i11&0xF)<<12 
-        | (i12&0xF)<<16 | (i13&0xF)<<20 | (i14&0xF)<<24 | (i15&0xF)<<28;
-
-    // Mask to zero out negative indexes 0 - 7
-    const int zlo = (i0<0?0:0xF) | (i1<0?0:0xF)<<4 | (i2<0?0:0xF)<<8 | (i3<0?0:0xF)<<12
-        | (i4<0?0:0xF)<<16 | (i5<0?0:0xF)<<20 | (i6<0?0:0xF)<<24 | (i7<0?0:0xF)<<28;
-
-    // Mask to zero out negative indexes 8 - 15
-    const int zhi = (i8<0?0:0xF) | (i9<0?0:0xF)<<4 | (i10<0?0:0xF)<<8 | (i11<0?0:0xF)<<12
-        | (i12<0?0:0xF)<<16 | (i13<0?0:0xF)<<20 | (i14<0?0:0xF)<<24 | (i15<0?0:0xF)<<28;
-
-    // zeroing needed
-    const bool dozero = ((i0|i1|i2|i3|i4|i5|i6|i7|i8|i9|i10|i11|i12|i13|i14|i15) & 0x80) != 0;
-
-    __m256i t1, mask;
-
-    // special case: all zero
-    if (zlo == 0 && zhi == 0) {
-        return _mm256_setzero_si256();
-    }
-
-    // special case: rotate 128 bits
-    if (i0>=0 && i0 < 16 && i1 ==((i0+1)&7) && i2 ==((i0+2)&7) && i3 ==((i0+3)&7) && i4 ==((i0+4)&7) && i5 ==((i0+5)&7) && i6 ==((i0+6)&7) && i7 ==((i0+7)&7) 
-        && i8 ==i0 +8 && i9 ==i1 +8 && i10==i2 +8 && i11==i3 +8 && i12==i4 +8 && i13==i5 +8 && i14==i6 +8 && i15==i7 +8 ) {
-        return _mm256_alignr_epi8(a, a, (i0 & 7) * 2);
-    }
+static inline Vec16s permute16(Vec16s const a) {
+    int constexpr indexs[16] = {  // indexes as array
+        i0, i1, i2, i3, i4, i5, i6, i7, i8, i9, i10, i11, i12, i13, i14, i15 };
+    __m256i y = a;  // result
+    // get flags for possibilities that fit the permutation pattern
+    constexpr uint64_t flags = perm_flags<Vec16s>(indexs);
 
-    // special case: rotate 256 bits
-    if (i0>=0 && i0 < 16     && i1 ==((i0+1 )&15) && i2 ==((i0+2 )&15) && i3 ==((i0+3 )&15) && i4 ==((i0+4 )&15) && i5 ==((i0+5 )&15) && i6 ==((i0+6 )&15) && i7 ==((i0+7 )&15) 
-        && i8 ==((i0+8 )&15) && i9 ==((i0+9 )&15) && i10==((i0+10)&15) && i11==((i0+11)&15) && i12==((i0+12)&15) && i13==((i0+13)&15) && i14==((i0+14)&15) && i15==((i0+15)&15)) {
-        t1 = _mm256_permute4x64_epi64(a, 0x4E);
-        return _mm256_alignr_epi8(a, t1, (i0 & 7) * 2);
-    }
-
-    // special case: no exchange of data between 64-bit sections, and same pattern in low and high 128 bits:
-    // can use VPSHUFLW or VPSHUFHW
-    if (((mlo ^ 0x44440000) & 0xCCCCCCCC & zlo) == 0 && ((mhi ^ 0xCCCC8888) & 0xCCCCCCCC & zhi) == 0
-        && ((mlo ^ mhi) & 0x33333333 & zlo & zhi) == 0) {
+    static_assert((flags & perm_outofrange) == 0, "Index out of range in permute function");
 
-        const int slo = (i0 >= 0 ? (i0&3) : i8 >= 0 ? (i8&3) : 0) | (i1 >= 0 ? (i1&3) : i9 >= 0 ? (i9&3) : 1) << 2 
-            | (i2 >= 0 ? (i2&3) : i10 >= 0 ? (i10&3) : 2) << 4 | (i3 >= 0 ? (i3&3) : i11 >= 0 ? (i11&3) : 3) << 6;
+    if constexpr ((flags & perm_allzero) != 0) return _mm256_setzero_si256();  // just return zero
 
-        const int shi = (i4 >= 0 ? (i4&3) : i12 >= 0 ? (i12&3) : 0) | (i5 >= 0 ? (i5&3) : i13 >= 0 ? (i13&3) : 1) << 2 
-            | (i6 >= 0 ? (i6&3) : i14 >= 0 ? (i14&3) : 2) << 4 | (i7 >= 0 ? (i7&3) : i15 >= 0 ? (i15&3) : 3) << 6;
+    if constexpr ((flags & perm_perm) != 0) {                   // permutation needed
 
-        if (shi == 0xE4 && slo == 0xE4) {             // no permute
-            if (dozero) {
-                // zero some elements
-                const __m256i maskz = constant8i<
-                    int((i0 <0?0:0xFFFF) | (i1 <0?0:0xFFFF0000)),
-                    int((i2 <0?0:0xFFFF) | (i3 <0?0:0xFFFF0000)),
-                    int((i4 <0?0:0xFFFF) | (i5 <0?0:0xFFFF0000)),
-                    int((i6 <0?0:0xFFFF) | (i7 <0?0:0xFFFF0000)),
-                    int((i8 <0?0:0xFFFF) | (i9 <0?0:0xFFFF0000)),
-                    int((i10<0?0:0xFFFF) | (i11<0?0:0xFFFF0000)),
-                    int((i12<0?0:0xFFFF) | (i13<0?0:0xFFFF0000)),
-                    int((i14<0?0:0xFFFF) | (i15<0?0:0xFFFF0000)) > ();                    
-                return _mm256_and_si256(a, maskz);
-            }
-            return a;                                 // do nothing
+        if constexpr ((flags & perm_largeblock) != 0) {         // use larger permutation
+            constexpr EList<int, 8> L = largeblock_perm<16>(indexs); // permutation pattern
+            y = permute8 <L.a[0], L.a[1], L.a[2], L.a[3], L.a[4], L.a[5], L.a[6], L.a[7]> (Vec8i(a));
+            if (!(flags & perm_addz)) return y;                 // no remaining zeroing
         }
-        if (shi == 0xE4 && !dozero) {
-            return _mm256_shufflelo_epi16(a, slo);    // low permute only
+        else if constexpr ((flags & perm_same_pattern) != 0) {  // same pattern in both lanes
+            // try to fit various instructions
+            if constexpr ((flags & perm_punpckh) != 0) {        // fits punpckhi
+                y = _mm256_unpackhi_epi16(y, y);
+            }
+            else if constexpr ((flags & perm_punpckl)!=0){      // fits punpcklo
+                y = _mm256_unpacklo_epi16(y, y);
+            }
+            else if constexpr ((flags & perm_rotate) != 0) {    // fits palignr. rotate within lanes
+                y = _mm256_alignr_epi8(a, a, (flags >> perm_rot_count) & 0xF);
+            }
+#if INSTRSET >= 10  // use rotate
+            else if constexpr ((flags & perm_swap) != 0) {      // swap adjacent elements. rotate 32 bits
+                y = _mm256_rol_epi32(a, 16);
+            }
+#endif
+            else {
+                // flags for 16 bit permute instructions
+                constexpr uint64_t flags16 = perm16_flags<Vec16s>(indexs);
+                constexpr bool L2L = (flags16 & 1) != 0;        // from low  to low  64-bit part
+                constexpr bool H2H = (flags16 & 2) != 0;        // from high to high 64-bit part
+                constexpr bool H2L = (flags16 & 4) != 0;        // from high to low  64-bit part
+                constexpr bool L2H = (flags16 & 8) != 0;        // from low  to high 64-bit part
+                constexpr uint8_t pL2L = uint8_t(flags16 >> 32);// low  to low  permute pattern
+                constexpr uint8_t pH2H = uint8_t(flags16 >> 40);// high to high permute pattern
+                constexpr uint8_t noperm = 0xE4;                // pattern for no permute
+                if constexpr (!H2L && !L2H) {                   // simple case. no crossing of 64-bit boundary
+                    if constexpr (L2L && pL2L != noperm) {
+                        y = _mm256_shufflelo_epi16(y, pL2L);    // permute low 64-bits
+                    }
+                    if constexpr (H2H && pH2H != noperm) {
+                        y = _mm256_shufflehi_epi16(y, pH2H);    // permute high 64-bits
+                    }
+                }
+                else {  // use pshufb
+                    constexpr EList <int8_t, 32> bm = pshufb_mask<Vec16s>(indexs);
+                    return _mm256_shuffle_epi8(a, Vec16s().load(bm.a));
+                }
+            }
         }
-        if (slo == 0xE4 && !dozero) {
-            return _mm256_shufflehi_epi16(a, shi);    // high permute only
+        else {  // different patterns in two lanes
+            if constexpr ((flags & perm_zext) != 0) {     // fits zero extension
+                y = _mm256_cvtepu16_epi32(_mm256_castsi256_si128(y));  // zero extension
+                if constexpr ((flags & perm_addz2) == 0) return y;
+            }
+#if INSTRSET >= 10 && defined (__AVX512VBMI2__)
+            else if constexpr ((flags & perm_compress) != 0) {
+                y = _mm256_maskz_compress_epi16(__mmask16(compress_mask(indexs)), y); // compress
+                if constexpr ((flags & perm_addz2) == 0) return y;
+            }
+            else if constexpr ((flags & perm_expand) != 0) {
+                y = _mm256_maskz_expand_epi16(__mmask16(expand_mask(indexs)), y); // expand
+                if constexpr ((flags & perm_addz2) == 0) return y;
+            }
+#endif  // AVX512VBMI2
+            else if constexpr ((flags & perm_cross_lane) == 0) {     // no lane crossing. Use pshufb
+                constexpr EList <int8_t, 32> bm = pshufb_mask<Vec16s>(indexs);
+                return _mm256_shuffle_epi8(a, Vec16s().load(bm.a));
+            }
+            else if constexpr ((flags & perm_rotate_big) != 0) {// fits full rotate
+                constexpr uint8_t rot = uint8_t(flags >> perm_rot_count) * 2; // rotate count
+                __m256i swap = _mm256_permute4x64_epi64(a,0x4E);// swap 128-bit halves
+                if (rot <= 16) {
+                    y = _mm256_alignr_epi8(swap, y, rot);
+                }
+                else {
+                    y = _mm256_alignr_epi8(y, swap, rot & 15);
+                }
+            }
+            else if constexpr ((flags & perm_broadcast) != 0 && (flags >> perm_rot_count) == 0) {
+                y = _mm256_broadcastw_epi16(_mm256_castsi256_si128(y)); // broadcast first element
+            }
+            else {  // full permute needed
+#if INSTRSET >= 10  // AVX512VL
+                constexpr EList <int16_t, 16> bm = perm_mask_broad<Vec16s>(indexs);
+                y = _mm256_permutexvar_epi16(Vec16s().load(bm.a), y);
+#else           // no full permute instruction available
+                __m256i swap = _mm256_permute4x64_epi64(y,0x4E);// swap high and low 128-bit lane
+                constexpr EList <int8_t, 32> bm1 = pshufb_mask<Vec16s, 1>(indexs);
+                constexpr EList <int8_t, 32> bm2 = pshufb_mask<Vec16s, 0>(indexs);
+                __m256i r1 = _mm256_shuffle_epi8(swap, Vec16s().load(bm1.a));
+                __m256i r2 = _mm256_shuffle_epi8(y,    Vec16s().load(bm2.a));
+                return       _mm256_or_si256(r1, r2);
+#endif
+            }
         }
     }
-    
-    // Check if we can use 32-bit permute. Even numbered indexes must be even and odd numbered
-    // indexes must be equal to the preceding index + 1, except for negative indexes.
-    if (((mlo ^ 0x10101010) & 0x11111111 & zlo) == 0 && ((mlo ^ mlo >> 4) & 0x0E0E0E0E & zlo & zlo >> 4) == 0 &&
-        ((mhi ^ 0x10101010) & 0x11111111 & zhi) == 0 && ((mhi ^ mhi >> 4) & 0x0E0E0E0E & zhi & zhi >> 4) == 0 ) {
-
-        const bool partialzero = int((i0^i1)|(i2^i3)|(i4^i5)|(i6^i7)|(i8^i9)|(i10^i11)|(i12^i13)|(i14^i15)) < 0; // part of a 32-bit block is zeroed
-        const int blank1 = partialzero ? -0x100 : -1;  // ignore or zero
-        const int n0 = i0 > 0 ? i0 /2 : i1 > 0 ? i1 /2 : blank1;  // indexes for 64 bit blend
-        const int n1 = i2 > 0 ? i2 /2 : i3 > 0 ? i3 /2 : blank1;
-        const int n2 = i4 > 0 ? i4 /2 : i5 > 0 ? i5 /2 : blank1;
-        const int n3 = i6 > 0 ? i6 /2 : i7 > 0 ? i7 /2 : blank1;
-        const int n4 = i8 > 0 ? i8 /2 : i9 > 0 ? i9 /2 : blank1;
-        const int n5 = i10> 0 ? i10/2 : i11> 0 ? i11/2 : blank1;
-        const int n6 = i12> 0 ? i12/2 : i13> 0 ? i13/2 : blank1;
-        const int n7 = i14> 0 ? i14/2 : i15> 0 ? i15/2 : blank1;
-        // do 32-bit permute
-        t1 = permute8i<n0,n1,n2,n3,n4,n5,n6,n7> (Vec8i(a));
-        if (blank1 == -1 || !dozero) {    
-            return  t1;
-        }
-        // need more zeroing
-        mask = constant8i<
-            int((i0 <0?0:0xFFFF) | (i1 <0?0:0xFFFF0000)),
-            int((i2 <0?0:0xFFFF) | (i3 <0?0:0xFFFF0000)),
-            int((i4 <0?0:0xFFFF) | (i5 <0?0:0xFFFF0000)),
-            int((i6 <0?0:0xFFFF) | (i7 <0?0:0xFFFF0000)),
-            int((i8 <0?0:0xFFFF) | (i9 <0?0:0xFFFF0000)),
-            int((i10<0?0:0xFFFF) | (i11<0?0:0xFFFF0000)),
-            int((i12<0?0:0xFFFF) | (i13<0?0:0xFFFF0000)),
-            int((i14<0?0:0xFFFF) | (i15<0?0:0xFFFF0000)) > ();                    
-        return _mm256_and_si256(t1, mask);
-    }
-
-    // special case: all elements from same half
-    if ((mlo & 0x88888888 & zlo) == 0 && ((mhi ^ 0x88888888) & 0x88888888 & zhi) == 0) {
-        mask = constant8i<
-            (i0  < 0 ? 0xFFFF : (i0  & 7) * 0x202 + 0x100) | (i1  < 0 ? 0xFFFF : (i1  & 7) * 0x202 + 0x100) << 16,
-            (i2  < 0 ? 0xFFFF : (i2  & 7) * 0x202 + 0x100) | (i3  < 0 ? 0xFFFF : (i3  & 7) * 0x202 + 0x100) << 16,
-            (i4  < 0 ? 0xFFFF : (i4  & 7) * 0x202 + 0x100) | (i5  < 0 ? 0xFFFF : (i5  & 7) * 0x202 + 0x100) << 16,
-            (i6  < 0 ? 0xFFFF : (i6  & 7) * 0x202 + 0x100) | (i7  < 0 ? 0xFFFF : (i7  & 7) * 0x202 + 0x100) << 16,
-            (i8  < 0 ? 0xFFFF : (i8  & 7) * 0x202 + 0x100) | (i9  < 0 ? 0xFFFF : (i9  & 7) * 0x202 + 0x100) << 16,
-            (i10 < 0 ? 0xFFFF : (i10 & 7) * 0x202 + 0x100) | (i11 < 0 ? 0xFFFF : (i11 & 7) * 0x202 + 0x100) << 16,
-            (i12 < 0 ? 0xFFFF : (i12 & 7) * 0x202 + 0x100) | (i13 < 0 ? 0xFFFF : (i13 & 7) * 0x202 + 0x100) << 16,
-            (i14 < 0 ? 0xFFFF : (i14 & 7) * 0x202 + 0x100) | (i15 < 0 ? 0xFFFF : (i15 & 7) * 0x202 + 0x100) << 16 > ();
-        return _mm256_shuffle_epi8(a, mask);
-    }
-
-    // special case: all elements from low half
-    if ((mlo & 0x88888888 & zlo) == 0 && (mhi & 0x88888888 & zhi) == 0) {
-        mask = constant8i<
-            (i0  < 0 ? 0xFFFF : (i0  & 7) * 0x202 + 0x100) | (i1  < 0 ? 0xFFFF : (i1  & 7) * 0x202 + 0x100) << 16,
-            (i2  < 0 ? 0xFFFF : (i2  & 7) * 0x202 + 0x100) | (i3  < 0 ? 0xFFFF : (i3  & 7) * 0x202 + 0x100) << 16,
-            (i4  < 0 ? 0xFFFF : (i4  & 7) * 0x202 + 0x100) | (i5  < 0 ? 0xFFFF : (i5  & 7) * 0x202 + 0x100) << 16,
-            (i6  < 0 ? 0xFFFF : (i6  & 7) * 0x202 + 0x100) | (i7  < 0 ? 0xFFFF : (i7  & 7) * 0x202 + 0x100) << 16,
-            (i8  < 0 ? 0xFFFF : (i8  & 7) * 0x202 + 0x100) | (i9  < 0 ? 0xFFFF : (i9  & 7) * 0x202 + 0x100) << 16,
-            (i10 < 0 ? 0xFFFF : (i10 & 7) * 0x202 + 0x100) | (i11 < 0 ? 0xFFFF : (i11 & 7) * 0x202 + 0x100) << 16,
-            (i12 < 0 ? 0xFFFF : (i12 & 7) * 0x202 + 0x100) | (i13 < 0 ? 0xFFFF : (i13 & 7) * 0x202 + 0x100) << 16,
-            (i14 < 0 ? 0xFFFF : (i14 & 7) * 0x202 + 0x100) | (i15 < 0 ? 0xFFFF : (i15 & 7) * 0x202 + 0x100) << 16 > ();
-        t1 = _mm256_inserti128_si256(a, _mm256_castsi256_si128(a), 1);  // low, low
-        return _mm256_shuffle_epi8(t1, mask);
-    }
-
-    // special case: all elements from high half
-    if (((mlo ^ 0x88888888) & 0x88888888 & zlo) == 0 && ((mhi ^ 0x88888888) & 0x88888888 & zhi) == 0) {
-        mask = constant8i<
-            (i0  < 0 ? 0xFFFF : (i0  & 7) * 0x202 + 0x100) | (i1  < 0 ? 0xFFFF : (i1  & 7) * 0x202 + 0x100) << 16,
-            (i2  < 0 ? 0xFFFF : (i2  & 7) * 0x202 + 0x100) | (i3  < 0 ? 0xFFFF : (i3  & 7) * 0x202 + 0x100) << 16,
-            (i4  < 0 ? 0xFFFF : (i4  & 7) * 0x202 + 0x100) | (i5  < 0 ? 0xFFFF : (i5  & 7) * 0x202 + 0x100) << 16,
-            (i6  < 0 ? 0xFFFF : (i6  & 7) * 0x202 + 0x100) | (i7  < 0 ? 0xFFFF : (i7  & 7) * 0x202 + 0x100) << 16,
-            (i8  < 0 ? 0xFFFF : (i8  & 7) * 0x202 + 0x100) | (i9  < 0 ? 0xFFFF : (i9  & 7) * 0x202 + 0x100) << 16,
-            (i10 < 0 ? 0xFFFF : (i10 & 7) * 0x202 + 0x100) | (i11 < 0 ? 0xFFFF : (i11 & 7) * 0x202 + 0x100) << 16,
-            (i12 < 0 ? 0xFFFF : (i12 & 7) * 0x202 + 0x100) | (i13 < 0 ? 0xFFFF : (i13 & 7) * 0x202 + 0x100) << 16,
-            (i14 < 0 ? 0xFFFF : (i14 & 7) * 0x202 + 0x100) | (i15 < 0 ? 0xFFFF : (i15 & 7) * 0x202 + 0x100) << 16 > ();
-        t1 = _mm256_permute4x64_epi64(a, 0xEE);  // high, high
-        return _mm256_shuffle_epi8(t1, mask);
-    }
-
-    // special case: all elements from opposite half
-    if (((mlo ^ 0x88888888) & 0x88888888 & zlo) == 0 && (mhi & 0x88888888 & zhi) == 0) {
-        mask = constant8i<
-            (i0  < 0 ? 0xFFFF : (i0  & 7) * 0x202 + 0x100) | (i1  < 0 ? 0xFFFF : (i1  & 7) * 0x202 + 0x100) << 16,
-            (i2  < 0 ? 0xFFFF : (i2  & 7) * 0x202 + 0x100) | (i3  < 0 ? 0xFFFF : (i3  & 7) * 0x202 + 0x100) << 16,
-            (i4  < 0 ? 0xFFFF : (i4  & 7) * 0x202 + 0x100) | (i5  < 0 ? 0xFFFF : (i5  & 7) * 0x202 + 0x100) << 16,
-            (i6  < 0 ? 0xFFFF : (i6  & 7) * 0x202 + 0x100) | (i7  < 0 ? 0xFFFF : (i7  & 7) * 0x202 + 0x100) << 16,
-            (i8  < 0 ? 0xFFFF : (i8  & 7) * 0x202 + 0x100) | (i9  < 0 ? 0xFFFF : (i9  & 7) * 0x202 + 0x100) << 16,
-            (i10 < 0 ? 0xFFFF : (i10 & 7) * 0x202 + 0x100) | (i11 < 0 ? 0xFFFF : (i11 & 7) * 0x202 + 0x100) << 16,
-            (i12 < 0 ? 0xFFFF : (i12 & 7) * 0x202 + 0x100) | (i13 < 0 ? 0xFFFF : (i13 & 7) * 0x202 + 0x100) << 16,
-            (i14 < 0 ? 0xFFFF : (i14 & 7) * 0x202 + 0x100) | (i15 < 0 ? 0xFFFF : (i15 & 7) * 0x202 + 0x100) << 16 > ();
-        t1 = _mm256_permute4x64_epi64(a, 0x4E);  // high, low
-        return _mm256_shuffle_epi8(t1, mask);
-    }
-
-    // general case: elements from both halves
-    const __m256i mmsame = constant8i<
-            ((i0 ^8) < 8 ? 0xFFFF : (i0  & 7) * 0x202 + 0x100) | ((i1 ^8) < 8 ? 0xFFFF : (i1  & 7) * 0x202 + 0x100) << 16,
-            ((i2 ^8) < 8 ? 0xFFFF : (i2  & 7) * 0x202 + 0x100) | ((i3 ^8) < 8 ? 0xFFFF : (i3  & 7) * 0x202 + 0x100) << 16,
-            ((i4 ^8) < 8 ? 0xFFFF : (i4  & 7) * 0x202 + 0x100) | ((i5 ^8) < 8 ? 0xFFFF : (i5  & 7) * 0x202 + 0x100) << 16,
-            ((i6 ^8) < 8 ? 0xFFFF : (i6  & 7) * 0x202 + 0x100) | ((i7 ^8) < 8 ? 0xFFFF : (i7  & 7) * 0x202 + 0x100) << 16,
-            (i8  < 8 ? 0xFFFF : (i8  & 7) * 0x202 + 0x100) | (i9  < 8 ? 0xFFFF : (i9  & 7) * 0x202 + 0x100) << 16,
-            (i10 < 8 ? 0xFFFF : (i10 & 7) * 0x202 + 0x100) | (i11 < 8 ? 0xFFFF : (i11 & 7) * 0x202 + 0x100) << 16,
-            (i12 < 8 ? 0xFFFF : (i12 & 7) * 0x202 + 0x100) | (i13 < 8 ? 0xFFFF : (i13 & 7) * 0x202 + 0x100) << 16,
-            (i14 < 8 ? 0xFFFF : (i14 & 7) * 0x202 + 0x100) | (i15 < 8 ? 0xFFFF : (i15 & 7) * 0x202 + 0x100) << 16 > ();
-
-    const __m256i mmopposite = constant8i<
-            (i0  < 8 ? 0xFFFF : (i0  & 7) * 0x202 + 0x100) | (i1  < 8 ? 0xFFFF : (i1  & 7) * 0x202 + 0x100) << 16,
-            (i2  < 8 ? 0xFFFF : (i2  & 7) * 0x202 + 0x100) | (i3  < 8 ? 0xFFFF : (i3  & 7) * 0x202 + 0x100) << 16,
-            (i4  < 8 ? 0xFFFF : (i4  & 7) * 0x202 + 0x100) | (i5  < 8 ? 0xFFFF : (i5  & 7) * 0x202 + 0x100) << 16,
-            (i6  < 8 ? 0xFFFF : (i6  & 7) * 0x202 + 0x100) | (i7  < 8 ? 0xFFFF : (i7  & 7) * 0x202 + 0x100) << 16,
-            ((i8 ^8) < 8 ? 0xFFFF : (i8  & 7) * 0x202 + 0x100) | ((i9 ^8) < 8 ? 0xFFFF : (i9  & 7) * 0x202 + 0x100) << 16,
-            ((i10^8) < 8 ? 0xFFFF : (i10 & 7) * 0x202 + 0x100) | ((i11^8) < 8 ? 0xFFFF : (i11 & 7) * 0x202 + 0x100) << 16,
-            ((i12^8) < 8 ? 0xFFFF : (i12 & 7) * 0x202 + 0x100) | ((i13^8) < 8 ? 0xFFFF : (i13 & 7) * 0x202 + 0x100) << 16,
-            ((i14^8) < 8 ? 0xFFFF : (i14 & 7) * 0x202 + 0x100) | ((i15^8) < 8 ? 0xFFFF : (i15 & 7) * 0x202 + 0x100) << 16 > ();
-
-    __m256i topp = _mm256_permute4x64_epi64(a, 0x4E);  // high, low
-    __m256i r1   = _mm256_shuffle_epi8(topp, mmopposite);
-    __m256i r2   = _mm256_shuffle_epi8(a, mmsame);
-    return         _mm256_or_si256(r1, r2);
+    if constexpr ((flags & perm_zeroing) != 0) {           // additional zeroing needed
+#if INSTRSET >= 10  // use compact mask
+        y = _mm256_maskz_mov_epi16(zero_mask<16>(indexs), y);
+#else               // use broad mask
+        constexpr EList <int16_t, 16> bm = zero_mask_broad<Vec16s>(indexs);
+        y = _mm256_and_si256(Vec16s().load(bm.a), y);
+#endif
+    }
+    return y;
 }
 
 template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7,
     int i8, int i9, int i10, int i11, int i12, int i13, int i14, int i15 >
-static inline Vec16us permute16us(Vec16us const & a) {
-    return Vec16us (permute16s<i0,i1,i2,i3,i4,i5,i6,i7,i8,i9,i10,i11,i12,i13,i14,i15> (a));
+static inline Vec16us permute16(Vec16us const a) {
+    return Vec16us (permute16<i0,i1,i2,i3,i4,i5,i6,i7,i8,i9,i10,i11,i12,i13,i14,i15> (Vec16s(a)));
 }
 
-template <int i0,  int i1,  int i2,  int i3,  int i4,  int i5,  int i6,  int i7, 
+
+template <int i0,  int i1,  int i2,  int i3,  int i4,  int i5,  int i6,  int i7,
           int i8,  int i9,  int i10, int i11, int i12, int i13, int i14, int i15,
           int i16, int i17, int i18, int i19, int i20, int i21, int i22, int i23,
           int i24, int i25, int i26, int i27, int i28, int i29, int i30, int i31 >
-static inline Vec32c permute32c(Vec32c const & a) {
-
-    // collect bit 4 of each index
-    const int m1 = 
-        (i0 &16)>>4  | (i1 &16)>>3  | (i2 &16)>>2  | (i3 &16)>>1  | (i4 &16)     | (i5 &16)<<1  | (i6 &16)<<2  | (i7 &16)<<3  | 
-        (i8 &16)<<4  | (i9 &16)<<5  | (i10&16)<<6  | (i11&16)<<7  | (i12&16)<<8  | (i13&16)<<9  | (i14&16)<<10 | (i15&16)<<11 | 
-        (i16&16)<<12 | (i17&16)<<13 | (i18&16)<<14 | (i19&16)<<15 | (i20&16)<<16 | (i21&16)<<17 | (i22&16)<<18 | (i23&16)<<19 | 
-        (i24&16)<<20 | (i25&16)<<21 | (i26&16)<<22 | (i27&16)<<23 | (i28&16)<<24 | (i29&16)<<25 | (i30&16)<<26 | (i31&16)<<27 ;
-
-    // check which elements to set to zero
-    const int mz = ~ (
-        (i0 <0)     | (i1 <0)<<1  | (i2 <0)<<2  | (i3 <0)<<3  | (i4 <0)<<4  | (i5 <0)<<5  | (i6 <0)<<6  | (i7 <0)<<7  | 
-        (i8 <0)<<8  | (i9 <0)<<9  | (i10<0)<<10 | (i11<0)<<11 | (i12<0)<<12 | (i13<0)<<13 | (i14<0)<<14 | (i15<0)<<15 | 
-        (i16<0)<<16 | (i17<0)<<17 | (i18<0)<<18 | (i19<0)<<19 | (i20<0)<<20 | (i21<0)<<21 | (i22<0)<<22 | (i23<0)<<23 | 
-        (i24<0)<<24 | (i25<0)<<25 | (i26<0)<<26 | (i27<0)<<27 | (i28<0)<<28 | (i29<0)<<29 | (i30<0)<<30 | (i31<0)<<31 );
-
-    // Combine indexes 0-7, 8-15, 16-23, 24-31 into a bitfields, with 8 bits for each
-    const uint64_t g0 = (i0 &0x1F)|(i1 &0x1F)<<8|(i2 &0x1F)<<16|(i3 &0x1F)<<24|(i4 &0x1FLL)<<32|(i5 &0x1FLL)<<40|(i6 &0x1FLL)<<48|(i7 &0x1FLL)<<56;
-    const uint64_t g1 = (i8 &0x1F)|(i9 &0x1F)<<8|(i10&0x1F)<<16|(i11&0x1F)<<24|(i12&0x1FLL)<<32|(i13&0x1FLL)<<40|(i14&0x1FLL)<<48|(i15&0x1FLL)<<56; 
-    const uint64_t g2 = (i16&0x1F)|(i17&0x1F)<<8|(i18&0x1F)<<16|(i19&0x1F)<<24|(i20&0x1FLL)<<32|(i21&0x1FLL)<<40|(i22&0x1FLL)<<48|(i23&0x1FLL)<<56; 
-    const uint64_t g3 = (i24&0x1F)|(i25&0x1F)<<8|(i26&0x1F)<<16|(i27&0x1F)<<24|(i28&0x1FLL)<<32|(i29&0x1FLL)<<40|(i30&0x1FLL)<<48|(i31&0x1FLL)<<56; 
-    
-    // Masks to zero out negative indexes
-    const uint64_t z0 = (i0 <0?0:0xFF)|(i1 <0?0:0xFF)<<8|(i2 <0?0:0xFF)<<16|(i3 <0?0:0xFF)<<24|(i4 <0?0:0xFFLL)<<32|(i5 <0?0:0xFFLL)<<40|(i6 <0?0:0xFFLL)<<48|(i7 <0?0:0xFFLL)<<56;
-    const uint64_t z1 = (i8 <0?0:0xFF)|(i9 <0?0:0xFF)<<8|(i10<0?0:0xFF)<<16|(i11<0?0:0xFF)<<24|(i12<0?0:0xFFLL)<<32|(i13<0?0:0xFFLL)<<40|(i14<0?0:0xFFLL)<<48|(i15<0?0:0xFFLL)<<56;
-    const uint64_t z2 = (i16<0?0:0xFF)|(i17<0?0:0xFF)<<8|(i18<0?0:0xFF)<<16|(i19<0?0:0xFF)<<24|(i20<0?0:0xFFLL)<<32|(i21<0?0:0xFFLL)<<40|(i22<0?0:0xFFLL)<<48|(i23<0?0:0xFFLL)<<56;
-    const uint64_t z3 = (i24<0?0:0xFF)|(i25<0?0:0xFF)<<8|(i26<0?0:0xFF)<<16|(i27<0?0:0xFF)<<24|(i28<0?0:0xFFLL)<<32|(i29<0?0:0xFFLL)<<40|(i30<0?0:0xFFLL)<<48|(i31<0?0:0xFFLL)<<56;
-
-    // zeroing needed
-    const bool dozero = ((i0|i1|i2|i3|i4|i5|i6|i7|i8|i9|i10|i11|i12|i13|i14|i15|i16|i17|i18|i19|i20|i21|i22|i23|i24|i25|i26|i27|i28|i29|i30|i31) & 0x80) != 0;
-
-    __m256i t1, mask;
-
-    // special case: all zero
-    if (mz == 0) return  _mm256_setzero_si256();
-
-    // special case: no permute
-    if ((i0 <0||i0 == 0) && (i1 <0||i1 == 1) && (i2 <0||i2 == 2) && (i3 <0||i3 == 3) && (i4 <0||i4 == 4) && (i5 <0||i5 == 5) && (i6 <0||i6 == 6) && (i7 <0||i7 == 7) &&
-        (i8 <0||i8 == 8) && (i9 <0||i9 == 9) && (i10<0||i10==10) && (i11<0||i11==11) && (i12<0||i12==12) && (i13<0||i13==13) && (i14<0||i14==14) && (i15<0||i15==15) &&
-        (i16<0||i16==16) && (i17<0||i17==17) && (i18<0||i18==18) && (i19<0||i19==19) && (i20<0||i20==20) && (i21<0||i21==21) && (i22<0||i22==22) && (i23<0||i23==23) &&
-        (i24<0||i24==24) && (i25<0||i25==25) && (i26<0||i26==26) && (i27<0||i27==27) && (i28<0||i28==28) && (i29<0||i29==29) && (i30<0||i30==30) && (i31<0||i31==31)) {
-        if (dozero) {
-            // zero some elements
-            mask = constant8i <
-                int((i0 <0?0:0xFF) | (i1 <0?0:0xFF00) | (i2 <0?0:0xFF0000) | (i3 <0?0:0xFF000000)),
-                int((i4 <0?0:0xFF) | (i5 <0?0:0xFF00) | (i6 <0?0:0xFF0000) | (i7 <0?0:0xFF000000)),
-                int((i8 <0?0:0xFF) | (i9 <0?0:0xFF00) | (i10<0?0:0xFF0000) | (i11<0?0:0xFF000000)),
-                int((i12<0?0:0xFF) | (i13<0?0:0xFF00) | (i14<0?0:0xFF0000) | (i15<0?0:0xFF000000)),
-                int((i16<0?0:0xFF) | (i17<0?0:0xFF00) | (i18<0?0:0xFF0000) | (i19<0?0:0xFF000000)),
-                int((i20<0?0:0xFF) | (i21<0?0:0xFF00) | (i22<0?0:0xFF0000) | (i23<0?0:0xFF000000)),
-                int((i24<0?0:0xFF) | (i25<0?0:0xFF00) | (i26<0?0:0xFF0000) | (i27<0?0:0xFF000000)),
-                int((i28<0?0:0xFF) | (i29<0?0:0xFF00) | (i30<0?0:0xFF0000) | (i31<0?0:0xFF000000)) > ();
-            return _mm256_and_si256(a, mask);
+static inline Vec32c permute32(Vec32c const a) {
+    int constexpr indexs[32] = {  // indexes as array
+        i0,  i1,  i2,  i3,  i4,  i5,  i6,  i7,  i8,  i9,  i10, i11, i12, i13, i14, i15,
+        i16, i17, i18, i19, i20, i21, i22, i23, i24, i25, i26, i27, i28, i29, i30, i31 };
+
+    __m256i y = a;  // result
+    // get flags for possibilities that fit the permutation pattern
+    constexpr uint64_t flags = perm_flags<Vec32c>(indexs);
+
+    static_assert((flags & perm_outofrange) == 0, "Index out of range in permute function");
+
+    if constexpr ((flags & perm_allzero) != 0) return _mm256_setzero_si256();  // just return zero
+
+    if constexpr ((flags & perm_perm) != 0) {                   // permutation needed
+
+        if constexpr ((flags & perm_largeblock) != 0) {         // use larger permutation
+            constexpr EList<int, 16> L = largeblock_perm<32>(indexs); // permutation pattern
+            y = permute16 <L.a[0], L.a[1], L.a[2], L.a[3], L.a[4], L.a[5], L.a[6], L.a[7],
+                L.a[8], L.a[9], L.a[10], L.a[11], L.a[12], L.a[13], L.a[14], L.a[15]> (Vec16s(a));
+            if (!(flags & perm_addz)) return y;                 // no remaining zeroing
         }
-        return a; // do nothing
-    }
-
-    // special case: rotate 128 bits
-    if (i0>=0 && i0 < 32     && i1 ==((i0+1 )&15) && i2 ==((i0+2 )&15) && i3 ==((i0+3 )&15) && i4 ==((i0+4 )&15) && i5 ==((i0+5 )&15) && i6 ==((i0+6 )&15) && i7 ==((i0+7 )&15) 
-        && i8 ==((i0+8 )&15) && i9 ==((i0+9 )&15) && i10==((i0+10)&15) && i11==((i0+11)&15) && i12==((i0+12)&15) && i13==((i0+13)&15) && i14==((i0+14)&15) && i15==((i0+15)&15)
-        && i16==i0 +16 && i17==i1 +16 && i18==i2 +16 && i19==i3 +16 && i20==i4 +16 && i21==i5 +16 && i22==i6 +16 && i23==i7 +16 
-        && i24==i8 +16 && i25==i9 +16 && i26==i10+16 && i27==i11+16 && i28==i12+16 && i29==i13+16 && i30==i14+16 && i31==i15+16 ) {
-        return _mm256_alignr_epi8(a, a, i0 & 15);
-    }
-
-    // special case: rotate 256 bits
-    if (i0>=0 && i0 < 32     && i1 ==((i0+1 )&31) && i2 ==((i0+2 )&31) && i3 ==((i0+3 )&31) && i4 ==((i0+4 )&31) && i5 ==((i0+5 )&31) && i6 ==((i0+6 )&31) && i7 ==((i0+7 )&31) 
-        && i8 ==((i0+8 )&31) && i9 ==((i0+9 )&31) && i10==((i0+10)&31) && i11==((i0+11)&31) && i12==((i0+12)&31) && i13==((i0+13)&31) && i14==((i0+14)&31) && i15==((i0+15)&31)
-        && i16==((i0+16)&31) && i17==((i0+17)&31) && i18==((i0+18)&31) && i19==((i0+19)&31) && i20==((i0+20)&31) && i21==((i0+21)&31) && i22==((i0+22)&31) && i23==((i0+23)&31)
-        && i24==((i0+24)&31) && i25==((i0+25)&31) && i26==((i0+26)&31) && i27==((i0+27)&31) && i28==((i0+28)&31) && i29==((i0+29)&31) && i30==((i0+30)&31) && i31==((i0+31)&31)) {
-        t1 = _mm256_permute4x64_epi64(a, 0x4E);
-        return _mm256_alignr_epi8(a, t1, i0 & 15);
-    }
-
-    // Check if we can use 16-bit permute. Even numbered indexes must be even and odd numbered
-    // indexes must be equal to the preceding index + 1, except for negative indexes.
-    if (((g0 ^ 0x0100010001000100) & 0x0101010101010101 & z0) == 0 && ((g0 ^ g0 >> 8) & 0x00FE00FE00FE00FE & z0 & z0 >> 8) == 0 &&
-        ((g1 ^ 0x0100010001000100) & 0x0101010101010101 & z1) == 0 && ((g1 ^ g1 >> 8) & 0x00FE00FE00FE00FE & z1 & z1 >> 8) == 0 &&
-        ((g2 ^ 0x0100010001000100) & 0x0101010101010101 & z2) == 0 && ((g2 ^ g2 >> 8) & 0x00FE00FE00FE00FE & z2 & z2 >> 8) == 0 &&
-        ((g3 ^ 0x0100010001000100) & 0x0101010101010101 & z3) == 0 && ((g3 ^ g3 >> 8) & 0x00FE00FE00FE00FE & z3 & z3 >> 8) == 0 ) {
-    
-        const bool partialzero = int((i0^i1)|(i2^i3)|(i4^i5)|(i6^i7)|(i8^i9)|(i10^i11)|(i12^i13)|(i14^i15)
-            |(i16^i17)|(i18^i19)|(i20^i21)|(i22^i23)|(i24^i25)|(i26^i27)|(i28^i29)|(i30^i31)) < 0; // part of a 16-bit block is zeroed
-        const int blank1 = partialzero ? -0x100 : -1;  // ignore or zero
-        const int n0 = i0 > 0 ? i0 /2 : i1 > 0 ? i1 /2 : blank1;  // indexes for 64 bit blend
-        const int n1 = i2 > 0 ? i2 /2 : i3 > 0 ? i3 /2 : blank1;
-        const int n2 = i4 > 0 ? i4 /2 : i5 > 0 ? i5 /2 : blank1;
-        const int n3 = i6 > 0 ? i6 /2 : i7 > 0 ? i7 /2 : blank1;
-        const int n4 = i8 > 0 ? i8 /2 : i9 > 0 ? i9 /2 : blank1;
-        const int n5 = i10> 0 ? i10/2 : i11> 0 ? i11/2 : blank1;
-        const int n6 = i12> 0 ? i12/2 : i13> 0 ? i13/2 : blank1;
-        const int n7 = i14> 0 ? i14/2 : i15> 0 ? i15/2 : blank1;
-        const int n8 = i16> 0 ? i16/2 : i17> 0 ? i17/2 : blank1;
-        const int n9 = i18> 0 ? i18/2 : i19> 0 ? i19/2 : blank1;
-        const int n10= i20> 0 ? i20/2 : i21> 0 ? i21/2 : blank1;
-        const int n11= i22> 0 ? i22/2 : i23> 0 ? i23/2 : blank1;
-        const int n12= i24> 0 ? i24/2 : i25> 0 ? i25/2 : blank1;
-        const int n13= i26> 0 ? i26/2 : i27> 0 ? i27/2 : blank1;
-        const int n14= i28> 0 ? i28/2 : i29> 0 ? i29/2 : blank1;
-        const int n15= i30> 0 ? i30/2 : i31> 0 ? i31/2 : blank1;
-        // do 16-bit permute
-        t1 = permute16s<n0,n1,n2,n3,n4,n5,n6,n7,n8,n9,n10,n11,n12,n13,n14,n15> (Vec16s(a));
-        if (blank1 == -1 || !dozero) {    
-            return  t1;
+        else if constexpr ((flags & perm_same_pattern) != 0) {  // same pattern in both lanes
+            if constexpr ((flags & perm_punpckh) != 0) {        // fits punpckhi
+                y = _mm256_unpackhi_epi8(y, y);
+            }
+            else if constexpr ((flags & perm_punpckl)!=0){      // fits punpcklo
+                y = _mm256_unpacklo_epi8(y, y);
+            }
+            else if constexpr ((flags & perm_rotate) != 0) {    // fits palignr. rotate within lanes
+                y = _mm256_alignr_epi8(a, a, (flags >> perm_rot_count) & 0xF);
+            }
+            else { // use pshufb
+                constexpr EList <int8_t, 32> bm = pshufb_mask<Vec32c>(indexs);
+                return _mm256_shuffle_epi8(a, Vec32c().load(bm.a));
+            }
+        }
+        else {  // different patterns in two lanes
+            if constexpr ((flags & perm_zext) != 0) {     // fits zero extension
+                y = _mm256_cvtepu8_epi16(_mm256_castsi256_si128(y));  // zero extension
+                if constexpr ((flags & perm_addz2) == 0) return y;
+            }
+#if INSTRSET >= 10 && defined (__AVX512VBMI2__)
+            else if constexpr ((flags & perm_compress) != 0) {
+                y = _mm256_maskz_compress_epi8(__mmask32(compress_mask(indexs)), y); // compress
+                if constexpr ((flags & perm_addz2) == 0) return y;
+            }
+            else if constexpr ((flags & perm_expand) != 0) {
+                y = _mm256_maskz_expand_epi8(__mmask32(expand_mask(indexs)), y); // expand
+                if constexpr ((flags & perm_addz2) == 0) return y;
+            }
+#endif  // AVX512VBMI2
+            else if constexpr ((flags & perm_cross_lane) == 0) {     // no lane crossing. Use pshufb
+                constexpr EList <int8_t, 32> bm = pshufb_mask<Vec32c>(indexs);
+                return _mm256_shuffle_epi8(a, Vec32c().load(bm.a));
+            }
+            else if constexpr ((flags & perm_rotate_big) != 0) {// fits full rotate
+                constexpr uint8_t rot = uint8_t(flags >> perm_rot_count); // rotate count
+                __m256i swap = _mm256_permute4x64_epi64(a,0x4E);// swap 128-bit halves
+                if (rot <= 16) {
+                    y = _mm256_alignr_epi8(swap, y, rot);
+                }
+                else {
+                    y = _mm256_alignr_epi8(y, swap, rot & 15);
+                }
+            }
+            else if constexpr ((flags & perm_broadcast) != 0 && (flags >> perm_rot_count) == 0) {
+                y = _mm256_broadcastb_epi8(_mm256_castsi256_si128(y)); // broadcast first element
+            }
+            else {  // full permute needed
+#if INSTRSET >= 10 && defined ( __AVX512VBMI__ ) // AVX512VBMI
+                constexpr EList <int8_t, 32> bm = perm_mask_broad<Vec32c>(indexs);
+                y = _mm256_permutexvar_epi8(Vec32c().load(bm.a), y);
+#else
+                // no full permute instruction available
+                __m256i swap = _mm256_permute4x64_epi64(y, 0x4E);  // swap high and low 128-bit lane
+                constexpr EList <int8_t, 32> bm1 = pshufb_mask<Vec32c, 1>(indexs);
+                constexpr EList <int8_t, 32> bm2 = pshufb_mask<Vec32c, 0>(indexs);
+                __m256i r1 = _mm256_shuffle_epi8(swap, Vec32c().load(bm1.a));
+                __m256i r2 = _mm256_shuffle_epi8(y,    Vec32c().load(bm2.a));
+                return       _mm256_or_si256(r1, r2);
+#endif
+            }
         }
-        // need more zeroing
-        mask = constant8i <
-            int((i0 <0?0:0xFF) | (i1 <0?0:0xFF00) | (i2 <0?0:0xFF0000) | (i3 <0?0:0xFF000000)),
-            int((i4 <0?0:0xFF) | (i5 <0?0:0xFF00) | (i6 <0?0:0xFF0000) | (i7 <0?0:0xFF000000)),
-            int((i8 <0?0:0xFF) | (i9 <0?0:0xFF00) | (i10<0?0:0xFF0000) | (i11<0?0:0xFF000000)),
-            int((i12<0?0:0xFF) | (i13<0?0:0xFF00) | (i14<0?0:0xFF0000) | (i15<0?0:0xFF000000)),
-            int((i16<0?0:0xFF) | (i17<0?0:0xFF00) | (i18<0?0:0xFF0000) | (i19<0?0:0xFF000000)),
-            int((i20<0?0:0xFF) | (i21<0?0:0xFF00) | (i22<0?0:0xFF0000) | (i23<0?0:0xFF000000)),
-            int((i24<0?0:0xFF) | (i25<0?0:0xFF00) | (i26<0?0:0xFF0000) | (i27<0?0:0xFF000000)),
-            int((i28<0?0:0xFF) | (i29<0?0:0xFF00) | (i30<0?0:0xFF0000) | (i31<0?0:0xFF000000)) > ();
-        return _mm256_and_si256(a, mask);
-    } 
-
-    // special case: all elements from same half
-    if (((m1 ^ 0xFFFF0000) & mz) == 0) {
-        mask = constant8i <
-            (i0  & 0xFF) | (i1  & 0xFF) << 8 | (i2  & 0xFF) << 16 | (i3  & 0xFF) << 24,
-            (i4  & 0xFF) | (i5  & 0xFF) << 8 | (i6  & 0xFF) << 16 | (i7  & 0xFF) << 24,
-            (i8  & 0xFF) | (i9  & 0xFF) << 8 | (i10 & 0xFF) << 16 | (i11 & 0xFF) << 24,
-            (i12 & 0xFF) | (i13 & 0xFF) << 8 | (i14 & 0xFF) << 16 | (i15 & 0xFF) << 24,
-            (i16 & 0xEF) | (i17 & 0xEF) << 8 | (i18 & 0xEF) << 16 | (i19 & 0xEF) << 24,
-            (i20 & 0xEF) | (i21 & 0xEF) << 8 | (i22 & 0xEF) << 16 | (i23 & 0xEF) << 24,
-            (i24 & 0xEF) | (i25 & 0xEF) << 8 | (i26 & 0xEF) << 16 | (i27 & 0xEF) << 24,
-            (i28 & 0xEF) | (i29 & 0xEF) << 8 | (i30 & 0xEF) << 16 | (i31 & 0xEF) << 24 > ();
-        return _mm256_shuffle_epi8(a, mask);
-    }
-
-    // special case: all elements from low half
-    if ((m1 & mz) == 0) {
-        mask = constant8i <
-            (i0  & 0xFF) | (i1  & 0xFF) << 8 | (i2  & 0xFF) << 16 | (i3  & 0xFF) << 24,
-            (i4  & 0xFF) | (i5  & 0xFF) << 8 | (i6  & 0xFF) << 16 | (i7  & 0xFF) << 24,
-            (i8  & 0xFF) | (i9  & 0xFF) << 8 | (i10 & 0xFF) << 16 | (i11 & 0xFF) << 24,
-            (i12 & 0xFF) | (i13 & 0xFF) << 8 | (i14 & 0xFF) << 16 | (i15 & 0xFF) << 24,
-            (i16 & 0xFF) | (i17 & 0xFF) << 8 | (i18 & 0xFF) << 16 | (i19 & 0xFF) << 24,
-            (i20 & 0xFF) | (i21 & 0xFF) << 8 | (i22 & 0xFF) << 16 | (i23 & 0xFF) << 24,
-            (i24 & 0xFF) | (i25 & 0xFF) << 8 | (i26 & 0xFF) << 16 | (i27 & 0xFF) << 24,
-            (i28 & 0xFF) | (i29 & 0xFF) << 8 | (i30 & 0xFF) << 16 | (i31 & 0xFF) << 24 > ();
-        t1 = _mm256_inserti128_si256(a, _mm256_castsi256_si128(a), 1);  // low, low
-        return _mm256_shuffle_epi8(t1, mask);
-    }
-
-    // special case: all elements from high half
-    if (((m1 ^ 0xFFFFFFFF) & mz) == 0) {
-        mask = constant8i <
-            (i0  & 0xEF) | (i1  & 0xEF) << 8 | (i2  & 0xEF) << 16 | (i3  & 0xEF) << 24,
-            (i4  & 0xEF) | (i5  & 0xEF) << 8 | (i6  & 0xEF) << 16 | (i7  & 0xEF) << 24,
-            (i8  & 0xEF) | (i9  & 0xEF) << 8 | (i10 & 0xEF) << 16 | (i11 & 0xEF) << 24,
-            (i12 & 0xEF) | (i13 & 0xEF) << 8 | (i14 & 0xEF) << 16 | (i15 & 0xEF) << 24,
-            (i16 & 0xEF) | (i17 & 0xEF) << 8 | (i18 & 0xEF) << 16 | (i19 & 0xEF) << 24,
-            (i20 & 0xEF) | (i21 & 0xEF) << 8 | (i22 & 0xEF) << 16 | (i23 & 0xEF) << 24,
-            (i24 & 0xEF) | (i25 & 0xEF) << 8 | (i26 & 0xEF) << 16 | (i27 & 0xEF) << 24,
-            (i28 & 0xEF) | (i29 & 0xEF) << 8 | (i30 & 0xEF) << 16 | (i31 & 0xEF) << 24 > ();
-        t1 = _mm256_permute4x64_epi64(a, 0xEE);  // high, high
-        return _mm256_shuffle_epi8(t1, mask);
-    }
-
-    // special case: all elements from opposite half
-    if (((m1 ^ 0x0000FFFF) & mz) == 0) {
-        mask = constant8i<
-            (i0  & 0xEF) | (i1  & 0xEF) << 8 | (i2  & 0xEF) << 16 | (i3  & 0xEF) << 24,
-            (i4  & 0xEF) | (i5  & 0xEF) << 8 | (i6  & 0xEF) << 16 | (i7  & 0xEF) << 24,
-            (i8  & 0xEF) | (i9  & 0xEF) << 8 | (i10 & 0xEF) << 16 | (i11 & 0xEF) << 24,
-            (i12 & 0xEF) | (i13 & 0xEF) << 8 | (i14 & 0xEF) << 16 | (i15 & 0xEF) << 24,
-            (i16 & 0xFF) | (i17 & 0xFF) << 8 | (i18 & 0xFF) << 16 | (i19 & 0xFF) << 24,
-            (i20 & 0xFF) | (i21 & 0xFF) << 8 | (i22 & 0xFF) << 16 | (i23 & 0xFF) << 24,
-            (i24 & 0xFF) | (i25 & 0xFF) << 8 | (i26 & 0xFF) << 16 | (i27 & 0xFF) << 24,
-            (i28 & 0xFF) | (i29 & 0xFF) << 8 | (i30 & 0xFF) << 16 | (i31 & 0xFF) << 24 > ();
-
-        t1 = _mm256_permute4x64_epi64(a, 0x4E);  // high, low
-        return _mm256_shuffle_epi8(t1, mask);
-    }
-
-    // general case: elements from both halves
-    const __m256i mmsame = constant8i <
-        ((i0 &0xF0)?0xFF:(i0 &15)) | ((i1 &0xF0)?0xFF:(i1 &15)) << 8 | ((i2 &0xF0)?0xFF:(i2 &15)) << 16 | ((i3 &0xF0)?0xFF:(i3 &15)) << 24, 
-        ((i4 &0xF0)?0xFF:(i4 &15)) | ((i5 &0xF0)?0xFF:(i5 &15)) << 8 | ((i6 &0xF0)?0xFF:(i6 &15)) << 16 | ((i7 &0xF0)?0xFF:(i7 &15)) << 24, 
-        ((i8 &0xF0)?0xFF:(i8 &15)) | ((i9 &0xF0)?0xFF:(i9 &15)) << 8 | ((i10&0xF0)?0xFF:(i10&15)) << 16 | ((i11&0xF0)?0xFF:(i11&15)) << 24, 
-        ((i12&0xF0)?0xFF:(i12&15)) | ((i13&0xF0)?0xFF:(i13&15)) << 8 | ((i14&0xF0)?0xFF:(i14&15)) << 16 | ((i15&0xF0)?0xFF:(i15&15)) << 24,
-        ((i16&0xF0)!=0x10?0xFF:(i16&15)) | ((i17&0xF0)!=0x10?0xFF:(i17&15)) << 8 | ((i18&0xF0)!=0x10?0xFF:(i18&15)) << 16 | ((i19&0xF0)!=0x10?0xFF:(i19&15)) << 24, 
-        ((i20&0xF0)!=0x10?0xFF:(i20&15)) | ((i21&0xF0)!=0x10?0xFF:(i21&15)) << 8 | ((i22&0xF0)!=0x10?0xFF:(i22&15)) << 16 | ((i23&0xF0)!=0x10?0xFF:(i23&15)) << 24, 
-        ((i24&0xF0)!=0x10?0xFF:(i24&15)) | ((i25&0xF0)!=0x10?0xFF:(i25&15)) << 8 | ((i26&0xF0)!=0x10?0xFF:(i26&15)) << 16 | ((i27&0xF0)!=0x10?0xFF:(i27&15)) << 24, 
-        ((i28&0xF0)!=0x10?0xFF:(i28&15)) | ((i29&0xF0)!=0x10?0xFF:(i29&15)) << 8 | ((i30&0xF0)!=0x10?0xFF:(i30&15)) << 16 | ((i31&0xF0)!=0x10?0xFF:(i31&15)) << 24 > ();
-
-    const __m256i mmopposite = constant8i <
-        ((i0 &0xF0)!=0x10?0xFF:(i0 &15)) | ((i1 &0xF0)!=0x10?0xFF:(i1 &15)) << 8 | ((i2 &0xF0)!=0x10?0xFF:(i2 &15)) << 16 | ((i3 &0xF0)!=0x10?0xFF:(i3 &15)) << 24, 
-        ((i4 &0xF0)!=0x10?0xFF:(i4 &15)) | ((i5 &0xF0)!=0x10?0xFF:(i5 &15)) << 8 | ((i6 &0xF0)!=0x10?0xFF:(i6 &15)) << 16 | ((i7 &0xF0)!=0x10?0xFF:(i7 &15)) << 24, 
-        ((i8 &0xF0)!=0x10?0xFF:(i8 &15)) | ((i9 &0xF0)!=0x10?0xFF:(i9 &15)) << 8 | ((i10&0xF0)!=0x10?0xFF:(i10&15)) << 16 | ((i11&0xF0)!=0x10?0xFF:(i11&15)) << 24, 
-        ((i12&0xF0)!=0x10?0xFF:(i12&15)) | ((i13&0xF0)!=0x10?0xFF:(i13&15)) << 8 | ((i14&0xF0)!=0x10?0xFF:(i14&15)) << 16 | ((i15&0xF0)!=0x10?0xFF:(i15&15)) << 24,
-        ((i16&0xF0)?0xFF:(i16&15)) | ((i17&0xF0)?0xFF:(i17&15)) << 8 | ((i18&0xF0)?0xFF:(i18&15)) << 16 | ((i19&0xF0)?0xFF:(i19&15)) << 24, 
-        ((i20&0xF0)?0xFF:(i20&15)) | ((i21&0xF0)?0xFF:(i21&15)) << 8 | ((i22&0xF0)?0xFF:(i22&15)) << 16 | ((i23&0xF0)?0xFF:(i23&15)) << 24, 
-        ((i24&0xF0)?0xFF:(i24&15)) | ((i25&0xF0)?0xFF:(i25&15)) << 8 | ((i26&0xF0)?0xFF:(i26&15)) << 16 | ((i27&0xF0)?0xFF:(i27&15)) << 24, 
-        ((i28&0xF0)?0xFF:(i28&15)) | ((i29&0xF0)?0xFF:(i29&15)) << 8 | ((i30&0xF0)?0xFF:(i30&15)) << 16 | ((i31&0xF0)?0xFF:(i31&15)) << 24 > ();
-
-    __m256i topp = _mm256_permute4x64_epi64(a, 0x4E);  // high, low
-    __m256i r1   = _mm256_shuffle_epi8(topp, mmopposite);
-    __m256i r2   = _mm256_shuffle_epi8(a, mmsame);
-    return         _mm256_or_si256(r1, r2);
+    }
+    if constexpr ((flags & perm_zeroing) != 0) { // additional zeroing needed
+#if INSTRSET >= 10  // use compact mask
+        y = _mm256_maskz_mov_epi8(zero_mask<32>(indexs), y);
+#else  // use broad mask
+        constexpr EList <int8_t, 32> bm = zero_mask_broad<Vec32c>(indexs);
+        y = _mm256_and_si256(Vec32c().load(bm.a), y);
+#endif
+    }
+    return y;
 }
 
 template <
-    int i0,  int i1,  int i2,  int i3,  int i4,  int i5,  int i6,  int i7, 
+    int i0,  int i1,  int i2,  int i3,  int i4,  int i5,  int i6,  int i7,
     int i8,  int i9,  int i10, int i11, int i12, int i13, int i14, int i15,
     int i16, int i17, int i18, int i19, int i20, int i21, int i22, int i23,
     int i24, int i25, int i26, int i27, int i28, int i29, int i30, int i31 >
-    static inline Vec32uc permute32uc(Vec32uc const & a) {
-        return Vec32uc (permute32c<i0,i1,i2,i3,i4,i5,i6,i7,i8,i9,i10,i11,i12,i13,i14,i15,    
-            i16,i17,i18,i19,i20,i21,i22,i23,i24,i25,i26,i27,i28,i29,i30,i31> (a));
+    static inline Vec32uc permute32(Vec32uc const a) {
+        return Vec32uc (permute32<i0,i1,i2,i3,i4,i5,i6,i7,i8,i9,i10,i11,i12,i13,i14,i15,
+            i16,i17,i18,i19,i20,i21,i22,i23,i24,i25,i26,i27,i28,i29,i30,i31> (Vec32c(a)));
 }
 
 
@@ -3890,624 +4284,359 @@ template <
 *
 *          Vector blend functions
 *
-******************************************************************************
-*
-* These blend functions can mix elements from two different vectors and
-* optionally set some elements to zero. 
-*
-* The indexes are inserted as template parameters in <>. These indexes must be
-* constants. Each template parameter is an index to the element you want to 
-* select, where higher indexes indicate an element from the second source
-* vector. For example, if each vector has 8 elements, then indexes 0 - 7
-* will select an element from the first vector and indexes 8 - 15 will select 
-* an element from the second vector. A negative index will generate zero.
-*
-* Example:
-* Vec8i a(100,101,102,103,104,105,106,107); // a is (100, 101, 102, 103, 104, 105, 106, 107)
-* Vec8i b(200,201,202,203,204,205,206,207); // b is (200, 201, 202, 203, 204, 205, 206, 207)
-* Vec8i c;
-* c = blend8i<1,0,9,8,7,-1,15,15> (a,b);    // c is (101, 100, 201, 200, 107,   0, 207, 207)
-*
-* A lot of the code here is metaprogramming aiming to find the instructions
-* that best fit the template parameters and instruction set. The metacode
-* will be reduced out to leave only a few vector instructions in release
-* mode with optimization on.
 *****************************************************************************/
 
-template <int i0,  int i1,  int i2,  int i3> 
-static inline Vec4q blend4q(Vec4q const & a, Vec4q const & b) {  
-
-    // Combine indexes into a single bitfield, with 8 bits for each
-    const int m1 = (i0 & 7) | (i1 & 7) << 8 | (i2 & 7) << 16 | (i3 & 7) << 24;
-
-    // Mask to zero out negative indexes
-    const int mz = (i0<0 ? 0 : 0xFF) | (i1<0 ? 0 : 0xFF) << 8 | (i2<0 ? 0 : 0xFF) << 16 | (i3<0 ? 0 : 0xFF) << 24;
-
-    // zeroing needed. An index of -0x100 means don't care
-    const bool dozero = ((i0|i1|i2|i3) & 0x80) != 0;
-
-    __m256i t1, mask;
-
-    // special case: 128 bit blend/permute
-    if (((m1 ^ 0x01000100) & 0x01010101 & mz) == 0 && (((m1 + 0x00010001) ^ (m1 >> 8)) & 0x00FF00FF & mz & mz >> 8) == 0) {
-        {
-            const int j0 = i0 >= 0 ? i0 / 2 : i1 >= 0 ? i1 / 2 : 4;  // index for low 128 bits
-            const int j1 = i2 >= 0 ? i2 / 2 : i3 >= 0 ? i3 / 2 : 4;  // index for high 128 bits
-            const bool partialzero = int((i0 ^ i1) | (i2 ^ i3)) < 0; // part of a 128-bit block is zeroed
-
-            switch (j0 | j1 << 4) {
-            case 0x00:
-                t1 = _mm256_inserti128_si256(a, _mm256_castsi256_si128(a), 1);  break;
-            case 0x02:
-                t1 = _mm256_inserti128_si256(b, _mm256_castsi256_si128(a), 1);  break;
-            case 0x04:
-                if (dozero && !partialzero) return _mm256_inserti128_si256(_mm256_setzero_si256(), _mm256_castsi256_si128(a), 1);
-                t1 = _mm256_inserti128_si256(a, _mm256_castsi256_si128(a), 1);  break;
-            case 0x12:
-                t1 = _mm256_inserti128_si256(a, _mm256_castsi256_si128(b), 0);  break;
-            case 0x14:
-                if (dozero && !partialzero) return _mm256_inserti128_si256(a,_mm_setzero_si128(), 0);
-                t1 = a;  break;
-            case 0x01: case 0x10: case 0x11: // all from a
-                return permute4q <i0, i1, i2, i3> (a);
-            case 0x20:
-                t1 = _mm256_inserti128_si256(a, _mm256_castsi256_si128(b), 1);  break;
-            case 0x22:
-                t1 = _mm256_inserti128_si256(b, _mm256_castsi256_si128(b), 1);  break;
-            case 0x24:
-                if (dozero && !partialzero) return _mm256_inserti128_si256(_mm256_setzero_si256(), _mm256_castsi256_si128(b), 1);
-                t1 = _mm256_inserti128_si256(b, _mm256_castsi256_si128(b), 1);  break;
-            case 0x30:
-                t1 = _mm256_inserti128_si256(b, _mm256_castsi256_si128(a), 0);  break;
-            case 0x34:
-                if (dozero && !partialzero) return _mm256_inserti128_si256(b,_mm_setzero_si128(), 0);
-                t1 = b;  break;
-            case 0x23: case 0x32: case 0x33:  // all from b
-                return permute4q <i0^4, i1^4, i2^4, i3^4> (b);
-            case 0x40:
-                if (dozero && !partialzero) return _mm256_castsi128_si256(_mm_and_si128(_mm256_castsi256_si128(a),_mm256_castsi256_si128(a)));
-                t1 = a;  break;
-            case 0x42:
-                if (dozero && !partialzero) return _mm256_castsi128_si256(_mm_and_si128(_mm256_castsi256_si128(b),_mm256_castsi256_si128(b)));
-                t1 = b;  break;
-            case 0x44:
-                return _mm256_setzero_si256();
-            default:
-                t1 = _mm256_permute2x128_si256(a, b, (j0&0x0F) | (j1&0x0F) << 4);
-            }
-        }
-        RETURNORZERO:
-        if (dozero) {
-            // zero some elements
-            const __m256i maskz = constant8i <
-                i0 < 0 ? 0 : -1, i0 < 0 ? 0 : -1, i1 < 0 ? 0 : -1, i1 < 0 ? 0 : -1, 
-                i2 < 0 ? 0 : -1, i2 < 0 ? 0 : -1, i3 < 0 ? 0 : -1, i3 < 0 ? 0 : -1 > ();
-            return _mm256_and_si256(t1, maskz);
-        }
-        return t1;
-    }
-
-    // special case: all from a
-    if ((m1 & 0x04040404 & mz) == 0) {
-        return permute4q <i0, i1, i2, i3> (a);
-    }
+// permute and blend Vec4q
+template <int i0, int i1, int i2, int i3>
+static inline Vec4q blend4(Vec4q const a, Vec4q const b) {
+    int constexpr indexs[4] = { i0, i1, i2, i3 };          // indexes as array
+    __m256i y = a;                                         // result
+    constexpr uint64_t flags = blend_flags<Vec4q>(indexs); // get flags for possibilities that fit the index pattern
 
-    // special case: all from b
-    if ((~m1 & 0x04040404 & mz) == 0) {
-        return permute4q <i0^4, i1^4, i2^4, i3^4> (b);
-    }
+    static_assert((flags & blend_outofrange) == 0, "Index out of range in blend function");
 
-    // special case: blend without permute
-    if (((m1 ^ 0x03020100) & 0xFBFBFBFB & mz) == 0) {
-        mask = constant8i <
-            (i0 & 4) ? -1 : 0, (i0 & 4) ? -1 : 0, (i1 & 4) ? -1 : 0, (i1 & 4) ? -1 : 0, 
-            (i2 & 4) ? -1 : 0, (i2 & 4) ? -1 : 0, (i3 & 4) ? -1 : 0, (i3 & 4) ? -1 : 0 > ();
-        t1 = _mm256_blendv_epi8(a, b, mask);  // blend
-        goto RETURNORZERO;
-    } 
+    if constexpr ((flags & blend_allzero) != 0) return _mm256_setzero_si256();  // just return zero
 
-    // special case: shift left
-    if (i0 > 0 && i0 < 4 && mz == -1 && (m1 ^ ((i0 & 3) * 0x01010101 + 0x03020100)) == 0) {
-        t1 = _mm256_permute2x128_si256(a, b, 0x21);
-        if (i0 < 2) return _mm256_alignr_epi8(t1, a, (i0 & 1) * 8);
-        else        return _mm256_alignr_epi8(b, t1, (i0 & 1) * 8);
+    if constexpr ((flags & blend_b) == 0) {                // nothing from b. just permute a
+        return permute4 <i0, i1, i2, i3> (a);
     }
-    // special case: shift right
-    if (i0 > 4 && i0 < 8 && mz == -1 && (m1 ^ 0x04040404 ^ ((i0 & 3) * 0x01010101 + 0x03020100)) == 0) {
-        t1 = _mm256_permute2x128_si256(b, a, 0x21);
-        if (i0 < 6) return _mm256_alignr_epi8(t1, b, (i0 & 1) * 8);
-        else        return _mm256_alignr_epi8(a, t1, (i0 & 1) * 8);
+    if constexpr ((flags & blend_a) == 0) {                // nothing from a. just permute b
+        return permute4 <i0<0?i0:i0&3, i1<0?i1:i1&3, i2<0?i2:i2&3, i3<0?i3:i3&3> (b);
     }
-    // special case: unpack low
-    if (((m1 ^ 0x06020400) & mz) == 0) {
-        t1 = _mm256_unpacklo_epi64(a, b);
-        goto RETURNORZERO;
+    if constexpr ((flags & (blend_perma | blend_permb)) == 0) { // no permutation, only blending
+        constexpr uint8_t mb = (uint8_t)make_bit_mask<4, 0x302>(indexs);  // blend mask
+#if INSTRSET >= 10 // AVX512VL
+        y = _mm256_mask_mov_epi64 (a, mb, b);
+#else  // AVX2
+        y = _mm256_blend_epi32(a, b, ((mb & 1) | (mb & 2) << 1 | (mb & 4) << 2 | (mb & 8) << 3) * 3); // duplicate each bit
+#endif
     }
-    // special case: unpack low
-    if (((m1 ^ 0x02060004) & mz) == 0) {
-        t1 = _mm256_unpacklo_epi64(b, a);
-        goto RETURNORZERO;
+    else if constexpr ((flags & blend_largeblock) != 0) {  // blend and permute 128-bit blocks
+        constexpr EList<int, 2> L = largeblock_perm<4>(indexs); // get 128-bit blend pattern
+        constexpr uint8_t pp = (L.a[0] & 0xF) | uint8_t(L.a[1] & 0xF) << 4;
+        y = _mm256_permute2x128_si256(a, b, pp);
     }
-    // special case: unpack high
-    if (((m1 ^ 0x07030501) & mz) == 0) {
-        t1 = _mm256_unpackhi_epi64(a, b);
-        goto RETURNORZERO;
+    // check if pattern fits special cases
+    else if constexpr ((flags & blend_punpcklab) != 0) {
+        y = _mm256_unpacklo_epi64 (a, b);
     }
-    // special case: unpack high
-    if (((m1 ^ 0x03070105) & mz) == 0) {
-        t1 = _mm256_unpackhi_epi64(b, a);
-        goto RETURNORZERO;
+    else if constexpr ((flags & blend_punpcklba) != 0) {
+        y = _mm256_unpacklo_epi64 (b, a);
     }
-
-    // general case: permute and blend and possibly zero
-    const int blank = dozero ? -1 : -0x100;  // ignore or zero
-
-    // permute and blend
-    __m256i ta = permute4q <
-        (i0 & 4) ? blank : i0, (i1 & 4) ? blank : i1, (i2 & 4) ? blank : i2, (i3 & 4) ? blank : i3 > (a);
-
-    __m256i tb = permute4q <
-        ((i0^4) & 4) ? blank : i0^4, ((i1^4) & 4) ? blank : i1^4, ((i2^4) & 4) ? blank : i2^4, ((i3^4) & 4) ? blank : i3^4 > (b);
-
-    if (blank == -1) {
-        // we have zeroed, need only to OR
-        return _mm256_or_si256(ta, tb);
+    else if constexpr ((flags & blend_punpckhab) != 0) {
+        y = _mm256_unpackhi_epi64 (a, b);
     }
-    // no zeroing, need to blend
-    mask = constant8i <
-        (i0 & 4) ? -1 : 0, (i0 & 4) ? -1 : 0, (i1 & 4) ? -1 : 0, (i1 & 4) ? -1 : 0, 
-        (i2 & 4) ? -1 : 0, (i2 & 4) ? -1 : 0, (i3 & 4) ? -1 : 0, (i3 & 4) ? -1 : 0 > ();
-
-    return _mm256_blendv_epi8(ta, tb, mask);  // blend
+    else if constexpr ((flags & blend_punpckhba) != 0) {
+        y = _mm256_unpackhi_epi64 (b, a);
+    }
+    else if constexpr ((flags & blend_rotateab) != 0) {
+        y = _mm256_alignr_epi8(a, b, flags >> blend_rotpattern);
+    }
+    else if constexpr ((flags & blend_rotateba) != 0) {
+        y = _mm256_alignr_epi8(b, a, flags >> blend_rotpattern);
+    }
+#if ALLOW_FP_PERMUTE  // allow floating point permute instructions on integer vectors
+    else if constexpr ((flags & blend_shufab) != 0) {      // use floating point instruction shufpd
+        y = _mm256_castpd_si256(_mm256_shuffle_pd(_mm256_castsi256_pd(a), _mm256_castsi256_pd(b), (flags >> blend_shufpattern) & 0xF));
+    }
+    else if constexpr ((flags & blend_shufba) != 0) {      // use floating point instruction shufpd
+        y = _mm256_castpd_si256(_mm256_shuffle_pd(_mm256_castsi256_pd(b), _mm256_castsi256_pd(a), (flags >> blend_shufpattern) & 0xF));
+    }
+#endif
+    else { // No special cases
+#if INSTRSET >= 10  // AVX512VL. use vpermi2q
+        __m256i const maskp = constant8ui<i0 & 15, 0, i1 & 15, 0, i2 & 15, 0, i3 & 15, 0>();
+        return _mm256_maskz_permutex2var_epi64 (zero_mask<4>(indexs), a, maskp, b);
+#else   // permute a and b separately, then blend.
+        constexpr EList<int, 8> L = blend_perm_indexes<4, 0>(indexs); // get permutation indexes
+        __m256i ya = permute4<L.a[0], L.a[1], L.a[2], L.a[3]>(a);
+        __m256i yb = permute4<L.a[4], L.a[5], L.a[6], L.a[7]>(b);
+        constexpr uint8_t mb = (uint8_t)make_bit_mask<4, 0x302>(indexs);  // blend mask
+        y = _mm256_blend_epi32(ya, yb, ((mb & 1) | (mb & 2) << 1 | (mb & 4) << 2 | (mb & 8) << 3) * 3); // duplicate each bit
+#endif
+    }
+    if constexpr ((flags & blend_zeroing) != 0) {          // additional zeroing needed
+#if INSTRSET >= 10  // use compact mask
+        y = _mm256_maskz_mov_epi64(zero_mask<4>(indexs), y);
+#else  // use broad mask
+        constexpr EList <int64_t, 4> bm = zero_mask_broad<Vec4q>(indexs);
+        y = _mm256_and_si256(Vec4q().load(bm.a), y);
+#endif
+    }
+    return y;
 }
 
-template <int i0, int i1, int i2, int i3> 
-static inline Vec4uq blend4uq(Vec4uq const & a, Vec4uq const & b) {
-    return Vec4uq( blend4q<i0,i1,i2,i3> (a,b));
+template <int i0, int i1, int i2, int i3>
+static inline Vec4uq blend4(Vec4uq const a, Vec4uq const b) {
+    return Vec4uq(blend4<i0,i1,i2,i3> (Vec4q(a),Vec4q(b)));
 }
 
 
-template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7> 
-static inline Vec8i blend8i(Vec8i const & a, Vec8i const & b) {  
+// permute and blend Vec8i
+template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7>
+static inline Vec8i blend8(Vec8i const a, Vec8i const b) {
+    int constexpr indexs[8] = { i0, i1, i2, i3, i4, i5, i6, i7 }; // indexes as array
+    __m256i y = a;                                         // result
+    constexpr uint64_t flags = blend_flags<Vec8i>(indexs); // get flags for possibilities that fit the index pattern
 
-    const int ior = i0 | i1 | i2 | i3 | i4 | i5 | i6 | i7;  // OR indexes
+    static_assert((flags & blend_outofrange) == 0, "Index out of range in blend function");
 
-    // is zeroing needed
-    const bool do_zero  = ior < 0 && (ior & 0x80); // at least one index is negative, and not -0x100
+    if constexpr ((flags & blend_allzero) != 0) return _mm256_setzero_si256();  // just return zero
 
-    // Combine all the indexes into a single bitfield, with 4 bits for each
-    const int m1 = (i0&0xF) | (i1&0xF)<<4 | (i2&0xF)<<8 | (i3&0xF)<<12 | (i4&0xF)<<16 | (i5&0xF)<<20 | (i6&0xF)<<24 | (i7&0xF)<<28;
+    if constexpr ((flags & blend_largeblock) != 0) {       // blend and permute 32-bit blocks
+        constexpr EList<int, 4> L = largeblock_perm<8>(indexs); // get 32-bit blend pattern
+        y = blend4<L.a[0], L.a[1], L.a[2], L.a[3]> (Vec4q(a), Vec4q(b));
+        if (!(flags & blend_addz)) return y;               // no remaining zeroing
+    }
+    else if constexpr ((flags & blend_b) == 0) {           // nothing from b. just permute a
+        return permute8 <i0, i1, i2, i3, i4, i5, i6, i7> (a);
+    }
+    else if constexpr ((flags & blend_a) == 0) {           // nothing from a. just permute b
+        constexpr EList<int, 16> L = blend_perm_indexes<8, 2>(indexs); // get permutation indexes
+        return permute8 < L.a[8], L.a[9], L.a[10], L.a[11], L.a[12], L.a[13], L.a[14], L.a[15] > (b);
+    }
+    else if constexpr ((flags & (blend_perma | blend_permb)) == 0) { // no permutation, only blending
+        constexpr uint8_t mb = (uint8_t)make_bit_mask<8, 0x303>(indexs);  // blend mask
+#if INSTRSET >= 10 // AVX512VL
+        y = _mm256_mask_mov_epi32 (a, mb, b);
+#else  // AVX2
+        y = _mm256_blend_epi32(a, b, mb);
+#endif
+    }
+    // check if pattern fits special cases
+    else if constexpr ((flags & blend_punpcklab) != 0) {
+        y = _mm256_unpacklo_epi32 (a, b);
+    }
+    else if constexpr ((flags & blend_punpcklba) != 0) {
+        y = _mm256_unpacklo_epi32 (b, a);
+    }
+    else if constexpr ((flags & blend_punpckhab) != 0) {
+        y = _mm256_unpackhi_epi32 (a, b);
+    }
+    else if constexpr ((flags & blend_punpckhba) != 0) {
+        y = _mm256_unpackhi_epi32 (b, a);
+    }
+    else if constexpr ((flags & blend_rotateab) != 0) {
+        y = _mm256_alignr_epi8(a, b, flags >> blend_rotpattern);
+    }
+    else if constexpr ((flags & blend_rotateba) != 0) {
+        y = _mm256_alignr_epi8(b, a, flags >> blend_rotpattern);
+    }
+#if ALLOW_FP_PERMUTE  // allow floating point permute instructions on integer vectors
+    else if constexpr ((flags & blend_shufab) != 0) {      // use floating point instruction shufpd
+        y = _mm256_castps_si256(_mm256_shuffle_ps(_mm256_castsi256_ps(a), _mm256_castsi256_ps(b), uint8_t(flags >> blend_shufpattern)));
+    }
+    else if constexpr ((flags & blend_shufba) != 0) {      // use floating point instruction shufpd
+        y = _mm256_castps_si256(_mm256_shuffle_ps(_mm256_castsi256_ps(b), _mm256_castsi256_ps(a), uint8_t(flags >> blend_shufpattern)));
+    }
+#endif
+    else { // No special cases
+#if INSTRSET >= 10  // AVX512VL. use vpermi2d
+        __m256i const maskp = constant8ui<i0 & 15, i1 & 15, i2 & 15, i3 & 15, i4 & 15, i5 & 15, i6 & 15, i7 & 15> ();
+        return _mm256_maskz_permutex2var_epi32 (zero_mask<8>(indexs), a, maskp, b);
+#else   // permute a and b separately, then blend.
+        constexpr EList<int, 16> L = blend_perm_indexes<8, 0>(indexs); // get permutation indexes
+        __m256i ya = permute8<L.a[0], L.a[1], L.a[2], L.a[3], L.a[4], L.a[5], L.a[6], L.a[7]>(a);
+        __m256i yb = permute8<L.a[8], L.a[9], L.a[10], L.a[11], L.a[12], L.a[13], L.a[14], L.a[15]>(b);
+        constexpr uint8_t mb = (uint8_t)make_bit_mask<8, 0x303>(indexs);  // blend mask
+        y = _mm256_blend_epi32(ya, yb, mb);
+#endif
+    }
+    if constexpr ((flags & blend_zeroing) != 0) {          // additional zeroing needed
+#if INSTRSET >= 10  // use compact mask
+        y = _mm256_maskz_mov_epi32(zero_mask<8>(indexs), y);
+#else  // use broad mask
+        constexpr EList <int32_t, 8> bm = zero_mask_broad<Vec8i>(indexs);
+        y = _mm256_and_si256(Vec8i().load(bm.a), y);
+#endif
+    }
+    return y;
+}
 
-    // Mask to zero out negative indexes
-    const int mz = (i0<0?0:0xF) | (i1<0?0:0xF)<<4 | (i2<0?0:0xF)<<8 | (i3<0?0:0xF)<<12 | (i4<0?0:0xF)<<16 | (i5<0?0:0xF)<<20 | (i6<0?0:0xF)<<24 | (i7<0?0:0xF)<<28;
+template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7>
+static inline Vec8ui blend8(Vec8ui const a, Vec8ui const b) {
+    return Vec8ui( blend8<i0,i1,i2,i3,i4,i5,i6,i7> (Vec8i(a),Vec8i(b)));
+}
 
-    __m256i t1, mask;
 
-    if (mz == 0) return _mm256_setzero_si256();  // all zero
+// permute and blend Vec16s
+template <int i0,  int i1,  int i2,  int i3,  int i4,  int i5,  int i6,  int i7,
+    int i8,  int i9,  int i10, int i11, int i12, int i13, int i14, int i15 >
+    static inline Vec16s blend16(Vec16s const a, Vec16s const b) {
+    int constexpr indexs[16] = {
+        i0, i1, i2, i3, i4, i5, i6, i7, i8, i9, i10, i11, i12, i13, i14, i15 };// indexes as array
+    __m256i y = a;                                         // result
+    constexpr uint64_t flags = blend_flags<Vec16s>(indexs);// get flags for possibilities that fit the index pattern
 
-    // special case: 64 bit blend/permute
-    if (((m1 ^ 0x10101010) & 0x11111111 & mz) == 0 && ((m1 ^ (m1 >> 4)) & 0x0E0E0E0E & mz & mz >> 4) == 0) {
-        // check if part of a 64-bit block is zeroed
-        const bool partialzero = int((i0^i1) | (i2^i3) | (i4^i5) | (i6^i7)) < 0; 
-        const int blank1 = partialzero ? -0x100 : -1;  // ignore if zeroing later anyway
-        // indexes for 64 bit blend
-        const int j0 = i0 >= 0 ? i0 / 2 : i1 >= 0 ? i1 / 2 : blank1;
-        const int j1 = i2 >= 0 ? i2 / 2 : i3 >= 0 ? i3 / 2 : blank1;
-        const int j2 = i4 >= 0 ? i4 / 2 : i5 >= 0 ? i5 / 2 : blank1;
-        const int j3 = i6 >= 0 ? i6 / 2 : i7 >= 0 ? i7 / 2 : blank1;
-        // 64-bit blend and permute
-        t1 = blend4q<j0,j1,j2,j3>(Vec4q(a), Vec4q(b));
-        if (partialzero && do_zero) {
-            // zero some elements
-            mask = constant8i< i0 < 0 ? 0 : -1, i1 < 0 ? 0 : -1, i2 < 0 ? 0 : -1, i3 < 0 ? 0 : -1, 
-                i4 < 0 ? 0 : -1, i5 < 0 ? 0 : -1, i6 < 0 ? 0 : -1, i7 < 0 ? 0 : -1 > ();
-            return _mm256_and_si256(t1, mask);
-        }
-        return t1;
-    }
+    static_assert((flags & blend_outofrange) == 0, "Index out of range in blend function");
 
-    if ((m1 & 0x88888888 & mz) == 0) {
-        // all from a
-        return permute8i<i0, i1, i2, i3, i4, i5, i6, i7> (a);
-    }
+    if constexpr ((flags & blend_allzero) != 0) return _mm256_setzero_si256();  // just return zero
 
-    if (((m1 ^ 0x88888888) & 0x88888888 & mz) == 0) {
-        // all from b
-        return permute8i<i0&~8, i1&~8, i2&~8, i3&~8, i4&~8, i5&~8, i6&~8, i7&~8> (b);
-    }
-
-    if ((((m1 & 0x77777777) ^ 0x76543210) & mz) == 0) {
-        // blend and zero, no permute
-        mask = constant8i<(i0&8)?0:-1, (i1&8)?0:-1, (i2&8)?0:-1, (i3&8)?0:-1, (i4&8)?0:-1, (i5&8)?0:-1, (i6&8)?0:-1, (i7&8)?0:-1> ();
-        t1   = select(mask, a, b);
-        if (!do_zero) return t1;
-        // zero some elements
-        mask = constant8i< (i0<0&&(i0&8)) ? 0 : -1, (i1<0&&(i1&8)) ? 0 : -1, (i2<0&&(i2&8)) ? 0 : -1, (i3<0&&(i3&8)) ? 0 : -1, 
-            (i4<0&&(i4&8)) ? 0 : -1, (i5<0&&(i5&8)) ? 0 : -1, (i6<0&&(i6&8)) ? 0 : -1, (i7<0&&(i7&8)) ? 0 : -1 > ();
-        return _mm256_and_si256(t1, mask);
-    }
-
-    // special case: shift left
-    if (i0 > 0 && i0 < 8 && mz == -1 && (m1 ^ ((i0 & 7) * 0x11111111u + 0x76543210u)) == 0) {
-        t1 = _mm256_permute2x128_si256(a, b, 0x21);
-        if (i0 < 4) return _mm256_alignr_epi8(t1, a, (i0 & 3) * 4);
-        else        return _mm256_alignr_epi8(b, t1, (i0 & 3) * 4);
-    }
-    // special case: shift right
-    if (i0 > 8 && i0 < 16 && mz == -1 && (m1 ^ 0x88888888 ^ ((i0 & 7) * 0x11111111u + 0x76543210u)) == 0) {
-        t1 = _mm256_permute2x128_si256(b, a, 0x21);
-        if (i0 < 12) return _mm256_alignr_epi8(t1, b, (i0 & 3) * 4);
-        else         return _mm256_alignr_epi8(a, t1, (i0 & 3) * 4);
-    }
-
-    // general case: permute and blend and possible zero
-    const int blank = do_zero ? -1 : -0x100;  // ignore or zero
-
-    Vec8i ta = permute8i <
-        (uint32_t)i0 < 8 ? i0 : blank,
-        (uint32_t)i1 < 8 ? i1 : blank,
-        (uint32_t)i2 < 8 ? i2 : blank,
-        (uint32_t)i3 < 8 ? i3 : blank,
-        (uint32_t)i4 < 8 ? i4 : blank,
-        (uint32_t)i5 < 8 ? i5 : blank,
-        (uint32_t)i6 < 8 ? i6 : blank,
-        (uint32_t)i7 < 8 ? i7 : blank > (a);
-    Vec8i tb = permute8i <
-        (uint32_t)(i0^8) < 8 ? (i0^8) : blank,
-        (uint32_t)(i1^8) < 8 ? (i1^8) : blank,
-        (uint32_t)(i2^8) < 8 ? (i2^8) : blank,
-        (uint32_t)(i3^8) < 8 ? (i3^8) : blank,
-        (uint32_t)(i4^8) < 8 ? (i4^8) : blank,
-        (uint32_t)(i5^8) < 8 ? (i5^8) : blank,
-        (uint32_t)(i6^8) < 8 ? (i6^8) : blank,
-        (uint32_t)(i7^8) < 8 ? (i7^8) : blank > (b);
-    if (blank == -1) {    
-        return  _mm256_or_si256(ta, tb); 
-    }
-    // no zeroing, need to blend
-    const int maskb = ((i0 >> 3) & 1) | ((i1 >> 2) & 2) | ((i2 >> 1) & 4) | (i3 & 8) | 
-        ((i4 << 1) & 0x10) | ((i5 << 2) & 0x20) | ((i6 << 3) & 0x40) | ((i7 << 4) & 0x80);
-    return _mm256_blend_epi32(ta, tb, maskb);  // blend
-}
-
-template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7> 
-static inline Vec8ui blend8ui(Vec8ui const & a, Vec8ui const & b) {
-    return Vec8ui( blend8i<i0,i1,i2,i3,i4,i5,i6,i7> (a,b));
-}
-
-
-template <int i0,  int i1,  int i2,  int i3,  int i4,  int i5,  int i6,  int i7, 
-          int i8,  int i9,  int i10, int i11, int i12, int i13, int i14, int i15 > 
-static inline Vec16s blend16s(Vec16s const & a, Vec16s const & b) {  
-    // collect bit 4 of each index
-    const int m1 = 
-        (i0 &16)>>4  | (i1 &16)>>3  | (i2 &16)>>2  | (i3 &16)>>1  | (i4 &16)     | (i5 &16)<<1  | (i6 &16)<<2  | (i7 &16)<<3  | 
-        (i8 &16)<<4  | (i9 &16)<<5  | (i10&16)<<6  | (i11&16)<<7  | (i12&16)<<8  | (i13&16)<<9  | (i14&16)<<10 | (i15&16)<<11 ;
-
-    // check which elements to set to zero
-    const int mz = 0x0000FFFF ^ (
-        (i0 <0)     | (i1 <0)<<1  | (i2 <0)<<2  | (i3 <0)<<3  | (i4 <0)<<4  | (i5 <0)<<5  | (i6 <0)<<6  | (i7 <0)<<7  | 
-        (i8 <0)<<8  | (i9 <0)<<9  | (i10<0)<<10 | (i11<0)<<11 | (i12<0)<<12 | (i13<0)<<13 | (i14<0)<<14 | (i15<0)<<15 );
-
-    __m256i t1, mask;
-
-    // special case: all zero
-    if (mz == 0) return  _mm256_setzero_si256();
-
-    // special case: all from a
-    if ((m1 & mz) == 0) {
-        return permute16s<i0,i1,i2,i3,i4,i5,i6,i7,i8,i9,i10,i11,i12,i13,i14,i15> (a);
-    }
-
-    // special case: all from b
-    if (((m1 ^ 0xFFFF) & mz) == 0) {
-        return permute16s<i0^16,i1^16,i2^16,i3^16,i4^16,i5^16,i6^16,i7^16,i8^16,i9^16,i10^16,i11^16,i12^16,i13^16,i14^16,i15^16 > (b);
-    }
-
-    // special case: blend without permute
-    if ((i0 <0||(i0 &15)== 0) && (i1 <0||(i1 &15)== 1) && (i2 <0||(i2 &15)== 2) && (i3 <0||(i3 &15)== 3) && 
-        (i4 <0||(i4 &15)== 4) && (i5 <0||(i5 &15)== 5) && (i6 <0||(i6 &15)== 6) && (i7 <0||(i7 &15)== 7) && 
-        (i8 <0||(i8 &15)== 8) && (i9 <0||(i9 &15)== 9) && (i10<0||(i10&15)==10) && (i11<0||(i11&15)==11) && 
-        (i12<0||(i12&15)==12) && (i13<0||(i13&15)==13) && (i14<0||(i14&15)==14) && (i15<0||(i15&15)==15)) {
-
-        mask = constant8i <
-            int(((i0 & 16) ? 0xFFFF : 0) | ((i1 & 16) ? 0xFFFF0000 : 0)),
-            int(((i2 & 16) ? 0xFFFF : 0) | ((i3 & 16) ? 0xFFFF0000 : 0)),
-            int(((i4 & 16) ? 0xFFFF : 0) | ((i5 & 16) ? 0xFFFF0000 : 0)),
-            int(((i6 & 16) ? 0xFFFF : 0) | ((i7 & 16) ? 0xFFFF0000 : 0)),
-            int(((i8 & 16) ? 0xFFFF : 0) | ((i9 & 16) ? 0xFFFF0000 : 0)),
-            int(((i10& 16) ? 0xFFFF : 0) | ((i11& 16) ? 0xFFFF0000 : 0)),
-            int(((i12& 16) ? 0xFFFF : 0) | ((i13& 16) ? 0xFFFF0000 : 0)),
-            int(((i14& 16) ? 0xFFFF : 0) | ((i15& 16) ? 0xFFFF0000 : 0)) > ();
-
-        t1 = _mm256_blendv_epi8(a, b, mask);  // blend
-
-        if (mz != 0xFFFF) {
-            // zero some elements
-            mask = constant8i <
-                int((i0  < 0 ? 0 : 0xFFFF) | (i1  < 0 ? 0 : 0xFFFF0000)),
-                int((i2  < 0 ? 0 : 0xFFFF) | (i3  < 0 ? 0 : 0xFFFF0000)),
-                int((i4  < 0 ? 0 : 0xFFFF) | (i5  < 0 ? 0 : 0xFFFF0000)),
-                int((i6  < 0 ? 0 : 0xFFFF) | (i7  < 0 ? 0 : 0xFFFF0000)),
-                int((i8  < 0 ? 0 : 0xFFFF) | (i9  < 0 ? 0 : 0xFFFF0000)),
-                int((i10 < 0 ? 0 : 0xFFFF) | (i11 < 0 ? 0 : 0xFFFF0000)),
-                int((i12 < 0 ? 0 : 0xFFFF) | (i13 < 0 ? 0 : 0xFFFF0000)),
-                int((i14 < 0 ? 0 : 0xFFFF) | (i15 < 0 ? 0 : 0xFFFF0000)) > ();
-            return _mm256_and_si256(t1, mask);
+    if constexpr ((flags & blend_largeblock) != 0) {       // blend and permute 32-bit blocks
+        constexpr EList<int, 8> L = largeblock_perm<16>(indexs); // get 32-bit blend pattern
+        y = blend8<L.a[0], L.a[1], L.a[2], L.a[3], L.a[4], L.a[5], L.a[6], L.a[7]> (Vec8i(a), Vec8i(b));
+        if (!(flags & blend_addz)) return y;               // no remaining zeroing
+    }
+    else if constexpr ((flags & blend_b) == 0) {           // nothing from b. just permute a
+        return permute16 <i0, i1, i2, i3, i4, i5, i6, i7, i8, i9, i10, i11, i12, i13, i14, i15> (a);
+    }
+    else if constexpr ((flags & blend_a) == 0) {           // nothing from a. just permute b
+        constexpr EList<int, 32> L = blend_perm_indexes<16, 2>(indexs); // get permutation indexes
+        return permute16 <
+            L.a[16], L.a[17], L.a[18], L.a[19], L.a[20], L.a[21], L.a[22], L.a[23],
+            L.a[24], L.a[25], L.a[26], L.a[27], L.a[28], L.a[29], L.a[30], L.a[31]> (b);
+    }
+    // check if pattern fits special cases
+    else if constexpr ((flags & blend_punpcklab) != 0) {
+        y = _mm256_unpacklo_epi16 (a, b);
+    }
+    else if constexpr ((flags & blend_punpcklba) != 0) {
+        y = _mm256_unpacklo_epi16 (b, a);
+    }
+    else if constexpr ((flags & blend_punpckhab) != 0) {
+        y = _mm256_unpackhi_epi16 (a, b);
+    }
+    else if constexpr ((flags & blend_punpckhba) != 0) {
+        y = _mm256_unpackhi_epi16 (b, a);
+    }
+    else if constexpr ((flags & blend_rotateab) != 0) {
+        y = _mm256_alignr_epi8(a, b, flags >> blend_rotpattern);
+    }
+    else if constexpr ((flags & blend_rotateba) != 0) {
+        y = _mm256_alignr_epi8(b, a, flags >> blend_rotpattern);
+    }
+    else { // No special cases
+#if INSTRSET >= 10  // AVX512VL. use vpermi2w
+        if constexpr ((flags & (blend_perma | blend_permb)) != 0) {
+            constexpr EList <int16_t, 16> bm = perm_mask_broad<Vec16s>(indexs);
+            return _mm256_maskz_permutex2var_epi16(zero_mask<16>(indexs), a, Vec16s().load(bm.a), b);
         }
-        return t1;
-    }
-
-    // special case: shift left
-    const int slb = i0 > 0 ? i0 : i15 - 15;
-    if (slb > 0 && slb < 16 
-        && (i0==slb+ 0||i0<0) && (i1==slb+ 1||i1<0) && (i2 ==slb+ 2||i2 <0) && (i3 ==slb+ 3||i3 <0) && (i4 ==slb+ 4||i4 <0) && (i5 ==slb+ 5||i5 <0) && (i6 ==slb+ 6||i6 <0) && (i7 ==slb+ 7||i7 <0)
-        && (i8==slb+ 8||i8<0) && (i9==slb+ 9||i9<0) && (i10==slb+10||i10<0) && (i11==slb+11||i11<0) && (i12==slb+12||i12<0) && (i13==slb+13||i13<0) && (i14==slb+14||i14<0) && (i15==slb+15||i15<0)) {
-        t1 = _mm256_permute2x128_si256(a, b, 0x21);
-        if (slb < 8) t1 = _mm256_alignr_epi8(t1, a, (slb & 7) * 2);
-        else         t1 = _mm256_alignr_epi8(b, t1, (slb & 7) * 2);
-        if (mz != 0xFFFF) {
-            // zero some elements
-            mask = constant8i <
-                int((i0  < 0 ? 0 : 0xFFFF) | (i1  < 0 ? 0 : 0xFFFF0000)),
-                int((i2  < 0 ? 0 : 0xFFFF) | (i3  < 0 ? 0 : 0xFFFF0000)),
-                int((i4  < 0 ? 0 : 0xFFFF) | (i5  < 0 ? 0 : 0xFFFF0000)),
-                int((i6  < 0 ? 0 : 0xFFFF) | (i7  < 0 ? 0 : 0xFFFF0000)),
-                int((i8  < 0 ? 0 : 0xFFFF) | (i9  < 0 ? 0 : 0xFFFF0000)),
-                int((i10 < 0 ? 0 : 0xFFFF) | (i11 < 0 ? 0 : 0xFFFF0000)),
-                int((i12 < 0 ? 0 : 0xFFFF) | (i13 < 0 ? 0 : 0xFFFF0000)),
-                int((i14 < 0 ? 0 : 0xFFFF) | (i15 < 0 ? 0 : 0xFFFF0000)) > ();
-            return _mm256_and_si256(t1, mask);
+#endif
+        // permute a and b separately, then blend.
+        Vec16s ya = a, yb = b;  // a and b permuted
+        constexpr EList<int, 32> L = blend_perm_indexes<16, 0>(indexs); // get permutation indexes
+        if constexpr ((flags & blend_perma) != 0) {
+            ya = permute16<
+                L.a[0], L.a[1], L.a[2], L.a[3], L.a[4], L.a[5], L.a[6], L.a[7],
+                L.a[8], L.a[9], L.a[10], L.a[11], L.a[12], L.a[13], L.a[14], L.a[15] >(ya);
         }
-        return t1;
-    }
-    // special case: shift right
-    const int srb = i0 > 0 ? (i0^16) : (i15^16) - 15;
-    if (srb > 0 && srb < 16
-        && ((i0 ^16)==srb+ 0||i0 <0) && ((i1 ^16)==srb+ 1||i1 <0) && ((i2 ^16)==srb+ 2||i2 <0) && ((i3 ^16)==srb+ 3||i3 <0) && ((i4 ^16)==srb+ 4||i4 <0) && ((i5 ^16)==srb+ 5||i5 <0) && ((i6 ^16)==srb+ 6||i6 <0) && ((i7 ^16)==srb+ 7||i7 <0)
-        && ((i8 ^16)==srb+ 8||i8 <0) && ((i9 ^16)==srb+ 9||i9 <0) && ((i10^16)==srb+10||i10<0) && ((i11^16)==srb+11||i11<0) && ((i12^16)==srb+12||i12<0) && ((i13^16)==srb+13||i13<0) && ((i14^16)==srb+14||i14<0) && ((i15^16)==srb+15||i15<0)) {
-        t1 = _mm256_permute2x128_si256(b, a, 0x21);
-        if (srb < 8) t1 = _mm256_alignr_epi8(t1, b, (srb & 7) * 2);
-        else         t1 = _mm256_alignr_epi8(a, t1, (srb & 7) * 2);
-        if (mz != 0xFFFF) {
-            // zero some elements
-            mask = constant8i <
-                int((i0  < 0 ? 0 : 0xFFFF) | (i1  < 0 ? 0 : 0xFFFF0000)),
-                int((i2  < 0 ? 0 : 0xFFFF) | (i3  < 0 ? 0 : 0xFFFF0000)),
-                int((i4  < 0 ? 0 : 0xFFFF) | (i5  < 0 ? 0 : 0xFFFF0000)),
-                int((i6  < 0 ? 0 : 0xFFFF) | (i7  < 0 ? 0 : 0xFFFF0000)),
-                int((i8  < 0 ? 0 : 0xFFFF) | (i9  < 0 ? 0 : 0xFFFF0000)),
-                int((i10 < 0 ? 0 : 0xFFFF) | (i11 < 0 ? 0 : 0xFFFF0000)),
-                int((i12 < 0 ? 0 : 0xFFFF) | (i13 < 0 ? 0 : 0xFFFF0000)),
-                int((i14 < 0 ? 0 : 0xFFFF) | (i15 < 0 ? 0 : 0xFFFF0000)) > ();
-            return _mm256_and_si256(t1, mask);
+        if constexpr ((flags & blend_permb) != 0) {
+            yb = permute16<
+            L.a[16], L.a[17], L.a[18], L.a[19], L.a[20], L.a[21], L.a[22], L.a[23],
+            L.a[24], L.a[25], L.a[26], L.a[27], L.a[28], L.a[29], L.a[30], L.a[31] >(yb);
         }
-        return t1;
-    }
-    
-    // general case: permute and blend and possibly zero
-    const int blank = (mz == 0xFFFF) ? -0x100 : -1;  // ignore or zero
-
-    // permute and blend
-    __m256i ta = permute16s <
-        (i0 &16)?blank:i0 , (i1 &16)?blank:i1 , (i2 &16)?blank:i2 , (i3 &16)?blank:i3 ,
-        (i4 &16)?blank:i4 , (i5 &16)?blank:i5 , (i6 &16)?blank:i6 , (i7 &16)?blank:i7 ,
-        (i8 &16)?blank:i8 , (i9 &16)?blank:i9 , (i10&16)?blank:i10, (i11&16)?blank:i11,
-        (i12&16)?blank:i12, (i13&16)?blank:i13, (i14&16)?blank:i14, (i15&16)?blank:i15 > (a);
-
-    __m256i tb = permute16s <
-        ((i0 ^16)&16)?blank:i0 ^16, ((i1 ^16)&16)?blank:i1 ^16, ((i2 ^16)&16)?blank:i2 ^16, ((i3 ^16)&16)?blank:i3 ^16, 
-        ((i4 ^16)&16)?blank:i4 ^16, ((i5 ^16)&16)?blank:i5 ^16, ((i6 ^16)&16)?blank:i6 ^16, ((i7 ^16)&16)?blank:i7 ^16, 
-        ((i8 ^16)&16)?blank:i8 ^16, ((i9 ^16)&16)?blank:i9 ^16, ((i10^16)&16)?blank:i10^16, ((i11^16)&16)?blank:i11^16,
-        ((i12^16)&16)?blank:i12^16, ((i13^16)&16)?blank:i13^16, ((i14^16)&16)?blank:i14^16, ((i15^16)&16)?blank:i15^16 > (b);
-
-    if (blank == -1) {
-        // we have zeroed, need only to OR
-        return _mm256_or_si256(ta, tb);
-    }
-    // no zeroing, need to blend
-    mask = constant8i <
-        int(((i0 & 16) ? 0xFFFF : 0) | ((i1 & 16) ? 0xFFFF0000 : 0)),
-        int(((i2 & 16) ? 0xFFFF : 0) | ((i3 & 16) ? 0xFFFF0000 : 0)),
-        int(((i4 & 16) ? 0xFFFF : 0) | ((i5 & 16) ? 0xFFFF0000 : 0)),
-        int(((i6 & 16) ? 0xFFFF : 0) | ((i7 & 16) ? 0xFFFF0000 : 0)),
-        int(((i8 & 16) ? 0xFFFF : 0) | ((i9 & 16) ? 0xFFFF0000 : 0)),
-        int(((i10& 16) ? 0xFFFF : 0) | ((i11& 16) ? 0xFFFF0000 : 0)),
-        int(((i12& 16) ? 0xFFFF : 0) | ((i13& 16) ? 0xFFFF0000 : 0)),
-        int(((i14& 16) ? 0xFFFF : 0) | ((i15& 16) ? 0xFFFF0000 : 0)) > ();
-
-    return _mm256_blendv_epi8(ta, tb, mask);  // blend
-}
-
-template <int i0, int i1, int i2,  int i3,  int i4,  int i5,  int i6,  int i7, 
-          int i8, int i9, int i10, int i11, int i12, int i13, int i14, int i15 > 
-static inline Vec16us blend16us(Vec16us const & a, Vec16us const & b) {
-    return Vec16us( blend16s<i0,i1,i2,i3,i4,i5,i6,i7,i8,i9,i10,i11,i12,i13,i14,i15> (a,b));
-}
-
-template <int i0,  int i1,  int i2,  int i3,  int i4,  int i5,  int i6,  int i7, 
+        constexpr uint16_t mb = (uint16_t)make_bit_mask<16, 0x304>(indexs);  // blend mask
+#if INSTRSET >= 10 // AVX512VL
+        y = _mm256_mask_mov_epi16 (ya, mb, yb);
+#else  // AVX2
+        if ((flags & blend_same_pattern) != 0) {           // same blend pattern in both 128-bit lanes
+            y = _mm256_blend_epi16(ya, yb, (uint8_t)mb);
+        }
+        else {
+            constexpr EList <int16_t, 16> bm = make_broad_mask<Vec16s>(mb);
+            y = _mm256_blendv_epi8 (ya, yb, Vec16s().load(bm.a));
+        }
+#endif
+    }
+    if constexpr ((flags & blend_zeroing) != 0) {          // additional zeroing needed
+#if INSTRSET >= 10  // use compact mask
+        y = _mm256_maskz_mov_epi16(zero_mask<16>(indexs), y);
+#else  // use broad mask
+        constexpr EList <int16_t, 16> bm = zero_mask_broad<Vec16s>(indexs);
+        y = _mm256_and_si256(Vec16s().load(bm.a), y);
+#endif
+    }
+    return y;
+}
+
+template <int i0, int i1, int i2,  int i3,  int i4,  int i5,  int i6,  int i7,
+          int i8, int i9, int i10, int i11, int i12, int i13, int i14, int i15 >
+static inline Vec16us blend16(Vec16us const a, Vec16us const b) {
+    return Vec16us( blend16<i0,i1,i2,i3,i4,i5,i6,i7,i8,i9,i10,i11,i12,i13,i14,i15> (Vec16s(a),Vec16s(b)));
+}
+
+
+// permute and blend Vec32c
+template <int i0,  int i1,  int i2,  int i3,  int i4,  int i5,  int i6,  int i7,
           int i8,  int i9,  int i10, int i11, int i12, int i13, int i14, int i15,
           int i16, int i17, int i18, int i19, int i20, int i21, int i22, int i23,
-          int i24, int i25, int i26, int i27, int i28, int i29, int i30, int i31 > 
-static inline Vec32c blend32c(Vec32c const & a, Vec32c const & b) {  
-    // collect bit 5 of each index
-    const int m1 = 
-        (i0 &32)>>5  | (i1 &32)>>4  | (i2 &32)>>3  | (i3 &32)>>2  | (i4 &32)>>1  | (i5 &32)     | (i6 &32)<<1  | (i7 &32)<<2  | 
-        (i8 &32)<<3  | (i9 &32)<<4  | (i10&32)<<5  | (i11&32)<<6  | (i12&32)<<7  | (i13&32)<<8  | (i14&32)<<9  | (i15&32)<<10 | 
-        (i16&32)<<11 | (i17&32)<<12 | (i18&32)<<13 | (i19&32)<<14 | (i20&32)<<15 | (i21&32)<<16 | (i22&32)<<17 | (i23&32)<<18 | 
-        (i24&32)<<19 | (i25&32)<<20 | (i26&32)<<21 | (i27&32)<<22 | (i28&32)<<23 | (i29&32)<<24 | (i30&32)<<25 | (i31&32)<<26 ;
-
-    // check which elements to set to zero
-    const int mz = ~ (
-        (i0 <0)     | (i1 <0)<<1  | (i2 <0)<<2  | (i3 <0)<<3  | (i4 <0)<<4  | (i5 <0)<<5  | (i6 <0)<<6  | (i7 <0)<<7  | 
-        (i8 <0)<<8  | (i9 <0)<<9  | (i10<0)<<10 | (i11<0)<<11 | (i12<0)<<12 | (i13<0)<<13 | (i14<0)<<14 | (i15<0)<<15 | 
-        (i16<0)<<16 | (i17<0)<<17 | (i18<0)<<18 | (i19<0)<<19 | (i20<0)<<20 | (i21<0)<<21 | (i22<0)<<22 | (i23<0)<<23 | 
-        (i24<0)<<24 | (i25<0)<<25 | (i26<0)<<26 | (i27<0)<<27 | (i28<0)<<28 | (i29<0)<<29 | (i30<0)<<30 | (i31<0)<<31 );
-
-    __m256i t1, mask;
-
-    // special case: all zero
-    if (mz == 0) return  _mm256_setzero_si256();
-
-    // special case: all from a
-    if ((m1 & mz) == 0) {
-        return permute32c<i0,i1,i2,i3,i4,i5,i6,i7,i8,i9,i10,i11,i12,i13,i14,i15,
-            i16,i17,i18,i19,i20,i21,i22,i23,i24,i25,i26,i27,i28,i29,i30,i31> (a);
-    }
-
-    // special case: all from b
-    if ((~m1 & mz) == 0) {
-        return permute32c<i0^32,i1^32,i2^32,i3^32,i4^32,i5^32,i6^32,i7^32,i8^32,i9^32,i10^32,i11^32,i12^32,i13^32,i14^32,i15^32,
-            i16^32,i17^32,i18^32,i19^32,i20^32,i21^32,i22^32,i23^32,i24^32,i25^32,i26^32,i27^32,i28^32,i29^32,i30^32,i31^32> (b);
-    }
-
-    // special case: blend without permute
-    if ((i0 <0||(i0 &31)== 0) && (i1 <0||(i1 &31)== 1) && (i2 <0||(i2 &31)== 2) && (i3 <0||(i3 &31)== 3) && 
-        (i4 <0||(i4 &31)== 4) && (i5 <0||(i5 &31)== 5) && (i6 <0||(i6 &31)== 6) && (i7 <0||(i7 &31)== 7) && 
-        (i8 <0||(i8 &31)== 8) && (i9 <0||(i9 &31)== 9) && (i10<0||(i10&31)==10) && (i11<0||(i11&31)==11) && 
-        (i12<0||(i12&31)==12) && (i13<0||(i13&31)==13) && (i14<0||(i14&31)==14) && (i15<0||(i15&31)==15) &&
-        (i16<0||(i16&31)==16) && (i17<0||(i17&31)==17) && (i18<0||(i18&31)==18) && (i19<0||(i19&31)==19) && 
-        (i20<0||(i20&31)==20) && (i21<0||(i21&31)==21) && (i22<0||(i22&31)==22) && (i23<0||(i23&31)==23) && 
-        (i24<0||(i24&31)==24) && (i25<0||(i25&31)==25) && (i26<0||(i26&31)==26) && (i27<0||(i27&31)==27) && 
-        (i28<0||(i28&31)==28) && (i29<0||(i29&31)==29) && (i30<0||(i30&31)==30) && (i31<0||(i31&31)==31) ) {
-
-        mask = constant8i <
-            int(((i0 <<2)&0x80) | ((i1 <<10)&0x8000) | ((i2 <<18)&0x800000) | (uint32_t(i3 <<26)&0x80000000)) ,
-            int(((i4 <<2)&0x80) | ((i5 <<10)&0x8000) | ((i6 <<18)&0x800000) | (uint32_t(i7 <<26)&0x80000000)) ,
-            int(((i8 <<2)&0x80) | ((i9 <<10)&0x8000) | ((i10<<18)&0x800000) | (uint32_t(i11<<26)&0x80000000)) ,
-            int(((i12<<2)&0x80) | ((i13<<10)&0x8000) | ((i14<<18)&0x800000) | (uint32_t(i15<<26)&0x80000000)) ,
-            int(((i16<<2)&0x80) | ((i17<<10)&0x8000) | ((i18<<18)&0x800000) | (uint32_t(i19<<26)&0x80000000)) ,
-            int(((i20<<2)&0x80) | ((i21<<10)&0x8000) | ((i22<<18)&0x800000) | (uint32_t(i23<<26)&0x80000000)) ,
-            int(((i24<<2)&0x80) | ((i25<<10)&0x8000) | ((i26<<18)&0x800000) | (uint32_t(i27<<26)&0x80000000)) ,
-            int(((i28<<2)&0x80) | ((i29<<10)&0x8000) | ((i30<<18)&0x800000) | (uint32_t(i31<<26)&0x80000000)) > ();
-
-        t1 = _mm256_blendv_epi8(a, b, mask);  // blend
-
-        if (mz != -1) {
-            // zero some elements
-            const __m256i maskz = constant8i <
-                int((i0 <0?0:0xFF) | (i1 <0?0:0xFF00) | (i2 <0?0:0xFF0000) | (i3 <0?0:0xFF000000)),
-                int((i4 <0?0:0xFF) | (i5 <0?0:0xFF00) | (i6 <0?0:0xFF0000) | (i7 <0?0:0xFF000000)),
-                int((i8 <0?0:0xFF) | (i9 <0?0:0xFF00) | (i10<0?0:0xFF0000) | (i11<0?0:0xFF000000)),
-                int((i12<0?0:0xFF) | (i13<0?0:0xFF00) | (i14<0?0:0xFF0000) | (i15<0?0:0xFF000000)),
-                int((i16<0?0:0xFF) | (i17<0?0:0xFF00) | (i18<0?0:0xFF0000) | (i19<0?0:0xFF000000)),
-                int((i20<0?0:0xFF) | (i21<0?0:0xFF00) | (i22<0?0:0xFF0000) | (i23<0?0:0xFF000000)),
-                int((i24<0?0:0xFF) | (i25<0?0:0xFF00) | (i26<0?0:0xFF0000) | (i27<0?0:0xFF000000)),
-                int((i28<0?0:0xFF) | (i29<0?0:0xFF00) | (i30<0?0:0xFF0000) | (i31<0?0:0xFF000000)) > ();
-            return _mm256_and_si256(t1, maskz);
+          int i24, int i25, int i26, int i27, int i28, int i29, int i30, int i31 >
+static inline Vec32c blend32(Vec32c const a, Vec32c const b) {
+    int constexpr indexs[32] = {
+        i0, i1, i2, i3, i4, i5, i6, i7, i8, i9, i10, i11, i12, i13, i14, i15,
+        i16, i17, i18, i19, i20, i21, i22, i23,i24, i25, i26, i27, i28, i29, i30, i31 };                  // indexes as array
+    __m256i y = a;                                         // result
+    constexpr uint64_t flags = blend_flags<Vec32c>(indexs);// get flags for possibilities that fit the index pattern
+
+    static_assert((flags & blend_outofrange) == 0, "Index out of range in blend function");
+
+    if constexpr ((flags & blend_allzero) != 0) return _mm256_setzero_si256();  // just return zero
+
+    if constexpr ((flags & blend_largeblock) != 0) {       // blend and permute 16-bit blocks
+        constexpr EList<int, 16> L = largeblock_perm<32>(indexs); // get 16-bit blend pattern
+        y = blend16 < L.a[0], L.a[1], L.a[2], L.a[3], L.a[4], L.a[5], L.a[6], L.a[7],
+            L.a[8], L.a[9], L.a[10], L.a[11], L.a[12], L.a[13], L.a[14], L.a[15] >
+            (Vec16s(a), Vec16s(b));
+        if (!(flags & blend_addz)) return y;               // no remaining zeroing
+    }
+    else if constexpr ((flags & blend_b) == 0) {           // nothing from b. just permute a
+        return permute32 <i0, i1, i2, i3, i4, i5, i6, i7, i8, i9, i10, i11, i12, i13, i14, i15,
+            i16, i17, i18, i19, i20, i21, i22, i23,i24, i25, i26, i27, i28, i29, i30, i31 > (a);
+    }
+    else if constexpr ((flags & blend_a) == 0) {           // nothing from a. just permute b
+        constexpr EList<int, 64> L = blend_perm_indexes<32, 2>(indexs); // get permutation indexes
+        return permute32 <
+            L.a[32], L.a[33], L.a[34], L.a[35], L.a[36], L.a[37], L.a[38], L.a[39],
+            L.a[40], L.a[41], L.a[42], L.a[43], L.a[44], L.a[45], L.a[46], L.a[47],
+            L.a[48], L.a[49], L.a[50], L.a[51], L.a[52], L.a[53], L.a[54], L.a[55],
+            L.a[56], L.a[57], L.a[58], L.a[59], L.a[60], L.a[61], L.a[62], L.a[63] > (b);
+    }
+    else { // No special cases
+#if INSTRSET >= 10 && defined (__AVX512VBMI__) // AVX512VL + AVX512VBMI. use vpermi2b
+        if constexpr ((flags & (blend_perma | blend_permb)) != 0) {
+            constexpr EList <int8_t, 32> bm = perm_mask_broad<Vec32c>(indexs);
+            return _mm256_maskz_permutex2var_epi8(zero_mask<32>(indexs), a, Vec32c().load(bm.a), b);
         }
-        return t1;
-    }
-
-    // special case: shift left
-    const int slb = i0 > 0 ? i0 : i31 - 31;
-    if (slb > 0 && slb < 32 
-        && (i0 ==slb+ 0||i0 <0) && (i1 ==slb+ 1||i1 <0) && (i2 ==slb+ 2||i2 <0) && (i3 ==slb+ 3||i3 <0)
-        && (i4 ==slb+ 4||i4 <0) && (i5 ==slb+ 5||i5 <0) && (i6 ==slb+ 6||i6 <0) && (i7 ==slb+ 7||i7 <0)
-        && (i8 ==slb+ 8||i8 <0) && (i9 ==slb+ 9||i9 <0) && (i10==slb+10||i10<0) && (i11==slb+11||i11<0)
-        && (i12==slb+12||i12<0) && (i13==slb+13||i13<0) && (i14==slb+14||i14<0) && (i15==slb+15||i15<0)
-        && (i16==slb+16||i16<0) && (i17==slb+17||i17<0) && (i18==slb+18||i18<0) && (i19==slb+19||i19<0)
-        && (i20==slb+20||i20<0) && (i21==slb+21||i21<0) && (i22==slb+22||i22<0) && (i23==slb+23||i23<0)
-        && (i24==slb+24||i24<0) && (i25==slb+25||i25<0) && (i26==slb+26||i26<0) && (i27==slb+27||i27<0)
-        && (i28==slb+28||i28<0) && (i29==slb+29||i29<0) && (i30==slb+30||i30<0) && (i31==slb+31||i31<0)) {
-        t1 = _mm256_permute2x128_si256(a, b, 0x21);
-        if (slb < 16) t1 = _mm256_alignr_epi8(t1, a, slb & 15);
-        else          t1 = _mm256_alignr_epi8(b, t1, slb & 15);
-        if (mz != -1) {
-            // zero some elements
-            const __m256i maskz = constant8i <
-                int((i0 <0?0:0xFF) | (i1 <0?0:0xFF00) | (i2 <0?0:0xFF0000) | (i3 <0?0:0xFF000000)),
-                int((i4 <0?0:0xFF) | (i5 <0?0:0xFF00) | (i6 <0?0:0xFF0000) | (i7 <0?0:0xFF000000)),
-                int((i8 <0?0:0xFF) | (i9 <0?0:0xFF00) | (i10<0?0:0xFF0000) | (i11<0?0:0xFF000000)),
-                int((i12<0?0:0xFF) | (i13<0?0:0xFF00) | (i14<0?0:0xFF0000) | (i15<0?0:0xFF000000)),
-                int((i16<0?0:0xFF) | (i17<0?0:0xFF00) | (i18<0?0:0xFF0000) | (i19<0?0:0xFF000000)),
-                int((i20<0?0:0xFF) | (i21<0?0:0xFF00) | (i22<0?0:0xFF0000) | (i23<0?0:0xFF000000)),
-                int((i24<0?0:0xFF) | (i25<0?0:0xFF00) | (i26<0?0:0xFF0000) | (i27<0?0:0xFF000000)),
-                int((i28<0?0:0xFF) | (i29<0?0:0xFF00) | (i30<0?0:0xFF0000) | (i31<0?0:0xFF000000)) > ();
-            return _mm256_and_si256(t1, maskz);
+#endif
+        // permute a and b separately, then blend.
+        Vec32c ya = a, yb = b;  // a and b permuted
+        constexpr EList<int, 64> L = blend_perm_indexes<32, 0>(indexs); // get permutation indexes
+        if constexpr ((flags & blend_perma) != 0) {
+            ya = permute32 <
+                L.a[0],  L.a[1],  L.a[2],  L.a[3],  L.a[4],  L.a[5],  L.a[6],  L.a[7],
+                L.a[8],  L.a[9],  L.a[10], L.a[11], L.a[12], L.a[13], L.a[14], L.a[15],
+                L.a[16], L.a[17], L.a[18], L.a[19], L.a[20], L.a[21], L.a[22], L.a[23],
+                L.a[24], L.a[25], L.a[26], L.a[27], L.a[28], L.a[29], L.a[30], L.a[31] > (ya);
         }
-        return t1;
-    }
-    // special case: shift right
-    const int srb = i0 > 0 ? (i0^32) : (i31^32) - 31;
-    if (srb > 0 && srb < 32
-        && ((i0 ^32)==srb+ 0||i0 <0) && ((i1 ^32)==srb+ 1||i1 <0) && ((i2 ^32)==srb+ 2||i2 <0) && ((i3 ^32)==srb+ 3||i3 <0)
-        && ((i4 ^32)==srb+ 4||i4 <0) && ((i5 ^32)==srb+ 5||i5 <0) && ((i6 ^32)==srb+ 6||i6 <0) && ((i7 ^32)==srb+ 7||i7 <0)
-        && ((i8 ^32)==srb+ 8||i8 <0) && ((i9 ^32)==srb+ 9||i9 <0) && ((i10^32)==srb+10||i10<0) && ((i11^32)==srb+11||i11<0)
-        && ((i12^32)==srb+12||i12<0) && ((i13^32)==srb+13||i13<0) && ((i14^32)==srb+14||i14<0) && ((i15^32)==srb+15||i15<0)
-        && ((i16^32)==srb+16||i16<0) && ((i17^32)==srb+17||i17<0) && ((i18^32)==srb+18||i18<0) && ((i19^32)==srb+19||i19<0)
-        && ((i20^32)==srb+20||i20<0) && ((i21^32)==srb+21||i21<0) && ((i22^32)==srb+22||i22<0) && ((i23^32)==srb+23||i23<0)
-        && ((i24^32)==srb+24||i24<0) && ((i25^32)==srb+25||i25<0) && ((i26^32)==srb+26||i26<0) && ((i27^32)==srb+27||i27<0)
-        && ((i28^32)==srb+28||i28<0) && ((i29^32)==srb+29||i29<0) && ((i30^32)==srb+30||i30<0) && ((i31^32)==srb+31||i31<0)) {
-        t1 = _mm256_permute2x128_si256(b, a, 0x21);
-        if (srb < 16) t1 = _mm256_alignr_epi8(t1, b, srb & 15);
-        else          t1 = _mm256_alignr_epi8(a, t1, srb & 15);
-        if (mz != -1) {
-            // zero some elements
-            const __m256i maskz = constant8i <
-                int((i0 <0?0:0xFF) | (i1 <0?0:0xFF00) | (i2 <0?0:0xFF0000) | (i3 <0?0:0xFF000000)),
-                int((i4 <0?0:0xFF) | (i5 <0?0:0xFF00) | (i6 <0?0:0xFF0000) | (i7 <0?0:0xFF000000)),
-                int((i8 <0?0:0xFF) | (i9 <0?0:0xFF00) | (i10<0?0:0xFF0000) | (i11<0?0:0xFF000000)),
-                int((i12<0?0:0xFF) | (i13<0?0:0xFF00) | (i14<0?0:0xFF0000) | (i15<0?0:0xFF000000)),
-                int((i16<0?0:0xFF) | (i17<0?0:0xFF00) | (i18<0?0:0xFF0000) | (i19<0?0:0xFF000000)),
-                int((i20<0?0:0xFF) | (i21<0?0:0xFF00) | (i22<0?0:0xFF0000) | (i23<0?0:0xFF000000)),
-                int((i24<0?0:0xFF) | (i25<0?0:0xFF00) | (i26<0?0:0xFF0000) | (i27<0?0:0xFF000000)),
-                int((i28<0?0:0xFF) | (i29<0?0:0xFF00) | (i30<0?0:0xFF0000) | (i31<0?0:0xFF000000)) > ();
-            return _mm256_and_si256(t1, maskz);
+        if constexpr ((flags & blend_permb) != 0) {
+            yb = permute32 <
+                L.a[32], L.a[33], L.a[34], L.a[35], L.a[36], L.a[37], L.a[38], L.a[39],
+                L.a[40], L.a[41], L.a[42], L.a[43], L.a[44], L.a[45], L.a[46], L.a[47],
+                L.a[48], L.a[49], L.a[50], L.a[51], L.a[52], L.a[53], L.a[54], L.a[55],
+                L.a[56], L.a[57], L.a[58], L.a[59], L.a[60], L.a[61], L.a[62], L.a[63] > (yb);
         }
-        return t1;
-    }
-
-    // general case: permute and blend and possible zero
-    const int blank = (mz == -1) ? -0x100 : -1;  // ignore or zero
-
-    // permute and blend
-    __m256i ta = permute32c <
-        (i0 &32)?blank:i0 , (i1 &32)?blank:i1 , (i2 &32)?blank:i2 , (i3 &32)?blank:i3 , 
-        (i4 &32)?blank:i4 , (i5 &32)?blank:i5 , (i6 &32)?blank:i6 , (i7 &32)?blank:i7 , 
-        (i8 &32)?blank:i8 , (i9 &32)?blank:i9 , (i10&32)?blank:i10, (i11&32)?blank:i11,
-        (i12&32)?blank:i12, (i13&32)?blank:i13, (i14&32)?blank:i14, (i15&32)?blank:i15, 
-        (i16&32)?blank:i16, (i17&32)?blank:i17, (i18&32)?blank:i18, (i19&32)?blank:i19, 
-        (i20&32)?blank:i20, (i21&32)?blank:i21, (i22&32)?blank:i22, (i23&32)?blank:i23, 
-        (i24&32)?blank:i24, (i25&32)?blank:i25, (i26&32)?blank:i26, (i27&32)?blank:i27, 
-        (i28&32)?blank:i28, (i29&32)?blank:i29, (i30&32)?blank:i30, (i31&32)?blank:i31 > (a);
-
-    __m256i tb = permute32c <
-        ((i0 ^32)&32)?blank:i0 ^32, ((i1 ^32)&32)?blank:i1 ^32, ((i2 ^32)&32)?blank:i2 ^32, ((i3 ^32)&32)?blank:i3 ^32, 
-        ((i4 ^32)&32)?blank:i4 ^32, ((i5 ^32)&32)?blank:i5 ^32, ((i6 ^32)&32)?blank:i6 ^32, ((i7 ^32)&32)?blank:i7 ^32, 
-        ((i8 ^32)&32)?blank:i8 ^32, ((i9 ^32)&32)?blank:i9 ^32, ((i10^32)&32)?blank:i10^32, ((i11^32)&32)?blank:i11^32,
-        ((i12^32)&32)?blank:i12^32, ((i13^32)&32)?blank:i13^32, ((i14^32)&32)?blank:i14^32, ((i15^32)&32)?blank:i15^32,
-        ((i16^32)&32)?blank:i16^32, ((i17^32)&32)?blank:i17^32, ((i18^32)&32)?blank:i18^32, ((i19^32)&32)?blank:i19^32,
-        ((i20^32)&32)?blank:i20^32, ((i21^32)&32)?blank:i21^32, ((i22^32)&32)?blank:i22^32, ((i23^32)&32)?blank:i23^32,
-        ((i24^32)&32)?blank:i24^32, ((i25^32)&32)?blank:i25^32, ((i26^32)&32)?blank:i26^32, ((i27^32)&32)?blank:i27^32,
-        ((i28^32)&32)?blank:i28^32, ((i29^32)&32)?blank:i29^32, ((i30^32)&32)?blank:i30^32, ((i31^32)&32)?blank:i31^32 > (b);
-
-    if (blank == -1) {
-        // we have zeroed, need only to OR
-        return _mm256_or_si256(ta, tb);
-    }
-    // no zeroing, need to blend
-    mask = constant8i <
-        int(((i0 <<2)&0x80) | ((i1 <<10)&0x8000) | ((i2 <<18)&0x800000) | (uint32_t(i3 <<26)&0x80000000)) ,
-        int(((i4 <<2)&0x80) | ((i5 <<10)&0x8000) | ((i6 <<18)&0x800000) | (uint32_t(i7 <<26)&0x80000000)) ,
-        int(((i8 <<2)&0x80) | ((i9 <<10)&0x8000) | ((i10<<18)&0x800000) | (uint32_t(i11<<26)&0x80000000)) ,
-        int(((i12<<2)&0x80) | ((i13<<10)&0x8000) | ((i14<<18)&0x800000) | (uint32_t(i15<<26)&0x80000000)) ,
-        int(((i16<<2)&0x80) | ((i17<<10)&0x8000) | ((i18<<18)&0x800000) | (uint32_t(i19<<26)&0x80000000)) ,
-        int(((i20<<2)&0x80) | ((i21<<10)&0x8000) | ((i22<<18)&0x800000) | (uint32_t(i23<<26)&0x80000000)) ,
-        int(((i24<<2)&0x80) | ((i25<<10)&0x8000) | ((i26<<18)&0x800000) | (uint32_t(i27<<26)&0x80000000)) ,
-        int(((i28<<2)&0x80) | ((i29<<10)&0x8000) | ((i30<<18)&0x800000) | (uint32_t(i31<<26)&0x80000000)) > ();
-
-    return _mm256_blendv_epi8(ta, tb, mask);  // blend
+        constexpr uint32_t mb = (uint32_t)make_bit_mask<32, 0x305>(indexs);// blend mask
+#if INSTRSET >= 10 // AVX512VL
+        y = _mm256_mask_mov_epi8 (ya, mb, yb);
+#else  // AVX2
+        constexpr EList <int8_t, 32> bm = make_broad_mask<Vec32c>(mb);
+        y = _mm256_blendv_epi8 (ya, yb, Vec32c().load(bm.a));
+#endif
+    }
+    if constexpr ((flags & blend_zeroing) != 0) {          // additional zeroing needed
+#if INSTRSET >= 10  // use compact mask
+        y = _mm256_maskz_mov_epi8(zero_mask<32>(indexs), y);
+#else  // use broad mask
+        constexpr EList <int8_t, 32> bm = zero_mask_broad<Vec32c>(indexs);
+        y = _mm256_and_si256(Vec32c().load(bm.a), y);
+#endif
+    }
+    return y;
 }
 
-template <
-    int i0,  int i1,  int i2,  int i3,  int i4,  int i5,  int i6,  int i7, 
-    int i8,  int i9,  int i10, int i11, int i12, int i13, int i14, int i15,
-    int i16, int i17, int i18, int i19, int i20, int i21, int i22, int i23,
-    int i24, int i25, int i26, int i27, int i28, int i29, int i30, int i31 >
-    static inline Vec32uc blend32uc(Vec32uc const & a, Vec32uc const & b) {
-        return Vec32uc (blend32c<i0,i1,i2,i3,i4,i5,i6,i7,i8,i9,i10,i11,i12,i13,i14,i15,    
-            i16,i17,i18,i19,i20,i21,i22,i23,i24,i25,i26,i27,i28,i29,i30,i31> (a, b));
+template <int ... i0>
+static inline Vec32uc blend32(Vec32uc const a, Vec32uc const b) {
+    return Vec32uc (blend32 <i0 ...> (Vec32c(a), Vec32c(b)));
 }
 
 
@@ -4520,32 +4649,16 @@ template <
 * These functions use vector elements as indexes into a table.
 * The table is given as one or more vectors or as an array.
 *
-* This can be used for several purposes:
-*  - table lookup
-*  - permute or blend with variable indexes
-*  - blend from more than two sources
-*  - gather non-contiguous data
-*
-* An index out of range may produce any value - the actual value produced is
-* implementation dependent and may be different for different instruction
-* sets. An index out of range does not produce an error message or exception.
-*
-* Example:
-* Vec8i a(2,0,0,6,4,3,5,0);                 // index a is (  2,   0,   0,   6,   4,   3,   5,   0)
-* Vec8i b(100,101,102,103,104,105,106,107); // table b is (100, 101, 102, 103, 104, 105, 106, 107)
-* Vec8i c;
-* c = lookup8 (a,b);                        // c is       (102, 100, 100, 106, 104, 103, 105, 100)
-*
 *****************************************************************************/
 
-static inline Vec32c lookup32(Vec32c const & index, Vec32c const & table) {
+static inline Vec32c lookup32(Vec32c const index, Vec32c const table) {
 #ifdef __XOP__  // AMD XOP instruction set. Use VPPERM
     Vec16c t0 = _mm_perm_epi8(table.get_low(), table.get_high(), index.get_low());
     Vec16c t1 = _mm_perm_epi8(table.get_low(), table.get_high(), index.get_high());
     return Vec32c(t0, t1);
 #else
-    Vec32c f0 = constant8i<0,0,0,0,0x10101010,0x10101010,0x10101010,0x10101010>();
-    Vec32c f1 = constant8i<0x10101010,0x10101010,0x10101010,0x10101010,0,0,0,0>();
+    Vec32c f0 = constant8ui<0,0,0,0,0x10101010,0x10101010,0x10101010,0x10101010>();
+    Vec32c f1 = constant8ui<0x10101010,0x10101010,0x10101010,0x10101010,0,0,0,0>();
     Vec32c tablef = _mm256_permute4x64_epi64(table, 0x4E);   // low and high parts swapped
     Vec32c r0 = _mm256_shuffle_epi8(table,  (index ^ f0) + 0x70);
     Vec32c r1 = _mm256_shuffle_epi8(tablef, (index ^ f1) + 0x70);
@@ -4554,18 +4667,21 @@ static inline Vec32c lookup32(Vec32c const & index, Vec32c const & table) {
 }
 
 template <int n>
-static inline Vec32c lookup(Vec32uc const & index, void const * table) {
-    if (n <=  0) return 0;
-    if (n <= 16) {
+static inline Vec32c lookup(Vec32uc const index, void const * table) {
+    if constexpr (n <=  0) return 0;
+    if constexpr (n <= 16) {
         Vec16c tt = Vec16c().load(table);
         Vec16c r0 = lookup16(index.get_low(),  tt);
         Vec16c r1 = lookup16(index.get_high(), tt);
         return Vec32c(r0, r1);
     }
-    if (n <= 32) return lookup32(index, Vec32c().load(table));
+    if constexpr (n <= 32) return lookup32(index, Vec32c().load(table));
     // n > 32. Limit index
     Vec32uc index1;
-    if ((n & (n-1)) == 0) {
+    if constexpr (n == INT_MAX) {
+        index1 = index;
+    }
+    else if constexpr ((n & (n-1)) == 0) {
         // n is a power of 2, make index modulo n
         index1 = Vec32uc(index) & uint8_t(n-1);
     }
@@ -4577,37 +4693,40 @@ static inline Vec32c lookup(Vec32uc const & index, void const * table) {
     Vec32c t0 = _mm256_i32gather_epi32((const int *)table, __m256i(mask0 & Vec8ui(index1)),      1); // positions 0, 4, 8,  ...
     Vec32c t1 = _mm256_i32gather_epi32((const int *)table, __m256i(mask0 & _mm256_srli_epi32(index1, 8)), 1); // positions 1, 5, 9,  ...
     Vec32c t2 = _mm256_i32gather_epi32((const int *)table, __m256i(mask0 & _mm256_srli_epi32(index1,16)), 1); // positions 2, 6, 10, ...
-    Vec32c t3 = _mm256_i32gather_epi32((const int *)table,         _mm256_srli_epi32(index1,24), 1); // positions 3, 7, 11, ...
-    t0 = t0 & mask0;
-    t1 = _mm256_slli_epi32(t1 & mask0,  8);
-    t2 = _mm256_slli_epi32(t2 & mask0, 16);
+    Vec32c t3 = _mm256_i32gather_epi32((const int *)table, _mm256_srli_epi32(index1,24), 1); // positions 3, 7, 11, ...
+    t0 = t0 & Vec32c(mask0);
+    t1 = _mm256_slli_epi32(t1 & Vec32c(mask0),  8);
+    t2 = _mm256_slli_epi32(t2 & Vec32c(mask0), 16);
     t3 = _mm256_slli_epi32(t3,         24);
     return (t0 | t3) | (t1 | t2);
 }
 
 template <int n>
-static inline Vec32c lookup(Vec32c const & index, void const * table) {
+static inline Vec32c lookup(Vec32c const index, void const * table) {
     return lookup<n>(Vec32uc(index), table);
 }
 
 
-static inline Vec16s lookup16(Vec16s const & index, Vec16s const & table) {
+static inline Vec16s lookup16(Vec16s const index, Vec16s const table) {
     return Vec16s(lookup32(Vec32c(index * 0x202 + 0x100), Vec32c(table)));
 }
 
 template <int n>
-static inline Vec16s lookup(Vec16s const & index, void const * table) {
-    if (n <=  0) return 0;
-    if (n <=  8) {
-        Vec8s table1 = Vec8s().load(table);        
-        return Vec16s(       
+static inline Vec16s lookup(Vec16s const index, void const * table) {
+    if constexpr (n <=  0) return 0;
+    if constexpr (n <=  8) {
+        Vec8s table1 = Vec8s().load(table);
+        return Vec16s(
             lookup8 (index.get_low(),  table1),
             lookup8 (index.get_high(), table1));
     }
-    if (n <= 16) return lookup16(index, Vec16s().load(table));
+    if constexpr (n <= 16) return lookup16(index, Vec16s().load(table));
     // n > 16. Limit index
     Vec16us index1;
-    if ((n & (n-1)) == 0) {
+    if constexpr (n == INT_MAX) {
+        index1 = index;
+    }
+    else if constexpr ((n & (n-1)) == 0) {
         // n is a power of 2, make index modulo n
         index1 = Vec16us(index) & (n-1);
     }
@@ -4617,21 +4736,21 @@ static inline Vec16s lookup(Vec16s const & index, void const * table) {
     }
     Vec16s t1 = _mm256_i32gather_epi32((const int *)table, __m256i(Vec8ui(index1) & 0x0000FFFF), 2);  // even positions
     Vec16s t2 = _mm256_i32gather_epi32((const int *)table, _mm256_srli_epi32(index1, 16) , 2);        // odd  positions
-    return blend16s<0,16,2,18,4,20,6,22,8,24,10,26,12,28,14,30>(t1, t2);
+    return blend16<0,16,2,18,4,20,6,22,8,24,10,26,12,28,14,30>(t1, t2);
 }
 
-static inline Vec8i lookup8(Vec8i const & index, Vec8i const & table) {
+static inline Vec8i lookup8(Vec8i const index, Vec8i const table) {
     return _mm256_permutevar8x32_epi32(table, index);
 }
 
 template <int n>
-static inline Vec8i lookup(Vec8i const & index, void const * table) {
-    if (n <= 0) return 0;
-    if (n <= 8) {
+static inline Vec8i lookup(Vec8i const index, void const * table) {
+    if constexpr (n <= 0) return 0;
+    if constexpr (n <= 8) {
         Vec8i table1 = Vec8i().load(table);
         return lookup8(index, table1);
     }
-    if (n <= 16) {
+    if constexpr (n <= 16) {
         Vec8i table1 = Vec8i().load(table);
         Vec8i table2 = Vec8i().load((int32_t const*)table + 8);
         Vec8i y1 = lookup8(index, table1);
@@ -4641,7 +4760,10 @@ static inline Vec8i lookup(Vec8i const & index, void const * table) {
     }
     // n > 16. Limit index
     Vec8ui index1;
-    if ((n & (n-1)) == 0) {
+    if constexpr (n == INT_MAX) {
+        index1 = index;
+    }
+    else if constexpr ((n & (n-1)) == 0) {
         // n is a power of 2, make index modulo n
         index1 = Vec8ui(index) & (n-1);
     }
@@ -4652,16 +4774,19 @@ static inline Vec8i lookup(Vec8i const & index, void const * table) {
     return _mm256_i32gather_epi32((const int *)table, index1, 4);
 }
 
-static inline Vec4q lookup4(Vec4q const & index, Vec4q const & table) {
+static inline Vec4q lookup4(Vec4q const index, Vec4q const table) {
     return Vec4q(lookup8(Vec8i(index * 0x200000002ll + 0x100000000ll), Vec8i(table)));
 }
 
 template <int n>
-static inline Vec4q lookup(Vec4q const & index, int64_t const * table) {
-    if (n <= 0) return 0;
+static inline Vec4q lookup(Vec4q const index, int64_t const * table) {
+    if constexpr (n <= 0) return 0;
     // n > 0. Limit index
     Vec4uq index1;
-    if ((n & (n-1)) == 0) {
+    if constexpr (n == INT_MAX) {
+        index1 = Vec4uq(index);
+    }
+    else if constexpr ((n & (n-1)) == 0) {
         // n is a power of 2, make index modulo n
         index1 = Vec4uq(index) & (n-1);
     }
@@ -4669,9 +4794,9 @@ static inline Vec4q lookup(Vec4q const & index, int64_t const * table) {
         // n is not a power of 2, limit to n-1.
         // There is no 64-bit min instruction, but we can use the 32-bit unsigned min,
         // since n is a 32-bit integer
-        index1 = Vec4uq(min(Vec8ui(index), constant8i<n-1, 0, n-1, 0, n-1, 0, n-1, 0>()));
+        index1 = Vec4uq(min(Vec8ui(index), constant8ui<n-1, 0, n-1, 0, n-1, 0, n-1, 0>()));
     }
-// old compilers can't agree how to define a 64 bit integer. Intel and MS use __int64, gcc use long long
+/* old compilers can't agree how to define a 64 bit integer. Intel and MS use __int64, gcc use long long
 #if defined (__clang__) && CLANG_VERSION < 30400
 // clang 3.3 uses const int * in accordance with official Intel doc., which is wrong. will be fixed
     return _mm256_i64gather_epi64((const int *)table, index1, 8);
@@ -4680,39 +4805,69 @@ static inline Vec4q lookup(Vec4q const & index, int64_t const * table) {
     return _mm256_i64gather_epi64((const int64_t *)table, index1, 8);
 #else
 // Gnu, Clang 3.4, MS 11.0
+*/
     return _mm256_i64gather_epi64((const long long *)table, index1, 8);
-#endif
+//#endif
 }
 
 
 /*****************************************************************************
 *
-*          Other permutations with variable indexes
+*          Byte shifts
 *
 *****************************************************************************/
 
 // Function shift_bytes_up: shift whole vector left by b bytes.
-// You may use a permute function instead if b is a compile-time constant
-static inline Vec32c shift_bytes_up(Vec32c const & a, int b) {
-    if (b < 16) {    
-        return Vec32c(shift_bytes_up(a.get_low(),b), shift_bytes_up(a.get_high(),b) | shift_bytes_down(a.get_low(),16-b));
+template <unsigned int b>
+static inline Vec32c shift_bytes_up(Vec32c const a) {
+    __m256i ahi, alo;
+    if constexpr (b == 0) return a;
+#if INSTRSET >= 10  // AVX512VL
+    else if constexpr ((b & 3) == 0) {  // b is divisible by 4
+        return _mm256_alignr_epi32(a, _mm256_setzero_si256(), (8 - (b >> 2)) & 7);
+    }
+#endif
+    else if constexpr (b < 16) {
+        alo = a;
+        ahi = _mm256_inserti128_si256 (_mm256_setzero_si256(), _mm256_castsi256_si128(a), 1);// shift a 16 bytes up, zero lower part
+    }
+    else if constexpr (b < 32) {
+        alo = _mm256_inserti128_si256 (_mm256_setzero_si256(), _mm256_castsi256_si128(a), 1);// shift a 16 bytes up, zero lower part
+        ahi = _mm256_setzero_si256();
     }
     else {
-        return Vec32c(Vec16c(0), shift_bytes_up(a.get_high(),b-16));
+        return _mm256_setzero_si256();                     // zero
     }
+    if constexpr ((b & 0xF) == 0) return alo;              // modulo 16. no more shift needeed
+    return _mm256_alignr_epi8(alo, ahi, 16-(b & 0xF));     // shift within 16-bytes lane
 }
 
 // Function shift_bytes_down: shift whole vector right by b bytes
-// You may use a permute function instead if b is a compile-time constant
-static inline Vec32c shift_bytes_down(Vec32c const & a, int b) {
-    if (b < 16) {    
-        return Vec32c(shift_bytes_down(a.get_low(),b) | shift_bytes_up(a.get_high(),16-b), shift_bytes_down(a.get_high(),b));
+template <unsigned int b>
+static inline Vec32c shift_bytes_down(Vec32c const a) {
+#if INSTRSET >= 10  // AVX512VL
+    if constexpr ((b & 3) == 0) {  // b is divisible by 4
+        return _mm256_alignr_epi32(_mm256_setzero_si256(), a, (b >> 2) & 7);
+    }
+#endif
+    __m256i ahi, alo;
+    if constexpr (b < 16) {
+        // shift a 16 bytes down, zero upper part
+        alo = _mm256_inserti128_si256(_mm256_setzero_si256(), _mm256_extracti128_si256(a, 1), 0);// make sure the upper part is zero (otherwise, an optimizing compiler can mess it up)
+        ahi = a;
+    }
+    else if constexpr (b < 32) {
+        alo = _mm256_setzero_si256();                      // zero
+        ahi = _mm256_inserti128_si256(_mm256_setzero_si256(), _mm256_extracti128_si256(a, 1), 0);// shift a 16 bytes down, zero upper part
     }
     else {
-        return Vec32c(shift_bytes_down(a.get_high(),b-16), Vec16c(0));
+        return _mm256_setzero_si256();                     // zero
     }
+    if constexpr ((b & 0xF) == 0) return ahi;              // modulo 16. no more shift needeed
+    return _mm256_alignr_epi8(alo, ahi, b & 0xF);          // shift within 16-bytes lane
 }
 
+
 /*****************************************************************************
 *
 *          Gather functions with fixed indexes
@@ -4721,48 +4876,37 @@ static inline Vec32c shift_bytes_down(Vec32c const & a, int b) {
 // Load elements from array a with indices i0, i1, i2, i3, i4, i5, i6, i7
 template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7>
 static inline Vec8i gather8i(void const * a) {
-    Static_error_check<(i0|i1|i2|i3|i4|i5|i6|i7)>=0> Negative_array_index;  // Error message if index is negative
-    const int i01min = i0 < i1 ? i0 : i1;
-    const int i23min = i2 < i3 ? i2 : i3;
-    const int i45min = i4 < i5 ? i4 : i5;
-    const int i67min = i6 < i7 ? i6 : i7;
-    const int i0123min = i01min < i23min ? i01min : i23min;
-    const int i4567min = i45min < i67min ? i45min : i67min;
-    const int imin = i0123min < i4567min ? i0123min : i4567min;
-    const int i01max = i0 > i1 ? i0 : i1;
-    const int i23max = i2 > i3 ? i2 : i3;
-    const int i45max = i4 > i5 ? i4 : i5;
-    const int i67max = i6 > i7 ? i6 : i7;
-    const int i0123max = i01max > i23max ? i01max : i23max;
-    const int i4567max = i45max > i67max ? i45max : i67max;
-    const int imax = i0123max > i4567max ? i0123max : i4567max;
-
-    if (imax - imin <= 7) {
+    int constexpr indexs[8] = { i0, i1, i2, i3, i4, i5, i6, i7 }; // indexes as array
+    constexpr int imin = min_index(indexs);
+    constexpr int imax = max_index(indexs);
+    static_assert(imin >= 0, "Negative index in gather function");
+
+    if constexpr (imax - imin <= 7) {
         // load one contiguous block and permute
-        if (imax > 7) {
+        if constexpr (imax > 7) {
             // make sure we don't read past the end of the array
             Vec8i b = Vec8i().load((int32_t const *)a + imax-7);
-            return permute8i<i0-imax+7, i1-imax+7, i2-imax+7, i3-imax+7, i4-imax+7, i5-imax+7, i6-imax+7, i7-imax+7>(b);
+            return permute8<i0-imax+7, i1-imax+7, i2-imax+7, i3-imax+7, i4-imax+7, i5-imax+7, i6-imax+7, i7-imax+7>(b);
         }
         else {
             Vec8i b = Vec8i().load((int32_t const *)a + imin);
-            return permute8i<i0-imin, i1-imin, i2-imin, i3-imin, i4-imin, i5-imin, i6-imin, i7-imin>(b);
+            return permute8<i0-imin, i1-imin, i2-imin, i3-imin, i4-imin, i5-imin, i6-imin, i7-imin>(b);
         }
     }
-    if ((i0<imin+8 || i0>imax-8) && (i1<imin+8 || i1>imax-8) && (i2<imin+8 || i2>imax-8) && (i3<imin+8 || i3>imax-8)
+    if constexpr ((i0<imin+8 || i0>imax-8) && (i1<imin+8 || i1>imax-8) && (i2<imin+8 || i2>imax-8) && (i3<imin+8 || i3>imax-8)
     &&  (i4<imin+8 || i4>imax-8) && (i5<imin+8 || i5>imax-8) && (i6<imin+8 || i6>imax-8) && (i7<imin+8 || i7>imax-8)) {
         // load two contiguous blocks and blend
         Vec8i b = Vec8i().load((int32_t const *)a + imin);
         Vec8i c = Vec8i().load((int32_t const *)a + imax-7);
-        const int j0 = i0<imin+8 ? i0-imin : 15-imax+i0;
-        const int j1 = i1<imin+8 ? i1-imin : 15-imax+i1;
-        const int j2 = i2<imin+8 ? i2-imin : 15-imax+i2;
-        const int j3 = i3<imin+8 ? i3-imin : 15-imax+i3;
-        const int j4 = i4<imin+8 ? i4-imin : 15-imax+i4;
-        const int j5 = i5<imin+8 ? i5-imin : 15-imax+i5;
-        const int j6 = i6<imin+8 ? i6-imin : 15-imax+i6;
-        const int j7 = i7<imin+8 ? i7-imin : 15-imax+i7;
-        return blend8i<j0, j1, j2, j3, j4, j5, j6, j7>(b, c);
+        constexpr int j0 = i0<imin+8 ? i0-imin : 15-imax+i0;
+        constexpr int j1 = i1<imin+8 ? i1-imin : 15-imax+i1;
+        constexpr int j2 = i2<imin+8 ? i2-imin : 15-imax+i2;
+        constexpr int j3 = i3<imin+8 ? i3-imin : 15-imax+i3;
+        constexpr int j4 = i4<imin+8 ? i4-imin : 15-imax+i4;
+        constexpr int j5 = i5<imin+8 ? i5-imin : 15-imax+i5;
+        constexpr int j6 = i6<imin+8 ? i6-imin : 15-imax+i6;
+        constexpr int j7 = i7<imin+8 ? i7-imin : 15-imax+i7;
+        return blend8<j0, j1, j2, j3, j4, j5, j6, j7>(b, c);
     }
     // use AVX2 gather
     return _mm256_i32gather_epi32((const int *)a, Vec8i(i0,i1,i2,i3,i4,i5,i6,i7), 4);
@@ -4770,26 +4914,24 @@ static inline Vec8i gather8i(void const * a) {
 
 template <int i0, int i1, int i2, int i3>
 static inline Vec4q gather4q(void const * a) {
-    Static_error_check<(i0|i1|i2|i3)>=0> Negative_array_index;  // Error message if index is negative
-    const int i01min = i0 < i1 ? i0 : i1;
-    const int i23min = i2 < i3 ? i2 : i3;
-    const int imin   = i01min < i23min ? i01min : i23min;
-    const int i01max = i0 > i1 ? i0 : i1;
-    const int i23max = i2 > i3 ? i2 : i3;
-    const int imax   = i01max > i23max ? i01max : i23max;
-    if (imax - imin <= 3) {
+    int constexpr indexs[4] = { i0, i1, i2, i3 }; // indexes as array
+    constexpr int imin = min_index(indexs);
+    constexpr int imax = max_index(indexs);
+    static_assert(imin >= 0, "Negative index in gather function");
+
+    if constexpr (imax - imin <= 3) {
         // load one contiguous block and permute
-        if (imax > 3) {
+        if constexpr (imax > 3) {
             // make sure we don't read past the end of the array
             Vec4q b = Vec4q().load((int64_t const *)a + imax-3);
-            return permute4q<i0-imax+3, i1-imax+3, i2-imax+3, i3-imax+3>(b);
+            return permute4<i0-imax+3, i1-imax+3, i2-imax+3, i3-imax+3>(b);
         }
         else {
             Vec4q b = Vec4q().load((int64_t const *)a + imin);
-            return permute4q<i0-imin, i1-imin, i2-imin, i3-imin>(b);
+            return permute4<i0-imin, i1-imin, i2-imin, i3-imin>(b);
         }
     }
-    if ((i0<imin+4 || i0>imax-4) && (i1<imin+4 || i1>imax-4) && (i2<imin+4 || i2>imax-4) && (i3<imin+4 || i3>imax-4)) {
+    if constexpr ((i0<imin+4 || i0>imax-4) && (i1<imin+4 || i1>imax-4) && (i2<imin+4 || i2>imax-4) && (i3<imin+4 || i3>imax-4)) {
         // load two contiguous blocks and blend
         Vec4q b = Vec4q().load((int64_t const *)a + imin);
         Vec4q c = Vec4q().load((int64_t const *)a + imax-3);
@@ -4797,20 +4939,10 @@ static inline Vec4q gather4q(void const * a) {
         const int j1 = i1<imin+4 ? i1-imin : 7-imax+i1;
         const int j2 = i2<imin+4 ? i2-imin : 7-imax+i2;
         const int j3 = i3<imin+4 ? i3-imin : 7-imax+i3;
-        return blend4q<j0, j1, j2, j3>(b, c);
+        return blend4<j0, j1, j2, j3>(b, c);
     }
     // use AVX2 gather
-    // old compilers can't agree how to define a 64 bit integer. Intel and MS use __int64, gcc use long long
-#if defined (__clang__) && CLANG_VERSION < 30400
-    // clang 3.3 uses const int * in accordance with official Intel doc., which is wrong. will be fixed
-    return _mm256_i32gather_epi64((const int *)a, Vec4i(i0,i1,i2,i3), 8);
-#elif defined (_MSC_VER) && _MSC_VER < 1700 && ! defined(__INTEL_COMPILER)
-    // Old MS and Intel use non-standard type __int64
-    return _mm256_i32gather_epi64((const int64_t *)a, Vec4i(i0,i1,i2,i3), 8);
-#else
-    // Gnu, Clang 3.4, MS 11.0
     return _mm256_i32gather_epi64((const long long *)a, Vec4i(i0,i1,i2,i3), 8);
-#endif
 }
 
 
@@ -4821,34 +4953,25 @@ static inline Vec4q gather4q(void const * a) {
 ******************************************************************************
 *
 * These functions write the elements of a vector to arbitrary positions in an
-* array in memory. Each vector element is written to an array position 
+* array in memory. Each vector element is written to an array position
 * determined by an index. An element is not written if the corresponding
 * index is out of range.
 * The indexes can be specified as constant template parameters or as an
 * integer vector.
-* 
-* The scatter functions are useful if the data are distributed in a sparce
-* manner into the array. If the array is dense then it is more efficient
-* to permute the data into the right positions and then write the whole
-* permuted vector into the array.
-*
-* Example:
-* Vec8q a(10,11,12,13,14,15,16,17);
-* int64_t b[16] = {0};
-* scatter<0,2,14,10,1,-1,5,9>(a,b); 
-* // Now, b = {10,14,11,0,0,16,0,0,0,17,13,0,0,0,12,0}
 *
 *****************************************************************************/
 
 template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7>
-static inline void scatter(Vec8i const & data, void * array) {
-#if defined (__AVX512VL__)
-    __m256i indx = constant8i<i0,i1,i2,i3,i4,i5,i6,i7>();
-    __mmask16 mask = uint16_t(i0>=0 | (i1>=0)<<1 | (i2>=0)<<2 | (i3>=0)<<3| (i4>=0)<<4| (i5>=0)<<5| (i6>=0)<<6| (i7>=0)<<7);
+static inline void scatter(Vec8i const data, void * array) {
+#if INSTRSET >= 10 //  __AVX512VL__
+    __m256i indx = constant8ui<i0,i1,i2,i3,i4,i5,i6,i7>();
+    __mmask8 mask = uint8_t((i0>=0) | ((i1>=0)<<1) | ((i2>=0)<<2) | ((i3>=0)<<3) |
+        ((i4>=0)<<4) | ((i5>=0)<<5) | ((i6>=0)<<6) | ((i7>=0)<<7));
     _mm256_mask_i32scatter_epi32((int*)array, mask, indx, data, 4);
-#elif defined (__AVX512F__)
-    __m512i indx = _mm512_castsi256_si512(constant8i<i0,i1,i2,i3,i4,i5,i6,i7>());
-    __mmask16 mask = uint16_t(i0>=0 | (i1>=0)<<1 | (i2>=0)<<2 | (i3>=0)<<3| (i4>=0)<<4| (i5>=0)<<5| (i6>=0)<<6| (i7>=0)<<7);
+#elif INSTRSET >= 9  //  __AVX512F__
+    __m512i indx = _mm512_castsi256_si512(constant8ui<i0,i1,i2,i3,i4,i5,i6,i7>());
+    __mmask16 mask = uint16_t((i0>=0) | ((i1>=0)<<1) | ((i2>=0)<<2) | ((i3>=0)<<3) |
+        ((i4>=0)<<4) | ((i5>=0)<<5) | ((i6>=0)<<6) | ((i7>=0)<<7));
     _mm512_mask_i32scatter_epi32((int*)array, mask, indx, _mm512_castsi256_si512(data), 4);
 #else
     int32_t* arr = (int32_t*)array;
@@ -4860,15 +4983,15 @@ static inline void scatter(Vec8i const & data, void * array) {
 }
 
 template <int i0, int i1, int i2, int i3>
-static inline void scatter(Vec4q const & data, void * array) {
-#if defined (__AVX512VL__)
-    __m128i indx = constant4i<i0,i1,i2,i3>();
-    __mmask16 mask = uint16_t(i0>=0 | (i1>=0)<<1 | (i2>=0)<<2 | (i3>=0)<<3);
+static inline void scatter(Vec4q const data, void * array) {
+#if INSTRSET >= 10 //  __AVX512VL__
+    __m128i indx = constant4ui<i0,i1,i2,i3>();
+    __mmask8 mask = uint8_t((i0>=0) | ((i1>=0)<<1) | ((i2>=0)<<2) | ((i3>=0)<<3));
     _mm256_mask_i32scatter_epi64((long long *)array, mask, indx, data, 8);
-#elif defined (__AVX512F__)
-    __m256i indx = _mm256_castsi128_si256(constant4i<i0,i1,i2,i3>());
-    __mmask16 mask = uint16_t(i0>=0 | (i1>=0)<<1 | (i2>=0)<<2 | (i3>=0)<<3);
-    _mm512_mask_i32scatter_epi64((long long*)array, mask, indx, _mm512_castsi256_si512(data), 8);
+#elif INSTRSET >= 9  //  __AVX512F__
+    __m256i indx = _mm256_castsi128_si256(constant4ui<i0,i1,i2,i3>());
+    __mmask16 mask = uint16_t((i0>=0) | ((i1>=0)<<1) | ((i2>=0)<<2) | ((i3>=0)<<3));
+    _mm512_mask_i32scatter_epi64((long long*)array, (__mmask8)mask, indx, _mm512_castsi256_si512(data), 8);
 #else
     int64_t* arr = (int64_t*)array;
     const int index[4] = {i0,i1,i2,i3};
@@ -4878,141 +5001,149 @@ static inline void scatter(Vec4q const & data, void * array) {
 #endif
 }
 
-static inline void scatter(Vec8i const & index, uint32_t limit, Vec8i const & data, void * array) {
-#if defined (__AVX512VL__)
-    __mmask16 mask = _mm256_cmplt_epu32_mask(index, Vec8ui(limit));
-    _mm256_mask_i32scatter_epi32((int*)array, mask, index, data, 4);
-#elif defined (__AVX512F__)
-    // 16 bit mask. upper 8 bits are (0<0) = false
-    __mmask16 mask = _mm512_cmplt_epu32_mask(_mm512_castsi256_si512(index), _mm512_castsi256_si512(Vec8ui(limit)));
-    _mm512_mask_i32scatter_epi32((int*)array, mask, _mm512_castsi256_si512(index), _mm512_castsi256_si512(data), 4);
+
+/*****************************************************************************
+*
+*          Scatter functions with variable indexes
+*
+*****************************************************************************/
+
+static inline void scatter(Vec8i const index, uint32_t limit, Vec8i const data, void * destination) {
+#if INSTRSET >= 10 //  __AVX512VL__
+    __mmask8 mask = _mm256_cmplt_epu32_mask(index, Vec8ui(limit));
+    _mm256_mask_i32scatter_epi32((int*)destination, mask, index, data, 4);
+#elif INSTRSET >= 9  //  __AVX512F__
+    // 16 bit mask, upper 8 bits are 0. Usually, we can rely on the upper bit of an extended vector to be zero, but we will mask then off the be sure
+    //__mmask16 mask = _mm512_cmplt_epu32_mask(_mm512_castsi256_si512(index), _mm512_castsi256_si512(Vec8ui(limit)));
+    __mmask16 mask = _mm512_mask_cmplt_epu32_mask(0xFF, _mm512_castsi256_si512(index), _mm512_castsi256_si512(Vec8ui(limit)));
+    _mm512_mask_i32scatter_epi32((int*)destination, mask, _mm512_castsi256_si512(index), _mm512_castsi256_si512(data), 4);
 #else
-    int32_t* arr = (int32_t*)array;
+    int32_t* arr = (int32_t*)destination;
     for (int i = 0; i < 8; i++) {
         if (uint32_t(index[i]) < limit) arr[index[i]] = data[i];
     }
 #endif
-} 
+}
 
-static inline void scatter(Vec4q const & index, uint32_t limit, Vec4q const & data, void * array) {
-#if defined (__AVX512VL__)
-    __mmask16 mask = _mm256_cmplt_epu64_mask(index, Vec4uq(uint64_t(limit)));
-    _mm256_mask_i64scatter_epi64((long long*)array, mask, index, data, 8);
-#elif defined (__AVX512F__)
-    // 16 bit mask. upper 8 bits are (0<0) = false
-    __mmask16 mask = _mm512_cmplt_epu64_mask(_mm512_castsi256_si512(index), _mm512_castsi256_si512(Vec4uq(uint64_t(limit))));
-    _mm512_mask_i64scatter_epi64((long long*)array, mask, _mm512_castsi256_si512(index), _mm512_castsi256_si512(data), 8);
+static inline void scatter(Vec4q const index, uint32_t limit, Vec4q const data, void * destination) {
+#if INSTRSET >= 10 //  __AVX512VL__
+    __mmask8 mask = _mm256_cmplt_epu64_mask(index, Vec4uq(uint64_t(limit)));
+    _mm256_mask_i64scatter_epi64((long long*)destination, mask, index, data, 8);
+#elif INSTRSET >= 9  //  __AVX512F__
+    // 16 bit mask. upper 12 bits are 0
+    __mmask16 mask = _mm512_mask_cmplt_epu64_mask(0xF, _mm512_castsi256_si512(index), _mm512_castsi256_si512(Vec4uq(uint64_t(limit))));
+    _mm512_mask_i64scatter_epi64((long long*)destination, (__mmask8)mask, _mm512_castsi256_si512(index), _mm512_castsi256_si512(data), 8);
 #else
-    int64_t* arr = (int64_t*)array;
+    int64_t* arr = (int64_t*)destination;
     for (int i = 0; i < 4; i++) {
         if (uint64_t(index[i]) < uint64_t(limit)) arr[index[i]] = data[i];
     }
 #endif
-} 
+}
 
-static inline void scatter(Vec4i const & index, uint32_t limit, Vec4q const & data, void * array) {
-#if defined (__AVX512VL__)
-    __mmask16 mask = _mm_cmplt_epu32_mask(index, Vec4ui(limit));
-    _mm256_mask_i32scatter_epi64((long long*)array, mask, index, data, 8);
-#elif defined (__AVX512F__)
-    // 16 bit mask. upper 8 bits are (0<0) = false
-    __mmask16 mask = _mm512_cmplt_epu32_mask(_mm512_castsi128_si512(index), _mm512_castsi128_si512(Vec4ui(limit)));
-    _mm512_mask_i32scatter_epi64((long long*)array, mask, _mm256_castsi128_si256(index), _mm512_castsi256_si512(data), 8);
+static inline void scatter(Vec4i const index, uint32_t limit, Vec4q const data, void * destination) {
+#if INSTRSET >= 10 //  __AVX512VL__
+    __mmask8 mask = _mm_cmplt_epu32_mask(index, Vec4ui(limit));
+    _mm256_mask_i32scatter_epi64((long long*)destination, mask, index, data, 8);
+#elif INSTRSET >= 9  //  __AVX512F__
+    // 16 bit mask. upper 12 bits are 0
+    __mmask16 mask = _mm512_mask_cmplt_epu32_mask(0xF, _mm512_castsi128_si512(index), _mm512_castsi128_si512(Vec4ui(limit)));
+    _mm512_mask_i32scatter_epi64((long long*)destination, (__mmask8)mask, _mm256_castsi128_si256(index), _mm512_castsi256_si512(data), 8);
 #else
-    int64_t* arr = (int64_t*)array;
+    int64_t* arr = (int64_t*)destination;
     for (int i = 0; i < 4; i++) {
         if (uint32_t(index[i]) < limit) arr[index[i]] = data[i];
     }
 #endif
-} 
+}
 
 /*****************************************************************************
 *
-*          Functions for conversion between integer sizes
+*          Functions for conversion between integer sizes and vector types
 *
 *****************************************************************************/
 
 // Extend 8-bit integers to 16-bit integers, signed and unsigned
 
 // Function extend_low : extends the low 16 elements to 16 bits with sign extension
-static inline Vec16s extend_low (Vec32c const & a) {
-    __m256i a2   = permute4q<0,-256,1,-256>(Vec4q(a));           // get bits 64-127 to position 128-191
+static inline Vec16s extend_low (Vec32c const a) {
+    __m256i a2   = _mm256_permute4x64_epi64(a, 0x10);            // get bits 64-127 to position 128-191
     __m256i sign = _mm256_cmpgt_epi8(_mm256_setzero_si256(),a2); // 0 > a2
     return         _mm256_unpacklo_epi8(a2, sign);               // interleave with sign extensions
 }
 
 // Function extend_high : extends the high 16 elements to 16 bits with sign extension
-static inline Vec16s extend_high (Vec32c const & a) {
-    __m256i a2   = permute4q<-256,2,-256,3>(Vec4q(a));           // get bits 128-191 to position 64-127
+static inline Vec16s extend_high (Vec32c const a) {
+    __m256i a2   = _mm256_permute4x64_epi64(a, 0xC8);            // get bits 128-191 to position 64-127
     __m256i sign = _mm256_cmpgt_epi8(_mm256_setzero_si256(),a2); // 0 > a2
     return         _mm256_unpackhi_epi8(a2, sign);               // interleave with sign extensions
 }
 
 // Function extend_low : extends the low 16 elements to 16 bits with zero extension
-static inline Vec16us extend_low (Vec32uc const & a) {
-    __m256i a2 = permute4q<0,-256,1,-256>(Vec4q(a));             // get bits 64-127 to position 128-191
+static inline Vec16us extend_low (Vec32uc const a) {
+    __m256i a2 = _mm256_permute4x64_epi64(a, 0x10);              // get bits 64-127 to position 128-191
     return    _mm256_unpacklo_epi8(a2, _mm256_setzero_si256());  // interleave with zero extensions
 }
 
 // Function extend_high : extends the high 19 elements to 16 bits with zero extension
-static inline Vec16us extend_high (Vec32uc const & a) {
-    __m256i a2 = permute4q<-256,2,-256,3>(Vec4q(a));             // get bits 128-191 to position 64-127
+static inline Vec16us extend_high (Vec32uc const a) {
+    __m256i a2 = _mm256_permute4x64_epi64(a, 0xC8);              // get bits 128-191 to position 64-127
     return  _mm256_unpackhi_epi8(a2, _mm256_setzero_si256());    // interleave with zero extensions
 }
 
 // Extend 16-bit integers to 32-bit integers, signed and unsigned
 
 // Function extend_low : extends the low 8 elements to 32 bits with sign extension
-static inline Vec8i extend_low (Vec16s const & a) {
-    __m256i a2   = permute4q<0,-256,1,-256>(Vec4q(a));           // get bits 64-127 to position 128-191
+static inline Vec8i extend_low (Vec16s const a) {
+    __m256i a2   = _mm256_permute4x64_epi64(a, 0x10);            // get bits 64-127 to position 128-191
     __m256i sign = _mm256_srai_epi16(a2, 15);                    // sign bit
     return         _mm256_unpacklo_epi16(a2 ,sign);              // interleave with sign extensions
 }
 
 // Function extend_high : extends the high 8 elements to 32 bits with sign extension
-static inline Vec8i extend_high (Vec16s const & a) {
-    __m256i a2 = permute4q<-256,2,-256,3>(Vec4q(a));             // get bits 128-191 to position 64-127
+static inline Vec8i extend_high (Vec16s const a) {
+    __m256i a2   = _mm256_permute4x64_epi64(a, 0xC8);            // get bits 128-191 to position 64-127
     __m256i sign = _mm256_srai_epi16(a2, 15);                    // sign bit
     return         _mm256_unpackhi_epi16(a2, sign);              // interleave with sign extensions
 }
 
 // Function extend_low : extends the low 8 elements to 32 bits with zero extension
-static inline Vec8ui extend_low (Vec16us const & a) {
-    __m256i a2 = permute4q<0,-256,1,-256>(Vec4q(a));             // get bits 64-127 to position 128-191
+static inline Vec8ui extend_low (Vec16us const a) {
+    __m256i a2   = _mm256_permute4x64_epi64(a, 0x10);            // get bits 64-127 to position 128-191
     return    _mm256_unpacklo_epi16(a2, _mm256_setzero_si256()); // interleave with zero extensions
 }
 
 // Function extend_high : extends the high 8 elements to 32 bits with zero extension
-static inline Vec8ui extend_high (Vec16us const & a) {
-    __m256i a2 = permute4q<-256,2,-256,3>(Vec4q(a));             // get bits 128-191 to position 64-127
+static inline Vec8ui extend_high (Vec16us const a) {
+    __m256i a2   = _mm256_permute4x64_epi64(a, 0xC8);            // get bits 128-191 to position 64-127
     return  _mm256_unpackhi_epi16(a2, _mm256_setzero_si256());   // interleave with zero extensions
 }
 
 // Extend 32-bit integers to 64-bit integers, signed and unsigned
 
 // Function extend_low : extends the low 4 elements to 64 bits with sign extension
-static inline Vec4q extend_low (Vec8i const & a) {
-    __m256i a2 = permute4q<0,-256,1,-256>(Vec4q(a));             // get bits 64-127 to position 128-191
+static inline Vec4q extend_low (Vec8i const a) {
+    __m256i a2   = _mm256_permute4x64_epi64(a, 0x10);            // get bits 64-127 to position 128-191
     __m256i sign = _mm256_srai_epi32(a2, 31);                    // sign bit
     return         _mm256_unpacklo_epi32(a2, sign);              // interleave with sign extensions
 }
 
 // Function extend_high : extends the high 4 elements to 64 bits with sign extension
-static inline Vec4q extend_high (Vec8i const & a) {
-    __m256i a2 = permute4q<-256,2,-256,3>(Vec4q(a));             // get bits 128-191 to position 64-127
+static inline Vec4q extend_high (Vec8i const a) {
+    __m256i a2   = _mm256_permute4x64_epi64(a, 0xC8);            // get bits 128-191 to position 64-127
     __m256i sign = _mm256_srai_epi32(a2, 31);                    // sign bit
     return         _mm256_unpackhi_epi32(a2, sign);              // interleave with sign extensions
 }
 
 // Function extend_low : extends the low 4 elements to 64 bits with zero extension
-static inline Vec4uq extend_low (Vec8ui const & a) {
-    __m256i a2 = permute4q<0,-256,1,-256>(Vec4q(a));             // get bits 64-127 to position 128-191
+static inline Vec4uq extend_low (Vec8ui const a) {
+    __m256i a2   = _mm256_permute4x64_epi64(a, 0x10);            // get bits 64-127 to position 128-191
     return  _mm256_unpacklo_epi32(a2, _mm256_setzero_si256());   // interleave with zero extensions
 }
 
 // Function extend_high : extends the high 4 elements to 64 bits with zero extension
-static inline Vec4uq extend_high (Vec8ui const & a) {
-    __m256i a2 = permute4q<-256,2,-256,3>(Vec4q(a));             // get bits 128-191 to position 64-127
+static inline Vec4uq extend_high (Vec8ui const a) {
+    __m256i a2   = _mm256_permute4x64_epi64(a, 0xC8);            // get bits 128-191 to position 64-127
     return  _mm256_unpackhi_epi32(a2, _mm256_setzero_si256());   // interleave with zero extensions
 }
 
@@ -5020,106 +5151,86 @@ static inline Vec4uq extend_high (Vec8ui const & a) {
 
 // Function compress : packs two vectors of 16-bit integers into one vector of 8-bit integers
 // Overflow wraps around
-static inline Vec32c compress (Vec16s const & low, Vec16s const & high) {
-    __m256i mask  = _mm256_set1_epi32(0x00FF00FF);            // mask for low bytes
-    __m256i lowm  = _mm256_and_si256(low, mask);              // bytes of low
-    __m256i highm = _mm256_and_si256(high, mask);             // bytes of high
-    __m256i pk    = _mm256_packus_epi16(lowm, highm);         // unsigned pack
-    return          _mm256_permute4x64_epi64(pk, 0xD8);       // put in right place
+static inline Vec32c compress (Vec16s const low, Vec16s const high) {
+    __m256i mask  = _mm256_set1_epi32(0x00FF00FF);         // mask for low bytes
+    __m256i lowm  = _mm256_and_si256(low, mask);           // bytes of low
+    __m256i highm = _mm256_and_si256(high, mask);          // bytes of high
+    __m256i pk    = _mm256_packus_epi16(lowm, highm);      // unsigned pack
+    return          _mm256_permute4x64_epi64(pk, 0xD8);    // put in right place
 }
 
 // Function compress : packs two vectors of 16-bit integers into one vector of 8-bit integers
 // Signed, with saturation
-static inline Vec32c compress_saturated (Vec16s const & low, Vec16s const & high) {
-    __m256i pk    = _mm256_packs_epi16(low,high);             // packed with signed saturation
-    return          _mm256_permute4x64_epi64(pk, 0xD8);       // put in right place
+static inline Vec32c compress_saturated (Vec16s const low, Vec16s const high) {
+    __m256i pk    = _mm256_packs_epi16(low,high);          // packed with signed saturation
+    return          _mm256_permute4x64_epi64(pk, 0xD8);    // put in right place
 }
 
 // Function compress : packs two vectors of 16-bit integers to one vector of 8-bit integers
 // Unsigned, overflow wraps around
-static inline Vec32uc compress (Vec16us const & low, Vec16us const & high) {
+static inline Vec32uc compress (Vec16us const low, Vec16us const high) {
     return  Vec32uc (compress((Vec16s)low, (Vec16s)high));
 }
 
 // Function compress : packs two vectors of 16-bit integers into one vector of 8-bit integers
 // Unsigned, with saturation
-static inline Vec32uc compress_saturated (Vec16us const & low, Vec16us const & high) {
-    __m256i maxval  = _mm256_set1_epi32(0x00FF00FF);          // maximum value
-    __m256i minval  = _mm256_setzero_si256();                 // minimum value = 0
-    __m256i low1    = _mm256_min_epu16(low,maxval);           // upper limit
-    __m256i high1   = _mm256_min_epu16(high,maxval);          // upper limit
-    __m256i low2    = _mm256_max_epu16(low1,minval);          // lower limit
-    __m256i high2   = _mm256_max_epu16(high1,minval);         // lower limit
-    __m256i pk      = _mm256_packus_epi16(low2,high2);        // this instruction saturates from signed 32 bit to unsigned 16 bit
-    return            _mm256_permute4x64_epi64(pk, 0xD8);     // put in right place
-}
-
-// Function compress : packs two vectors of 16-bit integers into one vector of 8-bit integers
-// Signed to unsigned, with saturation
-static inline Vec32uc compress_saturated_s2u (Vec16s const & low, Vec16s const & high) {
-    __m256i pk    = _mm256_packus_epi16(low,high);            // this instruction saturates from signed 16 bit to unsigned 8 bit
-    return          _mm256_permute4x64_epi64(pk, 0xD8);       // put in right place
+static inline Vec32uc compress_saturated (Vec16us const low, Vec16us const high) {
+    __m256i maxval  = _mm256_set1_epi32(0x00FF00FF);       // maximum value
+    __m256i low1    = _mm256_min_epu16(low,maxval);        // upper limit
+    __m256i high1   = _mm256_min_epu16(high,maxval);       // upper limit
+    __m256i pk      = _mm256_packus_epi16(low1,high1);     // this instruction saturates from signed 32 bit to unsigned 16 bit
+    return            _mm256_permute4x64_epi64(pk, 0xD8);  // put in right place
 }
 
 // Compress 32-bit integers to 16-bit integers, signed and unsigned, with and without saturation
 
 // Function compress : packs two vectors of 32-bit integers into one vector of 16-bit integers
 // Overflow wraps around
-static inline Vec16s compress (Vec8i const & low, Vec8i const & high) {
-    __m256i mask  = _mm256_set1_epi32(0x0000FFFF);            // mask for low words
-    __m256i lowm  = _mm256_and_si256(low,mask);               // bytes of low
-    __m256i highm = _mm256_and_si256(high,mask);              // bytes of high
-    __m256i pk    = _mm256_packus_epi32(lowm,highm);          // unsigned pack
-    return          _mm256_permute4x64_epi64(pk, 0xD8);       // put in right place
+static inline Vec16s compress (Vec8i const low, Vec8i const high) {
+    __m256i mask  = _mm256_set1_epi32(0x0000FFFF);         // mask for low words
+    __m256i lowm  = _mm256_and_si256(low,mask);            // words of low
+    __m256i highm = _mm256_and_si256(high,mask);           // words of high
+    __m256i pk    = _mm256_packus_epi32(lowm,highm);       // unsigned pack
+    return          _mm256_permute4x64_epi64(pk, 0xD8);    // put in right place
 }
 
 // Function compress : packs two vectors of 32-bit integers into one vector of 16-bit integers
 // Signed with saturation
-static inline Vec16s compress_saturated (Vec8i const & low, Vec8i const & high) {
-    __m256i pk    =  _mm256_packs_epi32(low,high);            // pack with signed saturation
-    return           _mm256_permute4x64_epi64(pk, 0xD8);      // put in right place
+static inline Vec16s compress_saturated (Vec8i const low, Vec8i const high) {
+    __m256i pk    =  _mm256_packs_epi32(low,high);         // pack with signed saturation
+    return           _mm256_permute4x64_epi64(pk, 0xD8);   // put in right place
 }
 
 // Function compress : packs two vectors of 32-bit integers into one vector of 16-bit integers
 // Overflow wraps around
-static inline Vec16us compress (Vec8ui const & low, Vec8ui const & high) {
+static inline Vec16us compress (Vec8ui const low, Vec8ui const high) {
     return Vec16us (compress((Vec8i)low, (Vec8i)high));
 }
 
 // Function compress : packs two vectors of 32-bit integers into one vector of 16-bit integers
 // Unsigned, with saturation
-static inline Vec16us compress_saturated (Vec8ui const & low, Vec8ui const & high) {
-    __m256i maxval  = _mm256_set1_epi32(0x0000FFFF);          // maximum value
-    __m256i minval  = _mm256_setzero_si256();                 // minimum value = 0
-    __m256i low1    = _mm256_min_epu32(low,maxval);           // upper limit
-    __m256i high1   = _mm256_min_epu32(high,maxval);          // upper limit
-    __m256i low2    = _mm256_max_epu32(low1,minval);          // lower limit
-    __m256i high2   = _mm256_max_epu32(high1,minval);         // lower limit
-    __m256i pk      = _mm256_packus_epi32(low2,high2);        // this instruction saturates from signed 32 bit to unsigned 16 bit
-    return            _mm256_permute4x64_epi64(pk, 0xD8);     // put in right place
-}
-
-// Function compress : packs two vectors of 32-bit integers into one vector of 16-bit integers
-// Signed to unsigned, with saturation
-static inline Vec16us compress_saturated_s2u (Vec8i const & low, Vec8i const & high) {
-    __m256i pk    =  _mm256_packus_epi32(low,high);           // this instruction saturates from signed 32 bit to unsigned 16 bit
-    return           _mm256_permute4x64_epi64(pk, 0xD8);      // put in right place
+static inline Vec16us compress_saturated (Vec8ui const low, Vec8ui const high) {
+    __m256i maxval  = _mm256_set1_epi32(0x0000FFFF);       // maximum value
+    __m256i low1    = _mm256_min_epu32(low,maxval);        // upper limit
+    __m256i high1   = _mm256_min_epu32(high,maxval);       // upper limit
+    __m256i pk      = _mm256_packus_epi32(low1,high1);     // this instruction saturates from signed 32 bit to unsigned 16 bit
+    return            _mm256_permute4x64_epi64(pk, 0xD8);  // put in right place
 }
 
 // Compress 64-bit integers to 32-bit integers, signed and unsigned, with and without saturation
 
 // Function compress : packs two vectors of 64-bit integers into one vector of 32-bit integers
 // Overflow wraps around
-static inline Vec8i compress (Vec4q const & low, Vec4q const & high) {
-    __m256i low2  = _mm256_shuffle_epi32(low,0xD8);           // low dwords of low  to pos. 0 and 32
-    __m256i high2 = _mm256_shuffle_epi32(high,0xD8);          // low dwords of high to pos. 0 and 32
-    __m256i pk    = _mm256_unpacklo_epi64(low2,high2);        // interleave
-    return          _mm256_permute4x64_epi64(pk, 0xD8);       // put in right place
+static inline Vec8i compress (Vec4q const low, Vec4q const high) {
+    __m256i low2  = _mm256_shuffle_epi32(low,0xD8);        // low dwords of low  to pos. 0 and 32
+    __m256i high2 = _mm256_shuffle_epi32(high,0xD8);       // low dwords of high to pos. 0 and 32
+    __m256i pk    = _mm256_unpacklo_epi64(low2,high2);     // interleave
+    return          _mm256_permute4x64_epi64(pk, 0xD8);    // put in right place
 }
 
 // Function compress : packs two vectors of 64-bit integers into one vector of 32-bit integers
 // Signed, with saturation
-static inline Vec8i compress_saturated (Vec4q const & a, Vec4q const & b) {
+static inline Vec8i compress_saturated (Vec4q const a, Vec4q const b) {
     Vec4q maxval = constant8ui<0x7FFFFFFF,0,0x7FFFFFFF,0,0x7FFFFFFF,0,0x7FFFFFFF,0>();
     Vec4q minval = constant8ui<0x80000000,0xFFFFFFFF,0x80000000,0xFFFFFFFF,0x80000000,0xFFFFFFFF,0x80000000,0xFFFFFFFF>();
     Vec4q a1  = min(a,maxval);
@@ -5131,26 +5242,132 @@ static inline Vec8i compress_saturated (Vec4q const & a, Vec4q const & b) {
 
 // Function compress : packs two vectors of 32-bit integers into one vector of 16-bit integers
 // Overflow wraps around
-static inline Vec8ui compress (Vec4uq const & low, Vec4uq const & high) {
+static inline Vec8ui compress (Vec4uq const low, Vec4uq const high) {
     return Vec8ui (compress((Vec4q)low, (Vec4q)high));
 }
 
 // Function compress : packs two vectors of 64-bit integers into one vector of 32-bit integers
 // Unsigned, with saturation
-static inline Vec8ui compress_saturated (Vec4uq const & low, Vec4uq const & high) {
-    __m256i zero     = _mm256_setzero_si256();                // 0
-    __m256i lowzero  = _mm256_cmpeq_epi32(low,zero);          // for each dword is zero
-    __m256i highzero = _mm256_cmpeq_epi32(high,zero);         // for each dword is zero
-    __m256i mone     = _mm256_set1_epi32(-1);                 // FFFFFFFF
-    __m256i lownz    = _mm256_xor_si256(lowzero,mone);        // for each dword is nonzero
-    __m256i highnz   = _mm256_xor_si256(highzero,mone);       // for each dword is nonzero
-    __m256i lownz2   = _mm256_srli_epi64(lownz,32);           // shift down to low dword
-    __m256i highnz2  = _mm256_srli_epi64(highnz,32);          // shift down to low dword
-    __m256i lowsatur = _mm256_or_si256(low,lownz2);           // low, saturated
-    __m256i hisatur  = _mm256_or_si256(high,highnz2);         // high, saturated
+static inline Vec8ui compress_saturated (Vec4uq const low, Vec4uq const high) {
+    __m256i zero     = _mm256_setzero_si256();             // 0
+    __m256i lowzero  = _mm256_cmpeq_epi32(low,zero);       // for each dword is zero
+    __m256i highzero = _mm256_cmpeq_epi32(high,zero);      // for each dword is zero
+    __m256i mone     = _mm256_set1_epi32(-1);              // FFFFFFFF
+    __m256i lownz    = _mm256_xor_si256(lowzero,mone);     // for each dword is nonzero
+    __m256i highnz   = _mm256_xor_si256(highzero,mone);    // for each dword is nonzero
+    __m256i lownz2   = _mm256_srli_epi64(lownz,32);        // shift down to low dword
+    __m256i highnz2  = _mm256_srli_epi64(highnz,32);       // shift down to low dword
+    __m256i lowsatur = _mm256_or_si256(low,lownz2);        // low, saturated
+    __m256i hisatur  = _mm256_or_si256(high,highnz2);      // high, saturated
     return  Vec8ui (compress(Vec4q(lowsatur), Vec4q(hisatur)));
 }
 
+// extend vectors to double size by adding zeroes
+
+#ifdef ZEXT_MISSING
+// GCC v. 9 is missing the _mm256_zextsi128_si256 intrinsic
+
+static inline Vec32c extend_z(Vec16c a) {
+    return Vec32c(a, Vec16c(0));
+}
+static inline Vec32uc extend_z(Vec16uc a) {
+    return Vec32uc(a, Vec16uc(0));
+}
+static inline Vec16s extend_z(Vec8s a) {
+    return Vec16s(a, Vec8s(0));
+}
+static inline Vec16us extend_z(Vec8us a) {
+    return Vec16us(a, Vec8us(0));
+}
+static inline Vec8i extend_z(Vec4i a) {
+    return Vec8i(a, Vec4i(0));
+}
+static inline Vec8ui extend_z(Vec4ui a) {
+    return Vec8ui(a, Vec4ui(0));
+}
+static inline Vec4q extend_z(Vec2q a) {
+    return Vec4q(a, Vec2q(0));
+}
+static inline Vec4uq extend_z(Vec2uq a) {
+    return Vec4uq(a, Vec2uq(0));
+}
+
+#else
+
+static inline Vec32c extend_z(Vec16c a) {
+    return _mm256_zextsi128_si256(a);
+}
+static inline Vec32uc extend_z(Vec16uc a) {
+    return _mm256_zextsi128_si256(a);
+}
+static inline Vec16s extend_z(Vec8s a) {
+    return _mm256_zextsi128_si256(a);
+}
+static inline Vec16us extend_z(Vec8us a) {
+    return _mm256_zextsi128_si256(a);
+}
+static inline Vec8i extend_z(Vec4i a) {
+    return _mm256_zextsi128_si256(a);
+}
+static inline Vec8ui extend_z(Vec4ui a) {
+    return _mm256_zextsi128_si256(a);
+}
+static inline Vec4q extend_z(Vec2q a) {
+    return _mm256_zextsi128_si256(a);
+}
+static inline Vec4uq extend_z(Vec2uq a) {
+    return _mm256_zextsi128_si256(a);
+}
+#endif // ZEXT_MISSING
+
+#if INSTRSET < 10  // broad boolean vectors
+#ifdef ZEXT_MISSING
+// GCC v. 9 is missing the _mm256_zextsi128_si256 intrinsic
+static inline Vec32cb extend_z(Vec16cb a) {
+    return Vec32cb(a, Vec16cb(false));
+}
+static inline Vec16sb extend_z(Vec8sb a) {
+    return Vec16sb(a, Vec8sb(false));
+}
+static inline Vec8ib extend_z(Vec4ib a) {
+    return Vec8ib(a, Vec4ib(false));
+}
+static inline Vec4qb extend_z(Vec2qb a) {
+    return Vec4qb(a, Vec2qb(false));
+}
+
+#else
+static inline Vec32cb extend_z(Vec16cb a) {
+    return _mm256_zextsi128_si256(a);
+}
+static inline Vec16sb extend_z(Vec8sb a) {
+    return _mm256_zextsi128_si256(a);
+}
+static inline Vec8ib extend_z(Vec4ib a) {
+    return _mm256_zextsi128_si256(a);
+}
+static inline Vec4qb extend_z(Vec2qb a) {
+    return _mm256_zextsi128_si256(a);
+}
+#endif  // ZEXT_MISSING
+
+#else    // compact boolean vectors
+
+static inline Vec32b extend_z(Vec16b a) {
+    return __mmask32(a);
+}
+static inline Vec16b extend_z(Vec8b a) {
+    return __mmask16(a);
+}
+static inline Vec8b extend_z(Vec4b a) {
+    return __mmask8(uint8_t(a) & 0x0F);
+}
+static inline Vec4b extend_z(Vec2b a) {
+    return __mmask8(uint8_t(a) & 0x03);
+} 
+
+#endif
+
 
 /*****************************************************************************
 *
@@ -5163,15 +5380,14 @@ static inline Vec8ui compress_saturated (Vec4uq const & low, Vec4uq const & high
 // vector operator / : divide each element by divisor
 
 // vector of 8 32-bit signed integers
-static inline Vec8i operator / (Vec8i const & a, Divisor_i const & d) {
+static inline Vec8i operator / (Vec8i const a, Divisor_i const d) {
     __m256i m   = _mm256_broadcastq_epi64(d.getm());       // broadcast multiplier
     __m256i sgn = _mm256_broadcastq_epi64(d.getsign());    // broadcast sign of d
     __m256i t1  = _mm256_mul_epi32(a,m);                   // 32x32->64 bit signed multiplication of even elements of a
     __m256i t2  = _mm256_srli_epi64(t1,32);                // high dword of even numbered results
     __m256i t3  = _mm256_srli_epi64(a,32);                 // get odd elements of a into position for multiplication
     __m256i t4  = _mm256_mul_epi32(t3,m);                  // 32x32->64 bit signed multiplication of odd elements
-    __m256i t5  = constant8i<0,-1,0,-1,0,-1,0,-1> ();      // mask for odd elements
-    __m256i t7  = _mm256_blendv_epi8(t2,t4,t5);            // blend two results
+    __m256i t7  = _mm256_blend_epi32(t2,t4,0xAA);
     __m256i t8  = _mm256_add_epi32(t7,a);                  // add
     __m256i t9  = _mm256_sra_epi32(t8,d.gets1());          // shift right artihmetic
     __m256i t10 = _mm256_srai_epi32(a,31);                 // sign of a
@@ -5181,22 +5397,21 @@ static inline Vec8i operator / (Vec8i const & a, Divisor_i const & d) {
 }
 
 // vector of 8 32-bit unsigned integers
-static inline Vec8ui operator / (Vec8ui const & a, Divisor_ui const & d) {
+static inline Vec8ui operator / (Vec8ui const a, Divisor_ui const d) {
     __m256i m   = _mm256_broadcastq_epi64(d.getm());       // broadcast multiplier
     __m256i t1  = _mm256_mul_epu32(a,m);                   // 32x32->64 bit unsigned multiplication of even elements of a
     __m256i t2  = _mm256_srli_epi64(t1,32);                // high dword of even numbered results
     __m256i t3  = _mm256_srli_epi64(a,32);                 // get odd elements of a into position for multiplication
     __m256i t4  = _mm256_mul_epu32(t3,m);                  // 32x32->64 bit unsigned multiplication of odd elements
-    __m256i t5  = constant8i<0,-1,0,-1,0,-1,0,-1> ();      // mask for odd elements
-    __m256i t7  = _mm256_blendv_epi8(t2,t4,t5);            // blend two results
+    __m256i t7  = _mm256_blend_epi32(t2,t4,0xAA);
     __m256i t8  = _mm256_sub_epi32(a,t7);                  // subtract
     __m256i t9  = _mm256_srl_epi32(t8,d.gets1());          // shift right logical
     __m256i t10 = _mm256_add_epi32(t7,t9);                 // add
-    return        _mm256_srl_epi32(t10,d.gets2());         // shift right logical 
+    return        _mm256_srl_epi32(t10,d.gets2());         // shift right logical
 }
 
 // vector of 16 16-bit signed integers
-static inline Vec16s operator / (Vec16s const & a, Divisor_s const & d) {
+static inline Vec16s operator / (Vec16s const a, Divisor_s const d) {
     __m256i m   = _mm256_broadcastq_epi64(d.getm());       // broadcast multiplier
     __m256i sgn = _mm256_broadcastq_epi64(d.getsign());    // broadcast sign of d
     __m256i t1  = _mm256_mulhi_epi16(a, m);                // multiply high signed words
@@ -5209,65 +5424,88 @@ static inline Vec16s operator / (Vec16s const & a, Divisor_s const & d) {
 }
 
 // vector of 16 16-bit unsigned integers
-static inline Vec16us operator / (Vec16us const & a, Divisor_us const & d) {
+static inline Vec16us operator / (Vec16us const a, Divisor_us const d) {
     __m256i m   = _mm256_broadcastq_epi64(d.getm());       // broadcast multiplier
     __m256i t1  = _mm256_mulhi_epu16(a, m);                // multiply high signed words
     __m256i t2  = _mm256_sub_epi16(a,t1);                  // subtract
     __m256i t3  = _mm256_srl_epi16(t2,d.gets1());          // shift right logical
     __m256i t4  = _mm256_add_epi16(t1,t3);                 // add
-    return        _mm256_srl_epi16(t4,d.gets2());          // shift right logical 
+    return        _mm256_srl_epi16(t4,d.gets2());          // shift right logical
 }
 
 // vector of 32 8-bit signed integers
-static inline Vec32c operator / (Vec32c const & a, Divisor_s const & d) {
+static inline Vec32c operator / (Vec32c const a, Divisor_s const d) {
+#if INSTRSET >= 10
+    // sign-extend even-numbered and odd-numbered elements to 16 bits
+    Vec16s  even = _mm256_srai_epi16(_mm256_slli_epi16(a, 8),8);
+    Vec16s  odd  = _mm256_srai_epi16(a, 8);
+    Vec16s  evend = even / d;         // divide even-numbered elements
+    Vec16s  oddd  = odd  / d;         // divide odd-numbered  elements
+            oddd  = _mm256_slli_epi16(oddd, 8); // shift left to put back in place
+    __m256i res  = _mm256_mask_mov_epi8(evend, 0xAAAAAAAA, oddd); // interleave even and odd
+    return res;
+#else
     // expand into two Vec16s
     Vec16s low  = extend_low(a) / d;
     Vec16s high = extend_high(a) / d;
     return compress(low,high);
+#endif
 }
 
+
 // vector of 32 8-bit unsigned integers
-static inline Vec32uc operator / (Vec32uc const & a, Divisor_us const & d) {
+static inline Vec32uc operator / (Vec32uc const a, Divisor_us const d) {
+    // zero-extend even-numbered and odd-numbered elements to 16 bits
+#if INSTRSET >= 10
+    Vec16us  even = _mm256_maskz_mov_epi8(__mmask32(0x55555555), a);
+    Vec16us  odd  = _mm256_srli_epi16(a, 8);
+    Vec16us  evend = even / d;          // divide even-numbered elements
+    Vec16us  oddd  = odd  / d;          // divide odd-numbered  elements
+    oddd  = _mm256_slli_epi16(oddd, 8); // shift left to put back in place
+    __m256i res  = _mm256_mask_mov_epi8(evend, 0xAAAAAAAA, oddd); // interleave even and odd
+    return res;
+#else
     // expand into two Vec16s
     Vec16us low  = extend_low(a) / d;
     Vec16us high = extend_high(a) / d;
     return compress(low,high);
+#endif
 }
 
 // vector operator /= : divide
-static inline Vec8i & operator /= (Vec8i & a, Divisor_i const & d) {
+static inline Vec8i & operator /= (Vec8i & a, Divisor_i const d) {
     a = a / d;
     return a;
 }
 
 // vector operator /= : divide
-static inline Vec8ui & operator /= (Vec8ui & a, Divisor_ui const & d) {
+static inline Vec8ui & operator /= (Vec8ui & a, Divisor_ui const d) {
     a = a / d;
     return a;
 }
 
 // vector operator /= : divide
-static inline Vec16s & operator /= (Vec16s & a, Divisor_s const & d) {
+static inline Vec16s & operator /= (Vec16s & a, Divisor_s const d) {
     a = a / d;
     return a;
 }
 
 
 // vector operator /= : divide
-static inline Vec16us & operator /= (Vec16us & a, Divisor_us const & d) {
+static inline Vec16us & operator /= (Vec16us & a, Divisor_us const d) {
     a = a / d;
     return a;
 
 }
 
 // vector operator /= : divide
-static inline Vec32c & operator /= (Vec32c & a, Divisor_s const & d) {
+static inline Vec32c & operator /= (Vec32c & a, Divisor_s const d) {
     a = a / d;
     return a;
 }
 
 // vector operator /= : divide
-static inline Vec32uc & operator /= (Vec32uc & a, Divisor_us const & d) {
+static inline Vec32uc & operator /= (Vec32uc & a, Divisor_us const d) {
     a = a / d;
     return a;
 }
@@ -5281,41 +5519,41 @@ static inline Vec32uc & operator /= (Vec32uc & a, Divisor_us const & d) {
 
 // Divide Vec8i by compile-time constant
 template <int32_t d>
-static inline Vec8i divide_by_i(Vec8i const & x) {
-    Static_error_check<(d!=0)> Dividing_by_zero;                     // Error message if dividing by zero
-    if (d ==  1) return  x;
-    if (d == -1) return -x;
-    if (uint32_t(d) == 0x80000000u) return Vec8i(x == Vec8i(0x80000000)) & 1; // prevent overflow when changing sign
-    const uint32_t d1 = d > 0 ? uint32_t(d) : -uint32_t(d);          // compile-time abs(d). (force GCC compiler to treat d as 32 bits, not 64 bits)
-    if ((d1 & (d1-1)) == 0) {
+static inline Vec8i divide_by_i(Vec8i const x) {
+    static_assert(d != 0, "Integer division by zero");
+    if constexpr (d ==  1) return  x;
+    if constexpr (d == -1) return -x;
+    if constexpr (uint32_t(d) == 0x80000000u) return Vec8i(x == Vec8i(0x80000000)) & 1; // prevent overflow when changing sign
+    constexpr uint32_t d1 = d > 0 ? uint32_t(d) : uint32_t(-d);// compile-time abs(d). (force GCC compiler to treat d as 32 bits, not 64 bits)
+    if constexpr ((d1 & (d1-1)) == 0) {
         // d1 is a power of 2. use shift
-        const int k = bit_scan_reverse_const(d1);
+        constexpr int k = bit_scan_reverse_const(d1);
         __m256i sign;
-        if (k > 1) sign = _mm256_srai_epi32(x, k-1); else sign = x;  // k copies of sign bit
-        __m256i bias    = _mm256_srli_epi32(sign, 32-k);             // bias = x >= 0 ? 0 : k-1
-        __m256i xpbias  = _mm256_add_epi32 (x, bias);                // x + bias
-        __m256i q       = _mm256_srai_epi32(xpbias, k);              // (x + bias) >> k
-        if (d > 0)      return q;                                    // d > 0: return  q
-        return _mm256_sub_epi32(_mm256_setzero_si256(), q);          // d < 0: return -q
+        if constexpr (k > 1) sign = _mm256_srai_epi32(x, k-1); else sign = x;  // k copies of sign bit
+        __m256i bias    = _mm256_srli_epi32(sign, 32-k);   // bias = x >= 0 ? 0 : k-1
+        __m256i xpbias  = _mm256_add_epi32 (x, bias);      // x + bias
+        __m256i q       = _mm256_srai_epi32(xpbias, k);    // (x + bias) >> k
+        if (d > 0)      return q;                          // d > 0: return  q
+        return _mm256_sub_epi32(_mm256_setzero_si256(), q);// d < 0: return -q
     }
     // general case
-    const int32_t sh = bit_scan_reverse_const(uint32_t(d1)-1);       // ceil(log2(d1)) - 1. (d1 < 2 handled by power of 2 case)
-    const int32_t mult = int(1 + (uint64_t(1) << (32+sh)) / uint32_t(d1) - (int64_t(1) << 32));   // multiplier
+    constexpr int32_t sh = bit_scan_reverse_const(uint32_t(d1)-1);// ceil(log2(d1)) - 1. (d1 < 2 handled by power of 2 case)
+    constexpr int32_t mult = int(1 + (uint64_t(1) << (32+sh)) / uint32_t(d1) - (int64_t(1) << 32));// multiplier
     const Divisor_i div(mult, sh, d < 0 ? -1 : 0);
     return x / div;
 }
 
 // define Vec8i a / const_int(d)
 template <int32_t d>
-static inline Vec8i operator / (Vec8i const & a, Const_int_t<d>) {
+static inline Vec8i operator / (Vec8i const a, Const_int_t<d>) {
     return divide_by_i<d>(a);
 }
 
 // define Vec8i a / const_uint(d)
 template <uint32_t d>
-static inline Vec8i operator / (Vec8i const & a, Const_uint_t<d>) {
-    Static_error_check< (d<0x80000000u) > Error_overflow_dividing_signed_by_unsigned; // Error: dividing signed by overflowing unsigned
-    return divide_by_i<int32_t(d)>(a);                               // signed divide
+static inline Vec8i operator / (Vec8i const a, Const_uint_t<d>) {
+    static_assert(d < 0x80000000u, "Dividing signed integer by overflowing unsigned");
+    return divide_by_i<int32_t(d)>(a);                     // signed divide
 }
 
 // vector operator /= : divide
@@ -5335,50 +5573,51 @@ static inline Vec8i & operator /= (Vec8i & a, Const_uint_t<d> b) {
 
 // Divide Vec8ui by compile-time constant
 template <uint32_t d>
-static inline Vec8ui divide_by_ui(Vec8ui const & x) {
-    Static_error_check<(d!=0)> Dividing_by_zero;                     // Error message if dividing by zero
-    if (d == 1) return x;                                            // divide by 1
-    const int b = bit_scan_reverse_const(d);                         // floor(log2(d))
-    if ((uint32_t(d) & (uint32_t(d)-1)) == 0) {
+static inline Vec8ui divide_by_ui(Vec8ui const x) {
+    static_assert(d != 0, "Integer division by zero");
+    if constexpr (d == 1) return x;                        // divide by 1
+    constexpr int b = bit_scan_reverse_const(d);           // floor(log2(d))
+    if constexpr ((uint32_t(d) & (uint32_t(d)-1)) == 0) {
         // d is a power of 2. use shift
-        return  _mm256_srli_epi32(x, b);                             // x >> b
+        return  _mm256_srli_epi32(x, b);                   // x >> b
     }
     // general case (d > 2)
-    uint32_t mult = uint32_t((uint64_t(1) << (b+32)) / d);           // multiplier = 2^(32+b) / d
-    const uint64_t rem = (uint64_t(1) << (b+32)) - uint64_t(d)*mult; // remainder 2^(32+b) % d
-    const bool round_down = (2*rem < d);                             // check if fraction is less than 0.5
-    if (!round_down) {
-        mult = mult + 1;                                             // round up mult
-    }
+    constexpr uint32_t mult = uint32_t((uint64_t(1) << (b+32)) / d); // multiplier = 2^(32+b) / d
+    constexpr uint64_t rem = (uint64_t(1) << (b+32)) - uint64_t(d)*mult; // remainder 2^(32+b) % d
+    constexpr bool round_down = (2*rem < d);               // check if fraction is less than 0.5
+    constexpr uint32_t mult1 = round_down ? mult : mult + 1;
     // do 32*32->64 bit unsigned multiplication and get high part of result
-    const __m256i multv = _mm256_set_epi32(0,mult,0,mult,0,mult,0,mult);// zero-extend mult and broadcast
-    __m256i t1 = _mm256_mul_epu32(x,multv);                          // 32x32->64 bit unsigned multiplication of x[0] and x[2]
-    if (round_down) {
-        t1      = _mm256_add_epi64(t1,multv);                        // compensate for rounding error. (x+1)*m replaced by x*m+m to avoid overflow
+#if INSTRSET >= 10
+    const __m256i multv = _mm256_maskz_set1_epi32(0x55, mult1);// zero-extend mult and broadcast
+#else
+    const __m256i multv = Vec4uq(uint64_t(mult1));         // zero-extend mult and broadcast
+#endif
+    __m256i t1 = _mm256_mul_epu32(x,multv);                // 32x32->64 bit unsigned multiplication of x[0] and x[2]
+    if constexpr (round_down) {
+        t1     = _mm256_add_epi64(t1,multv);               // compensate for rounding error. (x+1)*m replaced by x*m+m to avoid overflow
     }
-    __m256i t2 = _mm256_srli_epi64(t1,32);                           // high dword of result 0 and 2
-    __m256i t3 = _mm256_srli_epi64(x,32);                            // get x[1] and x[3] into position for multiplication
-    __m256i t4 = _mm256_mul_epu32(t3,multv);                         // 32x32->64 bit unsigned multiplication of x[1] and x[3]
-    if (round_down) {
-        t4      = _mm256_add_epi64(t4,multv);                        // compensate for rounding error. (x+1)*m replaced by x*m+m to avoid overflow
+    __m256i t2 = _mm256_srli_epi64(t1,32);                 // high dword of result 0 and 2
+    __m256i t3 = _mm256_srli_epi64(x,32);                  // get x[1] and x[3] into position for multiplication
+    __m256i t4 = _mm256_mul_epu32(t3,multv);               // 32x32->64 bit unsigned multiplication of x[1] and x[3]
+    if constexpr (round_down) {
+        t4     = _mm256_add_epi64(t4,multv);               // compensate for rounding error. (x+1)*m replaced by x*m+m to avoid overflow
     }
-    __m256i t5 = _mm256_set_epi32(-1,0,-1,0,-1,0,-1,0);              // mask of dword 1 and 3
-    __m256i t7 = _mm256_blendv_epi8(t2,t4,t5);                       // blend two results
-    Vec8ui  q  = _mm256_srli_epi32(t7, b);                           // shift right by b
-    return q;                                                        // no overflow possible
+    __m256i t7 = _mm256_blend_epi32(t2,t4,0xAA);
+    Vec8ui  q  = _mm256_srli_epi32(t7, b);                 // shift right by b
+    return  q;                                             // no overflow possible
 }
 
 // define Vec8ui a / const_uint(d)
 template <uint32_t d>
-static inline Vec8ui operator / (Vec8ui const & a, Const_uint_t<d>) {
+static inline Vec8ui operator / (Vec8ui const a, Const_uint_t<d>) {
     return divide_by_ui<d>(a);
 }
 
 // define Vec8ui a / const_int(d)
 template <int32_t d>
-static inline Vec8ui operator / (Vec8ui const & a, Const_int_t<d>) {
-    Static_error_check< (d>=0) > Error_dividing_unsigned_by_negative;// Error: dividing unsigned by negative is ambiguous
-    return divide_by_ui<d>(a);                                       // unsigned divide
+static inline Vec8ui operator / (Vec8ui const a, Const_int_t<d>) {
+    static_assert(d >= 0, "Dividing unsigned integer by negative is ambiguous");
+    return divide_by_ui<d>(a);                             // unsigned divide
 }
 
 // vector operator /= : divide
@@ -5396,45 +5635,45 @@ static inline Vec8ui & operator /= (Vec8ui & a, Const_int_t<d> b) {
 }
 
 
-// Divide Vec16s by compile-time constant 
+// Divide Vec16s by compile-time constant
 template <int d>
-static inline Vec16s divide_by_i(Vec16s const & x) {
-    const int16_t d0 = int16_t(d);                                   // truncate d to 16 bits
-    Static_error_check<(d0 != 0)> Dividing_by_zero;                  // Error message if dividing by zero
-    if (d0 ==  1) return  x;                                         // divide by  1
-    if (d0 == -1) return -x;                                         // divide by -1
-    if (uint16_t(d0) == 0x8000u) return Vec16s(x == Vec16s(0x8000)) & 1;// prevent overflow when changing sign
-    const uint16_t d1 = d0 > 0 ? d0 : -d0;                           // compile-time abs(d0)
-    if ((d1 & (d1-1)) == 0) {
+static inline Vec16s divide_by_i(Vec16s const x) {
+    constexpr int16_t d0 = int16_t(d);                     // truncate d to 16 bits
+    static_assert(d0 != 0, "Integer division by zero");
+    if constexpr (d0 ==  1) return  x;                     // divide by  1
+    if constexpr (d0 == -1) return -x;                     // divide by -1
+    if constexpr (uint16_t(d0) == 0x8000u) return Vec16s(x == Vec16s(0x8000)) & 1;// prevent overflow when changing sign
+    constexpr uint16_t d1 = d0 > 0 ? d0 : -d0;             // compile-time abs(d0)
+    if constexpr ((d1 & (d1-1)) == 0) {
         // d is a power of 2. use shift
-        const int k = bit_scan_reverse_const(uint32_t(d1));
+        constexpr int k = bit_scan_reverse_const(uint32_t(d1));
         __m256i sign;
-        if (k > 1) sign = _mm256_srai_epi16(x, k-1); else sign = x;  // k copies of sign bit
-        __m256i bias    = _mm256_srli_epi16(sign, 16-k);             // bias = x >= 0 ? 0 : k-1
-        __m256i xpbias  = _mm256_add_epi16 (x, bias);                // x + bias
-        __m256i q       = _mm256_srai_epi16(xpbias, k);              // (x + bias) >> k
-        if (d0 > 0)  return q;                                       // d0 > 0: return  q
-        return _mm256_sub_epi16(_mm256_setzero_si256(), q);          // d0 < 0: return -q
+        if constexpr (k > 1) sign = _mm256_srai_epi16(x, k-1); else sign = x;// k copies of sign bit
+        __m256i bias    = _mm256_srli_epi16(sign, 16-k);   // bias = x >= 0 ? 0 : k-1
+        __m256i xpbias  = _mm256_add_epi16 (x, bias);      // x + bias
+        __m256i q       = _mm256_srai_epi16(xpbias, k);    // (x + bias) >> k
+        if constexpr (d0 > 0)  return q;                   // d0 > 0: return  q
+        return _mm256_sub_epi16(_mm256_setzero_si256(), q);// d0 < 0: return -q
     }
     // general case
-    const int L = bit_scan_reverse_const(uint16_t(d1-1)) + 1;        // ceil(log2(d)). (d < 2 handled above)
-    const int16_t mult = int16_t(1 + (1u << (15+L)) / uint32_t(d1) - 0x10000);// multiplier
-    const int shift1 = L - 1;
+    constexpr int L = bit_scan_reverse_const(uint16_t(d1-1)) + 1;// ceil(log2(d)). (d < 2 handled above)
+    constexpr int16_t mult = int16_t(1 + (1u << (15+L)) / uint32_t(d1) - 0x10000);// multiplier
+    constexpr int shift1 = L - 1;
     const Divisor_s div(mult, shift1, d0 > 0 ? 0 : -1);
     return x / div;
 }
 
 // define Vec16s a / const_int(d)
 template <int d>
-static inline Vec16s operator / (Vec16s const & a, Const_int_t<d>) {
+static inline Vec16s operator / (Vec16s const a, Const_int_t<d>) {
     return divide_by_i<d>(a);
 }
 
 // define Vec16s a / const_uint(d)
 template <uint32_t d>
-static inline Vec16s operator / (Vec16s const & a, Const_uint_t<d>) {
-    Static_error_check< (d<0x8000u) > Error_overflow_dividing_signed_by_unsigned; // Error: dividing signed by overflowing unsigned
-    return divide_by_i<int(d)>(a);                                   // signed divide
+static inline Vec16s operator / (Vec16s const a, Const_uint_t<d>) {
+    static_assert(d < 0x8000u, "Dividing signed integer by overflowing unsigned");
+    return divide_by_i<int(d)>(a);                         // signed divide
 }
 
 // vector operator /= : divide
@@ -5454,49 +5693,47 @@ static inline Vec16s & operator /= (Vec16s & a, Const_uint_t<d> b) {
 
 // Divide Vec16us by compile-time constant
 template <uint32_t d>
-static inline Vec16us divide_by_ui(Vec16us const & x) {
-    const uint16_t d0 = uint16_t(d);                                 // truncate d to 16 bits
-    Static_error_check<(d0 != 0)> Dividing_by_zero;                  // Error message if dividing by zero
-    if (d0 == 1) return x;                                           // divide by 1
-    const int b = bit_scan_reverse_const(d0);                        // floor(log2(d))
-    if ((d0 & (d0-1)) == 0) {
+static inline Vec16us divide_by_ui(Vec16us const x) {
+    constexpr uint16_t d0 = uint16_t(d);                   // truncate d to 16 bits
+    static_assert(d0 != 0, "Integer division by zero");
+    if constexpr (d0 == 1) return x;                       // divide by 1
+    constexpr int b = bit_scan_reverse_const((uint32_t)d0);// floor(log2(d))
+    if constexpr ((d0 & (d0-1)) == 0) {
         // d is a power of 2. use shift
-        return  _mm256_srli_epi16(x, b);                             // x >> b
+        return  _mm256_srli_epi16(x, b);                   // x >> b
     }
     // general case (d > 2)
-    uint16_t mult = uint16_t((uint32_t(1) << (b+16)) / d0);          // multiplier = 2^(32+b) / d
-    const uint32_t rem = (uint32_t(1) << (b+16)) - uint32_t(d0)*mult;// remainder 2^(32+b) % d
-    const bool round_down = (2*rem < d0);                            // check if fraction is less than 0.5
+    constexpr uint16_t mult = uint16_t((uint32_t(1) << (b+16)) / d0);// multiplier = 2^(32+b) / d
+    constexpr uint32_t rem = (uint32_t(1) << (b+16)) - uint32_t(d0)*mult;// remainder 2^(32+b) % d
+    constexpr bool round_down = (2*rem < d0);              // check if fraction is less than 0.5
     Vec16us x1 = x;
-    if (round_down) {
-        x1 = x1 + 1;                                                 // round down mult and compensate by adding 1 to x
+    if constexpr (round_down) {
+        x1 = x1 + 1;                                       // round down mult and compensate by adding 1 to x
     }
-    else {
-        mult = mult + 1;                                             // round up mult. no compensation needed
-    }
-    const __m256i multv = _mm256_set1_epi16(mult);                   // broadcast mult
-    __m256i xm = _mm256_mulhi_epu16(x1, multv);                      // high part of 16x16->32 bit unsigned multiplication
-    Vec16us q    = _mm256_srli_epi16(xm, b);                         // shift right by b
-    if (round_down) {
-        Vec16sb overfl = (x1 == Vec16us(_mm256_setzero_si256()));     // check for overflow of x+1
-        return select(overfl, Vec16us(mult >> b), q);                // deal with overflow (rarely needed)
+    constexpr uint16_t mult1 = round_down ? mult : mult + 1;
+    const __m256i multv = _mm256_set1_epi16((int16_t)mult1);// broadcast mult
+    __m256i xm = _mm256_mulhi_epu16(x1, multv);            // high part of 16x16->32 bit unsigned multiplication
+    Vec16us q    = _mm256_srli_epi16(xm, b);               // shift right by b
+    if constexpr (round_down) {
+        Vec16sb overfl = (x1 == Vec16us(_mm256_setzero_si256())); // check for overflow of x+1
+        return select(overfl, Vec16us(uint16_t(mult1 >> (uint16_t)b)), q); // deal with overflow (rarely needed)
     }
     else {
-        return q;                                                    // no overflow possible
+        return q;                                          // no overflow possible
     }
 }
 
 // define Vec16us a / const_uint(d)
 template <uint32_t d>
-static inline Vec16us operator / (Vec16us const & a, Const_uint_t<d>) {
+static inline Vec16us operator / (Vec16us const a, Const_uint_t<d>) {
     return divide_by_ui<d>(a);
 }
 
 // define Vec16us a / const_int(d)
 template <int d>
-static inline Vec16us operator / (Vec16us const & a, Const_int_t<d>) {
-    Static_error_check< (d>=0) > Error_dividing_unsigned_by_negative;// Error: dividing unsigned by negative is ambiguous
-    return divide_by_ui<d>(a);                                       // unsigned divide
+static inline Vec16us operator / (Vec16us const a, Const_int_t<d>) {
+    static_assert(d >= 0, "Dividing unsigned integer by negative is ambiguous");
+    return divide_by_ui<d>(a);                             // unsigned divide
 }
 
 // vector operator /= : divide
@@ -5513,10 +5750,9 @@ static inline Vec16us & operator /= (Vec16us & a, Const_int_t<d> b) {
     return a;
 }
 
-
 // define Vec32c a / const_int(d)
 template <int d>
-static inline Vec32c operator / (Vec32c const & a, Const_int_t<d>) {
+static inline Vec32c operator / (Vec32c const a, Const_int_t<d>) {
     // expand into two Vec16s
     Vec16s low  = extend_low(a)  / Const_int_t<d>();
     Vec16s high = extend_high(a) / Const_int_t<d>();
@@ -5525,9 +5761,9 @@ static inline Vec32c operator / (Vec32c const & a, Const_int_t<d>) {
 
 // define Vec32c a / const_uint(d)
 template <uint32_t d>
-static inline Vec32c operator / (Vec32c const & a, Const_uint_t<d>) {
-    Static_error_check< (uint8_t(d)<0x80u) > Error_overflow_dividing_signed_by_unsigned; // Error: dividing signed by overflowing unsigned
-    return a / Const_int_t<d>();                                     // signed divide
+static inline Vec32c operator / (Vec32c const a, Const_uint_t<d>) {
+    static_assert(uint8_t(d) < 0x80u, "Dividing signed integer by overflowing unsigned");
+    return a / Const_int_t<d>();                           // signed divide
 }
 
 // vector operator /= : divide
@@ -5545,7 +5781,7 @@ static inline Vec32c & operator /= (Vec32c & a, Const_uint_t<d> b) {
 
 // define Vec32uc a / const_uint(d)
 template <uint32_t d>
-static inline Vec32uc operator / (Vec32uc const & a, Const_uint_t<d>) {
+static inline Vec32uc operator / (Vec32uc const a, Const_uint_t<d>) {
     // expand into two Vec16us
     Vec16us low  = extend_low(a)  / Const_uint_t<d>();
     Vec16us high = extend_high(a) / Const_uint_t<d>();
@@ -5554,9 +5790,9 @@ static inline Vec32uc operator / (Vec32uc const & a, Const_uint_t<d>) {
 
 // define Vec32uc a / const_int(d)
 template <int d>
-static inline Vec32uc operator / (Vec32uc const & a, Const_int_t<d>) {
-    Static_error_check< (int8_t(d)>=0) > Error_dividing_unsigned_by_negative;// Error: dividing unsigned by negative is ambiguous
-    return a / Const_uint_t<d>();                                    // unsigned divide
+static inline Vec32uc operator / (Vec32uc const a, Const_int_t<d>) {
+    static_assert(int8_t(d) >= 0, "Dividing unsigned integer by negative is ambiguous");
+    return a / Const_uint_t<d>();                          // unsigned divide
 }
 
 // vector operator /= : divide
@@ -5573,118 +5809,44 @@ static inline Vec32uc & operator /= (Vec32uc & a, Const_int_t<d> b) {
     return a;
 }
 
+
 /*****************************************************************************
 *
-*          Horizontal scan functions
+*          Boolean <-> bitfield conversion functions
 *
 *****************************************************************************/
 
-// Get index to the first element that is true. Return -1 if all are false
-static inline int horizontal_find_first(Vec32cb const & x) {
-    uint32_t a = _mm256_movemask_epi8(x);
-    if (a == 0) return -1;
-    int32_t b = bit_scan_forward(a);
-    return b;
-}
-
-static inline int horizontal_find_first(Vec16sb const & x) {
-    return horizontal_find_first(Vec32cb(x)) >> 1;
-}
-
-static inline int horizontal_find_first(Vec8ib const & x) {
-    return horizontal_find_first(Vec32cb(x)) >> 2;
-}
-
-static inline int horizontal_find_first(Vec4qb const & x) {
-    return horizontal_find_first(Vec32cb(x)) >> 3;
-}
-
-// Count the number of elements that are true
-static inline uint32_t horizontal_count(Vec32cb const & x) {
-    uint32_t a = _mm256_movemask_epi8(x);
-    return vml_popcnt(a);
-}
-
-static inline uint32_t horizontal_count(Vec16sb const & x) {
-    return horizontal_count(Vec32cb(x)) >> 1;
-}
-
-static inline uint32_t horizontal_count(Vec8ib const & x) {
-    return horizontal_count(Vec32cb(x)) >> 2;
-}
+#if INSTRSET >= 10  // compact boolean vectors, other sizes
 
-static inline uint32_t horizontal_count(Vec4qb const & x) {
-    return horizontal_count(Vec32cb(x)) >> 3;
+// to_bits: convert boolean vector to integer bitfield
+static inline uint32_t to_bits(Vec32b const x) {
+    return __mmask32(x);
 }
 
-/*****************************************************************************
-*
-*          Boolean <-> bitfield conversion functions
-*
-*****************************************************************************/
+#else
 
 // to_bits: convert boolean vector to integer bitfield
-static inline uint32_t to_bits(Vec32cb const & x) {
+static inline uint32_t to_bits(Vec32cb const x) {
     return (uint32_t)_mm256_movemask_epi8(x);
 }
 
-// to_Vec16c: convert integer bitfield to boolean vector
-static inline Vec32cb to_Vec32cb(uint32_t x) {
-    return Vec32cb(Vec32c(to_Vec16cb(uint16_t(x)), to_Vec16cb(uint16_t(x>>16))));
-}
-
-// to_bits: convert boolean vector to integer bitfield
-static inline uint16_t to_bits(Vec16sb const & x) {
+static inline uint16_t to_bits(Vec16sb const x) {
     __m128i a = _mm_packs_epi16(x.get_low(), x.get_high());  // 16-bit words to bytes
     return (uint16_t)_mm_movemask_epi8(a);
 }
 
-// to_Vec16sb: convert integer bitfield to boolean vector
-static inline Vec16sb to_Vec16sb(uint16_t x) {
-    return Vec16sb(Vec16s(to_Vec8sb(uint8_t(x)), to_Vec8sb(uint8_t(x>>8))));
-}
-
-#if INSTRSET < 9 || MAX_VECTOR_SIZE < 512
-// These functions are defined in Vectori512.h if AVX512 instruction set is used
-
-// to_bits: convert boolean vector to integer bitfield
-static inline uint8_t to_bits(Vec8ib const & x) {
+static inline uint8_t to_bits(Vec8ib const x) {
     __m128i a = _mm_packs_epi32(x.get_low(), x.get_high());  // 32-bit dwords to 16-bit words
     __m128i b = _mm_packs_epi16(a, a);  // 16-bit words to bytes
     return (uint8_t)_mm_movemask_epi8(b);
 }
 
-// to_Vec8ib: convert integer bitfield to boolean vector
-static inline Vec8ib to_Vec8ib(uint8_t x) {
-    return Vec8ib(Vec8i(to_Vec4ib(x), to_Vec4ib(x>>4)));
-}
-
-// to_bits: convert boolean vector to integer bitfield
-static inline uint8_t to_bits(Vec4qb const & x) {
-    uint32_t a = _mm256_movemask_epi8(x);
+static inline uint8_t to_bits(Vec4qb const x) {
+    uint32_t a = (uint32_t)_mm256_movemask_epi8(x);
     return ((a & 1) | ((a >> 7) & 2)) | (((a >> 14) & 4) | ((a >> 21) & 8));
 }
 
-// to_Vec4qb: convert integer bitfield to boolean vector
-static inline Vec4qb to_Vec4qb(uint8_t x) {
-    return  Vec4qb(Vec4q(-(x&1), -((x>>1)&1), -((x>>2)&1), -((x>>3)&1)));
-}
-
-#else  // function prototypes here only
-
-// to_bits: convert boolean vector to integer bitfield
-static inline uint8_t to_bits(Vec8ib x);
-
-// to_Vec8ib: convert integer bitfield to boolean vector
-static inline Vec8ib to_Vec8ib(uint8_t x);
-
-// to_bits: convert boolean vector to integer bitfield
-static inline uint8_t to_bits(Vec4qb x);
-
-// to_Vec4qb: convert integer bitfield to boolean vector
-static inline Vec4qb to_Vec4qb(uint8_t x);
-
-#endif  // INSTRSET < 9 || MAX_VECTOR_SIZE < 512
+#endif
 
 #ifdef VCL_NAMESPACE
 }
diff --git a/EEDI3/vectorclass/vectori256e.h b/EEDI3/vectorclass/vectori256e.h
index 4bdfb91..b3cdd8d 100644
--- a/EEDI3/vectorclass/vectori256e.h
+++ b/EEDI3/vectorclass/vectori256e.h
@@ -1,15 +1,17 @@
 /****************************  vectori256e.h   *******************************
 * Author:        Agner Fog
 * Date created:  2012-05-30
-* Last modified: 2017-02-19
-* Version:       1.27
-* Project:       vector classes
+* Last modified: 2023-06-03
+* Version:       2.02.01
+* Project:       vector class library
 * Description:
 * Header file defining 256-bit integer point vector classes as interface
 * to intrinsic functions. Emulated for processors without AVX2 instruction set.
 *
+* Instructions: see vcl_manual.pdf
+*
 * The following vector classes are defined here:
-* Vec256b   Vector of 256  1-bit unsigned  integers or Booleans
+* Vec256b   Vector of 256  bits. Used internally as base class
 * Vec32c    Vector of  32  8-bit signed    integers
 * Vec32uc   Vector of  32  8-bit unsigned  integers
 * Vec32cb   Vector of  32  Booleans for use with Vec32c and Vec32uc
@@ -23,83 +25,55 @@
 * Vec4uq    Vector of   4  64-bit unsigned integers
 * Vec4qb    Vector of   4  Booleans for use with Vec4q and Vec4uq
 *
-* For detailed instructions, see VectorClass.pdf
+* Each vector object is represented internally in the CPU as two 128-bit registers.
+* This header file defines operators and functions for these vectors.
 *
-* (c) Copyright 2012-2017 GNU General Public License http://www.gnu.org/licenses
+* (c) Copyright 2012-2023 Agner Fog.
+* Apache License version 2.0 or later.
 *****************************************************************************/
 
-// check combination of header files
-#if defined (VECTORI256_H)
-#if    VECTORI256_H != 1
-#error Two different versions of vectori256.h included
+#ifndef VECTORI256E_H
+#define VECTORI256E_H 1
+
+#ifndef VECTORCLASS_H
+#include "vectorclass.h"
 #endif
-#else
-#define VECTORI256_H  1
 
-#ifdef VECTORF256_H
-#error Please put header file vectori256.h or vectori256e.h before vectorf256e.h
+#if VECTORCLASS_H < 20200
+#error Incompatible versions of vector class library mixed
 #endif
 
+// check combination of header files
+#if defined (VECTORI256_H)
+#error Two different versions of vectori256.h included
+#endif
 
-#include "vectori128.h"
 
 #ifdef VCL_NAMESPACE
 namespace VCL_NAMESPACE {
 #endif
 
-/*****************************************************************************
-*
-*          base class Vec256ie
-*
-*****************************************************************************/
-// base class to replace Vec256ie when AVX2 is not supported
-class Vec256ie {
-protected:
-    __m128i y0;                         // low half
-    __m128i y1;                         // high half
-public:
-    Vec256ie(void) {};                  // default constructor
-    Vec256ie(__m128i x0, __m128i x1) {  // constructor to build from two __m128i
-        y0 = x0;  y1 = x1;
-    }
-    __m128i get_low() const {           // get low half
-        return y0;
-    }
-    __m128i get_high() const {          // get high half
-        return y1;
-    }
-};
-
 
 /*****************************************************************************
 *
-*          Vector of 256 1-bit unsigned integers or Booleans
+*          Vector of 256 bits. used as base class
 *
 *****************************************************************************/
 
-class Vec256b : public Vec256ie {
+class Vec256b {
+protected:
+    __m128i y0;                        // low half
+    __m128i y1;                        // high half
 public:
     // Default constructor:
-    Vec256b() {
+    Vec256b() = default;
+    Vec256b(__m128i x0, __m128i x1) {  // constructor to build from two __m128i
+        y0 = x0;  y1 = x1;
     }
-    // Constructor to broadcast the same value into all elements
-    // Removed because of undesired implicit conversions
-    //Vec256b(int i) {
-    //    y1 = y0 = _mm_set1_epi32(-(i & 1));}
-
     // Constructor to build from two Vec128b:
-    Vec256b(Vec128b const & a0, Vec128b const & a1) {
+    Vec256b(Vec128b const a0, Vec128b const a1) {
         y0 = a0;  y1 = a1;
     }
-    // Constructor to convert from type Vec256ie
-    Vec256b(Vec256ie const & x) {
-        y0 = x.get_low();  y1 = x.get_high();
-    }
-    // Assignment operator to convert from type Vec256ie
-    Vec256b & operator = (Vec256ie const & x) {
-        y0 = x.get_low();  y1 = x.get_high();
-        return *this;
-    }
     // Member function to load from array (unaligned)
     Vec256b & load(void const * p) {
         y0 = _mm_loadu_si128((__m128i const*)p);
@@ -126,37 +100,14 @@ public:
         _mm_store_si128((__m128i*)p,     y0);
         _mm_store_si128((__m128i*)p + 1, y1);
     }
-    // Member function to store into array using a non-temporal memory hint, aligned by 32
-    void stream(void * p) const {
+    // Member function storing to aligned uncached memory (non-temporal store).
+    // This may be more efficient than store_a when storing large blocks of memory if it 
+    // is unlikely that the data will stay in the cache until it is read again.
+    // Note: Will generate runtime error if p is not aligned by 32
+    void store_nt(void * p) const {
         _mm_stream_si128((__m128i*)p,     y0);
         _mm_stream_si128((__m128i*)p + 1, y1);
     }
-    // Member function to change a single bit
-    // Note: This function is inefficient. Use load function if changing more than one bit
-    Vec256b const & set_bit(uint32_t index, int value) {
-        if (index < 128) {
-            y0 = Vec128b(y0).set_bit(index, value);
-        }
-        else {
-            y1 = Vec128b(y1).set_bit(index-128, value);
-        }
-        return *this;
-    }
-    // Member function to get a single bit
-    // Note: This function is inefficient. Use store function if reading more than one bit
-    int get_bit(uint32_t index) const {
-        if (index < 128) {
-            return Vec128b(y0).get_bit(index);
-        }
-        else {
-            return Vec128b(y1).get_bit(index-128);
-        }
-    }
-    // Extract a single element. Use store function if extracting more than one element.
-    // Operator [] can only read an element, not write.
-    bool operator [] (uint32_t index) const {
-        return get_bit(index) != 0;
-    }
     // Member functions to split into two Vec128b:
     Vec128b get_low() const {
         return y0;
@@ -164,122 +115,88 @@ public:
     Vec128b get_high() const {
         return y1;
     }
-    static int size () {
+    static constexpr int size() {
         return 256;
     }
+    static constexpr int elementtype() {
+        return 1;
+    }
 };
 
 // Define operators for this class
 
 // vector operator & : bitwise and
-static inline Vec256b operator & (Vec256b const & a, Vec256b const & b) {
+static inline Vec256b operator & (Vec256b const a, Vec256b const b) {
     return Vec256b(a.get_low() & b.get_low(), a.get_high() & b.get_high());
 }
-static inline Vec256b operator && (Vec256b const & a, Vec256b const & b) {
+static inline Vec256b operator && (Vec256b const a, Vec256b const b) {
     return a & b;
 }
 
 // vector operator | : bitwise or
-static inline Vec256b operator | (Vec256b const & a, Vec256b const & b) {
+static inline Vec256b operator | (Vec256b const a, Vec256b const b) {
     return Vec256b(a.get_low() | b.get_low(), a.get_high() | b.get_high());
 }
-static inline Vec256b operator || (Vec256b const & a, Vec256b const & b) {
+static inline Vec256b operator || (Vec256b const a, Vec256b const b) {
     return a | b;
 }
 
 // vector operator ^ : bitwise xor
-static inline Vec256b operator ^ (Vec256b const & a, Vec256b const & b) {
+static inline Vec256b operator ^ (Vec256b const a, Vec256b const b) {
     return Vec256b(a.get_low() ^ b.get_low(), a.get_high() ^ b.get_high());
 }
 
 // vector operator ~ : bitwise not
-static inline Vec256b operator ~ (Vec256b const & a) {
+static inline Vec256b operator ~ (Vec256b const a) {
     return Vec256b(~a.get_low(), ~a.get_high());
 }
 
 // vector operator &= : bitwise and
-static inline Vec256b & operator &= (Vec256b & a, Vec256b const & b) {
+static inline Vec256b & operator &= (Vec256b & a, Vec256b const b) {
     a = a & b;
     return a;
 }
 
 // vector operator |= : bitwise or
-static inline Vec256b & operator |= (Vec256b & a, Vec256b const & b) {
+static inline Vec256b & operator |= (Vec256b & a, Vec256b const b) {
     a = a | b;
     return a;
 }
 
 // vector operator ^= : bitwise xor
-static inline Vec256b & operator ^= (Vec256b & a, Vec256b const & b) {
+static inline Vec256b & operator ^= (Vec256b & a, Vec256b const b) {
     a = a ^ b;
     return a;
 }
 
-// Define functions for this class
-
-static inline Vec256b zero_256b() {
-    Vec128b zero = _mm_setzero_si128();
-    return Vec256b(zero, zero);
-}
-
-// function andnot: a & ~ b
-static inline Vec256b andnot (Vec256b const & a, Vec256b const & b) {
-    return Vec256b(andnot(a.get_low(), b.get_low()), andnot(a.get_high(), b.get_high()));
-}
-
-
 /*****************************************************************************
 *
-*          Generate compile-time constant vector
+*          Functions for this class
 *
 *****************************************************************************/
-// Generate a constant vector of 8 integers stored in memory.
-// Can be converted to any integer vector type
-template <int32_t i0, int32_t i1, int32_t i2, int32_t i3, int32_t i4, int32_t i5, int32_t i6, int32_t i7>
-static inline Vec256ie constant8i() {
-    static const union {
-        int32_t i[8];
-        __m128i y[2];
-    } u = {{i0,i1,i2,i3,i4,i5,i6,i7}};
-    return Vec256ie(u.y[0], u.y[1]);
-}
 
-template <uint32_t i0, uint32_t i1, uint32_t i2, uint32_t i3, uint32_t i4, uint32_t i5, uint32_t i6, uint32_t i7>
-static inline Vec256ie constant8ui() {
-    return constant8i<int32_t(i0), int32_t(i1), int32_t(i2), int32_t(i3), int32_t(i4), int32_t(i5), int32_t(i6), int32_t(i7)>();
+// function andnot: a & ~ b
+static inline Vec256b andnot (Vec256b const a, Vec256b const b) {
+    return Vec256b(andnot(a.get_low(), b.get_low()), andnot(a.get_high(), b.get_high()));
 }
 
-
-/*****************************************************************************
-*
-*          selectb function
-*
-*****************************************************************************/
 // Select between two sources, byte by byte. Used in various functions and operators
 // Corresponds to this pseudocode:
 // for (int i = 0; i < 32; i++) result[i] = s[i] ? a[i] : b[i];
 // Each byte in s must be either 0 (false) or 0xFF (true). No other values are allowed.
-// Only bit 7 in each byte of s is checked, 
-static inline Vec256ie selectb (Vec256ie const & s, Vec256ie const & a, Vec256ie const & b) {
-    return Vec256ie(selectb(s.get_low(),  a.get_low(),  b.get_low()), 
-                    selectb(s.get_high(), a.get_high(), b.get_high()));
+// Only bit 7 in each byte of s is checked,
+static inline Vec256b selectb (Vec256b const s, Vec256b const a, Vec256b const b) {
+    return Vec256b(selectb(s.get_low(),  a.get_low(),  b.get_low()),
+                   selectb(s.get_high(), a.get_high(), b.get_high()));
 }
 
-
-
-/*****************************************************************************
-*
-*          Horizontal Boolean functions
-*
-*****************************************************************************/
-
 // horizontal_and. Returns true if all bits are 1
-static inline bool horizontal_and (Vec256b const & a) {
+static inline bool horizontal_and (Vec256b const a) {
     return horizontal_and(a.get_low() & a.get_high());
 }
 
 // horizontal_or. Returns true if at least one bit is 1
-static inline bool horizontal_or (Vec256b const & a) {
+static inline bool horizontal_or (Vec256b const a) {
     return horizontal_or(a.get_low() | a.get_high());
 }
 
@@ -293,31 +210,30 @@ static inline bool horizontal_or (Vec256b const & a) {
 class Vec32c : public Vec256b {
 public:
     // Default constructor:
-    Vec32c(){
-    }
+    Vec32c() = default;
     // Constructor to broadcast the same value into all elements:
     Vec32c(int i) {
         y1 = y0 = _mm_set1_epi8((char)i);
     }
     // Constructor to build from all elements:
     Vec32c(int8_t i0, int8_t i1, int8_t i2, int8_t i3, int8_t i4, int8_t i5, int8_t i6, int8_t i7,
-        int8_t i8, int8_t i9, int8_t i10, int8_t i11, int8_t i12, int8_t i13, int8_t i14, int8_t i15,        
+        int8_t i8, int8_t i9, int8_t i10, int8_t i11, int8_t i12, int8_t i13, int8_t i14, int8_t i15,
         int8_t i16, int8_t i17, int8_t i18, int8_t i19, int8_t i20, int8_t i21, int8_t i22, int8_t i23,
         int8_t i24, int8_t i25, int8_t i26, int8_t i27, int8_t i28, int8_t i29, int8_t i30, int8_t i31) {
         y0 = _mm_setr_epi8(i0,  i1,  i2,  i3,  i4,  i5,  i6,  i7,  i8,  i9,  i10, i11, i12, i13, i14, i15);
         y1 = _mm_setr_epi8(i16, i17, i18, i19, i20, i21, i22, i23, i24, i25, i26, i27, i28, i29, i30, i31);
     }
     // Constructor to build from two Vec16c:
-    Vec32c(Vec16c const & a0, Vec16c const & a1) {
+    Vec32c(Vec16c const a0, Vec16c const a1) {
         y0 = a0;  y1 = a1;
     }
-    // Constructor to convert from type Vec256ie
-    Vec32c(Vec256ie const & x) {
+    // Constructor to convert from type Vec256b
+    Vec32c(Vec256b const & x) {
         y0 = x.get_low();
         y1 = x.get_high();
     }
-    // Assignment operator to convert from type Vec256ie
-    Vec32c & operator = (Vec256ie const & x) {
+    // Assignment operator to convert from type Vec256b
+    Vec32c & operator = (Vec256b const x) {
         y0 = x.get_low();
         y1 = x.get_high();
         return *this;
@@ -377,9 +293,8 @@ public:
         return *this;
     }
     // Member function to change a single element in vector
-    // Note: This function is inefficient. Use load function if changing more than one element
-    Vec32c const & insert(uint32_t index, int8_t value) {
-        if (index < 16) {
+    Vec32c const insert(int index, int8_t value) {
+        if ((uint32_t)index < 16) {
             y0 = Vec16c(y0).insert(index, value);
         }
         else {
@@ -388,8 +303,8 @@ public:
         return *this;
     }
     // Member function extract a single element from vector
-    int8_t extract(uint32_t index) const {
-        if (index < 16) {
+    int8_t extract(int index) const {
+        if ((uint32_t)index < 16) {
             return Vec16c(y0).extract(index);
         }
         else {
@@ -398,7 +313,7 @@ public:
     }
     // Extract a single element. Use store function if extracting more than one element.
     // Operator [] can only read an element, not write.
-    int8_t operator [] (uint32_t index) const {
+    int8_t operator [] (int index) const {
         return extract(index);
     }
     // Member functions to split into two Vec16c:
@@ -408,9 +323,12 @@ public:
     Vec16c get_high() const {
         return y1;
     }
-    static int size () {
+    static constexpr int size() {
         return 32;
     }
+    static constexpr int elementtype() {
+        return 4;
+    }
 };
 
 
@@ -423,24 +341,14 @@ public:
 class Vec32cb : public Vec32c {
 public:
     // Default constructor:
-    Vec32cb(){}
-    // Constructor to build from all elements:
-    Vec32cb(bool x0, bool x1, bool x2, bool x3, bool x4, bool x5, bool x6, bool x7,
-        bool x8, bool x9, bool x10, bool x11, bool x12, bool x13, bool x14, bool x15,
-        bool x16, bool x17, bool x18, bool x19, bool x20, bool x21, bool x22, bool x23,
-        bool x24, bool x25, bool x26, bool x27, bool x28, bool x29, bool x30, bool x31) :
-        Vec32c(-int8_t(x0), -int8_t(x1), -int8_t(x2), -int8_t(x3), -int8_t(x4), -int8_t(x5), -int8_t(x6), -int8_t(x7), 
-            -int8_t(x8), -int8_t(x9), -int8_t(x10), -int8_t(x11), -int8_t(x12), -int8_t(x13), -int8_t(x14), -int8_t(x15),
-            -int8_t(x16), -int8_t(x17), -int8_t(x18), -int8_t(x19), -int8_t(x20), -int8_t(x21), -int8_t(x22), -int8_t(x23),
-            -int8_t(x24), -int8_t(x25), -int8_t(x26), -int8_t(x27), -int8_t(x28), -int8_t(x29), -int8_t(x30), -int8_t(x31))
-        {}
-    // Constructor to convert from type Vec256ie
-    Vec32cb(Vec256ie const & x) {
+    Vec32cb() = default;
+    // Constructor to convert from type Vec256b
+    Vec32cb(Vec256b const x) {
         y0 = x.get_low();
         y1 = x.get_high();
     }
-    // Assignment operator to convert from type Vec256ie
-    Vec32cb & operator = (Vec256ie const & x) {
+    // Assignment operator to convert from type Vec256b
+    Vec32cb & operator = (Vec256b const x) {
         y0 = x.get_low();
         y1 = x.get_high();
         return *this;
@@ -453,10 +361,9 @@ public:
         *this = Vec32cb(b);
         return *this;
     }
-private: // Prevent constructing from int, etc.
-    Vec32cb(int b);
-    Vec32cb & operator = (int x);
-public:
+    // Constructor to build from two Vec16cb:
+    Vec32cb(Vec16cb const a0, Vec16cb const a1) : Vec32c(Vec16c(a0), Vec16c(a1)) {
+    }
     // Member functions to split into two Vec16c:
     Vec16cb get_low() const {
         return y0;
@@ -467,16 +374,28 @@ public:
     Vec32cb & insert (int index, bool a) {
         Vec32c::insert(index, -(int)a);
         return *this;
-    }    
+    }
     // Member function extract a single element from vector
-    bool extract(uint32_t index) const {
+    bool extract(int index) const {
         return Vec32c::extract(index) != 0;
     }
     // Extract a single element. Use store function if extracting more than one element.
     // Operator [] can only read an element, not write.
-    bool operator [] (uint32_t index) const {
+    bool operator [] (int index) const {
         return extract(index);
     }
+    // Member function to change a bitfield to a boolean vector
+    Vec32cb & load_bits(uint32_t a) {
+        y0 = Vec16cb().load_bits(uint16_t(a));
+        y1 = Vec16cb().load_bits(uint16_t(a>>16));
+        return *this;
+    }
+    static constexpr int elementtype() {
+        return 3;
+    }
+    // Prevent constructing from int, etc.
+    Vec32cb(int b) = delete;
+    Vec32cb & operator = (int x) = delete;
 };
 
 
@@ -487,53 +406,63 @@ public:
 *****************************************************************************/
 
 // vector operator & : bitwise and
-static inline Vec32cb operator & (Vec32cb const & a, Vec32cb const & b) {
+static inline Vec32cb operator & (Vec32cb const a, Vec32cb const b) {
     return Vec32cb(Vec256b(a) & Vec256b(b));
 }
-static inline Vec32cb operator && (Vec32cb const & a, Vec32cb const & b) {
+static inline Vec32cb operator && (Vec32cb const a, Vec32cb const b) {
     return a & b;
 }
 // vector operator &= : bitwise and
-static inline Vec32cb & operator &= (Vec32cb & a, Vec32cb const & b) {
+static inline Vec32cb & operator &= (Vec32cb & a, Vec32cb const b) {
     a = a & b;
     return a;
 }
 
 // vector operator | : bitwise or
-static inline Vec32cb operator | (Vec32cb const & a, Vec32cb const & b) {
+static inline Vec32cb operator | (Vec32cb const a, Vec32cb const b) {
     return Vec32cb(Vec256b(a) | Vec256b(b));
 }
-static inline Vec32cb operator || (Vec32cb const & a, Vec32cb const & b) {
+static inline Vec32cb operator || (Vec32cb const a, Vec32cb const b) {
     return a | b;
 }
 // vector operator |= : bitwise or
-static inline Vec32cb & operator |= (Vec32cb & a, Vec32cb const & b) {
+static inline Vec32cb & operator |= (Vec32cb & a, Vec32cb const b) {
     a = a | b;
     return a;
 }
 
 // vector operator ^ : bitwise xor
-static inline Vec32cb operator ^ (Vec32cb const & a, Vec32cb const & b) {
+static inline Vec32cb operator ^ (Vec32cb const a, Vec32cb const b) {
     return Vec32cb(Vec256b(a) ^ Vec256b(b));
 }
 // vector operator ^= : bitwise xor
-static inline Vec32cb & operator ^= (Vec32cb & a, Vec32cb const & b) {
+static inline Vec32cb & operator ^= (Vec32cb & a, Vec32cb const b) {
     a = a ^ b;
     return a;
 }
 
+// vector operator == : xnor
+static inline Vec32cb operator == (Vec32cb const a, Vec32cb const b) {
+    return Vec32cb(Vec256b(a) ^ Vec256b(~b));
+}
+
+// vector operator != : xor
+static inline Vec32cb operator != (Vec32cb const a, Vec32cb const b) {
+    return Vec32cb(a ^ b);
+}
+
 // vector operator ~ : bitwise not
-static inline Vec32cb operator ~ (Vec32cb const & a) {
+static inline Vec32cb operator ~ (Vec32cb const a) {
     return Vec32cb( ~ Vec256b(a));
 }
 
 // vector operator ! : element not
-static inline Vec32cb operator ! (Vec32cb const & a) {
+static inline Vec32cb operator ! (Vec32cb const a) {
     return ~ a;
 }
 
 // vector function andnot
-static inline Vec32cb andnot (Vec32cb const & a, Vec32cb const & b) {
+static inline Vec32cb andnot (Vec32cb const a, Vec32cb const b) {
     return Vec32cb(andnot(Vec256b(a), Vec256b(b)));
 }
 
@@ -545,12 +474,12 @@ static inline Vec32cb andnot (Vec32cb const & a, Vec32cb const & b) {
 *****************************************************************************/
 
 // vector operator + : add element by element
-static inline Vec32c operator + (Vec32c const & a, Vec32c const & b) {
+static inline Vec32c operator + (Vec32c const a, Vec32c const b) {
     return Vec32c(a.get_low() + b.get_low(), a.get_high() + b.get_high());
 }
 
 // vector operator += : add
-static inline Vec32c & operator += (Vec32c & a, Vec32c const & b) {
+static inline Vec32c & operator += (Vec32c & a, Vec32c const b) {
     a = a + b;
     return a;
 }
@@ -569,17 +498,17 @@ static inline Vec32c & operator ++ (Vec32c & a) {
 }
 
 // vector operator - : subtract element by element
-static inline Vec32c operator - (Vec32c const & a, Vec32c const & b) {
+static inline Vec32c operator - (Vec32c const a, Vec32c const b) {
     return Vec32c(a.get_low() - b.get_low(), a.get_high() - b.get_high());
 }
 
 // vector operator - : unary minus
-static inline Vec32c operator - (Vec32c const & a) {
+static inline Vec32c operator - (Vec32c const a) {
     return Vec32c(-a.get_low(), -a.get_high());
 }
 
 // vector operator -= : add
-static inline Vec32c & operator -= (Vec32c & a, Vec32c const & b) {
+static inline Vec32c & operator -= (Vec32c & a, Vec32c const b) {
     a = a - b;
     return a;
 }
@@ -598,29 +527,29 @@ static inline Vec32c & operator -- (Vec32c & a) {
 }
 
 // vector operator * : multiply element by element
-static inline Vec32c operator * (Vec32c const & a, Vec32c const & b) {
+static inline Vec32c operator * (Vec32c const a, Vec32c const b) {
     return Vec32c(a.get_low() * b.get_low(), a.get_high() * b.get_high());
 }
 
 // vector operator *= : multiply
-static inline Vec32c & operator *= (Vec32c & a, Vec32c const & b) {
+static inline Vec32c & operator *= (Vec32c & a, Vec32c const b) {
     a = a * b;
     return a;
 }
 
 // vector of 32 8-bit signed integers
-static inline Vec32c operator / (Vec32c const & a, Divisor_s const & d) {
+static inline Vec32c operator / (Vec32c const a, Divisor_s const d) {
     return Vec32c(a.get_low() / d, a.get_high() / d);
 }
 
 // vector operator /= : divide
-static inline Vec32c & operator /= (Vec32c & a, Divisor_s const & d) {
+static inline Vec32c & operator /= (Vec32c & a, Divisor_s const d) {
     a = a / d;
     return a;
 }
 
 // vector operator << : shift left all elements
-static inline Vec32c operator << (Vec32c const & a, int b) {
+static inline Vec32c operator << (Vec32c const a, int b) {
     return Vec32c(a.get_low() << b, a.get_high() << b);
 }
 
@@ -631,7 +560,7 @@ static inline Vec32c & operator <<= (Vec32c & a, int b) {
 }
 
 // vector operator >> : shift right arithmetic all elements
-static inline Vec32c operator >> (Vec32c const & a, int b) {
+static inline Vec32c operator >> (Vec32c const a, int b) {
     return Vec32c(a.get_low() >> b, a.get_high() >> b);
 }
 
@@ -642,141 +571,149 @@ static inline Vec32c & operator >>= (Vec32c & a, int b) {
 }
 
 // vector operator == : returns true for elements for which a == b
-static inline Vec32cb operator == (Vec32c const & a, Vec32c const & b) {
+static inline Vec32cb operator == (Vec32c const a, Vec32c const b) {
     return Vec32c(a.get_low() == b.get_low(), a.get_high() == b.get_high());
 }
 
 // vector operator != : returns true for elements for which a != b
-static inline Vec32cb operator != (Vec32c const & a, Vec32c const & b) {
+static inline Vec32cb operator != (Vec32c const a, Vec32c const b) {
     return Vec32c(a.get_low() != b.get_low(), a.get_high() != b.get_high());
 }
 
 // vector operator > : returns true for elements for which a > b (signed)
-static inline Vec32cb operator > (Vec32c const & a, Vec32c const & b) {
+static inline Vec32cb operator > (Vec32c const a, Vec32c const b) {
     return Vec32c(a.get_low() > b.get_low(), a.get_high() > b.get_high());
 }
 
 // vector operator < : returns true for elements for which a < b (signed)
-static inline Vec32cb operator < (Vec32c const & a, Vec32c const & b) {
+static inline Vec32cb operator < (Vec32c const a, Vec32c const b) {
     return b > a;
 }
 
 // vector operator >= : returns true for elements for which a >= b (signed)
-static inline Vec32cb operator >= (Vec32c const & a, Vec32c const & b) {
+static inline Vec32cb operator >= (Vec32c const a, Vec32c const b) {
     return Vec32c(a.get_low() >= b.get_low(), a.get_high() >= b.get_high());
 }
 
 // vector operator <= : returns true for elements for which a <= b (signed)
-static inline Vec32cb operator <= (Vec32c const & a, Vec32c const & b) {
+static inline Vec32cb operator <= (Vec32c const a, Vec32c const b) {
     return b >= a;
 }
 
 // vector operator & : bitwise and
-static inline Vec32c operator & (Vec32c const & a, Vec32c const & b) {
+static inline Vec32c operator & (Vec32c const a, Vec32c const b) {
     return Vec32c(a.get_low() & b.get_low(), a.get_high() & b.get_high());
 }
-static inline Vec32c operator && (Vec32c const & a, Vec32c const & b) {
+static inline Vec32c operator && (Vec32c const a, Vec32c const b) {
     return a & b;
 }
 // vector operator &= : bitwise and
-static inline Vec32c & operator &= (Vec32c & a, Vec32c const & b) {
+static inline Vec32c & operator &= (Vec32c & a, Vec32c const b) {
     a = a & b;
     return a;
 }
 
 // vector operator | : bitwise or
-static inline Vec32c operator | (Vec32c const & a, Vec32c const & b) {
+static inline Vec32c operator | (Vec32c const a, Vec32c const b) {
     return Vec32c(a.get_low() | b.get_low(), a.get_high() | b.get_high());
 }
-static inline Vec32c operator || (Vec32c const & a, Vec32c const & b) {
+static inline Vec32c operator || (Vec32c const a, Vec32c const b) {
     return a | b;
 }
 // vector operator |= : bitwise or
-static inline Vec32c & operator |= (Vec32c & a, Vec32c const & b) {
+static inline Vec32c & operator |= (Vec32c & a, Vec32c const b) {
     a = a | b;
     return a;
 }
 
 // vector operator ^ : bitwise xor
-static inline Vec32c operator ^ (Vec32c const & a, Vec32c const & b) {
+static inline Vec32c operator ^ (Vec32c const a, Vec32c const b) {
     return Vec32c(a.get_low() ^ b.get_low(), a.get_high() ^ b.get_high());
 }
 // vector operator ^= : bitwise xor
-static inline Vec32c & operator ^= (Vec32c & a, Vec32c const & b) {
+static inline Vec32c & operator ^= (Vec32c & a, Vec32c const b) {
     a = a ^ b;
     return a;
 }
 
 // vector operator ~ : bitwise not
-static inline Vec32c operator ~ (Vec32c const & a) {
+static inline Vec32c operator ~ (Vec32c const a) {
     return Vec32c(~a.get_low(), ~a.get_high());
 }
 
 // vector operator ! : logical not, returns true for elements == 0
-static inline Vec32cb operator ! (Vec32c const & a) {
+static inline Vec32cb operator ! (Vec32c const a) {
     return Vec32c(!a.get_low(), !a.get_high());
 }
 
 // Functions for this class
 
-// Select between two operands. Corresponds to this pseudocode:
+// Select between two operands using broad boolean vectors. Corresponds to this pseudocode:
 // for (int i = 0; i < 16; i++) result[i] = s[i] ? a[i] : b[i];
 // Each byte in s must be either 0 (false) or -1 (true). No other values are allowed.
-static inline Vec32c select (Vec32cb const & s, Vec32c const & a, Vec32c const & b) {
+static inline Vec32c select (Vec32cb const s, Vec32c const a, Vec32c const b) {
     return selectb(s,a,b);
 }
 
 // Conditional add: For all vector elements i: result[i] = f[i] ? (a[i] + b[i]) : a[i]
-static inline Vec32c if_add (Vec32cb const & f, Vec32c const & a, Vec32c const & b) {
+static inline Vec32c if_add (Vec32cb const f, Vec32c const a, Vec32c const b) {
     return a + (Vec32c(f) & b);
 }
 
-// Horizontal add: Calculates the sum of all vector elements.
-// Overflow will wrap around
-static inline uint32_t horizontal_add (Vec32c const & a) {
-    return horizontal_add(a.get_low() + a.get_high());
+// Conditional subtract
+static inline Vec32c if_sub (Vec32cb const f, Vec32c const a, Vec32c const b) {
+    return a - (Vec32c(f) & b);
+}
+
+// Conditional multiply
+static inline Vec32c if_mul (Vec32cb const f, Vec32c const a, Vec32c const b) {
+    return select(f, a*b, a);
+}
+
+// Horizontal add: Calculates the sum of all vector elements. Overflow will wrap around
+static inline uint8_t horizontal_add (Vec32c const a) {
+    return (uint8_t)horizontal_add(a.get_low() + a.get_high());
 }
 
 // Horizontal add extended: Calculates the sum of all vector elements.
 // Each element is sign-extended before addition to avoid overflow
-static inline int32_t horizontal_add_x (Vec32c const & a) {
+static inline int32_t horizontal_add_x (Vec32c const a) {
     return horizontal_add_x(a.get_low()) + horizontal_add_x(a.get_high());
 }
 
-
 // function add_saturated: add element by element, signed with saturation
-static inline Vec32c add_saturated(Vec32c const & a, Vec32c const & b) {
+static inline Vec32c add_saturated(Vec32c const a, Vec32c const b) {
     return Vec32c(add_saturated(a.get_low(),b.get_low()), add_saturated(a.get_high(),b.get_high()));
 }
 
 // function sub_saturated: subtract element by element, signed with saturation
-static inline Vec32c sub_saturated(Vec32c const & a, Vec32c const & b) {
+static inline Vec32c sub_saturated(Vec32c const a, Vec32c const b) {
     return Vec32c(sub_saturated(a.get_low(),b.get_low()), sub_saturated(a.get_high(),b.get_high()));
 }
 
 // function max: a > b ? a : b
-static inline Vec32c max(Vec32c const & a, Vec32c const & b) {
+static inline Vec32c max(Vec32c const a, Vec32c const b) {
     return Vec32c(max(a.get_low(),b.get_low()), max(a.get_high(),b.get_high()));
 }
 
 // function min: a < b ? a : b
-static inline Vec32c min(Vec32c const & a, Vec32c const & b) {
+static inline Vec32c min(Vec32c const a, Vec32c const b) {
     return Vec32c(min(a.get_low(),b.get_low()), min(a.get_high(),b.get_high()));
 }
 
 // function abs: a >= 0 ? a : -a
-static inline Vec32c abs(Vec32c const & a) {
+static inline Vec32c abs(Vec32c const a) {
     return Vec32c(abs(a.get_low()), abs(a.get_high()));
 }
 
 // function abs_saturated: same as abs, saturate if overflow
-static inline Vec32c abs_saturated(Vec32c const & a) {
+static inline Vec32c abs_saturated(Vec32c const a) {
     return Vec32c(abs_saturated(a.get_low()), abs_saturated(a.get_high()));
 }
 
 // function rotate_left all elements
 // Use negative count to rotate right
-static inline Vec32c rotate_left(Vec32c const & a, int b) {
+static inline Vec32c rotate_left(Vec32c const a, int b) {
     return Vec32c(rotate_left(a.get_low(),b), rotate_left(a.get_high(),b));
 }
 
@@ -790,30 +727,29 @@ static inline Vec32c rotate_left(Vec32c const & a, int b) {
 class Vec32uc : public Vec32c {
 public:
     // Default constructor:
-    Vec32uc(){
-    }
+    Vec32uc() = default;
     // Constructor to broadcast the same value into all elements:
     Vec32uc(uint32_t i) {
         y1 = y0 = _mm_set1_epi8((char)i);
     }
     // Constructor to build from all elements:
     Vec32uc(uint8_t i0, uint8_t i1, uint8_t i2, uint8_t i3, uint8_t i4, uint8_t i5, uint8_t i6, uint8_t i7,
-        uint8_t i8, uint8_t i9, uint8_t i10, uint8_t i11, uint8_t i12, uint8_t i13, uint8_t i14, uint8_t i15,        
+        uint8_t i8, uint8_t i9, uint8_t i10, uint8_t i11, uint8_t i12, uint8_t i13, uint8_t i14, uint8_t i15,
         uint8_t i16, uint8_t i17, uint8_t i18, uint8_t i19, uint8_t i20, uint8_t i21, uint8_t i22, uint8_t i23,
         uint8_t i24, uint8_t i25, uint8_t i26, uint8_t i27, uint8_t i28, uint8_t i29, uint8_t i30, uint8_t i31) {
-        y0 = _mm_setr_epi8(i0,  i1,  i2,  i3,  i4,  i5,  i6,  i7,  i8,  i9,  i10, i11, i12, i13, i14, i15);
-        y1 = _mm_setr_epi8(i16, i17, i18, i19, i20, i21, i22, i23, i24, i25, i26, i27, i28, i29, i30, i31);
+        y0 = _mm_setr_epi8((int8_t)i0,  (int8_t)i1,  (int8_t)i2,  (int8_t)i3,  (int8_t)i4,  (int8_t)i5,  (int8_t)i6,  (int8_t)i7,  (int8_t)i8,  (int8_t)i9,  (int8_t)i10, (int8_t)i11, (int8_t)i12, (int8_t)i13, (int8_t)i14, (int8_t)i15);
+        y1 = _mm_setr_epi8((int8_t)i16, (int8_t)i17, (int8_t)i18, (int8_t)i19, (int8_t)i20, (int8_t)i21, (int8_t)i22, (int8_t)i23, (int8_t)i24, (int8_t)i25, (int8_t)i26, (int8_t)i27, (int8_t)i28, (int8_t)i29, (int8_t)i30, (int8_t)i31);
     }
     // Constructor to build from two Vec16uc:
-    Vec32uc(Vec16uc const & a0, Vec16uc const & a1) {
+    Vec32uc(Vec16uc const a0, Vec16uc const a1) {
         y0 = a0;  y1 = a1;
     }
-    // Constructor to convert from type Vec256ie
-    Vec32uc(Vec256ie const & x) {
+    // Constructor to convert from type Vec256b
+    Vec32uc(Vec256b const x) {
         y0 = x.get_low();  y1 = x.get_high();
     }
-    // Assignment operator to convert from type Vec256ie
-    Vec32uc & operator = (Vec256ie const & x) {
+    // Assignment operator to convert from type Vec256b
+    Vec32uc & operator = (Vec256b const x) {
         y0 = x.get_low();  y1 = x.get_high();
         return *this;
     }
@@ -830,18 +766,17 @@ public:
         return *this;
     }
     // Member function to change a single element in vector
-    // Note: This function is inefficient. Use load function if changing more than one element
-    Vec32uc const & insert(uint32_t index, uint8_t value) {
-        Vec32c::insert(index, value);
+    Vec32uc const insert(int index, uint8_t value) {
+        Vec32c::insert(index, (int8_t)value);
         return *this;
     }
     // Member function extract a single element from vector
-    uint8_t extract(uint32_t index) const {
-        return Vec32c::extract(index);
+    uint8_t extract(int index) const {
+        return (uint8_t)Vec32c::extract(index);
     }
     // Extract a single element. Use store function if extracting more than one element.
     // Operator [] can only read an element, not write.
-    uint8_t operator [] (uint32_t index) const {
+    uint8_t operator [] (int index) const {
         return extract(index);
     }
     // Member functions to split into two Vec16uc:
@@ -851,53 +786,56 @@ public:
     Vec16uc get_high() const {
         return y1;
     }
+    static constexpr int elementtype() {
+        return 5;
+    }
 };
 
 // Define operators for this class
 
 // vector operator + : add
-static inline Vec32uc operator + (Vec32uc const & a, Vec32uc const & b) {
-    return Vec32uc(a.get_low() + b.get_low(), a.get_high() + b.get_high()); 
+static inline Vec32uc operator + (Vec32uc const a, Vec32uc const b) {
+    return Vec32uc(a.get_low() + b.get_low(), a.get_high() + b.get_high());
 }
 
 // vector operator - : subtract
-static inline Vec32uc operator - (Vec32uc const & a, Vec32uc const & b) {
-    return Vec32uc(a.get_low() - b.get_low(), a.get_high() - b.get_high()); 
+static inline Vec32uc operator - (Vec32uc const a, Vec32uc const b) {
+    return Vec32uc(a.get_low() - b.get_low(), a.get_high() - b.get_high());
 }
 
 // vector operator * : multiply
-static inline Vec32uc operator * (Vec32uc const & a, Vec32uc const & b) {
-    return Vec32uc(a.get_low() * b.get_low(), a.get_high() * b.get_high()); 
+static inline Vec32uc operator * (Vec32uc const a, Vec32uc const b) {
+    return Vec32uc(a.get_low() * b.get_low(), a.get_high() * b.get_high());
 }
 
 // vector operator / : divide
-static inline Vec32uc operator / (Vec32uc const & a, Divisor_us const & d) {
+static inline Vec32uc operator / (Vec32uc const a, Divisor_us const d) {
     return Vec32uc(a.get_low() / d, a.get_high() / d);
 }
 
 // vector operator /= : divide
-static inline Vec32uc & operator /= (Vec32uc & a, Divisor_us const & d) {
+static inline Vec32uc & operator /= (Vec32uc & a, Divisor_us const d) {
     a = a / d;
     return a;
 }
 
 // vector operator << : shift left all elements
-static inline Vec32uc operator << (Vec32uc const & a, uint32_t b) {
-    return Vec32uc(a.get_low() << b, a.get_high() << b); 
+static inline Vec32uc operator << (Vec32uc const a, uint32_t b) {
+    return Vec32uc(a.get_low() << b, a.get_high() << b);
 }
 
 // vector operator << : shift left all elements
-static inline Vec32uc operator << (Vec32uc const & a, int32_t b) {
+static inline Vec32uc operator << (Vec32uc const a, int32_t b) {
     return a << (uint32_t)b;
 }
 
 // vector operator >> : shift right logical all elements
-static inline Vec32uc operator >> (Vec32uc const & a, uint32_t b) {
-    return Vec32uc(a.get_low() >> b, a.get_high() >> b); 
+static inline Vec32uc operator >> (Vec32uc const a, uint32_t b) {
+    return Vec32uc(a.get_low() >> b, a.get_high() >> b);
 }
 
 // vector operator >> : shift right logical all elements
-static inline Vec32uc operator >> (Vec32uc const & a, int32_t b) {
+static inline Vec32uc operator >> (Vec32uc const a, int32_t b) {
     return a >> (uint32_t)b;
 }
 
@@ -908,48 +846,48 @@ static inline Vec32uc & operator >>= (Vec32uc & a, uint32_t b) {
 }
 
 // vector operator >= : returns true for elements for which a >= b (unsigned)
-static inline Vec32cb operator >= (Vec32uc const & a, Vec32uc const & b) {
-    return Vec32c(a.get_low() >= b.get_low(), a.get_high() >= b.get_high()); 
+static inline Vec32cb operator >= (Vec32uc const a, Vec32uc const b) {
+    return Vec32c(a.get_low() >= b.get_low(), a.get_high() >= b.get_high());
 }
 
 // vector operator <= : returns true for elements for which a <= b (unsigned)
-static inline Vec32cb operator <= (Vec32uc const & a, Vec32uc const & b) {
+static inline Vec32cb operator <= (Vec32uc const a, Vec32uc const b) {
     return b >= a;
 }
 
 // vector operator > : returns true for elements for which a > b (unsigned)
-static inline Vec32cb operator > (Vec32uc const & a, Vec32uc const & b) {
-    return Vec32c(a.get_low() > b.get_low(), a.get_high() > b.get_high()); 
+static inline Vec32cb operator > (Vec32uc const a, Vec32uc const b) {
+    return Vec32c(a.get_low() > b.get_low(), a.get_high() > b.get_high());
 }
 
 // vector operator < : returns true for elements for which a < b (unsigned)
-static inline Vec32cb operator < (Vec32uc const & a, Vec32uc const & b) {
+static inline Vec32cb operator < (Vec32uc const a, Vec32uc const b) {
     return b > a;
 }
 
 // vector operator & : bitwise and
-static inline Vec32uc operator & (Vec32uc const & a, Vec32uc const & b) {
+static inline Vec32uc operator & (Vec32uc const a, Vec32uc const b) {
     return Vec32uc(a.get_low() & b.get_low(), a.get_high() & b.get_high());
 }
-static inline Vec32uc operator && (Vec32uc const & a, Vec32uc const & b) {
+static inline Vec32uc operator && (Vec32uc const a, Vec32uc const b) {
     return a & b;
 }
 
 // vector operator | : bitwise or
-static inline Vec32uc operator | (Vec32uc const & a, Vec32uc const & b) {
+static inline Vec32uc operator | (Vec32uc const a, Vec32uc const b) {
     return Vec32uc(a.get_low() | b.get_low(), a.get_high() | b.get_high());
 }
-static inline Vec32uc operator || (Vec32uc const & a, Vec32uc const & b) {
+static inline Vec32uc operator || (Vec32uc const a, Vec32uc const b) {
     return a | b;
 }
 
 // vector operator ^ : bitwise xor
-static inline Vec32uc operator ^ (Vec32uc const & a, Vec32uc const & b) {
+static inline Vec32uc operator ^ (Vec32uc const a, Vec32uc const b) {
     return Vec32uc(a.get_low() ^ b.get_low(), a.get_high() ^ b.get_high());
 }
 
 // vector operator ~ : bitwise not
-static inline Vec32uc operator ~ (Vec32uc const & a) {
+static inline Vec32uc operator ~ (Vec32uc const a) {
     return Vec32uc(~a.get_low(), ~a.get_high());
 }
 
@@ -959,50 +897,58 @@ static inline Vec32uc operator ~ (Vec32uc const & a) {
 // for (int i = 0; i < 32; i++) result[i] = s[i] ? a[i] : b[i];
 // Each byte in s must be either 0 (false) or -1 (true). No other values are allowed.
 // (s is signed)
-static inline Vec32uc select (Vec32cb const & s, Vec32uc const & a, Vec32uc const & b) {
+static inline Vec32uc select (Vec32cb const s, Vec32uc const a, Vec32uc const b) {
     return selectb(s,a,b);
 }
 
 // Conditional add: For all vector elements i: result[i] = f[i] ? (a[i] + b[i]) : a[i]
-static inline Vec32uc if_add (Vec32cb const & f, Vec32uc const & a, Vec32uc const & b) {
+static inline Vec32uc if_add (Vec32cb const f, Vec32uc const a, Vec32uc const b) {
     return a + (Vec32uc(f) & b);
 }
 
-// Horizontal add: Calculates the sum of all vector elements.
-// Overflow will wrap around
+// Conditional subtract
+static inline Vec32uc if_sub (Vec32cb const f, Vec32uc const a, Vec32uc const b) {
+    return a - (Vec32uc(f) & b);
+}
+
+// Conditional multiply
+static inline Vec32uc if_mul (Vec32cb const f, Vec32uc const a, Vec32uc const b) {
+    return select(f, a*b, a);
+}
+
+// Horizontal add: Calculates the sum of all vector elements. Overflow will wrap around
 // (Note: horizontal_add_x(Vec32uc) is slightly faster)
-static inline uint32_t horizontal_add (Vec32uc const & a) {
+static inline uint32_t horizontal_add (Vec32uc const a) {
     return horizontal_add(a.get_low() + a.get_high());
 }
 
 // Horizontal add extended: Calculates the sum of all vector elements.
 // Each element is zero-extended before addition to avoid overflow
-static inline uint32_t horizontal_add_x (Vec32uc const & a) {
+static inline uint32_t horizontal_add_x (Vec32uc const a) {
     return horizontal_add_x(a.get_low()) + horizontal_add_x(a.get_high());
 }
 
 // function add_saturated: add element by element, unsigned with saturation
-static inline Vec32uc add_saturated(Vec32uc const & a, Vec32uc const & b) {
-    return Vec32uc(add_saturated(a.get_low(),b.get_low()), add_saturated(a.get_high(),b.get_high())); 
+static inline Vec32uc add_saturated(Vec32uc const a, Vec32uc const b) {
+    return Vec32uc(add_saturated(a.get_low(),b.get_low()), add_saturated(a.get_high(),b.get_high()));
 }
 
 // function sub_saturated: subtract element by element, unsigned with saturation
-static inline Vec32uc sub_saturated(Vec32uc const & a, Vec32uc const & b) {
-    return Vec32uc(sub_saturated(a.get_low(),b.get_low()), sub_saturated(a.get_high(),b.get_high())); 
+static inline Vec32uc sub_saturated(Vec32uc const a, Vec32uc const b) {
+    return Vec32uc(sub_saturated(a.get_low(),b.get_low()), sub_saturated(a.get_high(),b.get_high()));
 }
 
 // function max: a > b ? a : b
-static inline Vec32uc max(Vec32uc const & a, Vec32uc const & b) {
-    return Vec32uc(max(a.get_low(),b.get_low()), max(a.get_high(),b.get_high())); 
+static inline Vec32uc max(Vec32uc const a, Vec32uc const b) {
+    return Vec32uc(max(a.get_low(),b.get_low()), max(a.get_high(),b.get_high()));
 }
 
 // function min: a < b ? a : b
-static inline Vec32uc min(Vec32uc const & a, Vec32uc const & b) {
-    return Vec32uc(min(a.get_low(),b.get_low()), min(a.get_high(),b.get_high())); 
+static inline Vec32uc min(Vec32uc const a, Vec32uc const b) {
+    return Vec32uc(min(a.get_low(),b.get_low()), min(a.get_high(),b.get_high()));
 }
 
 
-    
 /*****************************************************************************
 *
 *          Vector of 16 16-bit signed integers
@@ -1012,8 +958,7 @@ static inline Vec32uc min(Vec32uc const & a, Vec32uc const & b) {
 class Vec16s : public Vec256b {
 public:
     // Default constructor:
-    Vec16s() {
-    }
+    Vec16s() = default;
     // Constructor to broadcast the same value into all elements:
     Vec16s(int i) {
         y1 = y0 = _mm_set1_epi16((int16_t)i);
@@ -1025,15 +970,15 @@ public:
         y1 = _mm_setr_epi16(i8, i9, i10, i11, i12, i13, i14, i15);
     }
     // Constructor to build from two Vec8s:
-    Vec16s(Vec8s const & a0, Vec8s const & a1) {
+    Vec16s(Vec8s const a0, Vec8s const a1) {
         y0 = a0;  y1 = a1;
     }
-    // Constructor to convert from type Vec256ie
-    Vec16s(Vec256ie const & x) {
+    // Constructor to convert from type Vec256b
+    Vec16s(Vec256b const & x) {
         y0 = x.get_low();  y1 = x.get_high();
     }
-    // Assignment operator to convert from type Vec256ie
-    Vec16s & operator = (Vec256ie const & x) {
+    // Assignment operator to convert from type Vec256b
+    Vec16s & operator = (Vec256b const x) {
         y0 = x.get_low();  y1 = x.get_high();
         return *this;
     }
@@ -1049,12 +994,6 @@ public:
         y1 = _mm_load_si128((__m128i const*)p + 1);
         return *this;
     }
-    // Member function to load 16 8-bit unsigned integers from array
-    Vec16s & load_16uc(void const * p) {
-        y0 = Vec8s().load_8uc(p);
-        y1 = Vec8s().load_8uc((uint8_t const*)p + 8);
-        return *this;
-    }
     // Partial load. Load n elements and set the rest to 0
     Vec16s & load_partial(int n, void const * p) {
         if (n <= 0) {
@@ -1089,13 +1028,12 @@ public:
     }
     // cut off vector to n elements. The last 16-n elements are set to zero
     Vec16s & cutoff(int n) {
-        *this = Vec32c(*this).cutoff(n * 2);
+        *this = Vec16s(Vec32c(*this).cutoff(n * 2));
         return *this;
     }
     // Member function to change a single element in vector
-    // Note: This function is inefficient. Use load function if changing more than one element
-    Vec16s const & insert(uint32_t index, int16_t value) {
-        if (index < 8) {
+    Vec16s const insert(int index, int16_t value) {
+        if ((uint32_t)index < 8) {
             y0 = Vec8s(y0).insert(index, value);
         }
         else {
@@ -1104,8 +1042,8 @@ public:
         return *this;
     }
     // Member function extract a single element from vector
-    int16_t extract(uint32_t index) const {
-        if (index < 8) {
+    int16_t extract(int index) const {
+        if ((uint32_t)index < 8) {
             return Vec8s(y0).extract(index);
         }
         else {
@@ -1114,7 +1052,7 @@ public:
     }
     // Extract a single element. Use store function if extracting more than one element.
     // Operator [] can only read an element, not write.
-    int16_t operator [] (uint32_t index) const {
+    int16_t operator [] (int index) const {
         return extract(index);
     }
     // Member functions to split into two Vec8s:
@@ -1124,9 +1062,12 @@ public:
     Vec8s get_high() const {
         return y1;
     }
-    static int size () {
+    static constexpr int size() {
         return 16;
     }
+    static constexpr int elementtype() {
+        return 6;
+    }
 };
 
 
@@ -1139,20 +1080,19 @@ public:
 class Vec16sb : public Vec16s {
 public:
     // Default constructor:
-    Vec16sb() {
-    }
+    Vec16sb() = default;
     // Constructor to build from all elements:
     Vec16sb(bool x0, bool x1, bool x2, bool x3, bool x4, bool x5, bool x6, bool x7,
         bool x8, bool x9, bool x10, bool x11, bool x12, bool x13, bool x14, bool x15) :
-        Vec16s(-int16_t(x0), -int16_t(x1), -int16_t(x2), -int16_t(x3), -int16_t(x4), -int16_t(x5), -int16_t(x6), -int16_t(x7), 
+        Vec16s(-int16_t(x0), -int16_t(x1), -int16_t(x2), -int16_t(x3), -int16_t(x4), -int16_t(x5), -int16_t(x6), -int16_t(x7),
             -int16_t(x8), -int16_t(x9), -int16_t(x10), -int16_t(x11), -int16_t(x12), -int16_t(x13), -int16_t(x14), -int16_t(x15))
         {}
-    // Constructor to convert from type Vec256ie
-    Vec16sb(Vec256ie const & x) {
+    // Constructor to convert from type Vec256b
+    Vec16sb(Vec256b const x) {
         y0 = x.get_low();  y1 = x.get_high();
     }
-    // Assignment operator to convert from type Vec256ie
-    Vec16sb & operator = (Vec256ie const & x) {
+    // Assignment operator to convert from type Vec256b
+    Vec16sb & operator = (Vec256b const x) {
         y0 = x.get_low();  y1 = x.get_high();
         return *this;
     }
@@ -1164,10 +1104,9 @@ public:
         *this = Vec16sb(b);
         return *this;
     }
-private: // Prevent constructing from int, etc.
-    Vec16sb(int b);
-    Vec16sb & operator = (int x);
-public:
+    // Constructor to build from two Vec8sb:
+    Vec16sb(Vec8sb const a0, Vec8sb const a1) : Vec16s(Vec8s(a0), Vec8s(a1)) {
+    }
     // Member functions to split into two Vec8s:
     Vec8sb get_low() const {
         return y0;
@@ -1178,16 +1117,28 @@ public:
     Vec16sb & insert (int index, bool a) {
         Vec16s::insert(index, -(int)a);
         return *this;
-    }    
+    }
     // Member function extract a single element from vector
-    bool extract(uint32_t index) const {
+    bool extract(int index) const {
         return Vec16s::extract(index) != 0;
     }
     // Extract a single element. Use store function if extracting more than one element.
     // Operator [] can only read an element, not write.
-    bool operator [] (uint32_t index) const {
+    bool operator [] (int index) const {
         return extract(index);
     }
+    // Member function to change a bitfield to a boolean vector
+    Vec16sb & load_bits(uint16_t a) {
+        y0 = Vec8sb().load_bits(uint8_t(a));
+        y1 = Vec8sb().load_bits(uint8_t(a>>8));
+        return *this;
+    }
+    static constexpr int elementtype() {
+        return 3;
+    }
+    // Prevent constructing from int, etc.
+    Vec16sb(int b) = delete;
+    Vec16sb & operator = (int x) = delete;
 };
 
 
@@ -1198,53 +1149,63 @@ public:
 *****************************************************************************/
 
 // vector operator & : bitwise and
-static inline Vec16sb operator & (Vec16sb const & a, Vec16sb const & b) {
+static inline Vec16sb operator & (Vec16sb const a, Vec16sb const b) {
     return Vec16sb(Vec256b(a) & Vec256b(b));
 }
-static inline Vec16sb operator && (Vec16sb const & a, Vec16sb const & b) {
+static inline Vec16sb operator && (Vec16sb const a, Vec16sb const b) {
     return a & b;
 }
 // vector operator &= : bitwise and
-static inline Vec16sb & operator &= (Vec16sb & a, Vec16sb const & b) {
+static inline Vec16sb & operator &= (Vec16sb & a, Vec16sb const b) {
     a = a & b;
     return a;
 }
 
 // vector operator | : bitwise or
-static inline Vec16sb operator | (Vec16sb const & a, Vec16sb const & b) {
+static inline Vec16sb operator | (Vec16sb const a, Vec16sb const b) {
     return Vec16sb(Vec256b(a) | Vec256b(b));
 }
-static inline Vec16sb operator || (Vec16sb const & a, Vec16sb const & b) {
+static inline Vec16sb operator || (Vec16sb const a, Vec16sb const b) {
     return a | b;
 }
 // vector operator |= : bitwise or
-static inline Vec16sb & operator |= (Vec16sb & a, Vec16sb const & b) {
+static inline Vec16sb & operator |= (Vec16sb & a, Vec16sb const b) {
     a = a | b;
     return a;
 }
 
 // vector operator ^ : bitwise xor
-static inline Vec16sb operator ^ (Vec16sb const & a, Vec16sb const & b) {
+static inline Vec16sb operator ^ (Vec16sb const a, Vec16sb const b) {
     return Vec16sb(Vec256b(a) ^ Vec256b(b));
 }
 // vector operator ^= : bitwise xor
-static inline Vec16sb & operator ^= (Vec16sb & a, Vec16sb const & b) {
+static inline Vec16sb & operator ^= (Vec16sb & a, Vec16sb const b) {
     a = a ^ b;
     return a;
 }
 
+// vector operator == : xnor
+static inline Vec16sb operator == (Vec16sb const a, Vec16sb const b) {
+    return Vec16sb(Vec256b(a) ^ Vec256b(~b));
+}
+
+// vector operator != : xor
+static inline Vec16sb operator != (Vec16sb const a, Vec16sb const b) {
+    return Vec16sb(a ^ b);
+}
+
 // vector operator ~ : bitwise not
-static inline Vec16sb operator ~ (Vec16sb const & a) {
+static inline Vec16sb operator ~ (Vec16sb const a) {
     return Vec16sb( ~ Vec256b(a));
 }
 
 // vector operator ! : element not
-static inline Vec16sb operator ! (Vec16sb const & a) {
+static inline Vec16sb operator ! (Vec16sb const a) {
     return ~ a;
 }
 
 // vector function andnot
-static inline Vec16sb andnot (Vec16sb const & a, Vec16sb const & b) {
+static inline Vec16sb andnot (Vec16sb const a, Vec16sb const b) {
     return Vec16sb(andnot(Vec256b(a), Vec256b(b)));
 }
 
@@ -1256,12 +1217,12 @@ static inline Vec16sb andnot (Vec16sb const & a, Vec16sb const & b) {
 *****************************************************************************/
 
 // vector operator + : add element by element
-static inline Vec16s operator + (Vec16s const & a, Vec16s const & b) {
+static inline Vec16s operator + (Vec16s const a, Vec16s const b) {
     return Vec16s(a.get_low() + b.get_low(), a.get_high() + b.get_high());
 }
 
 // vector operator += : add
-static inline Vec16s & operator += (Vec16s & a, Vec16s const & b) {
+static inline Vec16s & operator += (Vec16s & a, Vec16s const b) {
     a = a + b;
     return a;
 }
@@ -1280,17 +1241,17 @@ static inline Vec16s & operator ++ (Vec16s & a) {
 }
 
 // vector operator - : subtract element by element
-static inline Vec16s operator - (Vec16s const & a, Vec16s const & b) {
+static inline Vec16s operator - (Vec16s const a, Vec16s const b) {
     return Vec16s(a.get_low() - b.get_low(), a.get_high() - b.get_high());
 }
 
 // vector operator - : unary minus
-static inline Vec16s operator - (Vec16s const & a) {
+static inline Vec16s operator - (Vec16s const a) {
     return Vec16s(-a.get_low(), -a.get_high());
 }
 
 // vector operator -= : subtract
-static inline Vec16s & operator -= (Vec16s & a, Vec16s const & b) {
+static inline Vec16s & operator -= (Vec16s & a, Vec16s const b) {
     a = a - b;
     return a;
 }
@@ -1309,29 +1270,29 @@ static inline Vec16s & operator -- (Vec16s & a) {
 }
 
 // vector operator * : multiply element by element
-static inline Vec16s operator * (Vec16s const & a, Vec16s const & b) {
+static inline Vec16s operator * (Vec16s const a, Vec16s const b) {
     return Vec16s(a.get_low() * b.get_low(), a.get_high() * b.get_high());
 }
 
 // vector operator *= : multiply
-static inline Vec16s & operator *= (Vec16s & a, Vec16s const & b) {
+static inline Vec16s & operator *= (Vec16s & a, Vec16s const b) {
     a = a * b;
     return a;
 }
 
 // vector operator / : divide all elements by same integer
-static inline Vec16s operator / (Vec16s const & a, Divisor_s const & d) {
+static inline Vec16s operator / (Vec16s const a, Divisor_s const d) {
     return Vec16s(a.get_low() / d, a.get_high() / d);
 }
 
 // vector operator /= : divide
-static inline Vec16s & operator /= (Vec16s & a, Divisor_s const & d) {
+static inline Vec16s & operator /= (Vec16s & a, Divisor_s const d) {
     a = a / d;
     return a;
 }
 
 // vector operator << : shift left
-static inline Vec16s operator << (Vec16s const & a, int b) {
+static inline Vec16s operator << (Vec16s const a, int b) {
     return Vec16s(a.get_low() << b, a.get_high() << b);
 }
 
@@ -1342,7 +1303,7 @@ static inline Vec16s & operator <<= (Vec16s & a, int b) {
 }
 
 // vector operator >> : shift right arithmetic
-static inline Vec16s operator >> (Vec16s const & a, int b) {
+static inline Vec16s operator >> (Vec16s const a, int b) {
     return Vec16s(a.get_low() >> b, a.get_high() >> b);
 }
 
@@ -1353,141 +1314,145 @@ static inline Vec16s & operator >>= (Vec16s & a, int b) {
 }
 
 // vector operator == : returns true for elements for which a == b
-static inline Vec16sb operator == (Vec16s const & a, Vec16s const & b) {
+static inline Vec16sb operator == (Vec16s const a, Vec16s const b) {
     return Vec16s(a.get_low() == b.get_low(), a.get_high() == b.get_high());
 }
 
 // vector operator != : returns true for elements for which a != b
-static inline Vec16sb operator != (Vec16s const & a, Vec16s const & b) {
+static inline Vec16sb operator != (Vec16s const a, Vec16s const b) {
     return Vec16s(a.get_low() != b.get_low(), a.get_high() != b.get_high());
 }
 
 // vector operator > : returns true for elements for which a > b
-static inline Vec16sb operator > (Vec16s const & a, Vec16s const & b) {
+static inline Vec16sb operator > (Vec16s const a, Vec16s const b) {
     return Vec16s(a.get_low() > b.get_low(), a.get_high() > b.get_high());
 }
 
 // vector operator < : returns true for elements for which a < b
-static inline Vec16sb operator < (Vec16s const & a, Vec16s const & b) {
+static inline Vec16sb operator < (Vec16s const a, Vec16s const b) {
     return b > a;
 }
 
 // vector operator >= : returns true for elements for which a >= b (signed)
-static inline Vec16sb operator >= (Vec16s const & a, Vec16s const & b) {
+static inline Vec16sb operator >= (Vec16s const a, Vec16s const b) {
     return Vec16s(a.get_low() >= b.get_low(), a.get_high() >= b.get_high());
 }
 
 // vector operator <= : returns true for elements for which a <= b (signed)
-static inline Vec16sb operator <= (Vec16s const & a, Vec16s const & b) {
+static inline Vec16sb operator <= (Vec16s const a, Vec16s const b) {
     return b >= a;
 }
 
 // vector operator & : bitwise and
-static inline Vec16s operator & (Vec16s const & a, Vec16s const & b) {
+static inline Vec16s operator & (Vec16s const a, Vec16s const b) {
     return Vec16s(a.get_low() & b.get_low(), a.get_high() & b.get_high());
 }
-static inline Vec16s operator && (Vec16s const & a, Vec16s const & b) {
+static inline Vec16s operator && (Vec16s const a, Vec16s const b) {
     return a & b;
 }
 // vector operator &= : bitwise and
-static inline Vec16s & operator &= (Vec16s & a, Vec16s const & b) {
+static inline Vec16s & operator &= (Vec16s & a, Vec16s const b) {
     a = a & b;
     return a;
 }
 
 // vector operator | : bitwise or
-static inline Vec16s operator | (Vec16s const & a, Vec16s const & b) {
+static inline Vec16s operator | (Vec16s const a, Vec16s const b) {
     return Vec16s(a.get_low() | b.get_low(), a.get_high() | b.get_high());
 }
-static inline Vec16s operator || (Vec16s const & a, Vec16s const & b) {
+static inline Vec16s operator || (Vec16s const a, Vec16s const b) {
     return a | b;
 }
 // vector operator |= : bitwise or
-static inline Vec16s & operator |= (Vec16s & a, Vec16s const & b) {
+static inline Vec16s & operator |= (Vec16s & a, Vec16s const b) {
     a = a | b;
     return a;
 }
 
 // vector operator ^ : bitwise xor
-static inline Vec16s operator ^ (Vec16s const & a, Vec16s const & b) {
+static inline Vec16s operator ^ (Vec16s const a, Vec16s const b) {
     return Vec16s(a.get_low() ^ b.get_low(), a.get_high() ^ b.get_high());
 }
 // vector operator ^= : bitwise xor
-static inline Vec16s & operator ^= (Vec16s & a, Vec16s const & b) {
+static inline Vec16s & operator ^= (Vec16s & a, Vec16s const b) {
     a = a ^ b;
     return a;
 }
 
 // vector operator ~ : bitwise not
-static inline Vec16s operator ~ (Vec16s const & a) {
+static inline Vec16s operator ~ (Vec16s const a) {
     return Vec16s(~Vec256b(a));
 }
 
-// vector operator ! : logical not, returns true for elements == 0
-static inline Vec16sb operator ! (Vec16s const & a) {
-    return Vec16s(!a.get_low(), !a.get_high());
-}
-
 // Functions for this class
 
 // Select between two operands. Corresponds to this pseudocode:
 // for (int i = 0; i < 16; i++) result[i] = s[i] ? a[i] : b[i];
 // Each byte in s must be either 0 (false) or -1 (true). No other values are allowed.
 // (s is signed)
-static inline Vec16s select (Vec16sb const & s, Vec16s const & a, Vec16s const & b) {
+static inline Vec16s select (Vec16sb const s, Vec16s const a, Vec16s const b) {
     return selectb(s,a,b);
 }
 
 // Conditional add: For all vector elements i: result[i] = f[i] ? (a[i] + b[i]) : a[i]
-static inline Vec16s if_add (Vec16sb const & f, Vec16s const & a, Vec16s const & b) {
+static inline Vec16s if_add (Vec16sb const f, Vec16s const a, Vec16s const b) {
     return a + (Vec16s(f) & b);
 }
 
-// Horizontal add: Calculates the sum of all vector elements.
-// Overflow will wrap around
-static inline int32_t horizontal_add (Vec16s const & a) {
+// Conditional subtract
+static inline Vec16s if_sub (Vec16sb const f, Vec16s const a, Vec16s const b) {
+    return a - (Vec16s(f) & b);
+}
+
+// Conditional multiply
+static inline Vec16s if_mul (Vec16sb const f, Vec16s const a, Vec16s const b) {
+    return select(f, a*b, a);
+}
+
+// Horizontal add: Calculates the sum of all vector elements. Overflow will wrap around
+static inline int16_t horizontal_add (Vec16s const a) {
     return horizontal_add(a.get_low() + a.get_high());
 }
 
 // Horizontal add extended: Calculates the sum of all vector elements.
 // Elements are sign extended before adding to avoid overflow
-static inline int32_t horizontal_add_x (Vec16s const & a) {
+static inline int32_t horizontal_add_x (Vec16s const a) {
     return horizontal_add_x(a.get_low()) + horizontal_add_x(a.get_high());
 }
 
 // function add_saturated: add element by element, signed with saturation
-static inline Vec16s add_saturated(Vec16s const & a, Vec16s const & b) {
+static inline Vec16s add_saturated(Vec16s const a, Vec16s const b) {
     return Vec16s(add_saturated(a.get_low(),b.get_low()), add_saturated(a.get_high(),b.get_high()));
 }
 
 // function sub_saturated: subtract element by element, signed with saturation
-static inline Vec16s sub_saturated(Vec16s const & a, Vec16s const & b) {
+static inline Vec16s sub_saturated(Vec16s const a, Vec16s const b) {
     return Vec16s(sub_saturated(a.get_low(),b.get_low()), sub_saturated(a.get_high(),b.get_high()));
 }
 
 // function max: a > b ? a : b
-static inline Vec16s max(Vec16s const & a, Vec16s const & b) {
+static inline Vec16s max(Vec16s const a, Vec16s const b) {
     return Vec16s(max(a.get_low(),b.get_low()), max(a.get_high(),b.get_high()));
 }
 
 // function min: a < b ? a : b
-static inline Vec16s min(Vec16s const & a, Vec16s const & b) {
+static inline Vec16s min(Vec16s const a, Vec16s const b) {
     return Vec16s(min(a.get_low(),b.get_low()), min(a.get_high(),b.get_high()));
 }
 
 // function abs: a >= 0 ? a : -a
-static inline Vec16s abs(Vec16s const & a) {
+static inline Vec16s abs(Vec16s const a) {
     return Vec16s(abs(a.get_low()), abs(a.get_high()));
 }
 
 // function abs_saturated: same as abs, saturate if overflow
-static inline Vec16s abs_saturated(Vec16s const & a) {
+static inline Vec16s abs_saturated(Vec16s const a) {
     return Vec16s(abs_saturated(a.get_low()), abs_saturated(a.get_high()));
 }
 
 // function rotate_left all elements
 // Use negative count to rotate right
-static inline Vec16s rotate_left(Vec16s const & a, int b) {
+static inline Vec16s rotate_left(Vec16s const a, int b) {
     return Vec16s(rotate_left(a.get_low(),b), rotate_left(a.get_high(),b));
 }
 
@@ -1501,8 +1466,7 @@ static inline Vec16s rotate_left(Vec16s const & a, int b) {
 class Vec16us : public Vec16s {
 public:
     // Default constructor:
-    Vec16us(){
-    }
+    Vec16us() = default;
     // Constructor to broadcast the same value into all elements:
     Vec16us(uint32_t i) {
         y1 = y0 = _mm_set1_epi16((int16_t)i);
@@ -1510,19 +1474,19 @@ public:
     // Constructor to build from all elements:
     Vec16us(uint16_t i0, uint16_t i1, uint16_t i2,  uint16_t i3,  uint16_t i4,  uint16_t i5,  uint16_t i6,  uint16_t i7,
             uint16_t i8, uint16_t i9, uint16_t i10, uint16_t i11, uint16_t i12, uint16_t i13, uint16_t i14, uint16_t i15) {
-        y0 = _mm_setr_epi16(i0, i1, i2,  i3,  i4,  i5,  i6,  i7);
-        y1 = _mm_setr_epi16(i8, i9, i10, i11, i12, i13, i14, i15 );
+        y0 = _mm_setr_epi16((int16_t)i0, (int16_t)i1, (int16_t)i2,  (int16_t)i3,  (int16_t)i4,  (int16_t)i5,  (int16_t)i6,  (int16_t)i7);
+        y1 = _mm_setr_epi16((int16_t)i8, (int16_t)i9, (int16_t)i10, (int16_t)i11, (int16_t)i12, (int16_t)i13, (int16_t)i14, (int16_t)i15);
     }
     // Constructor to build from two Vec8us:
-    Vec16us(Vec8us const & a0, Vec8us const & a1) {
+    Vec16us(Vec8us const a0, Vec8us const a1) {
         y0 = a0;  y1 = a1;
     }
-    // Constructor to convert from type Vec256ie
-    Vec16us(Vec256ie const & x) {
+    // Constructor to convert from type Vec256b
+    Vec16us(Vec256b const x) {
         y0 = x.get_low();  y1 = x.get_high();
     }
-    // Assignment operator to convert from type Vec256ie
-    Vec16us & operator = (Vec256ie const & x) {
+    // Assignment operator to convert from type Vec256b
+    Vec16us & operator = (Vec256b const x) {
         y0 = x.get_low();  y1 = x.get_high();
         return *this;
     }
@@ -1539,18 +1503,17 @@ public:
         return *this;
     }
     // Member function to change a single element in vector
-    // Note: This function is inefficient. Use load function if changing more than one element
-    Vec16us const & insert(uint32_t index, uint16_t value) {
-        Vec16s::insert(index, value);
+    Vec16us const insert(int index, uint16_t value) {
+        Vec16s::insert(index, (int16_t)value);
         return *this;
     }
     // Member function extract a single element from vector
-    uint16_t extract(uint32_t index) const {
-        return Vec16s::extract(index);
+    uint16_t extract(int index) const {
+        return (uint16_t)Vec16s::extract(index);
     }
     // Extract a single element. Use store function if extracting more than one element.
     // Operator [] can only read an element, not write.
-    uint16_t operator [] (uint32_t index) const {
+    uint16_t operator [] (int index) const {
         return extract(index);
     }
     // Member functions to split into two Vec8us:
@@ -1560,43 +1523,46 @@ public:
     Vec8us get_high() const {
         return y1;
     }
+    static constexpr int elementtype() {
+        return 7;
+    }
 };
 
 // Define operators for this class
 
 // vector operator + : add
-static inline Vec16us operator + (Vec16us const & a, Vec16us const & b) {
+static inline Vec16us operator + (Vec16us const a, Vec16us const b) {
     return Vec16us(a.get_low() + b.get_low(), a.get_high() + b.get_high());
 }
 
 // vector operator - : subtract
-static inline Vec16us operator - (Vec16us const & a, Vec16us const & b) {
+static inline Vec16us operator - (Vec16us const a, Vec16us const b) {
     return Vec16us(a.get_low() - b.get_low(), a.get_high() - b.get_high());
 }
 
 // vector operator * : multiply
-static inline Vec16us operator * (Vec16us const & a, Vec16us const & b) {
+static inline Vec16us operator * (Vec16us const a, Vec16us const b) {
     return Vec16us(a.get_low() * b.get_low(), a.get_high() * b.get_high());
 }
 
 // vector operator / : divide
-static inline Vec16us operator / (Vec16us const & a, Divisor_us const & d) {
+static inline Vec16us operator / (Vec16us const a, Divisor_us const d) {
     return Vec16us(a.get_low() / d, a.get_high() / d);
 }
 
 // vector operator /= : divide
-static inline Vec16us & operator /= (Vec16us & a, Divisor_us const & d) {
+static inline Vec16us & operator /= (Vec16us & a, Divisor_us const d) {
     a = a / d;
     return a;
 }
 
 // vector operator >> : shift right logical all elements
-static inline Vec16us operator >> (Vec16us const & a, uint32_t b) {
+static inline Vec16us operator >> (Vec16us const a, uint32_t b) {
     return Vec16us(a.get_low() >> b, a.get_high() >> b);
 }
 
 // vector operator >> : shift right logical all elements
-static inline Vec16us operator >> (Vec16us const & a, int b) {
+static inline Vec16us operator >> (Vec16us const a, int b) {
     return a >> (uint32_t)b;
 }
 
@@ -1607,58 +1573,58 @@ static inline Vec16us & operator >>= (Vec16us & a, uint32_t b) {
 }
 
 // vector operator << : shift left all elements
-static inline Vec16us operator << (Vec16us const & a, uint32_t b) {
+static inline Vec16us operator << (Vec16us const a, uint32_t b) {
     return Vec16us(a.get_low() << b, a.get_high() << b);
 }
 
 // vector operator << : shift left all elements
-static inline Vec16us operator << (Vec16us const & a, int32_t b) {
+static inline Vec16us operator << (Vec16us const a, int32_t b) {
     return a << (uint32_t)b;
 }
 
 // vector operator >= : returns true for elements for which a >= b (unsigned)
-static inline Vec16sb operator >= (Vec16us const & a, Vec16us const & b) {
+static inline Vec16sb operator >= (Vec16us const a, Vec16us const b) {
     return Vec16s(a.get_low() >= b.get_low(), a.get_high() >= b.get_high());
 }
 
 // vector operator <= : returns true for elements for which a <= b (unsigned)
-static inline Vec16sb operator <= (Vec16us const & a, Vec16us const & b) {
+static inline Vec16sb operator <= (Vec16us const a, Vec16us const b) {
     return b >= a;
 }
 
 // vector operator > : returns true for elements for which a > b (unsigned)
-static inline Vec16sb operator > (Vec16us const & a, Vec16us const & b) {
+static inline Vec16sb operator > (Vec16us const a, Vec16us const b) {
     return Vec16s(a.get_low() > b.get_low(), a.get_high() > b.get_high());
 }
 
 // vector operator < : returns true for elements for which a < b (unsigned)
-static inline Vec16sb operator < (Vec16us const & a, Vec16us const & b) {
+static inline Vec16sb operator < (Vec16us const a, Vec16us const b) {
     return b > a;
 }
 
 // vector operator & : bitwise and
-static inline Vec16us operator & (Vec16us const & a, Vec16us const & b) {
+static inline Vec16us operator & (Vec16us const a, Vec16us const b) {
     return Vec16us(a.get_low() & b.get_low(), a.get_high() & b.get_high());
 }
-static inline Vec16us operator && (Vec16us const & a, Vec16us const & b) {
+static inline Vec16us operator && (Vec16us const a, Vec16us const b) {
     return a & b;
 }
 
 // vector operator | : bitwise or
-static inline Vec16us operator | (Vec16us const & a, Vec16us const & b) {
+static inline Vec16us operator | (Vec16us const a, Vec16us const b) {
     return Vec16us(a.get_low() | b.get_low(), a.get_high() | b.get_high());
 }
-static inline Vec16us operator || (Vec16us const & a, Vec16us const & b) {
+static inline Vec16us operator || (Vec16us const a, Vec16us const b) {
     return a | b;
 }
 
 // vector operator ^ : bitwise xor
-static inline Vec16us operator ^ (Vec16us const & a, Vec16us const & b) {
+static inline Vec16us operator ^ (Vec16us const a, Vec16us const b) {
     return Vec16us(a.get_low() ^ b.get_low(), a.get_high() ^ b.get_high());
 }
 
 // vector operator ~ : bitwise not
-static inline Vec16us operator ~ (Vec16us const & a) {
+static inline Vec16us operator ~ (Vec16us const a) {
     return Vec16us(~ Vec256b(a));
 }
 
@@ -1669,49 +1635,57 @@ static inline Vec16us operator ~ (Vec16us const & a) {
 // for (int i = 0; i < 8; i++) result[i] = s[i] ? a[i] : b[i];
 // Each word in s must be either 0 (false) or -1 (true). No other values are allowed.
 // (s is signed)
-static inline Vec16us select (Vec16sb const & s, Vec16us const & a, Vec16us const & b) {
+static inline Vec16us select (Vec16sb const s, Vec16us const a, Vec16us const b) {
     return selectb(s,a,b);
 }
 
 // Conditional add: For all vector elements i: result[i] = f[i] ? (a[i] + b[i]) : a[i]
-static inline Vec16us if_add (Vec16sb const & f, Vec16us const & a, Vec16us const & b) {
+static inline Vec16us if_add (Vec16sb const f, Vec16us const a, Vec16us const b) {
     return a + (Vec16us(f) & b);
 }
 
-// Horizontal add: Calculates the sum of all vector elements.
-// Overflow will wrap around
-static inline uint32_t horizontal_add (Vec16us const & a) {
+// Conditional subtract
+static inline Vec16us if_sub (Vec16sb const f, Vec16us const a, Vec16us const b) {
+    return a - (Vec16us(f) & b);
+}
+
+// Conditional multiply
+static inline Vec16us if_mul (Vec16sb const f, Vec16us const a, Vec16us const b) {
+    return select(f, a*b, a);
+}
+
+// Horizontal add: Calculates the sum of all vector elements. Overflow will wrap around
+static inline uint32_t horizontal_add (Vec16us const a) {
     return horizontal_add(a.get_low() + a.get_high());
 }
 
 // Horizontal add extended: Calculates the sum of all vector elements.
 // Each element is zero-extended before addition to avoid overflow
-static inline uint32_t horizontal_add_x (Vec16us const & a) {
+static inline uint32_t horizontal_add_x (Vec16us const a) {
     return horizontal_add_x(a.get_low()) + horizontal_add_x(a.get_high());
 }
 
 // function add_saturated: add element by element, unsigned with saturation
-static inline Vec16us add_saturated(Vec16us const & a, Vec16us const & b) {
+static inline Vec16us add_saturated(Vec16us const a, Vec16us const b) {
     return Vec16us(add_saturated(a.get_low(),b.get_low()), add_saturated(a.get_high(),b.get_high()));
 }
 
 // function sub_saturated: subtract element by element, unsigned with saturation
-static inline Vec16us sub_saturated(Vec16us const & a, Vec16us const & b) {
+static inline Vec16us sub_saturated(Vec16us const a, Vec16us const b) {
     return Vec16us(sub_saturated(a.get_low(),b.get_low()), sub_saturated(a.get_high(),b.get_high()));
 }
 
 // function max: a > b ? a : b
-static inline Vec16us max(Vec16us const & a, Vec16us const & b) {
+static inline Vec16us max(Vec16us const a, Vec16us const b) {
     return Vec16us(max(a.get_low(),b.get_low()), max(a.get_high(),b.get_high()));
 }
 
 // function min: a < b ? a : b
-static inline Vec16us min(Vec16us const & a, Vec16us const & b) {
+static inline Vec16us min(Vec16us const a, Vec16us const b) {
     return Vec16us(min(a.get_low(),b.get_low()), min(a.get_high(),b.get_high()));
 }
 
 
-
 /*****************************************************************************
 *
 *          Vector of 8 32-bit signed integers
@@ -1721,8 +1695,7 @@ static inline Vec16us min(Vec16us const & a, Vec16us const & b) {
 class Vec8i : public Vec256b {
 public:
     // Default constructor:
-    Vec8i() {
-    }
+    Vec8i() = default;
     // Constructor to broadcast the same value into all elements:
     Vec8i(int i) {
         y1 = y0 = _mm_set1_epi32(i);
@@ -1733,15 +1706,15 @@ public:
         y1 = _mm_setr_epi32(i4, i5, i6, i7);
     }
     // Constructor to build from two Vec4i:
-    Vec8i(Vec4i const & a0, Vec4i const & a1) {
+    Vec8i(Vec4i const a0, Vec4i const a1) {
         y0 = a0;  y1 = a1;
     }
-    // Constructor to convert from type Vec256ie
-    Vec8i(Vec256ie const & x) {
+    // Constructor to convert from type Vec256b
+    Vec8i(Vec256b const & x) {
         y0 = x.get_low();  y1 = x.get_high();
     }
-    // Assignment operator to convert from type Vec256ie
-    Vec8i & operator = (Vec256ie const & x) {
+    // Assignment operator to convert from type Vec256b
+    Vec8i & operator = (Vec256b const x) {
         y0 = x.get_low();  y1 = x.get_high();
         return *this;
     }
@@ -1757,18 +1730,6 @@ public:
         y1 = _mm_load_si128((__m128i const*)p + 1);
         return *this;
     }
-    // Member function to load 8 8-bit unsigned integers from array
-    Vec8i & load_8uc(void const * p) {
-        y0 = Vec4i().load_4uc(p);
-        y1 = Vec4i().load_4uc((uint8_t const*)p + 4);
-        return *this;
-    }
-    // Member function to load 8 16-bit unsigned integers from array
-    Vec8i & load_8us(void const * p) {
-        y0 = Vec4i().load_4us(p);
-        y1 = Vec4i().load_4us((uint16_t const*)p + 4);
-        return *this;
-    }
     // Partial load. Load n elements and set the rest to 0
     Vec8i & load_partial(int n, void const * p) {
         if (n <= 0) {
@@ -1807,9 +1768,8 @@ public:
         return *this;
     }
     // Member function to change a single element in vector
-    // Note: This function is inefficient. Use load function if changing more than one element
-    Vec8i const & insert(uint32_t index, int32_t value) {
-        if (index < 4) {
+    Vec8i const insert(int index, int32_t value) {
+        if ((uint32_t)index < 4) {
             y0 = Vec4i(y0).insert(index, value);
         }
         else {
@@ -1818,9 +1778,8 @@ public:
         return *this;
     }
     // Member function extract a single element from vector
-    // Note: This function is inefficient. Use store function if extracting more than one element
-    int32_t extract(uint32_t index) const {
-        if (index < 4) {
+    int32_t extract(int index) const {
+        if ((uint32_t)index < 4) {
             return Vec4i(y0).extract(index);
         }
         else {
@@ -1829,7 +1788,7 @@ public:
     }
     // Extract a single element. Use store function if extracting more than one element.
     // Operator [] can only read an element, not write.
-    int32_t operator [] (uint32_t index) const {
+    int32_t operator [] (int index) const {
         return extract(index);
     }
     // Member functions to split into two Vec4i:
@@ -1839,7 +1798,10 @@ public:
     Vec4i get_high() const {
         return y1;
     }
-    static int size () {
+    static constexpr int size() {
+        return 8;
+    }
+    static constexpr int elementtype() {
         return 8;
     }
 };
@@ -1854,18 +1816,17 @@ public:
 class Vec8ib : public Vec8i {
 public:
     // Default constructor:
-    Vec8ib() {
-    }
+    Vec8ib() = default;
     // Constructor to build from all elements:
     Vec8ib(bool x0, bool x1, bool x2, bool x3, bool x4, bool x5, bool x6, bool x7) :
         Vec8i(-int32_t(x0), -int32_t(x1), -int32_t(x2), -int32_t(x3), -int32_t(x4), -int32_t(x5), -int32_t(x6), -int32_t(x7))
         {}
-    // Constructor to convert from type Vec256ie
-    Vec8ib(Vec256ie const & x) {
+    // Constructor to convert from type Vec256b
+    Vec8ib(Vec256b const x) {
         y0 = x.get_low();  y1 = x.get_high();
     }
-    // Assignment operator to convert from type Vec256ie
-    Vec8ib & operator = (Vec256ie const & x) {
+    // Assignment operator to convert from type Vec256b
+    Vec8ib & operator = (Vec256b const x) {
         y0 = x.get_low();  y1 = x.get_high();
         return *this;
     }
@@ -1877,10 +1838,9 @@ public:
         *this = Vec8ib(b);
         return *this;
     }
-private: // Prevent constructing from int, etc.
-    Vec8ib(int b);
-    Vec8ib & operator = (int x);
-public:
+    // Constructor to build from two Vec4ib:
+    Vec8ib(Vec4ib const a0, Vec4ib const a1) : Vec8i(Vec4i(a0), Vec4i(a1)) {
+    }
     // Member functions to split into two Vec4i:
     Vec4ib get_low() const {
         return y0;
@@ -1893,15 +1853,26 @@ public:
         return *this;
     }
     // Member function extract a single element from vector
-    // Note: This function is inefficient. Use store function if extracting more than one element
-    bool extract(uint32_t index) const {
+    bool extract(int index) const {
         return Vec8i::extract(index) != 0;
     }
     // Extract a single element. Use store function if extracting more than one element.
     // Operator [] can only read an element, not write.
-    bool operator [] (uint32_t index) const {
+    bool operator [] (int index) const {
         return extract(index);
     }
+    // Member function to change a bitfield to a boolean vector
+    Vec8ib & load_bits(uint8_t a) {
+        y0 = Vec4ib().load_bits(uint16_t(a));
+        y1 = Vec4ib().load_bits(uint16_t(a>>4));
+        return *this;
+    }
+    static constexpr int elementtype() {
+        return 3;
+    }
+    // Prevent constructing from int, etc.
+    Vec8ib(int b) = delete;
+    Vec8ib & operator = (int x) = delete;
 };
 
 /*****************************************************************************
@@ -1911,57 +1882,66 @@ public:
 *****************************************************************************/
 
 // vector operator & : bitwise and
-static inline Vec8ib operator & (Vec8ib const & a, Vec8ib const & b) {
+static inline Vec8ib operator & (Vec8ib const a, Vec8ib const b) {
     return Vec8ib(Vec256b(a) & Vec256b(b));
 }
-static inline Vec8ib operator && (Vec8ib const & a, Vec8ib const & b) {
+static inline Vec8ib operator && (Vec8ib const a, Vec8ib const b) {
     return a & b;
 }
 // vector operator &= : bitwise and
-static inline Vec8ib & operator &= (Vec8ib & a, Vec8ib const & b) {
+static inline Vec8ib & operator &= (Vec8ib & a, Vec8ib const b) {
     a = a & b;
     return a;
 }
 
 // vector operator | : bitwise or
-static inline Vec8ib operator | (Vec8ib const & a, Vec8ib const & b) {
+static inline Vec8ib operator | (Vec8ib const a, Vec8ib const b) {
     return Vec8ib(Vec256b(a) | Vec256b(b));
 }
-static inline Vec8ib operator || (Vec8ib const & a, Vec8ib const & b) {
+static inline Vec8ib operator || (Vec8ib const a, Vec8ib const b) {
     return a | b;
 }
 // vector operator |= : bitwise or
-static inline Vec8ib & operator |= (Vec8ib & a, Vec8ib const & b) {
+static inline Vec8ib & operator |= (Vec8ib & a, Vec8ib const b) {
     a = a | b;
     return a;
 }
 
 // vector operator ^ : bitwise xor
-static inline Vec8ib operator ^ (Vec8ib const & a, Vec8ib const & b) {
+static inline Vec8ib operator ^ (Vec8ib const a, Vec8ib const b) {
     return Vec8ib(Vec256b(a) ^ Vec256b(b));
 }
 // vector operator ^= : bitwise xor
-static inline Vec8ib & operator ^= (Vec8ib & a, Vec8ib const & b) {
+static inline Vec8ib & operator ^= (Vec8ib & a, Vec8ib const b) {
     a = a ^ b;
     return a;
 }
 
+// vector operator == : xnor
+static inline Vec8ib operator == (Vec8ib const a, Vec8ib const b) {
+    return Vec8ib(Vec256b(a) ^ Vec256b(~b));
+}
+
+// vector operator != : xor
+static inline Vec8ib operator != (Vec8ib const a, Vec8ib const b) {
+    return Vec8ib(a ^ b);
+}
+
 // vector operator ~ : bitwise not
-static inline Vec8ib operator ~ (Vec8ib const & a) {
+static inline Vec8ib operator ~ (Vec8ib const a) {
     return Vec8ib( ~ Vec256b(a));
 }
 
 // vector operator ! : element not
-static inline Vec8ib operator ! (Vec8ib const & a) {
+static inline Vec8ib operator ! (Vec8ib const a) {
     return ~ a;
 }
 
 // vector function andnot
-static inline Vec8ib andnot (Vec8ib const & a, Vec8ib const & b) {
+static inline Vec8ib andnot (Vec8ib const a, Vec8ib const b) {
     return Vec8ib(andnot(Vec256b(a), Vec256b(b)));
 }
 
-
 /*****************************************************************************
 *
 *          Operators for Vec8i
@@ -1969,12 +1949,12 @@ static inline Vec8ib andnot (Vec8ib const & a, Vec8ib const & b) {
 *****************************************************************************/
 
 // vector operator + : add element by element
-static inline Vec8i operator + (Vec8i const & a, Vec8i const & b) {
+static inline Vec8i operator + (Vec8i const a, Vec8i const b) {
     return Vec8i(a.get_low() + b.get_low(), a.get_high() + b.get_high());
 }
 
 // vector operator += : add
-static inline Vec8i & operator += (Vec8i & a, Vec8i const & b) {
+static inline Vec8i & operator += (Vec8i & a, Vec8i const b) {
     a = a + b;
     return a;
 }
@@ -1993,17 +1973,17 @@ static inline Vec8i & operator ++ (Vec8i & a) {
 }
 
 // vector operator - : subtract element by element
-static inline Vec8i operator - (Vec8i const & a, Vec8i const & b) {
+static inline Vec8i operator - (Vec8i const a, Vec8i const b) {
     return Vec8i(a.get_low() - b.get_low(), a.get_high() - b.get_high());
 }
 
 // vector operator - : unary minus
-static inline Vec8i operator - (Vec8i const & a) {
+static inline Vec8i operator - (Vec8i const a) {
     return Vec8i(-a.get_low(), -a.get_high());
 }
 
 // vector operator -= : subtract
-static inline Vec8i & operator -= (Vec8i & a, Vec8i const & b) {
+static inline Vec8i & operator -= (Vec8i & a, Vec8i const b) {
     a = a - b;
     return a;
 }
@@ -2022,29 +2002,29 @@ static inline Vec8i & operator -- (Vec8i & a) {
 }
 
 // vector operator * : multiply element by element
-static inline Vec8i operator * (Vec8i const & a, Vec8i const & b) {
+static inline Vec8i operator * (Vec8i const a, Vec8i const b) {
     return Vec8i(a.get_low() * b.get_low(), a.get_high() * b.get_high());
 }
 
 // vector operator *= : multiply
-static inline Vec8i & operator *= (Vec8i & a, Vec8i const & b) {
+static inline Vec8i & operator *= (Vec8i & a, Vec8i const b) {
     a = a * b;
     return a;
 }
 
 // vector operator / : divide all elements by same integer
-static inline Vec8i operator / (Vec8i const & a, Divisor_i const & d) {
+static inline Vec8i operator / (Vec8i const a, Divisor_i const d) {
     return Vec8i(a.get_low() / d, a.get_high() / d);
 }
 
 // vector operator /= : divide
-static inline Vec8i & operator /= (Vec8i & a, Divisor_i const & d) {
+static inline Vec8i & operator /= (Vec8i & a, Divisor_i const d) {
     a = a / d;
     return a;
 }
 
 // vector operator << : shift left
-static inline Vec8i operator << (Vec8i const & a, int32_t b) {
+static inline Vec8i operator << (Vec8i const a, int32_t b) {
     return Vec8i(a.get_low() << b, a.get_high() << b);
 }
 
@@ -2055,7 +2035,7 @@ static inline Vec8i & operator <<= (Vec8i & a, int32_t b) {
 }
 
 // vector operator >> : shift right arithmetic
-static inline Vec8i operator >> (Vec8i const & a, int32_t b) {
+static inline Vec8i operator >> (Vec8i const a, int32_t b) {
     return Vec8i(a.get_low() >> b, a.get_high() >> b);
 }
 
@@ -2066,78 +2046,78 @@ static inline Vec8i & operator >>= (Vec8i & a, int32_t b) {
 }
 
 // vector operator == : returns true for elements for which a == b
-static inline Vec8ib operator == (Vec8i const & a, Vec8i const & b) {
+static inline Vec8ib operator == (Vec8i const a, Vec8i const b) {
     return Vec8i(a.get_low() == b.get_low(), a.get_high() == b.get_high());
 }
 
 // vector operator != : returns true for elements for which a != b
-static inline Vec8ib operator != (Vec8i const & a, Vec8i const & b) {
+static inline Vec8ib operator != (Vec8i const a, Vec8i const b) {
     return Vec8i(a.get_low() != b.get_low(), a.get_high() != b.get_high());
 }
-  
+
 // vector operator > : returns true for elements for which a > b
-static inline Vec8ib operator > (Vec8i const & a, Vec8i const & b) {
+static inline Vec8ib operator > (Vec8i const a, Vec8i const b) {
     return Vec8i(a.get_low() > b.get_low(), a.get_high() > b.get_high());
 }
 
 // vector operator < : returns true for elements for which a < b
-static inline Vec8ib operator < (Vec8i const & a, Vec8i const & b) {
+static inline Vec8ib operator < (Vec8i const a, Vec8i const b) {
     return b > a;
 }
 
 // vector operator >= : returns true for elements for which a >= b (signed)
-static inline Vec8ib operator >= (Vec8i const & a, Vec8i const & b) {
+static inline Vec8ib operator >= (Vec8i const a, Vec8i const b) {
     return Vec8i(a.get_low() >= b.get_low(), a.get_high() >= b.get_high());
 }
 
 // vector operator <= : returns true for elements for which a <= b (signed)
-static inline Vec8ib operator <= (Vec8i const & a, Vec8i const & b) {
+static inline Vec8ib operator <= (Vec8i const a, Vec8i const b) {
     return b >= a;
 }
 
 // vector operator & : bitwise and
-static inline Vec8i operator & (Vec8i const & a, Vec8i const & b) {
+static inline Vec8i operator & (Vec8i const a, Vec8i const b) {
     return Vec8i(a.get_low() & b.get_low(), a.get_high() & b.get_high());
 }
-static inline Vec8i operator && (Vec8i const & a, Vec8i const & b) {
+static inline Vec8i operator && (Vec8i const a, Vec8i const b) {
     return a & b;
 }
 // vector operator &= : bitwise and
-static inline Vec8i & operator &= (Vec8i & a, Vec8i const & b) {
+static inline Vec8i & operator &= (Vec8i & a, Vec8i const b) {
     a = a & b;
     return a;
 }
 
 // vector operator | : bitwise or
-static inline Vec8i operator | (Vec8i const & a, Vec8i const & b) {
+static inline Vec8i operator | (Vec8i const a, Vec8i const b) {
     return Vec8i(a.get_low() | b.get_low(), a.get_high() | b.get_high());
 }
-static inline Vec8i operator || (Vec8i const & a, Vec8i const & b) {
+static inline Vec8i operator || (Vec8i const a, Vec8i const b) {
     return a | b;
 }
 // vector operator |= : bitwise or
-static inline Vec8i & operator |= (Vec8i & a, Vec8i const & b) {
+static inline Vec8i & operator |= (Vec8i & a, Vec8i const b) {
     a = a | b;
     return a;
 }
 
 // vector operator ^ : bitwise xor
-static inline Vec8i operator ^ (Vec8i const & a, Vec8i const & b) {
+static inline Vec8i operator ^ (Vec8i const a, Vec8i const b) {
     return Vec8i(a.get_low() ^ b.get_low(), a.get_high() ^ b.get_high());
 }
 // vector operator ^= : bitwise xor
-static inline Vec8i & operator ^= (Vec8i & a, Vec8i const & b) {
+static inline Vec8i & operator ^= (Vec8i & a, Vec8i const b) {
     a = a ^ b;
     return a;
 }
 
 // vector operator ~ : bitwise not
-static inline Vec8i operator ~ (Vec8i const & a) {
+static inline Vec8i operator ~ (Vec8i const a) {
     return Vec8i(~a.get_low(), ~a.get_high());
 }
 
 // vector operator ! : returns true for elements == 0
-static inline Vec8ib operator ! (Vec8i const & a) {
+static inline Vec8ib operator ! (Vec8i const a) {
     return Vec8i(!a.get_low(), !a.get_high());
 }
 
@@ -2147,60 +2127,69 @@ static inline Vec8ib operator ! (Vec8i const & a) {
 // for (int i = 0; i < 8; i++) result[i] = s[i] ? a[i] : b[i];
 // Each byte in s must be either 0 (false) or -1 (true). No other values are allowed.
 // (s is signed)
-static inline Vec8i select (Vec8ib const & s, Vec8i const & a, Vec8i const & b) {
+static inline Vec8i select (Vec8ib const s, Vec8i const a, Vec8i const b) {
     return selectb(s,a,b);
 }
 
 // Conditional add: For all vector elements i: result[i] = f[i] ? (a[i] + b[i]) : a[i]
-static inline Vec8i if_add (Vec8ib const & f, Vec8i const & a, Vec8i const & b) {
+static inline Vec8i if_add (Vec8ib const f, Vec8i const a, Vec8i const b) {
     return a + (Vec8i(f) & b);
 }
 
-// Horizontal add: Calculates the sum of all vector elements.
-// Overflow will wrap around
-static inline int32_t horizontal_add (Vec8i const & a) {
+// Conditional subtract
+static inline Vec8i if_sub (Vec8ib const f, Vec8i const a, Vec8i const b) {
+    return a - (Vec8i(f) & b);
+}
+
+// Conditional multiply
+static inline Vec8i if_mul (Vec8ib const f, Vec8i const a, Vec8i const b) {
+    return select(f, a*b, a);
+}
+
+// Horizontal add: Calculates the sum of all vector elements. Overflow will wrap around
+static inline int32_t horizontal_add (Vec8i const a) {
     return horizontal_add(a.get_low() + a.get_high());
 }
 
 // Horizontal add extended: Calculates the sum of all vector elements.
 // Elements are sign extended before adding to avoid overflow
-static inline int64_t horizontal_add_x (Vec8i const & a) {
+static inline int64_t horizontal_add_x (Vec8i const a) {
     return horizontal_add_x(a.get_low()) + horizontal_add_x(a.get_high());
 }
 
 // function add_saturated: add element by element, signed with saturation
-static inline Vec8i add_saturated(Vec8i const & a, Vec8i const & b) {
+static inline Vec8i add_saturated(Vec8i const a, Vec8i const b) {
     return Vec8i(add_saturated(a.get_low(),b.get_low()), add_saturated(a.get_high(),b.get_high()));
 }
 
 // function sub_saturated: subtract element by element, signed with saturation
-static inline Vec8i sub_saturated(Vec8i const & a, Vec8i const & b) {
+static inline Vec8i sub_saturated(Vec8i const a, Vec8i const b) {
     return Vec8i(sub_saturated(a.get_low(),b.get_low()), sub_saturated(a.get_high(),b.get_high()));
 }
 
 // function max: a > b ? a : b
-static inline Vec8i max(Vec8i const & a, Vec8i const & b) {
+static inline Vec8i max(Vec8i const a, Vec8i const b) {
     return Vec8i(max(a.get_low(),b.get_low()), max(a.get_high(),b.get_high()));
 }
 
 // function min: a < b ? a : b
-static inline Vec8i min(Vec8i const & a, Vec8i const & b) {
+static inline Vec8i min(Vec8i const a, Vec8i const b) {
     return Vec8i(min(a.get_low(),b.get_low()), min(a.get_high(),b.get_high()));
 }
 
 // function abs: a >= 0 ? a : -a
-static inline Vec8i abs(Vec8i const & a) {
+static inline Vec8i abs(Vec8i const a) {
     return Vec8i(abs(a.get_low()), abs(a.get_high()));
 }
 
 // function abs_saturated: same as abs, saturate if overflow
-static inline Vec8i abs_saturated(Vec8i const & a) {
+static inline Vec8i abs_saturated(Vec8i const a) {
     return Vec8i(abs_saturated(a.get_low()), abs_saturated(a.get_high()));
 }
 
 // function rotate_left all elements
 // Use negative count to rotate right
-static inline Vec8i rotate_left(Vec8i const & a, int b) {
+static inline Vec8i rotate_left(Vec8i const a, int b) {
     return Vec8i(rotate_left(a.get_low(),b), rotate_left(a.get_high(),b));
 }
 
@@ -2214,27 +2203,26 @@ static inline Vec8i rotate_left(Vec8i const & a, int b) {
 class Vec8ui : public Vec8i {
 public:
     // Default constructor:
-    Vec8ui() {
-    }
+    Vec8ui() = default;
     // Constructor to broadcast the same value into all elements:
     Vec8ui(uint32_t i) {
-        y1 = y0 = _mm_set1_epi32(i);
+        y1 = y0 = _mm_set1_epi32(int32_t(i));
     }
     // Constructor to build from all elements:
     Vec8ui(uint32_t i0, uint32_t i1, uint32_t i2, uint32_t i3, uint32_t i4, uint32_t i5, uint32_t i6, uint32_t i7) {
-        y0 = _mm_setr_epi32(i0, i1, i2, i3);
-        y1 = _mm_setr_epi32(i4, i5, i6, i7);
+        y0 = _mm_setr_epi32((int32_t)i0, (int32_t)i1, (int32_t)i2, (int32_t)i3);
+        y1 = _mm_setr_epi32((int32_t)i4, (int32_t)i5, (int32_t)i6, (int32_t)i7);
     }
     // Constructor to build from two Vec4ui:
-    Vec8ui(Vec4ui const & a0, Vec4ui const & a1) {
+    Vec8ui(Vec4ui const a0, Vec4ui const a1) {
         y0 = a0;  y1 = a1;
     }
-    // Constructor to convert from type Vec256ie
-    Vec8ui(Vec256ie const & x) {
+    // Constructor to convert from type Vec256b
+    Vec8ui(Vec256b const x) {
         y0 = x.get_low();  y1 = x.get_high();
     }
-    // Assignment operator to convert from type Vec256ie
-    Vec8ui & operator = (Vec256ie const & x) {
+    // Assignment operator to convert from type Vec256b
+    Vec8ui & operator = (Vec256b const x) {
         y0 = x.get_low();  y1 = x.get_high();
         return *this;
     }
@@ -2251,18 +2239,17 @@ public:
         return *this;
     }
     // Member function to change a single element in vector
-    // Note: This function is inefficient. Use load function if changing more than one element
-    Vec8ui const & insert(uint32_t index, uint32_t value) {
-        Vec8i::insert(index, value);
+    Vec8ui const insert(int index, uint32_t value) {
+        Vec8i::insert(index, (int32_t)value);
         return *this;
     }
     // Member function extract a single element from vector
-    uint32_t extract(uint32_t index) const {
-        return Vec8i::extract(index);
+    uint32_t extract(int index) const {
+        return (uint32_t)Vec8i::extract(index);
     }
     // Extract a single element. Use store function if extracting more than one element.
     // Operator [] can only read an element, not write.
-    uint32_t operator [] (uint32_t index) const {
+    uint32_t operator [] (int index) const {
         return extract(index);
     }
     // Member functions to split into two Vec4ui:
@@ -2272,43 +2259,46 @@ public:
     Vec4ui get_high() const {
         return y1;
     }
+    static constexpr int elementtype() {
+        return 9;
+    }
 };
 
 // Define operators for this class
 
 // vector operator + : add
-static inline Vec8ui operator + (Vec8ui const & a, Vec8ui const & b) {
+static inline Vec8ui operator + (Vec8ui const a, Vec8ui const b) {
     return Vec8ui (Vec8i(a) + Vec8i(b));
 }
 
 // vector operator - : subtract
-static inline Vec8ui operator - (Vec8ui const & a, Vec8ui const & b) {
+static inline Vec8ui operator - (Vec8ui const a, Vec8ui const b) {
     return Vec8ui (Vec8i(a) - Vec8i(b));
 }
 
 // vector operator * : multiply
-static inline Vec8ui operator * (Vec8ui const & a, Vec8ui const & b) {
+static inline Vec8ui operator * (Vec8ui const a, Vec8ui const b) {
     return Vec8ui (Vec8i(a) * Vec8i(b));
 }
 
 // vector operator / : divide all elements by same integer
-static inline Vec8ui operator / (Vec8ui const & a, Divisor_ui const & d) {
+static inline Vec8ui operator / (Vec8ui const a, Divisor_ui const d) {
     return Vec8ui(a.get_low() / d, a.get_high() / d);
 }
 
 // vector operator /= : divide
-static inline Vec8ui & operator /= (Vec8ui & a, Divisor_ui const & d) {
+static inline Vec8ui & operator /= (Vec8ui & a, Divisor_ui const d) {
     a = a / d;
     return a;
 }
 
 // vector operator >> : shift right logical all elements
-static inline Vec8ui operator >> (Vec8ui const & a, uint32_t b) {
+static inline Vec8ui operator >> (Vec8ui const a, uint32_t b) {
     return Vec8ui(a.get_low() >> b, a.get_high() >> b);
 }
 
 // vector operator >> : shift right logical all elements
-static inline Vec8ui operator >> (Vec8ui const & a, int32_t b) {
+static inline Vec8ui operator >> (Vec8ui const a, int32_t b) {
     return a >> (uint32_t)b;
 }
 
@@ -2316,67 +2306,67 @@ static inline Vec8ui operator >> (Vec8ui const & a, int32_t b) {
 static inline Vec8ui & operator >>= (Vec8ui & a, uint32_t b) {
     a = a >> b;
     return a;
-} 
+}
 
 // vector operator >>= : shift right logical
 static inline Vec8ui & operator >>= (Vec8ui & a, int32_t b) {
     a = a >> b;
     return a;
-} 
+}
 
 // vector operator << : shift left all elements
-static inline Vec8ui operator << (Vec8ui const & a, uint32_t b) {
+static inline Vec8ui operator << (Vec8ui const a, uint32_t b) {
     return Vec8ui ((Vec8i)a << (int32_t)b);
 }
 
 // vector operator << : shift left all elements
-static inline Vec8ui operator << (Vec8ui const & a, int32_t b) {
+static inline Vec8ui operator << (Vec8ui const a, int32_t b) {
     return Vec8ui ((Vec8i)a << (int32_t)b);
 }
 
 // vector operator > : returns true for elements for which a > b (unsigned)
-static inline Vec8ib operator > (Vec8ui const & a, Vec8ui const & b) {
+static inline Vec8ib operator > (Vec8ui const a, Vec8ui const b) {
     return Vec8i(a.get_low() > b.get_low(), a.get_high() > b.get_high());
 }
 
 // vector operator < : returns true for elements for which a < b (unsigned)
-static inline Vec8ib operator < (Vec8ui const & a, Vec8ui const & b) {
+static inline Vec8ib operator < (Vec8ui const a, Vec8ui const b) {
     return b > a;
 }
 
 // vector operator >= : returns true for elements for which a >= b (unsigned)
-static inline Vec8ib operator >= (Vec8ui const & a, Vec8ui const & b) {
+static inline Vec8ib operator >= (Vec8ui const a, Vec8ui const b) {
     return Vec8i(a.get_low() >= b.get_low(), a.get_high() >= b.get_high());
 }
 
 // vector operator <= : returns true for elements for which a <= b (unsigned)
-static inline Vec8ib operator <= (Vec8ui const & a, Vec8ui const & b) {
+static inline Vec8ib operator <= (Vec8ui const a, Vec8ui const b) {
     return b >= a;
 }
 
 // vector operator & : bitwise and
-static inline Vec8ui operator & (Vec8ui const & a, Vec8ui const & b) {
+static inline Vec8ui operator & (Vec8ui const a, Vec8ui const b) {
     return Vec8ui(a.get_low() & b.get_low(), a.get_high() & b.get_high());
 }
-static inline Vec8ui operator && (Vec8ui const & a, Vec8ui const & b) {
+static inline Vec8ui operator && (Vec8ui const a, Vec8ui const b) {
     return a & b;
 }
 
 // vector operator | : bitwise or
-static inline Vec8ui operator | (Vec8ui const & a, Vec8ui const & b) {
+static inline Vec8ui operator | (Vec8ui const a, Vec8ui const b) {
     return Vec8ui(a.get_low() | b.get_low(), a.get_high() | b.get_high());
 }
-static inline Vec8ui operator || (Vec8ui const & a, Vec8ui const & b) {
+static inline Vec8ui operator || (Vec8ui const a, Vec8ui const b) {
     return a | b;
 }
 
 // vector operator ^ : bitwise xor
-static inline Vec8ui operator ^ (Vec8ui const & a, Vec8ui const & b) {
+static inline Vec8ui operator ^ (Vec8ui const a, Vec8ui const b) {
     return Vec8ui(a.get_low() ^ b.get_low(), a.get_high() ^ b.get_high());
 }
 
 // vector operator ~ : bitwise not
-static inline Vec8ui operator ~ (Vec8ui const & a) {
+static inline Vec8ui operator ~ (Vec8ui const a) {
     return Vec8ui(~a.get_low(), ~a.get_high());
 }
 
@@ -2386,49 +2376,57 @@ static inline Vec8ui operator ~ (Vec8ui const & a) {
 // for (int i = 0; i < 16; i++) result[i] = s[i] ? a[i] : b[i];
 // Each word in s must be either 0 (false) or -1 (true). No other values are allowed.
 // (s is signed)
-static inline Vec8ui select (Vec8ib const & s, Vec8ui const & a, Vec8ui const & b) {
+static inline Vec8ui select (Vec8ib const s, Vec8ui const a, Vec8ui const b) {
     return selectb(s,a,b);
 }
 
 // Conditional add: For all vector elements i: result[i] = f[i] ? (a[i] + b[i]) : a[i]
-static inline Vec8ui if_add (Vec8ib const & f, Vec8ui const & a, Vec8ui const & b) {
+static inline Vec8ui if_add (Vec8ib const f, Vec8ui const a, Vec8ui const b) {
     return a + (Vec8ui(f) & b);
 }
 
-// Horizontal add: Calculates the sum of all vector elements.
-// Overflow will wrap around
-static inline uint32_t horizontal_add (Vec8ui const & a) {
-    return horizontal_add((Vec8i)a);
+// Conditional subtract
+static inline Vec8ui if_sub (Vec8ib const f, Vec8ui const a, Vec8ui const b) {
+    return a - (Vec8ui(f) & b);
+}
+
+// Conditional multiply
+static inline Vec8ui if_mul (Vec8ib const f, Vec8ui const a, Vec8ui const b) {
+    return select(f, a*b, a);
+}
+
+// Horizontal add: Calculates the sum of all vector elements. Overflow will wrap around
+static inline uint32_t horizontal_add (Vec8ui const a) {
+    return (uint32_t)horizontal_add((Vec8i)a);
 }
 
 // Horizontal add extended: Calculates the sum of all vector elements.
 // Elements are zero extended before adding to avoid overflow
-static inline uint64_t horizontal_add_x (Vec8ui const & a) {
+static inline uint64_t horizontal_add_x (Vec8ui const a) {
     return horizontal_add_x(a.get_low()) + horizontal_add_x(a.get_high());
 }
 
 // function add_saturated: add element by element, unsigned with saturation
-static inline Vec8ui add_saturated(Vec8ui const & a, Vec8ui const & b) {
+static inline Vec8ui add_saturated(Vec8ui const a, Vec8ui const b) {
     return Vec8ui(add_saturated(a.get_low(),b.get_low()), add_saturated(a.get_high(),b.get_high()));
 }
 
 // function sub_saturated: subtract element by element, unsigned with saturation
-static inline Vec8ui sub_saturated(Vec8ui const & a, Vec8ui const & b) {
+static inline Vec8ui sub_saturated(Vec8ui const a, Vec8ui const b) {
     return Vec8ui(sub_saturated(a.get_low(),b.get_low()), sub_saturated(a.get_high(),b.get_high()));
 }
 
 // function max: a > b ? a : b
-static inline Vec8ui max(Vec8ui const & a, Vec8ui const & b) {
+static inline Vec8ui max(Vec8ui const a, Vec8ui const b) {
     return Vec8ui(max(a.get_low(),b.get_low()), max(a.get_high(),b.get_high()));
 }
 
 // function min: a < b ? a : b
-static inline Vec8ui min(Vec8ui const & a, Vec8ui const & b) {
+static inline Vec8ui min(Vec8ui const a, Vec8ui const b) {
     return Vec8ui(min(a.get_low(),b.get_low()), min(a.get_high(),b.get_high()));
 }
 
 
-
 /*****************************************************************************
 *
 *          Vector of 4 64-bit signed integers
@@ -2438,8 +2436,7 @@ static inline Vec8ui min(Vec8ui const & a, Vec8ui const & b) {
 class Vec4q : public Vec256b {
 public:
     // Default constructor:
-    Vec4q() {
-    }
+    Vec4q() = default;
     // Constructor to broadcast the same value into all elements:
     Vec4q(int64_t i) {
         y0 = y1 = Vec2q(i);
@@ -2450,15 +2447,15 @@ public:
         y1 = Vec2q(i2,i3);
     }
     // Constructor to build from two Vec2q:
-    Vec4q(Vec2q const & a0, Vec2q const & a1) {
+    Vec4q(Vec2q const a0, Vec2q const a1) {
         y0 = a0;  y1 = a1;
     }
-    // Constructor to convert from type Vec256ie
-    Vec4q(Vec256ie const & x) {
+    // Constructor to convert from type Vec256b
+    Vec4q(Vec256b const & x) {
         y0 = x.get_low();  y1 = x.get_high();
     }
-    // Assignment operator to convert from type Vec256ie
-    Vec4q & operator = (Vec256ie const & x) {
+    // Assignment operator to convert from type Vec256b
+    Vec4q & operator = (Vec256b const x) {
         y0 = x.get_low();  y1 = x.get_high();
         return *this;
     }
@@ -2512,9 +2509,8 @@ public:
         return *this;
     }
     // Member function to change a single element in vector
-    // Note: This function is inefficient. Use load function if changing more than one element
-    Vec4q const & insert(uint32_t index, int64_t value) {
-        if (index < 2) {
+    Vec4q const insert(int index, int64_t value) {
+        if ((uint32_t)index < 2) {
             y0 = Vec2q(y0).insert(index, value);
         }
         else {
@@ -2523,9 +2519,8 @@ public:
         return *this;
     }
     // Member function extract a single element from vector
-    // Note: This function is inefficient. Use store function if extracting more than one element
-    int64_t extract(uint32_t index) const {
-        if (index < 2) {
+    int64_t extract(int index) const {
+        if ((uint32_t)index < 2) {
             return Vec2q(y0).extract(index);
         }
         else {
@@ -2534,7 +2529,7 @@ public:
     }
     // Extract a single element. Use store function if extracting more than one element.
     // Operator [] can only read an element, not write.
-    int64_t operator [] (uint32_t index) const {
+    int64_t operator [] (int index) const {
         return extract(index);
     }
     // Member functions to split into two Vec2q:
@@ -2544,9 +2539,12 @@ public:
     Vec2q get_high() const {
         return y1;
     }
-    static int size () {
+    static constexpr int size() {
         return 4;
     }
+    static constexpr int elementtype() {
+        return 10;
+    }
 };
 
 
@@ -2559,18 +2557,17 @@ public:
 class Vec4qb : public Vec4q {
 public:
     // Default constructor:
-    Vec4qb() {
-    }
+    Vec4qb() = default;
     // Constructor to build from all elements:
     Vec4qb(bool x0, bool x1, bool x2, bool x3) :
         Vec4q(-int64_t(x0), -int64_t(x1), -int64_t(x2), -int64_t(x3)) {
     }
-    // Constructor to convert from type Vec256ie
-    Vec4qb(Vec256ie const & x) {
+    // Constructor to convert from type Vec256b
+    Vec4qb(Vec256b const x) {
         y0 = x.get_low();  y1 = x.get_high();
     }
-    // Assignment operator to convert from type Vec256ie
-    Vec4qb & operator = (Vec256ie const & x) {
+    // Assignment operator to convert from type Vec256b
+    Vec4qb & operator = (Vec256b const x) {
         y0 = x.get_low();  y1 = x.get_high();
         return *this;
     }
@@ -2582,10 +2579,9 @@ public:
         *this = Vec4qb(b);
         return *this;
     }
-private: // Prevent constructing from int, etc.
-    Vec4qb(int b);
-    Vec4qb & operator = (int x);
-public:
+    // Constructor to build from two Vec2qb:
+    Vec4qb(Vec2qb const a0, Vec2qb const a1) : Vec4q(Vec2q(a0), Vec2q(a1)) {
+    }
     // Member functions to split into two Vec2qb:
     Vec2qb get_low() const {
         return y0;
@@ -2596,17 +2592,28 @@ public:
     Vec4qb & insert (int index, bool a) {
         Vec4q::insert(index, -(int64_t)a);
         return *this;
-    }    
+    }
     // Member function extract a single element from vector
-    // Note: This function is inefficient. Use store function if extracting more than one element
-    bool extract(uint32_t index) const {
+    bool extract(int index) const {
         return Vec4q::extract(index) != 0;
     }
     // Extract a single element. Use store function if extracting more than one element.
     // Operator [] can only read an element, not write.
-    bool operator [] (uint32_t index) const {
+    bool operator [] (int index) const {
         return extract(index);
     }
+    // Member function to change a bitfield to a boolean vector
+    Vec4qb & load_bits(uint8_t a) {
+        y0 = Vec2qb().load_bits(a);
+        y1 = Vec2qb().load_bits(uint8_t(a>>2u));
+        return *this;
+    }
+    static constexpr int elementtype() {
+        return 3;
+    }
+    // Prevent constructing from int, etc.
+    Vec4qb(int b) = delete;
+    Vec4qb & operator = (int x) = delete;
 };
 
 
@@ -2617,53 +2624,63 @@ public:
 *****************************************************************************/
 
 // vector operator & : bitwise and
-static inline Vec4qb operator & (Vec4qb const & a, Vec4qb const & b) {
+static inline Vec4qb operator & (Vec4qb const a, Vec4qb const b) {
     return Vec4qb(Vec256b(a) & Vec256b(b));
 }
-static inline Vec4qb operator && (Vec4qb const & a, Vec4qb const & b) {
+static inline Vec4qb operator && (Vec4qb const a, Vec4qb const b) {
     return a & b;
 }
 // vector operator &= : bitwise and
-static inline Vec4qb & operator &= (Vec4qb & a, Vec4qb const & b) {
+static inline Vec4qb & operator &= (Vec4qb & a, Vec4qb const b) {
     a = a & b;
     return a;
 }
 
 // vector operator | : bitwise or
-static inline Vec4qb operator | (Vec4qb const & a, Vec4qb const & b) {
+static inline Vec4qb operator | (Vec4qb const a, Vec4qb const b) {
     return Vec4qb(Vec256b(a) | Vec256b(b));
 }
-static inline Vec4qb operator || (Vec4qb const & a, Vec4qb const & b) {
+static inline Vec4qb operator || (Vec4qb const a, Vec4qb const b) {
     return a | b;
 }
 // vector operator |= : bitwise or
-static inline Vec4qb & operator |= (Vec4qb & a, Vec4qb const & b) {
+static inline Vec4qb & operator |= (Vec4qb & a, Vec4qb const b) {
     a = a | b;
     return a;
 }
 
 // vector operator ^ : bitwise xor
-static inline Vec4qb operator ^ (Vec4qb const & a, Vec4qb const & b) {
+static inline Vec4qb operator ^ (Vec4qb const a, Vec4qb const b) {
     return Vec4qb(Vec256b(a) ^ Vec256b(b));
 }
 // vector operator ^= : bitwise xor
-static inline Vec4qb & operator ^= (Vec4qb & a, Vec4qb const & b) {
+static inline Vec4qb & operator ^= (Vec4qb & a, Vec4qb const b) {
     a = a ^ b;
     return a;
 }
 
+// vector operator == : xnor
+static inline Vec4qb operator == (Vec4qb const a, Vec4qb const b) {
+    return Vec4qb(Vec256b(a) ^ Vec256b(~b));
+}
+
+// vector operator != : xor
+static inline Vec4qb operator != (Vec4qb const a, Vec4qb const b) {
+    return Vec4qb(a ^ b);
+}
+
 // vector operator ~ : bitwise not
-static inline Vec4qb operator ~ (Vec4qb const & a) {
+static inline Vec4qb operator ~ (Vec4qb const a) {
     return Vec4qb( ~ Vec256b(a));
 }
 
 // vector operator ! : element not
-static inline Vec4qb operator ! (Vec4qb const & a) {
+static inline Vec4qb operator ! (Vec4qb const a) {
     return ~ a;
 }
 
 // vector function andnot
-static inline Vec4qb andnot (Vec4qb const & a, Vec4qb const & b) {
+static inline Vec4qb andnot (Vec4qb const a, Vec4qb const b) {
     return Vec4qb(andnot(Vec256b(a), Vec256b(b)));
 }
 
@@ -2675,12 +2692,12 @@ static inline Vec4qb andnot (Vec4qb const & a, Vec4qb const & b) {
 *****************************************************************************/
 
 // vector operator + : add element by element
-static inline Vec4q operator + (Vec4q const & a, Vec4q const & b) {
+static inline Vec4q operator + (Vec4q const a, Vec4q const b) {
     return Vec4q(a.get_low() + b.get_low(), a.get_high() + b.get_high());
 }
 
 // vector operator += : add
-static inline Vec4q & operator += (Vec4q & a, Vec4q const & b) {
+static inline Vec4q & operator += (Vec4q & a, Vec4q const b) {
     a = a + b;
     return a;
 }
@@ -2699,17 +2716,17 @@ static inline Vec4q & operator ++ (Vec4q & a) {
 }
 
 // vector operator - : subtract element by element
-static inline Vec4q operator - (Vec4q const & a, Vec4q const & b) {
+static inline Vec4q operator - (Vec4q const a, Vec4q const b) {
     return Vec4q(a.get_low() - b.get_low(), a.get_high() - b.get_high());
 }
 
 // vector operator - : unary minus
-static inline Vec4q operator - (Vec4q const & a) {
+static inline Vec4q operator - (Vec4q const a) {
     return Vec4q(-a.get_low(), -a.get_high());
 }
 
 // vector operator -= : subtract
-static inline Vec4q & operator -= (Vec4q & a, Vec4q const & b) {
+static inline Vec4q & operator -= (Vec4q & a, Vec4q const b) {
     a = a - b;
     return a;
 }
@@ -2728,18 +2745,18 @@ static inline Vec4q & operator -- (Vec4q & a) {
 }
 
 // vector operator * : multiply element by element
-static inline Vec4q operator * (Vec4q const & a, Vec4q const & b) {
+static inline Vec4q operator * (Vec4q const a, Vec4q const b) {
     return Vec4q(a.get_low() * b.get_low(), a.get_high() * b.get_high());
 }
 
 // vector operator *= : multiply
-static inline Vec4q & operator *= (Vec4q & a, Vec4q const & b) {
+static inline Vec4q & operator *= (Vec4q & a, Vec4q const b) {
     a = a * b;
     return a;
 }
 
 // vector operator << : shift left
-static inline Vec4q operator << (Vec4q const & a, int32_t b) {
+static inline Vec4q operator << (Vec4q const a, int32_t b) {
     return Vec4q(a.get_low() << b, a.get_high() << b);
 }
 
@@ -2750,7 +2767,7 @@ static inline Vec4q & operator <<= (Vec4q & a, int32_t b) {
 }
 
 // vector operator >> : shift right arithmetic
-static inline Vec4q operator >> (Vec4q const & a, int32_t b) {
+static inline Vec4q operator >> (Vec4q const a, int32_t b) {
     return Vec4q(a.get_low() >> b, a.get_high() >> b);
 }
 
@@ -2761,79 +2778,78 @@ static inline Vec4q & operator >>= (Vec4q & a, int32_t b) {
 }
 
 // vector operator == : returns true for elements for which a == b
-static inline Vec4qb operator == (Vec4q const & a, Vec4q const & b) {
+static inline Vec4qb operator == (Vec4q const a, Vec4q const b) {
     return Vec4q(a.get_low() == b.get_low(), a.get_high() == b.get_high());
 }
 
 // vector operator != : returns true for elements for which a != b
-static inline Vec4qb operator != (Vec4q const & a, Vec4q const & b) {
+static inline Vec4qb operator != (Vec4q const a, Vec4q const b) {
     return Vec4q(a.get_low() != b.get_low(), a.get_high() != b.get_high());
 }
-  
+
 // vector operator < : returns true for elements for which a < b
-static inline Vec4qb operator < (Vec4q const & a, Vec4q const & b) {
+static inline Vec4qb operator < (Vec4q const a, Vec4q const b) {
     return Vec4q(a.get_low() < b.get_low(), a.get_high() < b.get_high());
 }
 
 // vector operator > : returns true for elements for which a > b
-static inline Vec4qb operator > (Vec4q const & a, Vec4q const & b) {
+static inline Vec4qb operator > (Vec4q const a, Vec4q const b) {
     return b < a;
 }
 
 // vector operator >= : returns true for elements for which a >= b (signed)
-static inline Vec4qb operator >= (Vec4q const & a, Vec4q const & b) {
+static inline Vec4qb operator >= (Vec4q const a, Vec4q const b) {
     return Vec4q(a.get_low() >= b.get_low(), a.get_high() >= b.get_high());
 }
 
 // vector operator <= : returns true for elements for which a <= b (signed)
-static inline Vec4qb operator <= (Vec4q const & a, Vec4q const & b) {
+static inline Vec4qb operator <= (Vec4q const a, Vec4q const b) {
     return b >= a;
 }
 
 // vector operator & : bitwise and
-static inline Vec4q operator & (Vec4q const & a, Vec4q const & b) {
+static inline Vec4q operator & (Vec4q const a, Vec4q const b) {
     return Vec4q(a.get_low() & b.get_low(), a.get_high() & b.get_high());
 }
-static inline Vec4q operator && (Vec4q const & a, Vec4q const & b) {
+static inline Vec4q operator && (Vec4q const a, Vec4q const b) {
     return a & b;
 }
 // vector operator &= : bitwise and
-static inline Vec4q & operator &= (Vec4q & a, Vec4q const & b) {
+static inline Vec4q & operator &= (Vec4q & a, Vec4q const b) {
     a = a & b;
     return a;
 }
 
 // vector operator | : bitwise or
-static inline Vec4q operator | (Vec4q const & a, Vec4q const & b) {
+static inline Vec4q operator | (Vec4q const a, Vec4q const b) {
     return Vec4q(a.get_low() | b.get_low(), a.get_high() | b.get_high());
 }
-static inline Vec4q operator || (Vec4q const & a, Vec4q const & b) {
+static inline Vec4q operator || (Vec4q const a, Vec4q const b) {
     return a | b;
 }
 // vector operator |= : bitwise or
-static inline Vec4q & operator |= (Vec4q & a, Vec4q const & b) {
+static inline Vec4q & operator |= (Vec4q & a, Vec4q const b) {
     a = a | b;
     return a;
 }
 
 // vector operator ^ : bitwise xor
-static inline Vec4q operator ^ (Vec4q const & a, Vec4q const & b) {
+static inline Vec4q operator ^ (Vec4q const a, Vec4q const b) {
     return Vec4q(a.get_low() ^ b.get_low(), a.get_high() ^ b.get_high());
 }
 // vector operator ^= : bitwise xor
-static inline Vec4q & operator ^= (Vec4q & a, Vec4q const & b) {
+static inline Vec4q & operator ^= (Vec4q & a, Vec4q const b) {
     a = a ^ b;
     return a;
 }
 
-
 // vector operator ~ : bitwise not
-static inline Vec4q operator ~ (Vec4q const & a) {
+static inline Vec4q operator ~ (Vec4q const a) {
     return Vec4q(~a.get_low(), ~a.get_high());
 }
 
 // vector operator ! : logical not, returns true for elements == 0
-static inline Vec4qb operator ! (Vec4q const & a) {
+static inline Vec4qb operator ! (Vec4q const a) {
     return Vec4q(!a.get_low(), !a.get_high());
 }
 
@@ -2843,44 +2859,53 @@ static inline Vec4qb operator ! (Vec4q const & a) {
 // for (int i = 0; i < 4; i++) result[i] = s[i] ? a[i] : b[i];
 // Each byte in s must be either 0 (false) or -1 (true). No other values are allowed.
 // (s is signed)
-static inline Vec4q select (Vec4qb const & s, Vec4q const & a, Vec4q const & b) {
+static inline Vec4q select (Vec4qb const s, Vec4q const a, Vec4q const b) {
     return selectb(s,a,b);
 }
 
 // Conditional add: For all vector elements i: result[i] = f[i] ? (a[i] + b[i]) : a[i]
-static inline Vec4q if_add (Vec4qb const & f, Vec4q const & a, Vec4q const & b) {
+static inline Vec4q if_add (Vec4qb const f, Vec4q const a, Vec4q const b) {
     return a + (Vec4q(f) & b);
 }
 
-// Horizontal add: Calculates the sum of all vector elements.
-// Overflow will wrap around
-static inline int64_t horizontal_add (Vec4q const & a) {
+// Conditional subtract
+static inline Vec4q if_sub (Vec4qb const f, Vec4q const a, Vec4q const b) {
+    return a - (Vec4q(f) & b);
+}
+
+// Conditional multiply
+static inline Vec4q if_mul (Vec4qb const f, Vec4q const a, Vec4q const b) {
+    return select(f, a*b, a);
+}
+
+// Horizontal add: Calculates the sum of all vector elements. Overflow will wrap around
+static inline int64_t horizontal_add (Vec4q const a) {
     return horizontal_add(a.get_low() + a.get_high());
 }
 
 // function max: a > b ? a : b
-static inline Vec4q max(Vec4q const & a, Vec4q const & b) {
+static inline Vec4q max(Vec4q const a, Vec4q const b) {
     return Vec4q(max(a.get_low(),b.get_low()), max(a.get_high(),b.get_high()));
 }
 
 // function min: a < b ? a : b
-static inline Vec4q min(Vec4q const & a, Vec4q const & b) {
+static inline Vec4q min(Vec4q const a, Vec4q const b) {
     return Vec4q(min(a.get_low(),b.get_low()), min(a.get_high(),b.get_high()));
 }
 
 // function abs: a >= 0 ? a : -a
-static inline Vec4q abs(Vec4q const & a) {
+static inline Vec4q abs(Vec4q const a) {
     return Vec4q(abs(a.get_low()), abs(a.get_high()));
 }
 
 // function abs_saturated: same as abs, saturate if overflow
-static inline Vec4q abs_saturated(Vec4q const & a) {
+static inline Vec4q abs_saturated(Vec4q const a) {
     return Vec4q(abs_saturated(a.get_low()), abs_saturated(a.get_high()));
 }
 
 // function rotate_left all elements
 // Use negative count to rotate right
-static inline Vec4q rotate_left(Vec4q const & a, int b) {
+static inline Vec4q rotate_left(Vec4q const a, int b) {
     return Vec4q(rotate_left(a.get_low(),b), rotate_left(a.get_high(),b));
 }
 
@@ -2894,27 +2919,26 @@ static inline Vec4q rotate_left(Vec4q const & a, int b) {
 class Vec4uq : public Vec4q {
 public:
     // Default constructor:
-    Vec4uq() {
-    }
+    Vec4uq() = default;
     // Constructor to broadcast the same value into all elements:
     Vec4uq(uint64_t i) {
-        y1 = y0 = Vec2q(i);
+        y1 = y0 = Vec2q((int64_t)i);
     }
     // Constructor to build from all elements:
     Vec4uq(uint64_t i0, uint64_t i1, uint64_t i2, uint64_t i3) {
-        y0 = Vec2q(i0, i1);
-        y1 = Vec2q(i2, i3);
+        y0 = Vec2q((int64_t)i0, (int64_t)i1);
+        y1 = Vec2q((int64_t)i2, (int64_t)i3);
     }
     // Constructor to build from two Vec2uq:
-    Vec4uq(Vec2uq const & a0, Vec2uq const & a1) {
+    Vec4uq(Vec2uq const a0, Vec2uq const a1) {
         y0 = a0;  y1 = a1;
     }
-    // Constructor to convert from type Vec256ie
-    Vec4uq(Vec256ie const & x) {
+    // Constructor to convert from type Vec256b
+    Vec4uq(Vec256b const x) {
         y0 = x.get_low();  y1 = x.get_high();
     }
-    // Assignment operator to convert from type Vec256ie
-    Vec4uq & operator = (Vec256ie const & x) {
+    // Assignment operator to convert from type Vec256b
+    Vec4uq & operator = (Vec256b const x) {
         y0 = x.get_low();  y1 = x.get_high();
         return *this;
     }
@@ -2931,18 +2955,17 @@ public:
         return *this;
     }
     // Member function to change a single element in vector
-    // Note: This function is inefficient. Use load function if changing more than one element
-    Vec4uq const & insert(uint32_t index, uint64_t value) {
-        Vec4q::insert(index, value);
+    Vec4uq const insert(int index, uint64_t value) {
+        Vec4q::insert(index, (int64_t)value);
         return *this;
     }
     // Member function extract a single element from vector
-    uint64_t extract(uint32_t index) const {
-        return Vec4q::extract(index);
+    uint64_t extract(int index) const {
+        return (uint64_t)Vec4q::extract(index);
     }
     // Extract a single element. Use store function if extracting more than one element.
     // Operator [] can only read an element, not write.
-    uint64_t operator [] (uint32_t index) const {
+    uint64_t operator [] (int index) const {
         return extract(index);
     }
     // Member functions to split into two Vec2uq:
@@ -2952,32 +2975,35 @@ public:
     Vec2uq get_high() const {
         return y1;
     }
+    static constexpr int elementtype() {
+        return 11;
+    }
 };
 
 // Define operators for this class
 
 // vector operator + : add
-static inline Vec4uq operator + (Vec4uq const & a, Vec4uq const & b) {
+static inline Vec4uq operator + (Vec4uq const a, Vec4uq const b) {
     return Vec4uq (Vec4q(a) + Vec4q(b));
 }
 
 // vector operator - : subtract
-static inline Vec4uq operator - (Vec4uq const & a, Vec4uq const & b) {
+static inline Vec4uq operator - (Vec4uq const a, Vec4uq const b) {
     return Vec4uq (Vec4q(a) - Vec4q(b));
 }
 
 // vector operator * : multiply element by element
-static inline Vec4uq operator * (Vec4uq const & a, Vec4uq const & b) {
+static inline Vec4uq operator * (Vec4uq const a, Vec4uq const b) {
     return Vec4uq (Vec4q(a) * Vec4q(b));
 }
 
 // vector operator >> : shift right logical all elements
-static inline Vec4uq operator >> (Vec4uq const & a, uint32_t b) {
+static inline Vec4uq operator >> (Vec4uq const a, uint32_t b) {
     return Vec4uq(a.get_low() >> b, a.get_high() >> b);
 }
 
 // vector operator >> : shift right logical all elements
-static inline Vec4uq operator >> (Vec4uq const & a, int32_t b) {
+static inline Vec4uq operator >> (Vec4uq const a, int32_t b) {
     return a >> (uint32_t)b;
 }
 
@@ -2985,61 +3011,61 @@ static inline Vec4uq operator >> (Vec4uq const & a, int32_t b) {
 static inline Vec4uq & operator >>= (Vec4uq & a, uint32_t b) {
     a = a >> b;
     return a;
-} 
+}
 
 // vector operator << : shift left all elements
-static inline Vec4uq operator << (Vec4uq const & a, uint32_t b) {
+static inline Vec4uq operator << (Vec4uq const a, uint32_t b) {
     return Vec4uq ((Vec4q)a << (int32_t)b);
 }
 
 // vector operator << : shift left all elements
-static inline Vec4uq operator << (Vec4uq const & a, int32_t b) {
+static inline Vec4uq operator << (Vec4uq const a, int32_t b) {
     return Vec4uq ((Vec4q)a << b);
 }
 
 // vector operator > : returns true for elements for which a > b (unsigned)
-static inline Vec4qb operator > (Vec4uq const & a, Vec4uq const & b) {
+static inline Vec4qb operator > (Vec4uq const a, Vec4uq const b) {
     return Vec4q(a.get_low() > b.get_low(), a.get_high() > b.get_high());
 }
 
 // vector operator < : returns true for elements for which a < b (unsigned)
-static inline Vec4qb operator < (Vec4uq const & a, Vec4uq const & b) {
+static inline Vec4qb operator < (Vec4uq const a, Vec4uq const b) {
     return b > a;
 }
 
 // vector operator >= : returns true for elements for which a >= b (unsigned)
-static inline Vec4qb operator >= (Vec4uq const & a, Vec4uq const & b) {
+static inline Vec4qb operator >= (Vec4uq const a, Vec4uq const b) {
     return Vec4q(a.get_low() >= b.get_low(), a.get_high() >= b.get_high());
 }
 
 // vector operator <= : returns true for elements for which a <= b (unsigned)
-static inline Vec4qb operator <= (Vec4uq const & a, Vec4uq const & b) {
+static inline Vec4qb operator <= (Vec4uq const a, Vec4uq const b) {
     return b >= a;
 }
 
 // vector operator & : bitwise and
-static inline Vec4uq operator & (Vec4uq const & a, Vec4uq const & b) {
+static inline Vec4uq operator & (Vec4uq const a, Vec4uq const b) {
     return Vec4uq(a.get_low() & b.get_low(), a.get_high() & b.get_high());
 }
-static inline Vec4uq operator && (Vec4uq const & a, Vec4uq const & b) {
+static inline Vec4uq operator && (Vec4uq const a, Vec4uq const b) {
     return a & b;
 }
 
 // vector operator | : bitwise or
-static inline Vec4uq operator | (Vec4uq const & a, Vec4uq const & b) {
+static inline Vec4uq operator | (Vec4uq const a, Vec4uq const b) {
     return Vec4q(a.get_low() | b.get_low(), a.get_high() | b.get_high());
 }
-static inline Vec4uq operator || (Vec4uq const & a, Vec4uq const & b) {
+static inline Vec4uq operator || (Vec4uq const a, Vec4uq const b) {
     return a | b;
 }
 
 // vector operator ^ : bitwise xor
-static inline Vec4uq operator ^ (Vec4uq const & a, Vec4uq const & b) {
+static inline Vec4uq operator ^ (Vec4uq const a, Vec4uq const b) {
     return Vec4uq(a.get_low() ^ b.get_low(), a.get_high() ^ b.get_high());
 }
 
 // vector operator ~ : bitwise not
-static inline Vec4uq operator ~ (Vec4uq const & a) {
+static inline Vec4uq operator ~ (Vec4uq const a) {
     return Vec4uq(~a.get_low(), ~a.get_high());
 }
 
@@ -3049,28 +3075,37 @@ static inline Vec4uq operator ~ (Vec4uq const & a) {
 // for (int i = 0; i < 4; i++) result[i] = s[i] ? a[i] : b[i];
 // Each word in s must be either 0 (false) or -1 (true). No other values are allowed.
 // (s is signed)
-static inline Vec4uq select (Vec4qb const & s, Vec4uq const & a, Vec4uq const & b) {
+static inline Vec4uq select (Vec4qb const s, Vec4uq const a, Vec4uq const b) {
     return selectb(s,a,b);
 }
 
 // Conditional add: For all vector elements i: result[i] = f[i] ? (a[i] + b[i]) : a[i]
-static inline Vec4uq if_add (Vec4qb const & f, Vec4uq const & a, Vec4uq const & b) {
+static inline Vec4uq if_add (Vec4qb const f, Vec4uq const a, Vec4uq const b) {
     return a + (Vec4uq(f) & b);
 }
 
-// Horizontal add: Calculates the sum of all vector elements.
-// Overflow will wrap around
-static inline uint64_t horizontal_add (Vec4uq const & a) {
-    return horizontal_add((Vec4q)a);
+// Conditional subtract
+static inline Vec4uq if_sub (Vec4qb const f, Vec4uq const a, Vec4uq const b) {
+    return a - (Vec4uq(f) & b);
+}
+
+// Conditional multiply
+static inline Vec4uq if_mul (Vec4qb const f, Vec4uq const a, Vec4uq const b) {
+    return select(f, a*b, a);
+}
+
+// Horizontal add: Calculates the sum of all vector elements. Overflow will wrap around
+static inline uint64_t horizontal_add (Vec4uq const a) {
+    return (uint64_t)horizontal_add((Vec4q)a);
 }
 
 // function max: a > b ? a : b
-static inline Vec4uq max(Vec4uq const & a, Vec4uq const & b) {
+static inline Vec4uq max(Vec4uq const a, Vec4uq const b) {
     return Vec4uq(max(a.get_low(),b.get_low()), max(a.get_high(),b.get_high()));
 }
 
 // function min: a < b ? a : b
-static inline Vec4uq min(Vec4uq const & a, Vec4uq const & b) {
+static inline Vec4uq min(Vec4uq const a, Vec4uq const b) {
     return Vec4uq(min(a.get_low(),b.get_low()), min(a.get_high(),b.get_high()));
 }
 
@@ -3082,82 +3117,67 @@ static inline Vec4uq min(Vec4uq const & a, Vec4uq const & b) {
 ******************************************************************************
 *
 * These permute functions can reorder the elements of a vector and optionally
-* set some elements to zero. 
-*
-* The indexes are inserted as template parameters in <>. These indexes must be
-* constants. Each template parameter is an index to the element you want to select.
-* An index of -1 will generate zero. An index of -256 means don't care.
+* set some elements to zero. See Vectori128.h for description
 *
-* Example:
-
-* Vec8i a(10,11,12,13,14,15,16,17);      // a is (10,11,12,13,14,15,16,17)
-* Vec8i b;
-* b = permute8i<0,2,7,7,-1,-1,1,1>(a);   // b is (10,12,17,17, 0, 0,11,11)
-*
-* A lot of the code here is metaprogramming aiming to find the instructions
-* that best fit the template parameters and instruction set. The metacode
-* will be reduced out to leave only a few vector instructions in release
-* mode with optimization on.
 *****************************************************************************/
 
-
-// Shuffle vector of 4 64-bit integers.
-// Index -1 gives 0, index -256 means don't care.
+// permute vector of 4 64-bit integers.
+// Index -1 gives 0, index V_DC means don't care.
 template <int i0, int i1, int i2, int i3 >
-static inline Vec4q permute4q(Vec4q const & a) {
-    return Vec4q(blend2q<i0,i1> (a.get_low(), a.get_high()),
-                 blend2q<i2,i3> (a.get_low(), a.get_high()));
+static inline Vec4q permute4(Vec4q const a) {
+    return Vec4q(blend2<i0,i1> (a.get_low(), a.get_high()),
+                 blend2<i2,i3> (a.get_low(), a.get_high()));
 }
 
 template <int i0, int i1, int i2, int i3>
-static inline Vec4uq permute4uq(Vec4uq const & a) {
-    return Vec4uq (permute4q<i0,i1,i2,i3> (a));
+static inline Vec4uq permute4(Vec4uq const a) {
+    return Vec4uq (permute4<i0,i1,i2,i3> (Vec4q(a)));
 }
 
-// Shuffle vector of 8 32-bit integers.
-// Index -1 gives 0, index -256 means don't care.
+// permute vector of 8 32-bit integers.
+// Index -1 gives 0, index V_DC means don't care.
 template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7 >
-static inline Vec8i permute8i(Vec8i const & a) {
-    return Vec8i(blend4i<i0,i1,i2,i3> (a.get_low(), a.get_high()), 
-                 blend4i<i4,i5,i6,i7> (a.get_low(), a.get_high()));
+static inline Vec8i permute8(Vec8i const a) {
+    return Vec8i(blend4<i0,i1,i2,i3> (a.get_low(), a.get_high()),
+                 blend4<i4,i5,i6,i7> (a.get_low(), a.get_high()));
 }
 
 template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7 >
-static inline Vec8ui permute8ui(Vec8ui const & a) {
-    return Vec8ui (permute8i<i0,i1,i2,i3,i4,i5,i6,i7> (a));
+static inline Vec8ui permute8(Vec8ui const a) {
+    return Vec8ui (permute8<i0,i1,i2,i3,i4,i5,i6,i7> (Vec8i(a)));
 }
 
-// Shuffle vector of 16 16-bit integers.
-// Index -1 gives 0, index -256 means don't care.
+// permute vector of 16 16-bit integers.
+// Index -1 gives 0, index V_DC means don't care.
 template <int i0, int i1, int i2,  int i3,  int i4,  int i5,  int i6,  int i7,
           int i8, int i9, int i10, int i11, int i12, int i13, int i14, int i15 >
-static inline Vec16s permute16s(Vec16s const & a) {
-    return Vec16s(blend8s<i0,i1,i2 ,i3 ,i4 ,i5 ,i6 ,i7 > (a.get_low(), a.get_high()), 
-                  blend8s<i8,i9,i10,i11,i12,i13,i14,i15> (a.get_low(), a.get_high()));
+static inline Vec16s permute16(Vec16s const a) {
+    return Vec16s(blend8<i0,i1,i2 ,i3 ,i4 ,i5 ,i6 ,i7 > (a.get_low(), a.get_high()),
+                  blend8<i8,i9,i10,i11,i12,i13,i14,i15> (a.get_low(), a.get_high()));
 }
 
 template <int i0, int i1, int i2,  int i3,  int i4,  int i5,  int i6,  int i7,
           int i8, int i9, int i10, int i11, int i12, int i13, int i14, int i15 >
-static inline Vec16us permute16us(Vec16us const & a) {
-    return Vec16us (permute16s<i0,i1,i2,i3,i4,i5,i6,i7,i8,i9,i10,i11,i12,i13,i14,i15> (a));
+static inline Vec16us permute16(Vec16us const a) {
+    return Vec16us (permute16<i0,i1,i2,i3,i4,i5,i6,i7,i8,i9,i10,i11,i12,i13,i14,i15> (Vec16s(a)));
 }
 
-template <int i0,  int i1,  int i2,  int i3,  int i4,  int i5,  int i6,  int i7, 
+template <int i0,  int i1,  int i2,  int i3,  int i4,  int i5,  int i6,  int i7,
           int i8,  int i9,  int i10, int i11, int i12, int i13, int i14, int i15,
           int i16, int i17, int i18, int i19, int i20, int i21, int i22, int i23,
           int i24, int i25, int i26, int i27, int i28, int i29, int i30, int i31 >
-static inline Vec32c permute32c(Vec32c const & a) {
-    return Vec32c(blend16c<i0, i1, i2 ,i3 ,i4 ,i5 ,i6 ,i7, i8, i9, i10,i11,i12,i13,i14,i15> (a.get_low(), a.get_high()), 
-                  blend16c<i16,i17,i18,i19,i20,i21,i22,i23,i24,i25,i26,i27,i28,i29,i30,i31> (a.get_low(), a.get_high()));
+static inline Vec32c permute32(Vec32c const a) {
+    return Vec32c(blend16<i0, i1, i2 ,i3 ,i4 ,i5 ,i6 ,i7, i8, i9, i10,i11,i12,i13,i14,i15> (a.get_low(), a.get_high()),
+                  blend16<i16,i17,i18,i19,i20,i21,i22,i23,i24,i25,i26,i27,i28,i29,i30,i31> (a.get_low(), a.get_high()));
 }
 
-template <int i0,  int i1,  int i2,  int i3,  int i4,  int i5,  int i6,  int i7, 
+template <int i0,  int i1,  int i2,  int i3,  int i4,  int i5,  int i6,  int i7,
           int i8,  int i9,  int i10, int i11, int i12, int i13, int i14, int i15,
           int i16, int i17, int i18, int i19, int i20, int i21, int i22, int i23,
           int i24, int i25, int i26, int i27, int i28, int i29, int i30, int i31 >
-    static inline Vec32uc permute32uc(Vec32uc const & a) {
-        return Vec32uc (permute32c<i0,i1,i2,i3,i4,i5,i6,i7,i8,i9,i10,i11,i12,i13,i14,i15,    
-            i16,i17,i18,i19,i20,i21,i22,i23,i24,i25,i26,i27,i28,i29,i30,i31> (a));
+    static inline Vec32uc permute32(Vec32uc const a) {
+        return Vec32uc (permute32<i0,i1,i2,i3,i4,i5,i6,i7,i8,i9,i10,i11,i12,i13,i14,i15,
+            i16,i17,i18,i19,i20,i21,i22,i23,i24,i25,i26,i27,i28,i29,i30,i31> (Vec32c(a)));
 }
 
 
@@ -3165,479 +3185,69 @@ template <int i0,  int i1,  int i2,  int i3,  int i4,  int i5,  int i6,  int i7,
 *
 *          Vector blend functions
 *
-******************************************************************************
-*
-* These blend functions can mix elements from two different vectors and
-* optionally set some elements to zero. 
-*
-* The indexes are inserted as template parameters in <>. These indexes must be
-* constants. Each template parameter is an index to the element you want to 
-* select, where higher indexes indicate an element from the second source
-* vector. For example, if each vector has 8 elements, then indexes 0 - 7
-* will select an element from the first vector and indexes 8 - 15 will select 
-* an element from the second vector. A negative index will generate zero.
-*
-* Example:
-* Vec8i a(100,101,102,103,104,105,106,107); // a is (100, 101, 102, 103, 104, 105, 106, 107)
-* Vec8i b(200,201,202,203,204,205,206,207); // b is (200, 201, 202, 203, 204, 205, 206, 207)
-* Vec8i c;
-* c = blend8i<1,0,9,8,7,-1,15,15> (a,b);    // c is (101, 100, 201, 200, 107,   0, 207, 207)
-*
-* A lot of the code here is metaprogramming aiming to find the instructions
-* that best fit the template parameters and instruction set. The metacode
-* will be reduced out to leave only a few vector instructions in release
-* mode with optimization on.
 *****************************************************************************/
 
-// helper function used below
-template <int n>
-static inline Vec2q select4(Vec4q const & a, Vec4q const & b) {
-    switch (n) {
-    case 0:
-        return a.get_low();
-    case 1:
-        return a.get_high();
-    case 2:
-        return b.get_low();
-    case 3:
-        return b.get_high();
-    }
-    return _mm_setzero_si128();
-}
-
 // blend vectors Vec4q
 template <int i0, int i1, int i2, int i3>
-static inline Vec4q blend4q(Vec4q const & a, Vec4q const & b) {
-    const int j0 = i0 >= 0 ? i0/2 : i0;
-    const int j1 = i1 >= 0 ? i1/2 : i1;
-    const int j2 = i2 >= 0 ? i2/2 : i2;
-    const int j3 = i3 >= 0 ? i3/2 : i3;    
-    Vec2q x0, x1;
-
-    if (j0 == j1 || i0 < 0 || i1 < 0) {  // both from same
-        const int k0 = j0 >= 0 ? j0 : j1;
-        x0 = permute2q<i0 & -7, i1 & -7> (select4<k0> (a,b));
-    }
-    else {
-        x0 = blend2q<i0 & -7, (i1 & -7) | 2> (select4<j0>(a,b), select4<j1>(a,b));
-    }
-    if (j2 == j3 || i2 < 0 || i3 < 0) {  // both from same
-        const int k1 = j2 >= 0 ? j2 : j3;
-        x1 = permute2q<i2 & -7, i3 & -7> (select4<k1> (a,b));
-    }
-    else {
-        x1 = blend2q<i2 & -7, (i3 & -7) | 2> (select4<j2>(a,b), select4<j3>(a,b));
-    }
-    return Vec4q(x0,x1);
-}
-
-template <int i0, int i1, int i2, int i3> 
-static inline Vec4uq blend4uq(Vec4uq const & a, Vec4uq const & b) {
-    return Vec4uq( blend4q<i0,i1,i2,i3> (a,b));
-}
-
-// helper function used below
-template <int n>
-static inline Vec4i select4(Vec8i const & a, Vec8i const & b) {
-    switch (n) {
-    case 0:
-        return a.get_low();
-    case 1:
-        return a.get_high();
-    case 2:
-        return b.get_low();
-    case 3:
-        return b.get_high();
-    }
-    return _mm_setzero_si128();
+static inline Vec4q blend4(Vec4q const& a, Vec4q const& b) {
+    Vec2q x0 = blend_half<Vec4q, i0, i1>(a, b);
+    Vec2q x1 = blend_half<Vec4q, i2, i3>(a, b);
+    return Vec4q(x0, x1);
 }
 
 // blend vectors Vec8i
 template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7>
-static inline Vec8i blend8i(Vec8i const & a, Vec8i const & b) {
-    const int j0 = i0 >= 0 ? i0/4 : i0;
-    const int j1 = i1 >= 0 ? i1/4 : i1;
-    const int j2 = i2 >= 0 ? i2/4 : i2;
-    const int j3 = i3 >= 0 ? i3/4 : i3;
-    const int j4 = i4 >= 0 ? i4/4 : i4;
-    const int j5 = i5 >= 0 ? i5/4 : i5;
-    const int j6 = i6 >= 0 ? i6/4 : i6;
-    const int j7 = i7 >= 0 ? i7/4 : i7;
-    Vec4i x0, x1;
-
-    const int r0 = j0 >= 0 ? j0 : j1 >= 0 ? j1 : j2 >= 0 ? j2 : j3;
-    const int r1 = j4 >= 0 ? j4 : j5 >= 0 ? j5 : j6 >= 0 ? j6 : j7;
-    const int s0 = (j1 >= 0 && j1 != r0) ? j1 : (j2 >= 0 && j2 != r0) ? j2 : j3;
-    const int s1 = (j5 >= 0 && j5 != r1) ? j5 : (j6 >= 0 && j6 != r1) ? j6 : j7;
-
-    // Combine all the indexes into a single bitfield, with 4 bits for each
-    const int m1 = (i0&0xF) | (i1&0xF)<<4 | (i2&0xF)<<8 | (i3&0xF)<<12 | (i4&0xF)<<16 | (i5&0xF)<<20 | (i6&0xF)<<24 | (i7&0xF)<<28;
-
-    // Mask to zero out negative indexes
-    const int mz = (i0<0?0:0xF) | (i1<0?0:0xF)<<4 | (i2<0?0:0xF)<<8 | (i3<0?0:0xF)<<12 | (i4<0?0:0xF)<<16 | (i5<0?0:0xF)<<20 | (i6<0?0:0xF)<<24 | (i7<0?0:0xF)<<28;
-
-    if (r0 < 0) {
-        x0 = _mm_setzero_si128();
-    }
-    else if (((m1 ^ r0*0x4444) & 0xCCCC & mz) == 0) { 
-        // i0 - i3 all from same source
-        x0 = permute4i<i0 & -13, i1 & -13, i2 & -13, i3 & -13> (select4<r0> (a,b));
-    }
-    else if ((j2 < 0 || j2 == r0 || j2 == s0) && (j3 < 0 || j3 == r0 || j3 == s0)) { 
-        // i0 - i3 all from two sources
-        const int k0 =  i0 >= 0 ? i0 & 3 : i0;
-        const int k1 = (i1 >= 0 ? i1 & 3 : i1) | (j1 == s0 ? 4 : 0);
-        const int k2 = (i2 >= 0 ? i2 & 3 : i2) | (j2 == s0 ? 4 : 0);
-        const int k3 = (i3 >= 0 ? i3 & 3 : i3) | (j3 == s0 ? 4 : 0);
-        x0 = blend4i<k0,k1,k2,k3> (select4<r0>(a,b), select4<s0>(a,b));
-    }
-    else {
-        // i0 - i3 from three or four different sources
-        x0 = blend4i<0,1,6,7> (
-             blend4i<i0 & -13, (i1 & -13) | 4, -0x100, -0x100> (select4<j0>(a,b), select4<j1>(a,b)),
-             blend4i<-0x100, -0x100, i2 & -13, (i3 & -13) | 4> (select4<j2>(a,b), select4<j3>(a,b)));
-    }
-
-    if (r1 < 0) {
-        x1 = _mm_setzero_si128();
-    }
-    else if (((m1 ^ uint32_t(r1)*0x44440000u) & 0xCCCC0000 & mz) == 0) { 
-        // i4 - i7 all from same source
-        x1 = permute4i<i4 & -13, i5 & -13, i6 & -13, i7 & -13> (select4<r1> (a,b));
-    }
-    else if ((j6 < 0 || j6 == r1 || j6 == s1) && (j7 < 0 || j7 == r1 || j7 == s1)) { 
-        // i4 - i7 all from two sources
-        const int k4 =  i4 >= 0 ? i4 & 3 : i4;
-        const int k5 = (i5 >= 0 ? i5 & 3 : i5) | (j5 == s1 ? 4 : 0);
-        const int k6 = (i6 >= 0 ? i6 & 3 : i6) | (j6 == s1 ? 4 : 0);
-        const int k7 = (i7 >= 0 ? i7 & 3 : i7) | (j7 == s1 ? 4 : 0);
-        x1 = blend4i<k4,k5,k6,k7> (select4<r1>(a,b), select4<s1>(a,b));
-    }
-    else {
-        // i4 - i7 from three or four different sources
-        x1 = blend4i<0,1,6,7> (
-             blend4i<i4 & -13, (i5 & -13) | 4, -0x100, -0x100> (select4<j4>(a,b), select4<j5>(a,b)),
-             blend4i<-0x100, -0x100, i6 & -13, (i7 & -13) | 4> (select4<j6>(a,b), select4<j7>(a,b)));
-    }
-
-    return Vec8i(x0,x1);
+static inline Vec8i blend8(Vec8i const& a, Vec8i const& b) {
+    Vec4i x0 = blend_half<Vec8i, i0, i1, i2, i3>(a, b);
+    Vec4i x1 = blend_half<Vec8i, i4, i5, i6, i7>(a, b);
+    return Vec8i(x0, x1);
 }
 
-template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7> 
-static inline Vec8ui blend8ui(Vec8ui const & a, Vec8ui const & b) {
-    return Vec8ui( blend8i<i0,i1,i2,i3,i4,i5,i6,i7> (a,b));
+// blend vectors Vec16s
+template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7,
+    int i8, int i9, int i10, int i11, int i12, int i13, int i14, int i15>
+static inline Vec16s blend16(Vec16s const& a, Vec16s const& b) {
+    Vec8s x0 = blend_half<Vec16s, i0, i1, i2, i3, i4, i5, i6, i7>(a, b);
+    Vec8s x1 = blend_half<Vec16s, i8, i9, i10, i11, i12, i13, i14, i15>(a, b);
+    return Vec16s(x0, x1);
 }
 
-// helper function used below
-template <int n>
-static inline Vec8s select4(Vec16s const & a, Vec16s const & b) {
-    switch (n) {
-    case 0:
-        return a.get_low();
-    case 1:
-        return a.get_high();
-    case 2:
-        return b.get_low();
-    case 3:
-        return b.get_high();
-    }
-    return _mm_setzero_si128();
-}
-
-template <int i0,  int i1,  int i2,  int i3,  int i4,  int i5,  int i6,  int i7, 
-          int i8,  int i9,  int i10, int i11, int i12, int i13, int i14, int i15 > 
-static inline Vec16s blend16s(Vec16s const & a, Vec16s const & b) {
-
-    const int j0  = i0  >= 0 ? i0 /8 : i0;
-    const int j1  = i1  >= 0 ? i1 /8 : i1;
-    const int j2  = i2  >= 0 ? i2 /8 : i2;
-    const int j3  = i3  >= 0 ? i3 /8 : i3;
-    const int j4  = i4  >= 0 ? i4 /8 : i4;
-    const int j5  = i5  >= 0 ? i5 /8 : i5;
-    const int j6  = i6  >= 0 ? i6 /8 : i6;
-    const int j7  = i7  >= 0 ? i7 /8 : i7;
-    const int j8  = i8  >= 0 ? i8 /8 : i8;
-    const int j9  = i9  >= 0 ? i9 /8 : i9;
-    const int j10 = i10 >= 0 ? i10/8 : i10;
-    const int j11 = i11 >= 0 ? i11/8 : i11;
-    const int j12 = i12 >= 0 ? i12/8 : i12;
-    const int j13 = i13 >= 0 ? i13/8 : i13;
-    const int j14 = i14 >= 0 ? i14/8 : i14;
-    const int j15 = i15 >= 0 ? i15/8 : i15;
-
-    Vec8s x0, x1;
-
-    const int r0 = j0 >= 0 ? j0 : j1 >= 0 ? j1 : j2  >= 0 ? j2  : j3  >= 0 ? j3  : j4  >= 0 ? j4  : j5  >= 0 ? j5  : j6  >= 0 ? j6  : j7;
-    const int r1 = j8 >= 0 ? j8 : j9 >= 0 ? j9 : j10 >= 0 ? j10 : j11 >= 0 ? j11 : j12 >= 0 ? j12 : j13 >= 0 ? j13 : j14 >= 0 ? j14 : j15;
-    const int s0 = (j1 >= 0 && j1 != r0) ? j1 : (j2 >= 0 && j2 != r0) ? j2  : (j3 >= 0 && j3 != r0) ? j3 : (j4 >= 0 && j4 != r0) ? j4 : (j5 >= 0 && j5 != r0) ? j5 : (j6 >= 0 && j6 != r0) ? j6 : j7;
-    const int s1 = (j9 >= 0 && j9 != r1) ? j9 : (j10>= 0 && j10!= r1) ? j10 : (j11>= 0 && j11!= r1) ? j11: (j12>= 0 && j12!= r1) ? j12: (j13>= 0 && j13!= r1) ? j13: (j14>= 0 && j14!= r1) ? j14: j15;
-
-    if (r0 < 0) {
-        x0 = _mm_setzero_si128();
-    }
-    else if (r0 == s0) {
-        // i0 - i7 all from same source
-        x0 = permute8s<i0&-25, i1&-25, i2&-25, i3&-25, i4&-25, i5&-25, i6&-25, i7&-25> (select4<r0> (a,b));
-    }
-    else if ((j2<0||j2==r0||j2==s0) && (j3<0||j3==r0||j3 == s0) && (j4<0||j4==r0||j4 == s0) && (j5<0||j5==r0||j5 == s0) && (j6<0||j6==r0||j6 == s0) && (j7<0||j7==r0||j7 == s0)) { 
-        // i0 - i7 all from two sources
-        const int k0 =  i0 >= 0 ? i0 & 7 : i0;
-        const int k1 = (i1 >= 0 ? i1 & 7 : i1) | (j1 == s0 ? 8 : 0);
-        const int k2 = (i2 >= 0 ? i2 & 7 : i2) | (j2 == s0 ? 8 : 0);
-        const int k3 = (i3 >= 0 ? i3 & 7 : i3) | (j3 == s0 ? 8 : 0);
-        const int k4 = (i4 >= 0 ? i4 & 7 : i4) | (j4 == s0 ? 8 : 0);
-        const int k5 = (i5 >= 0 ? i5 & 7 : i5) | (j5 == s0 ? 8 : 0);
-        const int k6 = (i6 >= 0 ? i6 & 7 : i6) | (j6 == s0 ? 8 : 0);
-        const int k7 = (i7 >= 0 ? i7 & 7 : i7) | (j7 == s0 ? 8 : 0);
-        x0 = blend8s<k0,k1,k2,k3,k4,k5,k6,k7> (select4<r0>(a,b), select4<s0>(a,b));
-    }
-    else {
-        // i0 - i7 from three or four different sources
-        const int n0 = j0 >= 0 ? j0 /2*8 + 0 : j0;
-        const int n1 = j1 >= 0 ? j1 /2*8 + 1 : j1;
-        const int n2 = j2 >= 0 ? j2 /2*8 + 2 : j2;
-        const int n3 = j3 >= 0 ? j3 /2*8 + 3 : j3;
-        const int n4 = j4 >= 0 ? j4 /2*8 + 4 : j4;
-        const int n5 = j5 >= 0 ? j5 /2*8 + 5 : j5;
-        const int n6 = j6 >= 0 ? j6 /2*8 + 6 : j6;
-        const int n7 = j7 >= 0 ? j7 /2*8 + 7 : j7;
-        x0 = blend8s<n0, n1, n2, n3, n4, n5, n6, n7> (
-             blend8s< j0   & 2 ? -256 : i0 &15,  j1   & 2 ? -256 : i1 &15,  j2   & 2 ? -256 : i2 &15,  j3   & 2 ? -256 : i3 &15,  j4   & 2 ? -256 : i4 &15,  j5   & 2 ? -256 : i5 &15,  j6   & 2 ? -256 : i6 &15,  j7   & 2 ? -256 : i7 &15> (a.get_low(),a.get_high()),
-             blend8s<(j0^2)& 6 ? -256 : i0 &15, (j1^2)& 6 ? -256 : i1 &15, (j2^2)& 6 ? -256 : i2 &15, (j3^2)& 6 ? -256 : i3 &15, (j4^2)& 6 ? -256 : i4 &15, (j5^2)& 6 ? -256 : i5 &15, (j6^2)& 6 ? -256 : i6 &15, (j7^2)& 6 ? -256 : i7 &15> (b.get_low(),b.get_high()));
-    }
-
-    if (r1 < 0) {
-        x1 = _mm_setzero_si128();
-    }
-    else if (r1 == s1) {
-        // i8 - i15 all from same source
-        x1 = permute8s<i8&-25, i9&-25, i10&-25, i11&-25, i12&-25, i13&-25, i14&-25, i15&-25> (select4<r1> (a,b));
-    }
-    else if ((j10<0||j10==r1||j10==s1) && (j11<0||j11==r1||j11==s1) && (j12<0||j12==r1||j12==s1) && (j13<0||j13==r1||j13==s1) && (j14<0||j14==r1||j14==s1) && (j15<0||j15==r1||j15==s1)) { 
-        // i8 - i15 all from two sources
-        const int k8 =  i8 >= 0 ? i8 & 7 : i8;
-        const int k9 = (i9 >= 0 ? i9 & 7 : i9 ) | (j9 == s1 ? 8 : 0);
-        const int k10= (i10>= 0 ? i10& 7 : i10) | (j10== s1 ? 8 : 0);
-        const int k11= (i11>= 0 ? i11& 7 : i11) | (j11== s1 ? 8 : 0);
-        const int k12= (i12>= 0 ? i12& 7 : i12) | (j12== s1 ? 8 : 0);
-        const int k13= (i13>= 0 ? i13& 7 : i13) | (j13== s1 ? 8 : 0);
-        const int k14= (i14>= 0 ? i14& 7 : i14) | (j14== s1 ? 8 : 0);
-        const int k15= (i15>= 0 ? i15& 7 : i15) | (j15== s1 ? 8 : 0);
-        x1 = blend8s<k8,k9,k10,k11,k12,k13,k14,k15> (select4<r1>(a,b), select4<s1>(a,b));
-    }
-    else {
-        // i8 - i15 from three or four different sources
-        const int n8 = j8 >= 0 ? j8 /2*8 + 0 : j8 ;
-        const int n9 = j9 >= 0 ? j9 /2*8 + 1 : j9 ;
-        const int n10= j10>= 0 ? j10/2*8 + 2 : j10;
-        const int n11= j11>= 0 ? j11/2*8 + 3 : j11;
-        const int n12= j12>= 0 ? j12/2*8 + 4 : j12;
-        const int n13= j13>= 0 ? j13/2*8 + 5 : j13;
-        const int n14= j14>= 0 ? j14/2*8 + 6 : j14;
-        const int n15= j15>= 0 ? j15/2*8 + 7 : j15;
-        x1 = blend8s<n8, n9, n10, n11, n12, n13, n14, n15> (
-             blend8s< j8   & 2 ? -256 : i8 &15,  j9   & 2 ? -256 : i9 &15,  j10   & 2 ? -256 : i10 &15,  j11   & 2 ? -256 : i11 &15,  j12   & 2 ? -256 : i12 &15,  j13   & 2 ? -256 : i13 &15,  j14   & 2 ? -256 : i14 &15,  j15   & 2 ? -256 : i15 &15> (a.get_low(),a.get_high()),
-             blend8s<(j8^2)& 6 ? -256 : i8 &15, (j9^2)& 6 ? -256 : i9 &15, (j10^2)& 6 ? -256 : i10 &15, (j11^2)& 6 ? -256 : i11 &15, (j12^2)& 6 ? -256 : i12 &15, (j13^2)& 6 ? -256 : i13 &15, (j14^2)& 6 ? -256 : i14 &15, (j15^2)& 6 ? -256 : i15 &15> (b.get_low(),b.get_high()));
-    }
-    return Vec16s(x0,x1);
+template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7,
+    int i8,  int i9,  int i10, int i11, int i12, int i13, int i14, int i15,
+    int i16, int i17, int i18, int i19, int i20, int i21, int i22, int i23,
+    int i24, int i25, int i26, int i27, int i28, int i29, int i30, int i31 >
+    static inline Vec32c blend32(Vec32c const& a, Vec32c const& b) {
+    Vec16c x0 = blend_half<Vec32c, i0, i1, i2, i3, i4, i5, i6, i7, i8, i9, i10, i11, i12, i13, i14, i15>(a, b);
+    Vec16c x1 = blend_half<Vec32c, i16, i17, i18, i19, i20, i21, i22, i23, i24, i25, i26, i27, i28, i29, i30, i31>(a, b);
+    return Vec32c(x0, x1);
 }
 
-template <int i0, int i1, int i2,  int i3,  int i4,  int i5,  int i6,  int i7, 
-          int i8, int i9, int i10, int i11, int i12, int i13, int i14, int i15 > 
-static inline Vec16us blend16us(Vec16us const & a, Vec16us const & b) {
-    return Vec16us( blend16s<i0,i1,i2,i3,i4,i5,i6,i7,i8,i9,i10,i11,i12,i13,i14,i15> (a,b));
+// unsigned types:
+
+template <int i0, int i1, int i2, int i3>
+static inline Vec4uq blend4(Vec4uq const a, Vec4uq const b) {
+    return Vec4uq( blend4<i0,i1,i2,i3> (Vec4q(a),Vec4q(b)));
 }
 
-// helper function used below
-template <int n>
-static inline Vec16c select4(Vec32c const & a, Vec32c const & b) {
-    switch (n) {
-    case 0:
-        return a.get_low();
-    case 1:
-        return a.get_high();
-    case 2:
-        return b.get_low();
-    case 3:
-        return b.get_high();
-    }
-    return _mm_setzero_si128();
+template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7>
+static inline Vec8ui blend8(Vec8ui const a, Vec8ui const b) {
+    return Vec8ui( blend8<i0,i1,i2,i3,i4,i5,i6,i7> (Vec8i(a),Vec8i(b)));
 }
 
-template <int i0,  int i1,  int i2,  int i3,  int i4,  int i5,  int i6,  int i7, 
-          int i8,  int i9,  int i10, int i11, int i12, int i13, int i14, int i15,
-          int i16, int i17, int i18, int i19, int i20, int i21, int i22, int i23,
-          int i24, int i25, int i26, int i27, int i28, int i29, int i30, int i31 > 
-static inline Vec32c blend32c(Vec32c const & a, Vec32c const & b) {  
-
-    // j0 - j31 indicate one of four 16-byte sources
-    const int j0  = i0  >= 0 ? i0 /16 : i0;
-    const int j1  = i1  >= 0 ? i1 /16 : i1;
-    const int j2  = i2  >= 0 ? i2 /16 : i2;
-    const int j3  = i3  >= 0 ? i3 /16 : i3;
-    const int j4  = i4  >= 0 ? i4 /16 : i4;
-    const int j5  = i5  >= 0 ? i5 /16 : i5;
-    const int j6  = i6  >= 0 ? i6 /16 : i6;
-    const int j7  = i7  >= 0 ? i7 /16 : i7;
-    const int j8  = i8  >= 0 ? i8 /16 : i8;
-    const int j9  = i9  >= 0 ? i9 /16 : i9;
-    const int j10 = i10 >= 0 ? i10/16 : i10;
-    const int j11 = i11 >= 0 ? i11/16 : i11;
-    const int j12 = i12 >= 0 ? i12/16 : i12;
-    const int j13 = i13 >= 0 ? i13/16 : i13;
-    const int j14 = i14 >= 0 ? i14/16 : i14;
-    const int j15 = i15 >= 0 ? i15/16 : i15;
-    const int j16 = i16 >= 0 ? i16/16 : i16;
-    const int j17 = i17 >= 0 ? i17/16 : i17;
-    const int j18 = i18 >= 0 ? i18/16 : i18;
-    const int j19 = i19 >= 0 ? i19/16 : i19;
-    const int j20 = i20 >= 0 ? i20/16 : i20;
-    const int j21 = i21 >= 0 ? i21/16 : i21;
-    const int j22 = i22 >= 0 ? i22/16 : i22;
-    const int j23 = i23 >= 0 ? i23/16 : i23;
-    const int j24 = i24 >= 0 ? i24/16 : i24;
-    const int j25 = i25 >= 0 ? i25/16 : i25;
-    const int j26 = i26 >= 0 ? i26/16 : i26;
-    const int j27 = i27 >= 0 ? i27/16 : i27;
-    const int j28 = i28 >= 0 ? i28/16 : i28;
-    const int j29 = i29 >= 0 ? i29/16 : i29;
-    const int j30 = i30 >= 0 ? i30/16 : i30;
-    const int j31 = i31 >= 0 ? i31/16 : i31;
-
-    Vec16c x0, x1;
-
-    // r0, s0 = first two sources of low  destination (i0  - i15)
-    // r1, s1 = first two sources of high destination (i16 - i31)
-    const int r0 = j0 >= 0 ? j0 : j1 >= 0 ? j1 : j2  >= 0 ? j2  : j3  >= 0 ? j3  : j4  >= 0 ? j4  : j5  >= 0 ? j5  : j6  >= 0 ? j6  : j7 >= 0 ? j7 : 
-                   j8 >= 0 ? j8 : j9 >= 0 ? j9 : j10 >= 0 ? j10 : j11 >= 0 ? j11 : j12 >= 0 ? j12 : j13 >= 0 ? j13 : j14 >= 0 ? j14 : j15;
-    const int r1 = j16>= 0 ? j16: j17>= 0 ? j17: j18 >= 0 ? j18 : j19 >= 0 ? j19 : j20 >= 0 ? j20 : j21 >= 0 ? j21 : j22 >= 0 ? j22 : j23>= 0 ? j23: 
-                   j24>= 0 ? j24: j25>= 0 ? j25: j26 >= 0 ? j26 : j27 >= 0 ? j27 : j28 >= 0 ? j28 : j29 >= 0 ? j29 : j30 >= 0 ? j30 : j31;
-    const int s0 = (j1 >=0&&j1 !=r0)?j1  : (j2 >=0&&j2 !=r0)?j2  : (j3 >=0&&j3 !=r0)?j3  : (j4 >=0&&j4 !=r0)?j4  : (j5 >=0&&j5 !=r0)?j5  : (j6 >=0&&j6 !=r0)?j6 : (j7 >=0&&j7 !=r0)?j7 : 
-                   (j8 >=0&&j8 !=r0)?j8  : (j9 >=0&&j9 !=r0)?j9  : (j10>=0&&j10!=r0)?j10 : (j11>=0&&j11!=r0)?j11 : (j12>=0&&j12!=r0)?j12 : (j13>=0&&j13!=r0)?j13: (j14>=0&&j14!=r0)?j14: j15;
-    const int s1 = (j17>=0&&j17!=r1)?j17 : (j18>=0&&j18!=r1)?j18 : (j19>=0&&j19!=r1)?j19 : (j20>=0&&j20!=r1)?j20 : (j21>=0&&j21!=r1)?j21 : (j22>=0&&j22!=r1)?j22: (j23>=0&&j23!=r1)?j23: 
-                   (j24>=0&&j24!=r1)?j24 : (j25>=0&&j25!=r1)?j25 : (j26>=0&&j26!=r1)?j26 : (j27>=0&&j27!=r1)?j27 : (j28>=0&&j28!=r1)?j28 : (j29>=0&&j29!=r1)?j29: (j30>=0&&j30!=r1)?j30: j31;
-
-    if (r0 < 0) {
-        x0 = _mm_setzero_si128();
-    }
-    else if (r0 == s0) {
-        // i0 - i15 all from same source
-        x0 = permute16c< i0&-49, i1&-49, i2 &-49, i3 &-49, i4 &-49, i5 &-49, i6 &-49, i7 &-49,
-                         i8&-49, i9&-49, i10&-49, i11&-49, i12&-49, i13&-49, i14&-49, i15&-49 >
-             (select4<r0> (a,b));
-    }
-    else if ((j2 <0||j2 ==r0||j2 ==s0) && (j3 <0||j3 ==r0||j3 ==s0) && (j4 <0||j4 ==r0||j4 ==s0) && (j5 <0||j5 ==r0||j5 ==s0) && (j6 <0||j6 ==r0||j6 ==s0) && (j7 <0||j7 ==r0||j7 ==s0) && (j8 <0||j8 ==r0||j8 ==s0) && 
-             (j9 <0||j9 ==r0||j9 ==s0) && (j10<0||j10==r0||j10==s0) && (j11<0||j11==r0||j11==s0) && (j12<0||j12==r0||j12==s0) && (j13<0||j13==r0||j13==s0) && (j14<0||j14==r0||j14==s0) && (j15<0||j15==r0||j15==s0)) {
-        // i0 - i15 all from two sources
-        const int k0 =  i0 >= 0 ? i0 & 15 : i0;
-        const int k1 = (i1 >= 0 ? i1 & 15 : i1 ) | (j1 == s0 ? 16 : 0);
-        const int k2 = (i2 >= 0 ? i2 & 15 : i2 ) | (j2 == s0 ? 16 : 0);
-        const int k3 = (i3 >= 0 ? i3 & 15 : i3 ) | (j3 == s0 ? 16 : 0);
-        const int k4 = (i4 >= 0 ? i4 & 15 : i4 ) | (j4 == s0 ? 16 : 0);
-        const int k5 = (i5 >= 0 ? i5 & 15 : i5 ) | (j5 == s0 ? 16 : 0);
-        const int k6 = (i6 >= 0 ? i6 & 15 : i6 ) | (j6 == s0 ? 16 : 0);
-        const int k7 = (i7 >= 0 ? i7 & 15 : i7 ) | (j7 == s0 ? 16 : 0);
-        const int k8 = (i8 >= 0 ? i8 & 15 : i8 ) | (j8 == s0 ? 16 : 0);
-        const int k9 = (i9 >= 0 ? i9 & 15 : i9 ) | (j9 == s0 ? 16 : 0);
-        const int k10= (i10>= 0 ? i10& 15 : i10) | (j10== s0 ? 16 : 0);
-        const int k11= (i11>= 0 ? i11& 15 : i11) | (j11== s0 ? 16 : 0);
-        const int k12= (i12>= 0 ? i12& 15 : i12) | (j12== s0 ? 16 : 0);
-        const int k13= (i13>= 0 ? i13& 15 : i13) | (j13== s0 ? 16 : 0);
-        const int k14= (i14>= 0 ? i14& 15 : i14) | (j14== s0 ? 16 : 0);
-        const int k15= (i15>= 0 ? i15& 15 : i15) | (j15== s0 ? 16 : 0);
-        x0 = blend16c<k0,k1,k2,k3,k4,k5,k6,k7,k8,k9,k10,k11,k12,k13,k14,k15> (select4<r0>(a,b), select4<s0>(a,b));
-    }
-    else {
-        // i0 - i15 from three or four different sources
-        const int n0 = j0 >= 0 ? j0 /2*16 + 0 : j0;
-        const int n1 = j1 >= 0 ? j1 /2*16 + 1 : j1;
-        const int n2 = j2 >= 0 ? j2 /2*16 + 2 : j2;
-        const int n3 = j3 >= 0 ? j3 /2*16 + 3 : j3;
-        const int n4 = j4 >= 0 ? j4 /2*16 + 4 : j4;
-        const int n5 = j5 >= 0 ? j5 /2*16 + 5 : j5;
-        const int n6 = j6 >= 0 ? j6 /2*16 + 6 : j6;
-        const int n7 = j7 >= 0 ? j7 /2*16 + 7 : j7;
-        const int n8 = j8 >= 0 ? j8 /2*16 + 8 : j8;
-        const int n9 = j9 >= 0 ? j9 /2*16 + 9 : j9;
-        const int n10= j10>= 0 ? j10/2*16 +10 : j10;
-        const int n11= j11>= 0 ? j11/2*16 +11 : j11;
-        const int n12= j12>= 0 ? j12/2*16 +12 : j12;
-        const int n13= j13>= 0 ? j13/2*16 +13 : j13;
-        const int n14= j14>= 0 ? j14/2*16 +14 : j14;
-        const int n15= j15>= 0 ? j15/2*16 +15 : j15;
-
-        Vec16c x0a = blend16c< j0   & 2 ? -256 : i0 & 31,  j1   & 2 ? -256 : i1 & 31,  j2    & 2 ? -256 : i2 & 31,  j3    & 2 ? -256 : i3 & 31,  j4    & 2 ? -256 : i4 & 31,  j5    & 2 ? -256 : i5 & 31,  j6    & 2 ? -256 : i6 & 31,  j7    & 2 ? -256 : i7 & 31,
-                               j8   & 2 ? -256 : i8 & 31,  j9   & 2 ? -256 : i9 & 31,  j10   & 2 ? -256 : i10& 31,  j11   & 2 ? -256 : i11& 31,  j12   & 2 ? -256 : i12& 31,  j13   & 2 ? -256 : i13& 31,  j14   & 2 ? -256 : i14& 31,  j15   & 2 ? -256 : i15& 31 > (a.get_low(),a.get_high());
-        Vec16c x0b = blend16c<(j0^2)& 6 ? -256 : i0 & 31, (j1^2)& 6 ? -256 : i1 & 31, (j2 ^2)& 6 ? -256 : i2 & 31, (j3 ^2)& 6 ? -256 : i3 & 31, (j4 ^2)& 6 ? -256 : i4 & 31, (j5 ^2)& 6 ? -256 : i5 & 31, (j6 ^2)& 6 ? -256 : i6 & 31, (j7 ^2)& 6 ? -256 : i7 & 31,
-                              (j8^2)& 6 ? -256 : i8 & 31, (j9^2)& 6 ? -256 : i9 & 31, (j10^2)& 6 ? -256 : i10& 31, (j11^2)& 6 ? -256 : i11& 31, (j12^2)& 6 ? -256 : i12& 31, (j13^2)& 6 ? -256 : i13& 31, (j14^2)& 6 ? -256 : i14& 31, (j15^2)& 6 ? -256 : i15& 31 > (b.get_low(),b.get_high());
-        x0         = blend16c<n0, n1, n2, n3, n4, n5, n6, n7, n8, n9, n10, n11, n12, n13, n14, n15> (x0a, x0b);
-    }
-
-    if (r1 < 0) {
-        x1 = _mm_setzero_si128();
-    }
-    else if (r1 == s1) {
-        // i16 - i31 all from same source
-        x1 = permute16c< i16&-49, i17&-49, i18&-49, i19&-49, i20&-49, i21&-49, i22&-49, i23&-49,
-                         i24&-49, i25&-49, i26&-49, i27&-49, i28&-49, i29&-49, i30&-49, i31&-49 >
-             (select4<r1> (a,b));
-    }
-    else if ((j18<0||j18==r1||j18==s1) && (j19<0||j19==r1||j19==s1) && (j20<0||j20==r1||j20==s1) && (j21<0||j21==r1||j21==s1) && (j22<0||j22==r1||j22==s1) && (j23<0||j23==r1||j23==s1) && (j24<0||j24==r1||j24==s1) && 
-             (j25<0||j25==r1||j25==s1) && (j26<0||j26==r1||j26==s1) && (j27<0||j27==r1||j27==s1) && (j28<0||j28==r1||j28==s1) && (j29<0||j29==r1||j29==s1) && (j30<0||j30==r1||j30==s1) && (j31<0||j31==r1||j31==s1)) {
-        // i16 - i31 all from two sources
-        const int k16=  i16>= 0 ? i16& 15 : i16;
-        const int k17= (i17>= 0 ? i17& 15 : i17) | (j17== s1 ? 16 : 0);
-        const int k18= (i18>= 0 ? i18& 15 : i18) | (j18== s1 ? 16 : 0);
-        const int k19= (i19>= 0 ? i19& 15 : i19) | (j19== s1 ? 16 : 0);
-        const int k20= (i20>= 0 ? i20& 15 : i20) | (j20== s1 ? 16 : 0);
-        const int k21= (i21>= 0 ? i21& 15 : i21) | (j21== s1 ? 16 : 0);
-        const int k22= (i22>= 0 ? i22& 15 : i22) | (j22== s1 ? 16 : 0);
-        const int k23= (i23>= 0 ? i23& 15 : i23) | (j23== s1 ? 16 : 0);
-        const int k24= (i24>= 0 ? i24& 15 : i24) | (j24== s1 ? 16 : 0);
-        const int k25= (i25>= 0 ? i25& 15 : i25) | (j25== s1 ? 16 : 0);
-        const int k26= (i26>= 0 ? i26& 15 : i26) | (j26== s1 ? 16 : 0);
-        const int k27= (i27>= 0 ? i27& 15 : i27) | (j27== s1 ? 16 : 0);
-        const int k28= (i28>= 0 ? i28& 15 : i28) | (j28== s1 ? 16 : 0);
-        const int k29= (i29>= 0 ? i29& 15 : i29) | (j29== s1 ? 16 : 0);
-        const int k30= (i30>= 0 ? i30& 15 : i30) | (j30== s1 ? 16 : 0);
-        const int k31= (i31>= 0 ? i31& 15 : i31) | (j31== s1 ? 16 : 0);
-        x1 = blend16c<k16,k17,k18,k19,k20,k21,k22,k23,k24,k25,k26,k27,k28,k29,k30,k31> (select4<r1>(a,b), select4<s1>(a,b));
-    }
-    else {
-        // i16 - i31 from three or four different sources
-        const int n16= j16>= 0 ? j16/2*16 + 0 : j16;
-        const int n17= j17>= 0 ? j17/2*16 + 1 : j17;
-        const int n18= j18>= 0 ? j18/2*16 + 2 : j18;
-        const int n19= j19>= 0 ? j19/2*16 + 3 : j19;
-        const int n20= j20>= 0 ? j20/2*16 + 4 : j20;
-        const int n21= j21>= 0 ? j21/2*16 + 5 : j21;
-        const int n22= j22>= 0 ? j22/2*16 + 6 : j22;
-        const int n23= j23>= 0 ? j23/2*16 + 7 : j23;
-        const int n24= j24>= 0 ? j24/2*16 + 8 : j24;
-        const int n25= j25>= 0 ? j25/2*16 + 9 : j25; 
-        const int n26= j26>= 0 ? j26/2*16 +10 : j26;
-        const int n27= j27>= 0 ? j27/2*16 +11 : j27;
-        const int n28= j28>= 0 ? j28/2*16 +12 : j28;
-        const int n29= j29>= 0 ? j29/2*16 +13 : j29;
-        const int n30= j30>= 0 ? j30/2*16 +14 : j30;
-        const int n31= j31>= 0 ? j31/2*16 +15 : j31;
-        x1 = blend16c<n16, n17, n18, n19, n20, n21, n22, n23, n24, n25, n26, n27, n28, n29, n30, n31> (
-             blend16c< j16   & 2 ? -256 : i16& 31,  j17   & 2 ? -256 : i17& 31,  j18   & 2 ? -256 : i18& 31,  j19   & 2 ? -256 : i19& 31,  j20   & 2 ? -256 : i20& 31,  j21   & 2 ? -256 : i21& 31,  j22   & 2 ? -256 : i22& 31,  j23   & 2 ? -256 : i23& 31,
-                       j24   & 2 ? -256 : i24& 31,  j25   & 2 ? -256 : i25& 31,  j26   & 2 ? -256 : i26& 31,  j27   & 2 ? -256 : i27& 31,  j28   & 2 ? -256 : i28& 31,  j29   & 2 ? -256 : i29& 31,  j30   & 2 ? -256 : i30& 31,  j31   & 2 ? -256 : i31& 31 > (a.get_low(),a.get_high()),
-             blend16c<(j16^2)& 6 ? -256 : i16& 31, (j17^2)& 6 ? -256 : i17& 31, (j18^2)& 6 ? -256 : i18& 31, (j19^2)& 6 ? -256 : i19& 31, (j20^2)& 6 ? -256 : i20& 31, (j21^2)& 6 ? -256 : i21& 31, (j22^2)& 6 ? -256 : i22& 31, (j23^2)& 6 ? -256 : i23& 31,
-                      (j24^2)& 6 ? -256 : i24& 31, (j25^2)& 6 ? -256 : i25& 31, (j26^2)& 6 ? -256 : i26& 31, (j27^2)& 6 ? -256 : i27& 31, (j28^2)& 6 ? -256 : i28& 31, (j29^2)& 6 ? -256 : i29& 31, (j30^2)& 6 ? -256 : i30& 31, (j31^2)& 6 ? -256 : i31& 31 > (b.get_low(),b.get_high()));
-    }
-    return Vec32c(x0,x1);
+template <int i0, int i1, int i2,  int i3,  int i4,  int i5,  int i6,  int i7,
+          int i8, int i9, int i10, int i11, int i12, int i13, int i14, int i15 >
+static inline Vec16us blend16(Vec16us const a, Vec16us const b) {
+    return Vec16us( blend16<i0,i1,i2,i3,i4,i5,i6,i7,i8,i9,i10,i11,i12,i13,i14,i15> (Vec16s(a),Vec16s(b)));
 }
 
 template <
-    int i0,  int i1,  int i2,  int i3,  int i4,  int i5,  int i6,  int i7, 
+    int i0,  int i1,  int i2,  int i3,  int i4,  int i5,  int i6,  int i7,
     int i8,  int i9,  int i10, int i11, int i12, int i13, int i14, int i15,
     int i16, int i17, int i18, int i19, int i20, int i21, int i22, int i23,
     int i24, int i25, int i26, int i27, int i28, int i29, int i30, int i31 >
-    static inline Vec32uc blend32uc(Vec32uc const & a, Vec32uc const & b) {
-        return Vec32uc (blend32c<i0,i1,i2,i3,i4,i5,i6,i7,i8,i9,i10,i11,i12,i13,i14,i15,    
-            i16,i17,i18,i19,i20,i21,i22,i23,i24,i25,i26,i27,i28,i29,i30,i31> (a, b));
+    static inline Vec32uc blend32(Vec32uc const a, Vec32uc const b) {
+        return Vec32uc (blend32<i0,i1,i2,i3,i4,i5,i6,i7,i8,i9,i10,i11,i12,i13,i14,i15,
+            i16,i17,i18,i19,i20,i21,i22,i23,i24,i25,i26,i27,i28,i29,i30,i31> (Vec32c(a), Vec32c(b)));
 }
 
 
@@ -3650,25 +3260,9 @@ template <
 * These functions use vector elements as indexes into a table.
 * The table is given as one or more vectors or as an array.
 *
-* This can be used for several purposes:
-*  - table lookup
-*  - permute or blend with variable indexes
-*  - blend from more than two sources
-*  - gather non-contiguous data
-*
-* An index out of range may produce any value - the actual value produced is
-* implementation dependent and may be different for different instruction
-* sets. An index out of range does not produce an error message or exception.
-*
-* Example:
-* Vec8i a(2,0,0,6,4,3,5,0);                 // index a is (  2,   0,   0,   6,   4,   3,   5,   0)
-* Vec8i b(100,101,102,103,104,105,106,107); // table b is (100, 101, 102, 103, 104, 105, 106, 107)
-* Vec8i c;
-* c = lookup8 (a,b);                        // c is       (102, 100, 100, 106, 104, 103, 105, 100)
-*
 *****************************************************************************/
 
-static inline Vec32c lookup32(Vec32c const & index, Vec32c const & table) {
+static inline Vec32c lookup32(Vec32c const index, Vec32c const table) {
 #if defined (__XOP__)   // AMD XOP instruction set. Use VPPERM
     Vec16c t0 = _mm_perm_epi8(table.get_low(), table.get_high(), index.get_low());
     Vec16c t1 = _mm_perm_epi8(table.get_low(), table.get_high(), index.get_high());
@@ -3681,18 +3275,21 @@ static inline Vec32c lookup32(Vec32c const & index, Vec32c const & table) {
 }
 
 template <int n>
-static inline Vec32c lookup(Vec32uc const & index, void const * table) {
-    if (n <=  0) return 0;
-    if (n <= 16) {
+static inline Vec32c lookup(Vec32uc const index, void const * table) {
+    if constexpr (n <=  0) return 0;
+    if constexpr (n <= 16) {
         Vec16c tt = Vec16c().load(table);
         Vec16c r0 = lookup16(index.get_low(),  tt);
         Vec16c r1 = lookup16(index.get_high(), tt);
         return Vec32c(r0, r1);
     }
-    if (n <= 32) return lookup32(index, Vec32c().load(table));
+    if constexpr (n <= 32) return lookup32(index, Vec32c().load(table));
     // n > 32. Limit index
     Vec32uc index1;
-    if ((n & (n-1)) == 0) {
+    if constexpr (n == INT_MAX) {
+        index1 = index;
+    }
+    else if constexpr ((n & (n-1)) == 0) {
         // n is a power of 2, make index modulo n
         index1 = Vec32uc(index) & uint8_t(n-1);
     }
@@ -3709,30 +3306,32 @@ static inline Vec32c lookup(Vec32uc const & index, void const * table) {
 }
 
 template <int n>
-static inline Vec32c lookup(Vec32c const & index, void const * table) {
+static inline Vec32c lookup(Vec32c const index, void const * table) {
     return lookup<n>(Vec32uc(index), table);
 }
 
-
-static inline Vec16s lookup16(Vec16s const & index, Vec16s const & table) {
+static inline Vec16s lookup16(Vec16s const index, Vec16s const table) {
     Vec8s t0 = lookup16(index.get_low() , table.get_low(), table.get_high());
     Vec8s t1 = lookup16(index.get_high(), table.get_low(), table.get_high());
     return Vec16s(t0, t1);
 }
 
 template <int n>
-static inline Vec16s lookup(Vec16s const & index, void const * table) {
-    if (n <=  0) return 0;
-    if (n <=  8) {
-        Vec8s table1 = Vec8s().load(table);        
-        return Vec16s(       
+static inline Vec16s lookup(Vec16s const index, void const * table) {
+    if constexpr (n <=  0) return 0;
+    if constexpr (n <=  8) {
+        Vec8s table1 = Vec8s().load(table);
+        return Vec16s(
             lookup8 (index.get_low(),  table1),
             lookup8 (index.get_high(), table1));
     }
-    if (n <= 16) return lookup16(index, Vec16s().load(table));
+    if constexpr (n <= 16) return lookup16(index, Vec16s().load(table));
     // n > 16. Limit index
     Vec16us i1;
-    if ((n & (n-1)) == 0) {
+    if constexpr (n == INT_MAX) {
+        i1 = index;
+    }
+    else if constexpr ((n & (n-1)) == 0) {
         // n is a power of 2, make index modulo n
         i1 = Vec16us(index) & (n-1);
     }
@@ -3745,27 +3344,30 @@ static inline Vec16s lookup(Vec16s const & index, void const * table) {
         t[i1[8]],t[i1[9]],t[i1[10]],t[i1[11]],t[i1[12]],t[i1[13]],t[i1[14]],t[i1[15]]);
 }
 
-static inline Vec8i lookup8(Vec8i const & index, Vec8i const & table) {
+static inline Vec8i lookup8(Vec8i const index, Vec8i const table) {
     Vec4i t0 = lookup8(index.get_low() , table.get_low(), table.get_high());
     Vec4i t1 = lookup8(index.get_high(), table.get_low(), table.get_high());
     return Vec8i(t0, t1);
 }
 
 template <int n>
-static inline Vec8i lookup(Vec8i const & index, void const * table) {
-    if (n <= 0) return 0;
-    if (n <= 4) {
-        Vec4i table1 = Vec4i().load(table);        
-        return Vec8i(       
+static inline Vec8i lookup(Vec8i const index, void const * table) {
+    if constexpr (n <= 0) return 0;
+    if constexpr (n <= 4) {
+        Vec4i table1 = Vec4i().load(table);
+        return Vec8i(
             lookup4 (index.get_low(),  table1),
             lookup4 (index.get_high(), table1));
     }
-    if (n <= 8) {
+    if constexpr (n <= 8) {
         return lookup8(index, Vec8i().load(table));
     }
     // n > 8. Limit index
     Vec8ui i1;
-    if ((n & (n-1)) == 0) {
+    if constexpr (n == INT_MAX) {
+        i1 = index;
+    }
+    else if constexpr ((n & (n-1)) == 0) {
         // n is a power of 2, make index modulo n
         i1 = Vec8ui(index) & (n-1);
     }
@@ -3777,16 +3379,19 @@ static inline Vec8i lookup(Vec8i const & index, void const * table) {
     return Vec8i(t[i1[0]],t[i1[1]],t[i1[2]],t[i1[3]],t[i1[4]],t[i1[5]],t[i1[6]],t[i1[7]]);
 }
 
-static inline Vec4q lookup4(Vec4q const & index, Vec4q const & table) {
+static inline Vec4q lookup4(Vec4q const index, Vec4q const table) {
     return lookup8(Vec8i(index * 0x200000002ll + 0x100000000ll), Vec8i(table));
 }
 
 template <int n>
-static inline Vec4q lookup(Vec4q const & index, void const * table) {
-    if (n <= 0) return 0;
+static inline Vec4q lookup(Vec4q const index, void const * table) {
+    if constexpr (n <= 0) return 0;
     // n > 0. Limit index
     Vec4uq index1;
-    if ((n & (n-1)) == 0) {
+    if constexpr (n == INT_MAX) {
+        index1 = index;
+    }
+    else if constexpr ((n & (n-1)) == 0) {
         // n is a power of 2, make index modulo n
         index1 = Vec4uq(index) & (n-1);
     }
@@ -3794,42 +3399,45 @@ static inline Vec4q lookup(Vec4q const & index, void const * table) {
         // n is not a power of 2, limit to n-1.
         // There is no 64-bit min instruction, but we can use the 32-bit unsigned min,
         // since n is a 32-bit integer
-        index1 = Vec4uq(min(Vec8ui(index), constant8i<n-1, 0, n-1, 0, n-1, 0, n-1, 0>()));
+        index1 = Vec4uq(min(Vec8ui(index), Vec8ui(n-1, 0, n-1, 0, n-1, 0, n-1, 0)));
     }
     uint32_t ii[8];  index1.store(ii);  // use only lower 32 bits of each index
     int64_t const * tt = (int64_t const *)table;
-    return Vec4q(tt[ii[0]], tt[ii[2]], tt[ii[4]], tt[ii[6]]);    
+    return Vec4q(tt[ii[0]], tt[ii[2]], tt[ii[4]], tt[ii[6]]);
 }
 
 
 /*****************************************************************************
 *
-*          Other permutations with variable indexes
+*          Byte shifts
 *
 *****************************************************************************/
 
 // Function shift_bytes_up: shift whole vector left by b bytes.
-// You may use a permute function instead if b is a compile-time constant
-static inline Vec32c shift_bytes_up(Vec32c const & a, int b) {
-    if (b < 16) {    
-        return Vec32c(shift_bytes_up(a.get_low(),b), shift_bytes_up(a.get_high(),b) | shift_bytes_down(a.get_low(),16-b));
-    }
-    else {
-        return Vec32c(Vec16c(0), shift_bytes_up(a.get_high(),b-16));
+template <unsigned int b>
+static inline Vec32c shift_bytes_up(Vec32c const a) {
+    int8_t dat[64];
+    if (b < 32) {
+        Vec32c(0).store(dat);
+        a.store(dat+b);
+        return Vec32c().load(dat);
     }
+    else return 0;
 }
 
 // Function shift_bytes_down: shift whole vector right by b bytes
-// You may use a permute function instead if b is a compile-time constant
-static inline Vec32c shift_bytes_down(Vec32c const & a, int b) {
-    if (b < 16) {    
-        return Vec32c(shift_bytes_down(a.get_low(),b) | shift_bytes_up(a.get_high(),16-b), shift_bytes_down(a.get_high(),b));
-    }
-    else {
-        return Vec32c(shift_bytes_down(a.get_high(),b-16), Vec16c(0));
+template <unsigned int b>
+static inline Vec32c shift_bytes_down(Vec32c const a) {
+    int8_t dat[64];
+    if (b < 32) {
+        a.store(dat);
+        Vec32c(0).store(dat+32);
+        return Vec32c().load(dat+b);
     }
+    else return 0;
 }
 
+
 /*****************************************************************************
 *
 *          Gather functions with fixed indexes
@@ -3838,35 +3446,24 @@ static inline Vec32c shift_bytes_down(Vec32c const & a, int b) {
 // Load elements from array a with indices i0, i1, i2, i3, i4, i5, i6, i7
 template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7>
 static inline Vec8i gather8i(void const * a) {
-    Static_error_check<(i0|i1|i2|i3|i4|i5|i6|i7)>=0> Negative_array_index;  // Error message if index is negative
-    const int i01min = i0 < i1 ? i0 : i1;
-    const int i23min = i2 < i3 ? i2 : i3;
-    const int i45min = i4 < i5 ? i4 : i5;
-    const int i67min = i6 < i7 ? i6 : i7;
-    const int i0123min = i01min < i23min ? i01min : i23min;
-    const int i4567min = i45min < i67min ? i45min : i67min;
-    const int imin = i0123min < i4567min ? i0123min : i4567min;
-    const int i01max = i0 > i1 ? i0 : i1;
-    const int i23max = i2 > i3 ? i2 : i3;
-    const int i45max = i4 > i5 ? i4 : i5;
-    const int i67max = i6 > i7 ? i6 : i7;
-    const int i0123max = i01max > i23max ? i01max : i23max;
-    const int i4567max = i45max > i67max ? i45max : i67max;
-    const int imax = i0123max > i4567max ? i0123max : i4567max;
-
-    if (imax - imin <= 7) {
+    int constexpr indexs[8] = { i0, i1, i2, i3, i4, i5, i6, i7 }; // indexes as array
+    constexpr int imin = min_index(indexs);
+    constexpr int imax = max_index(indexs);
+    static_assert(imin >= 0, "Negative index in gather function");
+
+    if constexpr (imax - imin <= 7) {
         // load one contiguous block and permute
-        if (imax > 7) {
+        if constexpr (imax > 7) {
             // make sure we don't read past the end of the array
             Vec8i b = Vec8i().load((int32_t const *)a + imax-7);
-            return permute8i<i0-imax+7, i1-imax+7, i2-imax+7, i3-imax+7, i4-imax+7, i5-imax+7, i6-imax+7, i7-imax+7>(b);
+            return permute8<i0-imax+7, i1-imax+7, i2-imax+7, i3-imax+7, i4-imax+7, i5-imax+7, i6-imax+7, i7-imax+7>(b);
         }
         else {
             Vec8i b = Vec8i().load((int32_t const *)a + imin);
-            return permute8i<i0-imin, i1-imin, i2-imin, i3-imin, i4-imin, i5-imin, i6-imin, i7-imin>(b);
+            return permute8<i0-imin, i1-imin, i2-imin, i3-imin, i4-imin, i5-imin, i6-imin, i7-imin>(b);
         }
     }
-    if ((i0<imin+8 || i0>imax-8) && (i1<imin+8 || i1>imax-8) && (i2<imin+8 || i2>imax-8) && (i3<imin+8 || i3>imax-8)
+    if constexpr ((i0<imin+8 || i0>imax-8) && (i1<imin+8 || i1>imax-8) && (i2<imin+8 || i2>imax-8) && (i3<imin+8 || i3>imax-8)
     &&  (i4<imin+8 || i4>imax-8) && (i5<imin+8 || i5>imax-8) && (i6<imin+8 || i6>imax-8) && (i7<imin+8 || i7>imax-8)) {
         // load two contiguous blocks and blend
         Vec8i b = Vec8i().load((int32_t const *)a + imin);
@@ -3879,7 +3476,7 @@ static inline Vec8i gather8i(void const * a) {
         const int j5 = i5<imin+8 ? i5-imin : 15-imax+i5;
         const int j6 = i6<imin+8 ? i6-imin : 15-imax+i6;
         const int j7 = i7<imin+8 ? i7-imin : 15-imax+i7;
-        return blend8i<j0, j1, j2, j3, j4, j5, j6, j7>(b, c);
+        return blend8<j0, j1, j2, j3, j4, j5, j6, j7>(b, c);
     }
     // use lookup function
     return lookup<imax+1>(Vec8i(i0,i1,i2,i3,i4,i5,i6,i7), a);
@@ -3887,26 +3484,24 @@ static inline Vec8i gather8i(void const * a) {
 
 template <int i0, int i1, int i2, int i3>
 static inline Vec4q gather4q(void const * a) {
-    Static_error_check<(i0|i1|i2|i3)>=0> Negative_array_index;  // Error message if index is negative
-    const int i01min = i0 < i1 ? i0 : i1;
-    const int i23min = i2 < i3 ? i2 : i3;
-    const int imin   = i01min < i23min ? i01min : i23min;
-    const int i01max = i0 > i1 ? i0 : i1;
-    const int i23max = i2 > i3 ? i2 : i3;
-    const int imax   = i01max > i23max ? i01max : i23max;
-    if (imax - imin <= 3) {
+    int constexpr indexs[4] = { i0, i1, i2, i3 }; // indexes as array
+    constexpr int imin = min_index(indexs);
+    constexpr int imax = max_index(indexs);
+    static_assert(imin >= 0, "Negative index in gather function");
+
+    if constexpr (imax - imin <= 3) {
         // load one contiguous block and permute
-        if (imax > 3) {
+        if constexpr (imax > 3) {
             // make sure we don't read past the end of the array
             Vec4q b = Vec4q().load((int64_t const *)a + imax-3);
-            return permute4q<i0-imax+3, i1-imax+3, i2-imax+3, i3-imax+3>(b);
+            return permute4<i0-imax+3, i1-imax+3, i2-imax+3, i3-imax+3>(b);
         }
         else {
             Vec4q b = Vec4q().load((int64_t const *)a + imin);
-            return permute4q<i0-imin, i1-imin, i2-imin, i3-imin>(b);
+            return permute4<i0-imin, i1-imin, i2-imin, i3-imin>(b);
         }
     }
-    if ((i0<imin+4 || i0>imax-4) && (i1<imin+4 || i1>imax-4) && (i2<imin+4 || i2>imax-4) && (i3<imin+4 || i3>imax-4)) {
+    if constexpr ((i0<imin+4 || i0>imax-4) && (i1<imin+4 || i1>imax-4) && (i2<imin+4 || i2>imax-4) && (i3<imin+4 || i3>imax-4)) {
         // load two contiguous blocks and blend
         Vec4q b = Vec4q().load((int64_t const *)a + imin);
         Vec4q c = Vec4q().load((int64_t const *)a + imax-3);
@@ -3914,7 +3509,7 @@ static inline Vec4q gather4q(void const * a) {
         const int j1 = i1<imin+4 ? i1-imin : 7-imax+i1;
         const int j2 = i2<imin+4 ? i2-imin : 7-imax+i2;
         const int j3 = i3<imin+4 ? i3-imin : 7-imax+i3;
-        return blend4q<j0, j1, j2, j3>(b, c);
+        return blend4<j0, j1, j2, j3>(b, c);
     }
     // use lookup function
     return lookup<imax+1>(Vec4q(i0,i1,i2,i3), a);
@@ -3927,27 +3522,16 @@ static inline Vec4q gather4q(void const * a) {
 ******************************************************************************
 *
 * These functions write the elements of a vector to arbitrary positions in an
-* array in memory. Each vector element is written to an array position 
+* array in memory. Each vector element is written to an array position
 * determined by an index. An element is not written if the corresponding
 * index is out of range.
 * The indexes can be specified as constant template parameters or as an
 * integer vector.
-* 
-* The scatter functions are useful if the data are distributed in a sparce
-* manner into the array. If the array is dense then it is more efficient
-* to permute the data into the right positions and then write the whole
-* permuted vector into the array.
-*
-* Example:
-* Vec8q a(10,11,12,13,14,15,16,17);
-* int64_t b[16] = {0};
-* scatter<0,2,14,10,1,-1,5,9>(a,b); 
-* // Now, b = {10,14,11,0,0,16,0,0,0,17,13,0,0,0,12,0}
 *
 *****************************************************************************/
 
 template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7>
-static inline void scatter(Vec8i const & data, void * array) {
+static inline void scatter(Vec8i const data, void * array) {
     int32_t* arr = (int32_t*)array;
     const int index[8] = {i0,i1,i2,i3,i4,i5,i6,i7};
     for (int i = 0; i < 8; i++) {
@@ -3956,7 +3540,7 @@ static inline void scatter(Vec8i const & data, void * array) {
 }
 
 template <int i0, int i1, int i2, int i3>
-static inline void scatter(Vec4q const & data, void * array) {
+static inline void scatter(Vec4q const data, void * array) {
     int64_t* arr = (int64_t*)array;
     const int index[4] = {i0,i1,i2,i3};
     for (int i = 0; i < 4; i++) {
@@ -3964,96 +3548,98 @@ static inline void scatter(Vec4q const & data, void * array) {
     }
 }
 
-static inline void scatter(Vec8i const & index, uint32_t limit, Vec8i const & data, void * array) {
-    int32_t* arr = (int32_t*)array;
+// scatter functions with variable indexes
+
+static inline void scatter(Vec8i const index, uint32_t limit, Vec8i const data, void * destination) {
+    int32_t* arr = (int32_t*)destination;
     for (int i = 0; i < 8; i++) {
         if (uint32_t(index[i]) < limit) arr[index[i]] = data[i];
     }
 }
 
-static inline void scatter(Vec4q const & index, uint32_t limit, Vec4q const & data, void * array) {
-    int64_t* arr = (int64_t*)array;
+static inline void scatter(Vec4q const index, uint32_t limit, Vec4q const data, void * destination) {
+    int64_t* arr = (int64_t*)destination;
     for (int i = 0; i < 4; i++) {
         if (uint64_t(index[i]) < uint64_t(limit)) arr[index[i]] = data[i];
     }
-} 
+}
 
-static inline void scatter(Vec4i const & index, uint32_t limit, Vec4q const & data, void * array) {
-    int64_t* arr = (int64_t*)array;
+static inline void scatter(Vec4i const index, uint32_t limit, Vec4q const data, void * destination) {
+    int64_t* arr = (int64_t*)destination;
     for (int i = 0; i < 4; i++) {
         if (uint32_t(index[i]) < limit) arr[index[i]] = data[i];
     }
-} 
+}
 
 /*****************************************************************************
 *
-*          Functions for conversion between integer sizes
+*          Functions for conversion between integer sizes and vector types
 *
 *****************************************************************************/
 
 // Extend 8-bit integers to 16-bit integers, signed and unsigned
 
 // Function extend_low : extends the low 16 elements to 16 bits with sign extension
-static inline Vec16s extend_low (Vec32c const & a) {
+static inline Vec16s extend_low (Vec32c const a) {
     return Vec16s(extend_low(a.get_low()), extend_high(a.get_low()));
 }
 
 // Function extend_high : extends the high 16 elements to 16 bits with sign extension
-static inline Vec16s extend_high (Vec32c const & a) {
+static inline Vec16s extend_high (Vec32c const a) {
     return Vec16s(extend_low(a.get_high()), extend_high(a.get_high()));
 }
 
 // Function extend_low : extends the low 16 elements to 16 bits with zero extension
-static inline Vec16us extend_low (Vec32uc const & a) {
+static inline Vec16us extend_low (Vec32uc const a) {
     return Vec16us(extend_low(a.get_low()), extend_high(a.get_low()));
 }
 
 // Function extend_high : extends the high 19 elements to 16 bits with zero extension
-static inline Vec16us extend_high (Vec32uc const & a) {
+static inline Vec16us extend_high (Vec32uc const a) {
     return Vec16us(extend_low(a.get_high()), extend_high(a.get_high()));
 }
 
 // Extend 16-bit integers to 32-bit integers, signed and unsigned
 
 // Function extend_low : extends the low 8 elements to 32 bits with sign extension
-static inline Vec8i extend_low (Vec16s const & a) {
+static inline Vec8i extend_low (Vec16s const a) {
     return Vec8i(extend_low(a.get_low()), extend_high(a.get_low()));
 }
 
 // Function extend_high : extends the high 8 elements to 32 bits with sign extension
-static inline Vec8i extend_high (Vec16s const & a) {
+static inline Vec8i extend_high (Vec16s const a) {
     return Vec8i(extend_low(a.get_high()), extend_high(a.get_high()));
 }
 
 // Function extend_low : extends the low 8 elements to 32 bits with zero extension
-static inline Vec8ui extend_low (Vec16us const & a) {
+static inline Vec8ui extend_low (Vec16us const a) {
     return Vec8ui(extend_low(a.get_low()), extend_high(a.get_low()));
 }
 
 // Function extend_high : extends the high 8 elements to 32 bits with zero extension
-static inline Vec8ui extend_high (Vec16us const & a) {
+static inline Vec8ui extend_high (Vec16us const a) {
     return Vec8ui(extend_low(a.get_high()), extend_high(a.get_high()));
 }
 
 // Extend 32-bit integers to 64-bit integers, signed and unsigned
 
 // Function extend_low : extends the low 4 elements to 64 bits with sign extension
-static inline Vec4q extend_low (Vec8i const & a) {
+static inline Vec4q extend_low (Vec8i const a) {
     return Vec4q(extend_low(a.get_low()), extend_high(a.get_low()));
 }
 
 // Function extend_high : extends the high 4 elements to 64 bits with sign extension
-static inline Vec4q extend_high (Vec8i const & a) {
+static inline Vec4q extend_high (Vec8i const a) {
     return Vec4q(extend_low(a.get_high()), extend_high(a.get_high()));
 }
 
 // Function extend_low : extends the low 4 elements to 64 bits with zero extension
-static inline Vec4uq extend_low (Vec8ui const & a) {
+static inline Vec4uq extend_low (Vec8ui const a) {
     return Vec4uq(extend_low(a.get_low()), extend_high(a.get_low()));
 }
 
 // Function extend_high : extends the high 4 elements to 64 bits with zero extension
-static inline Vec4uq extend_high (Vec8ui const & a) {
+static inline Vec4uq extend_high (Vec8ui const a) {
     return Vec4uq(extend_low(a.get_high()), extend_high(a.get_high()));
 }
 
@@ -4061,25 +3647,25 @@ static inline Vec4uq extend_high (Vec8ui const & a) {
 
 // Function compress : packs two vectors of 16-bit integers into one vector of 8-bit integers
 // Overflow wraps around
-static inline Vec32c compress (Vec16s const & low, Vec16s const & high) {
+static inline Vec32c compress (Vec16s const low, Vec16s const high) {
     return Vec32c(compress(low.get_low(),low.get_high()), compress(high.get_low(),high.get_high()));
 }
 
 // Function compress : packs two vectors of 16-bit integers into one vector of 8-bit integers
 // Signed, with saturation
-static inline Vec32c compress_saturated (Vec16s const & low, Vec16s const & high) {
+static inline Vec32c compress_saturated (Vec16s const low, Vec16s const high) {
     return Vec32c(compress_saturated(low.get_low(),low.get_high()), compress_saturated(high.get_low(),high.get_high()));
 }
 
 // Function compress : packs two vectors of 16-bit integers to one vector of 8-bit integers
 // Unsigned, overflow wraps around
-static inline Vec32uc compress (Vec16us const & low, Vec16us const & high) {
+static inline Vec32uc compress (Vec16us const low, Vec16us const high) {
     return Vec32uc(compress(low.get_low(),low.get_high()), compress(high.get_low(),high.get_high()));
 }
 
 // Function compress : packs two vectors of 16-bit integers into one vector of 8-bit integers
 // Unsigned, with saturation
-static inline Vec32uc compress_saturated (Vec16us const & low, Vec16us const & high) {
+static inline Vec32uc compress_saturated (Vec16us const low, Vec16us const high) {
     return Vec32uc(compress_saturated(low.get_low(),low.get_high()), compress_saturated(high.get_low(),high.get_high()));
 }
 
@@ -4087,25 +3673,25 @@ static inline Vec32uc compress_saturated (Vec16us const & low, Vec16us const & h
 
 // Function compress : packs two vectors of 32-bit integers into one vector of 16-bit integers
 // Overflow wraps around
-static inline Vec16s compress (Vec8i const & low, Vec8i const & high) {
+static inline Vec16s compress (Vec8i const low, Vec8i const high) {
     return Vec16s(compress(low.get_low(),low.get_high()), compress(high.get_low(),high.get_high()));
 }
 
 // Function compress : packs two vectors of 32-bit integers into one vector of 16-bit integers
 // Signed with saturation
-static inline Vec16s compress_saturated (Vec8i const & low, Vec8i const & high) {
+static inline Vec16s compress_saturated (Vec8i const low, Vec8i const high) {
     return Vec16s(compress_saturated(low.get_low(),low.get_high()), compress_saturated(high.get_low(),high.get_high()));
 }
 
 // Function compress : packs two vectors of 32-bit integers into one vector of 16-bit integers
 // Overflow wraps around
-static inline Vec16us compress (Vec8ui const & low, Vec8ui const & high) {
+static inline Vec16us compress (Vec8ui const low, Vec8ui const high) {
     return Vec16us(compress(low.get_low(),low.get_high()), compress(high.get_low(),high.get_high()));
 }
 
 // Function compress : packs two vectors of 32-bit integers into one vector of 16-bit integers
 // Unsigned, with saturation
-static inline Vec16us compress_saturated (Vec8ui const & low, Vec8ui const & high) {
+static inline Vec16us compress_saturated (Vec8ui const low, Vec8ui const high) {
     return Vec16us(compress_saturated(low.get_low(),low.get_high()), compress_saturated(high.get_low(),high.get_high()));
 }
 
@@ -4113,28 +3699,68 @@ static inline Vec16us compress_saturated (Vec8ui const & low, Vec8ui const & hig
 
 // Function compress : packs two vectors of 64-bit integers into one vector of 32-bit integers
 // Overflow wraps around
-static inline Vec8i compress (Vec4q const & low, Vec4q const & high) {
+static inline Vec8i compress (Vec4q const low, Vec4q const high) {
     return Vec8i(compress(low.get_low(),low.get_high()), compress(high.get_low(),high.get_high()));
 }
 
 // Function compress : packs two vectors of 64-bit integers into one vector of 32-bit integers
 // Signed, with saturation
-static inline Vec8i compress_saturated (Vec4q const & low, Vec4q const & high) {
+static inline Vec8i compress_saturated (Vec4q const low, Vec4q const high) {
     return Vec8i(compress_saturated(low.get_low(),low.get_high()), compress_saturated(high.get_low(),high.get_high()));
 }
 
 // Function compress : packs two vectors of 32-bit integers into one vector of 16-bit integers
 // Overflow wraps around
-static inline Vec8ui compress (Vec4uq const & low, Vec4uq const & high) {
+static inline Vec8ui compress (Vec4uq const low, Vec4uq const high) {
     return Vec8ui (compress((Vec4q)low, (Vec4q)high));
 }
 
 // Function compress : packs two vectors of 64-bit integers into one vector of 32-bit integers
 // Unsigned, with saturation
-static inline Vec8ui compress_saturated (Vec4uq const & low, Vec4uq const & high) {
+static inline Vec8ui compress_saturated (Vec4uq const low, Vec4uq const high) {
     return Vec8ui(compress_saturated(low.get_low(),low.get_high()), compress_saturated(high.get_low(),high.get_high()));
 }
 
+// extend vectors to double size by adding zeroes
+static inline Vec32c extend_z(Vec16c a) {
+    return Vec32c(a, _mm_setzero_si128());
+}
+static inline Vec32uc extend_z(Vec16uc a) {
+    return Vec32uc(a, _mm_setzero_si128());
+}
+static inline Vec16s extend_z(Vec8s a) {
+    return Vec16s(a, _mm_setzero_si128());
+}
+static inline Vec16us extend_z(Vec8us a) {
+    return Vec16us(a, _mm_setzero_si128());
+}
+static inline Vec8i extend_z(Vec4i a) {
+    return Vec8i(a, _mm_setzero_si128());
+}
+static inline Vec8ui extend_z(Vec4ui a) {
+    return Vec8ui(a, _mm_setzero_si128());
+}
+static inline Vec4q extend_z(Vec2q a) {
+    return Vec4q(a, _mm_setzero_si128());
+}
+static inline Vec4uq extend_z(Vec2uq a) {
+    return Vec4uq(a, _mm_setzero_si128());
+} 
+
+static inline Vec32cb extend_z(Vec16cb a) {
+    return Vec32cb(a, _mm_setzero_si128());
+}
+static inline Vec16sb extend_z(Vec8sb a) {
+    return Vec16sb(a, _mm_setzero_si128());
+}
+static inline Vec8ib extend_z(Vec4ib a) {
+    return Vec8ib(a, _mm_setzero_si128());
+}
+static inline Vec4qb extend_z(Vec2qb a) {
+    return Vec4qb(a, _mm_setzero_si128());
+}
+
+
 
 /*****************************************************************************
 *
@@ -4144,20 +3770,20 @@ static inline Vec8ui compress_saturated (Vec4uq const & low, Vec4uq const & high
 
 // Divide Vec8i by compile-time constant
 template <int32_t d>
-static inline Vec8i divide_by_i(Vec8i const & a) {
-    return Vec8i( divide_by_i<d>(a.get_low()), divide_by_i<d>(a.get_high()));
+static inline Vec8i divide_by_i(Vec8i const a) {
+    return Vec8i(divide_by_i<d>(a.get_low()), divide_by_i<d>(a.get_high()));
 }
 
 // define Vec8i a / const_int(d)
 template <int32_t d>
-static inline Vec8i operator / (Vec8i const & a, Const_int_t<d>) {
+static inline Vec8i operator / (Vec8i const a, Const_int_t<d>) {
     return divide_by_i<d>(a);
 }
 
 // define Vec8i a / const_uint(d)
 template <uint32_t d>
-static inline Vec8i operator / (Vec8i const & a, Const_uint_t<d>) {
-    Static_error_check< (d<0x80000000u) > Error_overflow_dividing_signed_by_unsigned; // Error: dividing signed by overflowing unsigned
+static inline Vec8i operator / (Vec8i const a, Const_uint_t<d>) {
+    static_assert(d < 0x80000000u, "Dividing signed integer by overflowing unsigned");
     return divide_by_i<int32_t(d)>(a);                               // signed divide
 }
 
@@ -4178,20 +3804,20 @@ static inline Vec8i & operator /= (Vec8i & a, Const_uint_t<d> b) {
 
 // Divide Vec8ui by compile-time constant
 template <uint32_t d>
-static inline Vec8ui divide_by_ui(Vec8ui const & a) {
+static inline Vec8ui divide_by_ui(Vec8ui const a) {
     return Vec8ui( divide_by_ui<d>(a.get_low()), divide_by_ui<d>(a.get_high()));
 }
 
 // define Vec8ui a / const_uint(d)
 template <uint32_t d>
-static inline Vec8ui operator / (Vec8ui const & a, Const_uint_t<d>) {
+static inline Vec8ui operator / (Vec8ui const a, Const_uint_t<d>) {
     return divide_by_ui<d>(a);
 }
 
 // define Vec8ui a / const_int(d)
 template <int32_t d>
-static inline Vec8ui operator / (Vec8ui const & a, Const_int_t<d>) {
-    Static_error_check< (d>=0) > Error_dividing_unsigned_by_negative;// Error: dividing unsigned by negative is ambiguous
+static inline Vec8ui operator / (Vec8ui const a, Const_int_t<d>) {
+    static_assert(d >= 0, "Dividing unsigned integer by negative is ambiguous");
     return divide_by_ui<d>(a);                                       // unsigned divide
 }
 
@@ -4209,23 +3835,22 @@ static inline Vec8ui & operator /= (Vec8ui & a, Const_int_t<d> b) {
     return a;
 }
 
-
-// Divide Vec16s by compile-time constant 
+// Divide Vec16s by compile-time constant
 template <int d>
-static inline Vec16s divide_by_i(Vec16s const & a) {
+static inline Vec16s divide_by_i(Vec16s const a) {
     return Vec16s( divide_by_i<d>(a.get_low()), divide_by_i<d>(a.get_high()));
 }
 
 // define Vec16s a / const_int(d)
 template <int d>
-static inline Vec16s operator / (Vec16s const & a, Const_int_t<d>) {
+static inline Vec16s operator / (Vec16s const a, Const_int_t<d>) {
     return divide_by_i<d>(a);
 }
 
 // define Vec16s a / const_uint(d)
 template <uint32_t d>
-static inline Vec16s operator / (Vec16s const & a, Const_uint_t<d>) {
-    Static_error_check< (d<0x8000u) > Error_overflow_dividing_signed_by_unsigned; // Error: dividing signed by overflowing unsigned
+static inline Vec16s operator / (Vec16s const a, Const_uint_t<d>) {
+    static_assert(d < 0x8000u, "Dividing signed integer by overflowing unsigned");
     return divide_by_i<int(d)>(a);                                   // signed divide
 }
 
@@ -4243,23 +3868,22 @@ static inline Vec16s & operator /= (Vec16s & a, Const_uint_t<d> b) {
     return a;
 }
 
-
 // Divide Vec16us by compile-time constant
 template <uint32_t d>
-static inline Vec16us divide_by_ui(Vec16us const & a) {
+static inline Vec16us divide_by_ui(Vec16us const a) {
     return Vec16us( divide_by_ui<d>(a.get_low()), divide_by_ui<d>(a.get_high()));
 }
 
 // define Vec16us a / const_uint(d)
 template <uint32_t d>
-static inline Vec16us operator / (Vec16us const & a, Const_uint_t<d>) {
+static inline Vec16us operator / (Vec16us const a, Const_uint_t<d>) {
     return divide_by_ui<d>(a);
 }
 
 // define Vec16us a / const_int(d)
 template <int d>
-static inline Vec16us operator / (Vec16us const & a, Const_int_t<d>) {
-    Static_error_check< (d>=0) > Error_dividing_unsigned_by_negative;// Error: dividing unsigned by negative is ambiguous
+static inline Vec16us operator / (Vec16us const a, Const_int_t<d>) {
+    static_assert(d >= 0, "Dividing unsigned integer by negative is ambiguous");
     return divide_by_ui<d>(a);                                       // unsigned divide
 }
 
@@ -4277,10 +3901,9 @@ static inline Vec16us & operator /= (Vec16us & a, Const_int_t<d> b) {
     return a;
 }
 
-
 // define Vec32c a / const_int(d)
 template <int d>
-static inline Vec32c operator / (Vec32c const & a, Const_int_t<d>) {
+static inline Vec32c operator / (Vec32c const a, Const_int_t<d>) {
     // expand into two Vec16s
     Vec16s low  = extend_low(a)  / Const_int_t<d>();
     Vec16s high = extend_high(a) / Const_int_t<d>();
@@ -4289,8 +3912,8 @@ static inline Vec32c operator / (Vec32c const & a, Const_int_t<d>) {
 
 // define Vec32c a / const_uint(d)
 template <uint32_t d>
-static inline Vec32c operator / (Vec32c const & a, Const_uint_t<d>) {
-    Static_error_check< (uint8_t(d)<0x80u) > Error_overflow_dividing_signed_by_unsigned; // Error: dividing signed by overflowing unsigned
+static inline Vec32c operator / (Vec32c const a, Const_uint_t<d>) {
+    static_assert(uint8_t(d) < 0x80u, "Dividing signed integer by overflowing unsigned");
     return a / Const_int_t<d>();                                     // signed divide
 }
 
@@ -4309,7 +3932,7 @@ static inline Vec32c & operator /= (Vec32c & a, Const_uint_t<d> b) {
 
 // define Vec32uc a / const_uint(d)
 template <uint32_t d>
-static inline Vec32uc operator / (Vec32uc const & a, Const_uint_t<d>) {
+static inline Vec32uc operator / (Vec32uc const a, Const_uint_t<d>) {
     // expand into two Vec16us
     Vec16us low  = extend_low(a)  / Const_uint_t<d>();
     Vec16us high = extend_high(a) / Const_uint_t<d>();
@@ -4318,8 +3941,8 @@ static inline Vec32uc operator / (Vec32uc const & a, Const_uint_t<d>) {
 
 // define Vec32uc a / const_int(d)
 template <int d>
-static inline Vec32uc operator / (Vec32uc const & a, Const_int_t<d>) {
-    Static_error_check< (int8_t(d)>=0) > Error_dividing_unsigned_by_negative;// Error: dividing unsigned by negative is ambiguous
+static inline Vec32uc operator / (Vec32uc const a, Const_int_t<d>) {
+    static_assert(int8_t(d) >= 0, "Dividing unsigned integer by negative is ambiguous");
     return a / Const_uint_t<d>();                                    // unsigned divide
 }
 
@@ -4337,50 +3960,6 @@ static inline Vec32uc & operator /= (Vec32uc & a, Const_int_t<d> b) {
     return a;
 }
 
-/*****************************************************************************
-*
-*          Horizontal scan functions
-*
-*****************************************************************************/
-
-// Get index to the first element that is true. Return -1 if all are false
-static inline int horizontal_find_first(Vec32cb const & x) {
-    int a1 = horizontal_find_first(x.get_low());
-    if (a1 >= 0) return a1;
-    int a2 = horizontal_find_first(x.get_high());
-    if (a2 < 0) return a2;
-    return a2 + 16;;
-}
-
-static inline int horizontal_find_first(Vec16sb const & x) {
-    return horizontal_find_first(Vec32cb(x)) >> 1;
-}
-
-static inline int horizontal_find_first(Vec8ib const & x) {
-    return horizontal_find_first(Vec32cb(x)) >> 2;
-}
-
-static inline int horizontal_find_first(Vec4qb const & x) {
-    return horizontal_find_first(Vec32cb(x)) >> 3;
-}
-
-// Count the number of elements that are true
-static inline uint32_t horizontal_count(Vec32cb const & x) {
-    return horizontal_count(x.get_low()) + horizontal_count(x.get_high());
-}
-
-static inline uint32_t horizontal_count(Vec16sb const & x) {
-    return horizontal_count(Vec32cb(x)) >> 1;
-}
-
-static inline uint32_t horizontal_count(Vec8ib const & x) {
-    return horizontal_count(Vec32cb(x)) >> 2;
-}
-
-static inline uint32_t horizontal_count(Vec4qb const & x) {
-    return horizontal_count(Vec32cb(x)) >> 3;
-}
-
 /*****************************************************************************
 *
 *          Boolean <-> bitfield conversion functions
@@ -4388,47 +3967,27 @@ static inline uint32_t horizontal_count(Vec4qb const & x) {
 *****************************************************************************/
 
 // to_bits: convert boolean vector to integer bitfield
-static inline uint32_t to_bits(Vec32cb const & x) {
+static inline uint32_t to_bits(Vec32cb const x) {
     return to_bits(x.get_low()) | (uint32_t)to_bits(x.get_high()) << 16;
 }
 
-// to_Vec16c: convert integer bitfield to boolean vector
-static inline Vec32cb to_Vec32cb(uint32_t x) {
-    return Vec32c(to_Vec16cb(uint16_t(x)), to_Vec16cb(uint16_t(x>>16)));
-}
-
 // to_bits: convert boolean vector to integer bitfield
-static inline uint16_t to_bits(Vec16sb const & x) {
-    return to_bits(x.get_low()) | (uint16_t)to_bits(x.get_high()) << 8;
-}
-
-// to_Vec16sb: convert integer bitfield to boolean vector
-static inline Vec16sb to_Vec16sb(uint16_t x) {
-    return Vec16s(to_Vec8sb(uint8_t(x)), to_Vec8sb(uint8_t(x>>8)));
+static inline uint16_t to_bits(Vec16sb const x) {
+    return uint16_t(to_bits(x.get_low()) | (uint16_t)to_bits(x.get_high()) << 8);
 }
 
 // to_bits: convert boolean vector to integer bitfield
-static inline uint8_t to_bits(Vec8ib const & x) {
-    return to_bits(x.get_low()) | (uint8_t)to_bits(x.get_high()) << 4;
-}
-
-// to_Vec8ib: convert integer bitfield to boolean vector
-static inline Vec8ib to_Vec8ib(uint8_t x) {
-    return Vec8i(to_Vec4ib(x), to_Vec4ib(x>>4));
+static inline uint8_t to_bits(Vec8ib const x) {
+    return uint8_t(to_bits(x.get_low()) | (uint8_t)to_bits(x.get_high()) << 4);
 }
 
 // to_bits: convert boolean vector to integer bitfield
-static inline uint8_t to_bits(Vec4qb const & x) {
-    return to_bits(x.get_low()) | to_bits(x.get_high()) << 2;
-}
-
-// to_Vec16c: convert integer bitfield to boolean vector
-static inline Vec4qb to_Vec4qb(uint8_t x) {
-    return Vec4q(to_Vec2qb(x), to_Vec2qb(x>>2));
+static inline uint8_t to_bits(Vec4qb const x) {
+    return uint8_t(to_bits(x.get_low()) | to_bits(x.get_high()) << 2);
 }
 
 #ifdef VCL_NAMESPACE
 }
 #endif
 
-#endif // VECTORI256_H
+#endif // VECTORI256E_H
diff --git a/EEDI3/vectorclass/vectori512.h b/EEDI3/vectorclass/vectori512.h
index 9628153..c02431c 100644
--- a/EEDI3/vectorclass/vectori512.h
+++ b/EEDI3/vectorclass/vectori512.h
@@ -1,16 +1,14 @@
 /****************************  vectori512.h   *******************************
 * Author:        Agner Fog
 * Date created:  2014-07-23
-* Last modified: 2017-02-19
-* Version:       1.27
-* Project:       vector classes
+* Last modified: 2023-06-03
+* Version:       2.02.01
+* Project:       vector class library
 * Description:
-* Header file defining integer vector classes as interface to intrinsic 
-* functions in x86 microprocessors with AVX512 and later instruction sets.
+* Header file defining 512-bit integer vector classes for 32 and 64 bit integers.
+* For x86 microprocessors with AVX512F and later instruction sets.
 *
-* Instructions:
-* Use Gnu, Intel or Microsoft C++ compiler. Compile for the desired 
-* instruction set, which must be at least AVX512. 
+* Instructions: see vcl_manual.pdf
 *
 * The following vector classes are defined here:
 * Vec16i    Vector of  16  32-bit signed   integers
@@ -19,547 +17,67 @@
 * Vec8q     Vector of   8  64-bit signed   integers
 * Vec8uq    Vector of   8  64-bit unsigned integers
 * Vec8qb    Vector of   8  Booleans for use with Vec8q and Vec8uq
+* Other 512-bit integer vectors are defined in Vectori512s.h
 *
 * Each vector object is represented internally in the CPU as a 512-bit register.
 * This header file defines operators and functions for these vectors.
 *
-* For detailed instructions, see VectorClass.pdf
-*
-* (c) Copyright 2014-2017 GNU General Public License http://www.gnu.org/licenses
+* (c) Copyright 2012-2023 Agner Fog.
+* Apache License version 2.0 or later.
 *****************************************************************************/
 
-// check combination of header files
-#if defined (VECTORI512_H)
-#if    VECTORI512_H != 2
-#error Two different versions of vectori512.h included
-#endif
-#else
-#define VECTORI512_H  2
+#ifndef VECTORI512_H
+#define VECTORI512_H
 
-#ifdef VECTORF512_H
-#error Please put header file vectori512.h before vectorf512.h
+#ifndef VECTORCLASS_H
+#include "vectorclass.h"
 #endif
 
+#if VECTORCLASS_H < 20200
+#error Incompatible versions of vector class library mixed
+#endif
 
-#if INSTRSET < 9   // AVX512 required
-#error Wrong instruction set for vectori512.h, AVX512 required or use vectori512e.h
+// check combination of header files
+#ifdef VECTORI512E_H
+#error Two different versions of vectori512.h included
 #endif
 
-#include "vectori256.h"
 
 #ifdef VCL_NAMESPACE
 namespace VCL_NAMESPACE {
 #endif
 
-// Bug fix for missing intrinsics:
-// _mm512_cmpgt_epu32_mask, _mm512_cmpgt_epu64_mask
-// all typecast intrinsics
-// Fix expected in GCC version 4.9.3 or 5.0. https://gcc.gnu.org/bugzilla/show_bug.cgi?id=61878
-
-// questionable
-// _mm512_mask_mov_epi32 check select(). Doc at https://software.intel.com/en-us/node/513888 is wrong. Bug report filed
-
-
-#if defined (GCC_VERSION) && GCC_VERSION < 50000 && !defined(__INTEL_COMPILER) && !defined(__clang__)
-
-static inline  __m512i _mm512_castsi256_si512(__m256i x) {
-    union {
-        __m512i a;
-        __m256i b;
-    } u;
-    u.b = x;
-    return u.a;
-}
-
-static inline  __m256i _mm512_castsi512_si256(__m512i x) {
-    union {
-        __m512i a;
-        __m256i b;
-    } u;
-    u.a = x;
-    return u.b;
-}
-
-static inline  __m512i _mm512_castsi128_si512(__m128i x) {
-    union {
-        __m128i a;
-        __m512i b;
-    } u;
-    u.a = x;
-    return u.b;
-}
-
-static inline  __m128i _mm512_castsi512_si128(__m512i x) {
-    union {
-        __m512i a;
-        __m128i b;
-    } u;
-    u.a = x;
-    return u.b;
-}
-
-#endif
-
-
-/*****************************************************************************
-*
-*          Generate compile-time constant vector
-*
-*****************************************************************************/
-// Generate a constant vector of 8 integers stored in memory.
+// Generate a constant vector of 16 integers stored in memory.
 // Can be converted to any integer vector type
-template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7, 
-int i8, int i9, int i10, int i11, int i12, int i13, int i14, int i15>
-static inline __m512i constant16i() {
-    static const union {
-        int32_t i[16];
+template <uint32_t i0, uint32_t i1, uint32_t i2, uint32_t i3, uint32_t i4, uint32_t i5, uint32_t i6, uint32_t i7,
+uint32_t i8, uint32_t i9, uint32_t i10, uint32_t i11, uint32_t i12, uint32_t i13, uint32_t i14, uint32_t i15>
+static inline __m512i constant16ui() {
+    /*
+    const union {
+        uint32_t i[16];
         __m512i zmm;
     } u = {{i0,i1,i2,i3,i4,i5,i6,i7,i8,i9,i10,i11,i12,i13,i14,i15}};
     return u.zmm;
+    */
+    return _mm512_setr_epi32(i0,i1,i2,i3,i4,i5,i6,i7,i8,i9,i10,i11,i12,i13,i14,i15);
 }
 
 
 /*****************************************************************************
 *
-*          Boolean vector base classes for AVX512
-*
-*****************************************************************************/
-
-class Vec16b {
-protected:
-    __mmask16  m16; // Boolean vector
-public:
-    // Default constructor:
-    Vec16b () {
-    }
-    // Constructor to convert from type __mmask16 used in intrinsics:
-    Vec16b (__mmask16 x) {
-        m16 = x;
-    }
-    // Constructor to build from all elements:
-    Vec16b(bool b0, bool b1, bool b2, bool b3, bool b4, bool b5, bool b6, bool b7, 
-    bool b8, bool b9, bool b10, bool b11, bool b12, bool b13, bool b14, bool b15) {
-        m16 = uint16_t(b0 | b1<<1 | b2<<2 | b3<<3 | b4<<4 | b5<<5 | b6<<6 | b7<<7 |
-              b8<<8 | b9<<9 | b10<<10 | b11<<11 | b12<<12 | b13<<13 | b14<<14 | b15<<15);
-    }
-    // Constructor to broadcast single value:
-    Vec16b(bool b) {
-        m16 = __mmask16(-int16_t(b));
-    }
-private: // Prevent constructing from int, etc.
-    Vec16b(int b);
-public:
-    // Constructor to make from two halves
-    Vec16b (Vec8ib const & x0, Vec8ib const & x1) {
-        // = Vec16i(x0,x1) != 0;  (not defined yet)
-        __m512i z = _mm512_inserti64x4(_mm512_castsi256_si512(x0), x1, 1);
-        m16 = _mm512_cmpneq_epi32_mask(z, _mm512_setzero_epi32());
-    }        
-    // Assignment operator to convert from type __mmask16 used in intrinsics:
-    Vec16b & operator = (__mmask16 x) {
-        m16 = x;
-        return *this;
-    }
-    // Assignment operator to broadcast scalar value:
-    Vec16b & operator = (bool b) {
-        m16 = Vec16b(b);
-        return *this;
-    }
-private: // Prevent assigning int because of ambiguity
-    Vec16b & operator = (int x);
-public:
-    // Type cast operator to convert to __mmask16 used in intrinsics
-    operator __mmask16() const {
-        return m16;
-    }
-    // split into two halves
-    Vec8ib get_low() const {
-        return to_Vec8ib((uint8_t)m16);
-    }
-    Vec8ib get_high() const {
-        return to_Vec8ib((uint16_t)m16 >> 8);
-    }
-    // Member function to change a single element in vector
-    // Note: This function is inefficient. Use load function if changing more than one element
-    Vec16b const & insert(uint32_t index, bool value) {
-        m16 = __mmask16(((uint16_t)m16 & ~(1 << index)) | (int)value << index);
-        return *this;
-    }
-    // Member function extract a single element from vector
-    bool extract(uint32_t index) const {
-        return ((uint32_t)m16 >> index) & 1;
-    }
-    // Extract a single element. Operator [] can only read an element, not write.
-    bool operator [] (uint32_t index) const {
-        return extract(index);
-    }
-    static int size () {
-        return 16;
-    }
-};
-
-// Define operators for this class
-
-// vector operator & : bitwise and
-static inline Vec16b operator & (Vec16b a, Vec16b b) {
-    return _mm512_kand(a, b);
-}
-static inline Vec16b operator && (Vec16b a, Vec16b b) {
-    return a & b;
-}
-
-// vector operator | : bitwise or
-static inline Vec16b operator | (Vec16b a, Vec16b b) {
-    return _mm512_kor(a, b);
-}
-static inline Vec16b operator || (Vec16b a, Vec16b b) {
-    return a | b;
-}
-
-// vector operator ^ : bitwise xor
-static inline Vec16b operator ^ (Vec16b a, Vec16b b) {
-    return _mm512_kxor(a, b);
-}
-
-// vector operator ~ : bitwise not
-static inline Vec16b operator ~ (Vec16b a) {
-    return _mm512_knot(a);
-}
-
-// vector operator ! : element not
-static inline Vec16b operator ! (Vec16b a) {
-    return ~a;
-}
-
-// vector operator &= : bitwise and
-static inline Vec16b & operator &= (Vec16b & a, Vec16b b) {
-    a = a & b;
-    return a;
-}
-
-// vector operator |= : bitwise or
-static inline Vec16b & operator |= (Vec16b & a, Vec16b b) {
-    a = a | b;
-    return a;
-}
-
-// vector operator ^= : bitwise xor
-static inline Vec16b & operator ^= (Vec16b & a, Vec16b b) {
-    a = a ^ b;
-    return a;
-}
-
-
-/*****************************************************************************
-*
-*          Functions for boolean vectors
+*          Boolean vector classes for AVX512
 *
 *****************************************************************************/
 
-// function andnot: a & ~ b
-static inline Vec16b andnot (Vec16b a, Vec16b b) {
-    return _mm512_kandn(b, a);
-}
-
-// horizontal_and. Returns true if all bits are 1
-static inline bool horizontal_and (Vec16b const & a) {
-    return (uint16_t)(__mmask16)a == 0xFFFF;
-}
-
-// horizontal_or. Returns true if at least one bit is 1
-static inline bool horizontal_or (Vec16b const & a) {
-    return (uint16_t)(__mmask16)a != 0;
-}
+typedef Vec16b Vec16ib;
+typedef Vec16b Vec16uib;
+typedef Vec8b Vec8qb;
+typedef Vec8b Vec8uqb;
 
 
 /*****************************************************************************
 *
-*          Vec16ib: Vector of 16 Booleans for use with Vec16i and Vec16ui
-*
-*****************************************************************************/
-
-class Vec16ib : public Vec16b {
-public:
-    // Default constructor:
-    Vec16ib () {
-    }
-    Vec16ib (Vec16b x) {
-        m16 = x;
-    }
-    // Constructor to build from all elements:
-    Vec16ib(bool x0, bool x1, bool x2, bool x3, bool x4, bool x5, bool x6, bool x7,
-        bool x8, bool x9, bool x10, bool x11, bool x12, bool x13, bool x14, bool x15) :
-        Vec16b(x0, x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12, x13, x14, x15) {
-    }
-    // Constructor to convert from type __mmask16 used in intrinsics:
-    Vec16ib (__mmask16 x) {
-        m16 = x;
-    }
-    // Constructor to broadcast single value:
-    Vec16ib(bool b) : Vec16b(b) {}
-private: // Prevent constructing from int, etc.
-    Vec16ib(int b);
-public:
-    // Constructor to make from two halves
-    Vec16ib (Vec8ib const & x0, Vec8ib const & x1) {
-        m16 = Vec16b(x0, x1);
-    }
-    // Assignment operator to convert from type __mmask16 used in intrinsics:
-    Vec16ib & operator = (__mmask16 x) {
-        m16 = x;
-        return *this;
-    }
-    // Assignment operator to broadcast scalar value:
-    Vec16ib & operator = (bool b) {
-        m16 = Vec16b(b);
-        return *this;
-    }
-private: // Prevent assigning int because of ambiguity
-    Vec16ib & operator = (int x);
-public:
-};
-
-// Define operators for Vec16ib
-
-// vector operator & : bitwise and
-static inline Vec16ib operator & (Vec16ib a, Vec16ib b) {
-    return Vec16b(a) & Vec16b(b);
-}
-static inline Vec16ib operator && (Vec16ib a, Vec16ib b) {
-    return a & b;
-}
-
-// vector operator | : bitwise or
-static inline Vec16ib operator | (Vec16ib a, Vec16ib b) {
-    return Vec16b(a) | Vec16b(b);
-}
-static inline Vec16ib operator || (Vec16ib a, Vec16ib b) {
-    return a | b;
-}
-
-// vector operator ^ : bitwise xor
-static inline Vec16ib operator ^ (Vec16ib a, Vec16ib b) {
-    return Vec16b(a) ^ Vec16b(b);
-}
-
-// vector operator ~ : bitwise not
-static inline Vec16ib operator ~ (Vec16ib a) {
-    return ~Vec16b(a);
-}
-
-// vector operator ! : element not
-static inline Vec16ib operator ! (Vec16ib a) {
-    return ~a;
-}
-
-// vector operator &= : bitwise and
-static inline Vec16ib & operator &= (Vec16ib & a, Vec16ib b) {
-    a = a & b;
-    return a;
-}
-
-// vector operator |= : bitwise or
-static inline Vec16ib & operator |= (Vec16ib & a, Vec16ib b) {
-    a = a | b;
-    return a;
-}
-
-// vector operator ^= : bitwise xor
-static inline Vec16ib & operator ^= (Vec16ib & a, Vec16ib b) {
-    a = a ^ b;
-    return a;
-}
-
-// vector function andnot
-static inline Vec16ib andnot (Vec16ib a, Vec16ib b) {
-    return Vec16ib(andnot(Vec16b(a), Vec16b(b)));
-}
-
-
-/*****************************************************************************
-*
-*          Vec8b: Base class vector of 8 Booleans
-*
-*****************************************************************************/
-
-class Vec8b : public Vec16b {
-public:
-    // Default constructor:
-    Vec8b () {
-    }
-    // Constructor to convert from type __mmask16 used in intrinsics:
-    Vec8b (__mmask16 x) {
-        m16 = x;
-    }
-    // Constructor to build from all elements:
-    Vec8b(bool b0, bool b1, bool b2, bool b3, bool b4, bool b5, bool b6, bool b7) {
-        m16 = uint16_t(b0 | b1<<1 | b2<<2 | b3<<3 | b4<<4 | b5<<5 | b6<<6 | b7<<7);
-    }
-    Vec8b (Vec16b const & x) {
-        m16 = __mmask16(x);
-    }
-    // Constructor to broadcast single value:
-    Vec8b(bool b) {
-        m16 = __mmask16(-int8_t(b));
-    }
-    // Assignment operator to convert from type __mmask16 used in intrinsics:
-    Vec8b & operator = (__mmask16 x) {
-        m16 = x;
-        return *this;
-    }
-private: // Prevent constructing from int etc. because of ambiguity
-    Vec8b(int b);
-    Vec8b & operator = (int x);
-public:
-    // split into two halves
-    Vec4qb get_low() const {
-        return Vec4qb(Vec4q(_mm512_castsi512_si256(_mm512_maskz_set1_epi64(__mmask16(m16), -1LL))));
-    }
-    Vec4qb get_high() const {
-        return Vec8b(__mmask16(m16 >> 4)).get_low();
-    }
-    static int size () {
-        return 8;
-    }
-};
-
-/*****************************************************************************
-*
-*          Functions for boolean vectors
-*
-*****************************************************************************/
-
-// function andnot: a & ~ b
-static inline Vec8b andnot (Vec8b a, Vec8b b) {
-    return _mm512_kandn(b, a);
-}
-
-// horizontal_and. Returns true if all bits are 1
-static inline bool horizontal_and (Vec8b const & a) {
-    return (uint8_t)(__mmask16)a == 0xFF;
-}
-
-// horizontal_or. Returns true if at least one bit is 1
-static inline bool horizontal_or (Vec8b const & a) {
-    return (uint8_t)(__mmask16)a != 0;
-}
-
-
-/*****************************************************************************
-*
-*          Vec8qb: Vector of 8 Booleans for use with Vec8q and Vec8qu
-*
-*****************************************************************************/
-
-class Vec8qb : public Vec8b {
-public:
-    // Default constructor:
-    Vec8qb () {
-    }
-    Vec8qb (Vec16b x) {
-        m16 = x;
-    }
-    // Constructor to build from all elements:
-    Vec8qb(bool x0, bool x1, bool x2, bool x3, bool x4, bool x5, bool x6, bool x7) :
-        Vec8b(x0, x1, x2, x3, x4, x5, x6, x7) {
-    }
-    // Constructor to convert from type __mmask8 used in intrinsics:
-    Vec8qb (__mmask8 x) {
-        m16 = (__mmask16)x;
-    }
-    // Constructor to convert from type __mmask16 used in intrinsics:
-    Vec8qb (__mmask16 x) {
-        m16 = x;
-    }
-    // Assignment operator to convert from type __mmask16 used in intrinsics:
-    Vec8qb & operator = (__mmask16 x) {
-        m16 = x;
-        return *this;
-    }
-    // Constructor to broadcast single value:
-    Vec8qb(bool b) : Vec8b(b) {}
-    // Assignment operator to broadcast scalar:
-    Vec8qb & operator = (bool b) {
-        m16 = Vec8b(b);
-        return *this;
-    }
-private: // Prevent constructing from int, etc.
-    Vec8qb(int b);
-    Vec8qb & operator = (int x);
-public:
-    // Constructor to make from two halves
-    Vec8qb (Vec4qb const & x0, Vec4qb const & x1) {
-        // = Vec8q(x0,x1) != 0;  (not defined yet)
-        __m512i z = _mm512_inserti64x4(_mm512_castsi256_si512(x0), x1, 1);
-        m16 = _mm512_cmpneq_epi64_mask(z, _mm512_setzero_si512());
-    }        
-};
-
-// Define operators for Vec8qb
-
-// vector operator & : bitwise and
-static inline Vec8qb operator & (Vec8qb a, Vec8qb b) {
-    return Vec16b(a) & Vec16b(b);
-}
-static inline Vec8qb operator && (Vec8qb a, Vec8qb b) {
-    return a & b;
-}
-
-// vector operator | : bitwise or
-static inline Vec8qb operator | (Vec8qb a, Vec8qb b) {
-    return Vec16b(a) | Vec16b(b);
-}
-static inline Vec8qb operator || (Vec8qb a, Vec8qb b) {
-    return a | b;
-}
-
-// vector operator ^ : bitwise xor
-static inline Vec8qb operator ^ (Vec8qb a, Vec8qb b) {
-    return Vec16b(a) ^ Vec16b(b);
-}
-
-// vector operator ~ : bitwise not
-static inline Vec8qb operator ~ (Vec8qb a) {
-    return ~Vec16b(a);
-}
-
-// vector operator ! : element not
-static inline Vec8qb operator ! (Vec8qb a) {
-    return ~a;
-}
-
-// vector operator &= : bitwise and
-static inline Vec8qb & operator &= (Vec8qb & a, Vec8qb b) {
-    a = a & b;
-    return a;
-}
-
-// vector operator |= : bitwise or
-static inline Vec8qb & operator |= (Vec8qb & a, Vec8qb b) {
-    a = a | b;
-    return a;
-}
-
-// vector operator ^= : bitwise xor
-static inline Vec8qb & operator ^= (Vec8qb & a, Vec8qb b) {
-    a = a ^ b;
-    return a;
-}
-
-// to_bits: convert to integer bitfield
-static inline uint32_t to_bits(Vec8qb a) {
-    return (uint8_t)(__mmask16)a;
-}
-
-// vector function andnot
-static inline Vec8qb andnot (Vec8qb a, Vec8qb b) {
-    return Vec8qb(andnot(Vec16b(a), Vec16b(b)));
-}
-
-
-/*****************************************************************************
-*
-*          Vector of 512 1-bit unsigned integers (base class for Vec16i)
+*          Vector of 512 bits. Used as base class for Vec16i and Vec8q
 *
 *****************************************************************************/
 class Vec512b {
@@ -567,18 +85,17 @@ protected:
     __m512i zmm; // Integer vector
 public:
     // Default constructor:
-    Vec512b() {
-    }
+    Vec512b() = default;
     // Constructor to build from two Vec256b:
-    Vec512b(Vec256b const & a0, Vec256b const & a1) {
+    Vec512b(Vec256b const a0, Vec256b const a1) {
         zmm = _mm512_inserti64x4(_mm512_castsi256_si512(a0), a1, 1);
     }
     // Constructor to convert from type __m512i used in intrinsics:
-    Vec512b(__m512i const & x) {
+    Vec512b(__m512i const x) {
         zmm = x;
     }
     // Assignment operator to convert from type __m512i used in intrinsics:
-    Vec512b & operator = (__m512i const & x) {
+    Vec512b & operator = (__m512i const x) {
         zmm = x;
         return *this;
     }
@@ -608,39 +125,13 @@ public:
     void store_a(void * p) const {
         _mm512_store_si512(p, zmm);
     }
-    // Member function to store into array using a non-temporal memory hint, aligned by 64
-    void stream(void * p) const {
+    // Member function storing to aligned uncached memory (non-temporal store).
+    // This may be more efficient than store_a when storing large blocks of memory if it 
+    // is unlikely that the data will stay in the cache until it is read again.
+    // Note: Will generate runtime error if p is not aligned by 64
+    void store_nt(void * p) const {
         _mm512_stream_si512((__m512i*)p, zmm);
     }
-    // Member function to change a single bit, mainly for test purposes
-    // Note: This function is inefficient. Use load function if changing more than one bit
-    Vec512b const & set_bit(uint32_t index, int value) {
-        static uint64_t m[16] = {0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0};
-        int wi = (index >> 6) & 7;               // qword index
-        int bi = index & 0x3F;                   // bit index within qword w
-
-        __m512i mask = Vec512b().load(m+8-wi);   // 1 in qword number wi
-        mask = _mm512_sll_epi64(mask,_mm_cvtsi32_si128(bi)); // mask with bit number b set
-        if (value & 1) {
-            zmm = _mm512_or_si512(mask,zmm);
-        }
-        else {
-            zmm = _mm512_andnot_si512(mask,zmm);
-        }
-        return *this;
-    }
-    // Member function to get a single bit, mainly for test purposes
-    // Note: This function is inefficient. Use store function if reading more than one bit
-    int get_bit(uint32_t index) const {
-        union {
-            __m512i z;
-            uint8_t i[64];
-        } u;
-        u.z = zmm; 
-        int wi = (index >> 3) & 0x3F;            // byte index
-        int bi = index & 7;                      // bit index within byte w
-        return (u.i[wi] >> bi) & 1;
-    }
     // Member functions to split into two Vec256b:
     Vec256b get_low() const {
         return _mm512_castsi512_si256(zmm);
@@ -648,66 +139,63 @@ public:
     Vec256b get_high() const {
         return _mm512_extracti64x4_epi64(zmm,1);
     }
-    static int size () {
+    static constexpr int size() {
         return 512;
     }
+    static constexpr int elementtype() {
+        return 1;
+    }
+    typedef __m512i registertype;
 };
 
-
-// Define operators for this class
+// Define operators and functions for this class
 
 // vector operator & : bitwise and
-static inline Vec512b operator & (Vec512b const & a, Vec512b const & b) {
+static inline Vec512b operator & (Vec512b const a, Vec512b const b) {
     return _mm512_and_epi32(a, b);
 }
-static inline Vec512b operator && (Vec512b const & a, Vec512b const & b) {
+static inline Vec512b operator && (Vec512b const a, Vec512b const b) {
     return a & b;
 }
 
 // vector operator | : bitwise or
-static inline Vec512b operator | (Vec512b const & a, Vec512b const & b) {
+static inline Vec512b operator | (Vec512b const a, Vec512b const b) {
     return _mm512_or_epi32(a, b);
 }
-static inline Vec512b operator || (Vec512b const & a, Vec512b const & b) {
+static inline Vec512b operator || (Vec512b const a, Vec512b const b) {
     return a | b;
 }
 
 // vector operator ^ : bitwise xor
-static inline Vec512b operator ^ (Vec512b const & a, Vec512b const & b) {
+static inline Vec512b operator ^ (Vec512b const a, Vec512b const b) {
     return _mm512_xor_epi32(a, b);
 }
 
 // vector operator ~ : bitwise not
-static inline Vec512b operator ~ (Vec512b const & a) {
+static inline Vec512b operator ~ (Vec512b const a) {
     return _mm512_xor_epi32(a, _mm512_set1_epi32(-1));
 }
 
 // vector operator &= : bitwise and
-static inline Vec512b & operator &= (Vec512b & a, Vec512b const & b) {
+static inline Vec512b & operator &= (Vec512b & a, Vec512b const b) {
     a = a & b;
     return a;
 }
 
 // vector operator |= : bitwise or
-static inline Vec512b & operator |= (Vec512b & a, Vec512b const & b) {
+static inline Vec512b & operator |= (Vec512b & a, Vec512b const b) {
     a = a | b;
     return a;
 }
 
 // vector operator ^= : bitwise xor
-static inline Vec512b & operator ^= (Vec512b & a, Vec512b const & b) {
+static inline Vec512b & operator ^= (Vec512b & a, Vec512b const b) {
     a = a ^ b;
     return a;
 }
 
-// Define functions for this class
-
-static inline __m512i zero_512b() {
-    return _mm512_setzero_si512();
-}
-
 // function andnot: a & ~ b
-static inline Vec512b andnot (Vec512b const & a, Vec512b const & b) {
+static inline Vec512b andnot (Vec512b const a, Vec512b const b) {
     return _mm512_andnot_epi32(b, a);
 }
 
@@ -721,34 +209,33 @@ static inline Vec512b andnot (Vec512b const & a, Vec512b const & b) {
 class Vec16i: public Vec512b {
 public:
     // Default constructor:
-    Vec16i() {
-    };
+    Vec16i() = default;
     // Constructor to broadcast the same value into all elements:
     Vec16i(int i) {
         zmm = _mm512_set1_epi32(i);
-    };
+    }
     // Constructor to build from all elements:
     Vec16i(int32_t i0, int32_t i1, int32_t i2, int32_t i3, int32_t i4, int32_t i5, int32_t i6, int32_t i7,
         int32_t i8, int32_t i9, int32_t i10, int32_t i11, int32_t i12, int32_t i13, int32_t i14, int32_t i15) {
         zmm = _mm512_setr_epi32(i0, i1, i2, i3, i4, i5, i6, i7, i8, i9, i10, i11, i12, i13, i14, i15);
-    };
+    }
     // Constructor to build from two Vec8i:
-    Vec16i(Vec8i const & a0, Vec8i const & a1) {
+    Vec16i(Vec8i const a0, Vec8i const a1) {
         zmm = _mm512_inserti64x4(_mm512_castsi256_si512(a0), a1, 1);
     }
     // Constructor to convert from type __m512i used in intrinsics:
-    Vec16i(__m512i const & x) {
+    Vec16i(__m512i const x) {
         zmm = x;
-    };
+    }
     // Assignment operator to convert from type __m512i used in intrinsics:
-    Vec16i & operator = (__m512i const & x) {
+    Vec16i & operator = (__m512i const x) {
         zmm = x;
         return *this;
-    };
+    }
     // Type cast operator to convert to __m512i used in intrinsics
     operator __m512i() const {
         return zmm;
-    };
+    }
     // Member function to load from array (unaligned)
     Vec16i & load(void const * p) {
         zmm = _mm512_loadu_si512(p);
@@ -761,32 +248,31 @@ public:
     }
     // Partial load. Load n elements and set the rest to 0
     Vec16i & load_partial(int n, void const * p) {
-        zmm = _mm512_maskz_loadu_epi32(__mmask16((1 << n) - 1), p);
+        zmm = _mm512_maskz_loadu_epi32(__mmask16((1u << n) - 1), p);
         return *this;
     }
     // Partial store. Store n elements
     void store_partial(int n, void * p) const {
-        _mm512_mask_storeu_epi32(p, __mmask16((1 << n) - 1), zmm);
+        _mm512_mask_storeu_epi32(p, __mmask16((1u << n) - 1), zmm);
     }
     // cut off vector to n elements. The last 16-n elements are set to zero
     Vec16i & cutoff(int n) {
-        zmm = _mm512_maskz_mov_epi32(__mmask16((1 << n) - 1), zmm);
+        zmm = _mm512_maskz_mov_epi32(__mmask16((1u << n) - 1), zmm);
         return *this;
     }
     // Member function to change a single element in vector
-    Vec16i const & insert(uint32_t index, int32_t value) {
-        zmm = _mm512_mask_set1_epi32(zmm, __mmask16(1 << index), value);
+    Vec16i const insert(int index, int32_t value) {
+        zmm = _mm512_mask_set1_epi32(zmm, __mmask16(1u << index), value);
         return *this;
-    };
+    }
     // Member function extract a single element from vector
-    int32_t extract(uint32_t index) const {
-        int32_t a[16];
-        store(a);
-        return a[index & 15];
+    int32_t extract(int index) const {
+        __m512i x = _mm512_maskz_compress_epi32(__mmask16(1u << index), zmm);
+        return _mm_cvtsi128_si32(_mm512_castsi512_si128(x));
     }
     // Extract a single element. Use store function if extracting more than one element.
     // Operator [] can only read an element, not write.
-    int32_t operator [] (uint32_t index) const {
+    int32_t operator [] (int index) const {
         return extract(index);
     }
     // Member functions to split into two Vec8i:
@@ -796,21 +282,23 @@ public:
     Vec8i get_high() const {
         return _mm512_extracti64x4_epi64(zmm,1);
     }
-    static int size () {
+    static constexpr int size() {
         return 16;
     }
+    static constexpr int elementtype() {
+        return 8;
+    }
 };
 
 
 // Define operators for Vec16i
 
 // vector operator + : add element by element
-static inline Vec16i operator + (Vec16i const & a, Vec16i const & b) {
+static inline Vec16i operator + (Vec16i const a, Vec16i const b) {
     return _mm512_add_epi32(a, b);
 }
-
 // vector operator += : add
-static inline Vec16i & operator += (Vec16i & a, Vec16i const & b) {
+static inline Vec16i & operator += (Vec16i & a, Vec16i const b) {
     a = a + b;
     return a;
 }
@@ -821,7 +309,6 @@ static inline Vec16i operator ++ (Vec16i & a, int) {
     a = a + 1;
     return a0;
 }
-
 // prefix operator ++
 static inline Vec16i & operator ++ (Vec16i & a) {
     a = a + 1;
@@ -829,17 +316,15 @@ static inline Vec16i & operator ++ (Vec16i & a) {
 }
 
 // vector operator - : subtract element by element
-static inline Vec16i operator - (Vec16i const & a, Vec16i const & b) {
+static inline Vec16i operator - (Vec16i const a, Vec16i const b) {
     return _mm512_sub_epi32(a, b);
 }
-
 // vector operator - : unary minus
-static inline Vec16i operator - (Vec16i const & a) {
+static inline Vec16i operator - (Vec16i const a) {
     return _mm512_sub_epi32(_mm512_setzero_epi32(), a);
 }
-
 // vector operator -= : subtract
-static inline Vec16i & operator -= (Vec16i & a, Vec16i const & b) {
+static inline Vec16i & operator -= (Vec16i & a, Vec16i const b) {
     a = a - b;
     return a;
 }
@@ -850,7 +335,6 @@ static inline Vec16i operator -- (Vec16i & a, int) {
     a = a - 1;
     return a0;
 }
-
 // prefix operator --
 static inline Vec16i & operator -- (Vec16i & a) {
     a = a - 1;
@@ -858,25 +342,21 @@ static inline Vec16i & operator -- (Vec16i & a) {
 }
 
 // vector operator * : multiply element by element
-static inline Vec16i operator * (Vec16i const & a, Vec16i const & b) {
+static inline Vec16i operator * (Vec16i const a, Vec16i const b) {
     return _mm512_mullo_epi32(a, b);
 }
-
 // vector operator *= : multiply
-static inline Vec16i & operator *= (Vec16i & a, Vec16i const & b) {
+static inline Vec16i & operator *= (Vec16i & a, Vec16i const b) {
     a = a * b;
     return a;
 }
 
-// vector operator / : divide all elements by same integer
-// See bottom of file
-
+// vector operator / : divide all elements by same integer. See bottom of file
 
 // vector operator << : shift left
-static inline Vec16i operator << (Vec16i const & a, int32_t b) {
+static inline Vec16i operator << (Vec16i const a, int32_t b) {
     return _mm512_sll_epi32(a, _mm_cvtsi32_si128(b));
 }
-
 // vector operator <<= : shift left
 static inline Vec16i & operator <<= (Vec16i & a, int32_t b) {
     a = a << b;
@@ -884,10 +364,9 @@ static inline Vec16i & operator <<= (Vec16i & a, int32_t b) {
 }
 
 // vector operator >> : shift right arithmetic
-static inline Vec16i operator >> (Vec16i const & a, int32_t b) {
+static inline Vec16i operator >> (Vec16i const a, int32_t b) {
     return _mm512_sra_epi32(a, _mm_cvtsi32_si128(b));
 }
-
 // vector operator >>= : shift right arithmetic
 static inline Vec16i & operator >>= (Vec16i & a, int32_t b) {
     a = a >> b;
@@ -895,90 +374,98 @@ static inline Vec16i & operator >>= (Vec16i & a, int32_t b) {
 }
 
 // vector operator == : returns true for elements for which a == b
-static inline Vec16ib operator == (Vec16i const & a, Vec16i const & b) {
+static inline Vec16ib operator == (Vec16i const a, Vec16i const b) {
     return _mm512_cmpeq_epi32_mask(a, b);
 }
 
 // vector operator != : returns true for elements for which a != b
-static inline Vec16ib operator != (Vec16i const & a, Vec16i const & b) {
+static inline Vec16ib operator != (Vec16i const a, Vec16i const b) {
     return _mm512_cmpneq_epi32_mask(a, b);
 }
-  
+
 // vector operator > : returns true for elements for which a > b
-static inline Vec16ib operator > (Vec16i const & a, Vec16i const & b) {
-    return  _mm512_cmpgt_epi32_mask(a, b);
+static inline Vec16ib operator > (Vec16i const a, Vec16i const b) {
+    return  _mm512_cmp_epi32_mask(a, b, 6);
 }
 
 // vector operator < : returns true for elements for which a < b
-static inline Vec16ib operator < (Vec16i const & a, Vec16i const & b) {
-    return b > a;
+static inline Vec16ib operator < (Vec16i const a, Vec16i const b) {
+    return  _mm512_cmp_epi32_mask(a, b, 1);
 }
 
 // vector operator >= : returns true for elements for which a >= b (signed)
-static inline Vec16ib operator >= (Vec16i const & a, Vec16i const & b) {
-    return _mm512_cmpge_epi32_mask(a, b);
+static inline Vec16ib operator >= (Vec16i const a, Vec16i const b) {
+    return _mm512_cmp_epi32_mask(a, b, 5);
 }
 
 // vector operator <= : returns true for elements for which a <= b (signed)
-static inline Vec16ib operator <= (Vec16i const & a, Vec16i const & b) {
-    return b >= a;
+static inline Vec16ib operator <= (Vec16i const a, Vec16i const b) {
+    return _mm512_cmp_epi32_mask(a, b, 2);
 }
 
 // vector operator & : bitwise and
-static inline Vec16i operator & (Vec16i const & a, Vec16i const & b) {
+static inline Vec16i operator & (Vec16i const a, Vec16i const b) {
     return _mm512_and_epi32(a, b);
 }
-
 // vector operator &= : bitwise and
-static inline Vec16i & operator &= (Vec16i & a, Vec16i const & b) {
+static inline Vec16i & operator &= (Vec16i & a, Vec16i const b) {
     a = a & b;
     return a;
 }
 
 // vector operator | : bitwise or
-static inline Vec16i operator | (Vec16i const & a, Vec16i const & b) {
+static inline Vec16i operator | (Vec16i const a, Vec16i const b) {
     return _mm512_or_epi32(a, b);
 }
-
 // vector operator |= : bitwise or
-static inline Vec16i & operator |= (Vec16i & a, Vec16i const & b) {
+static inline Vec16i & operator |= (Vec16i & a, Vec16i const b) {
     a = a | b;
     return a;
 }
 
 // vector operator ^ : bitwise xor
-static inline Vec16i operator ^ (Vec16i const & a, Vec16i const & b) {
+static inline Vec16i operator ^ (Vec16i const a, Vec16i const b) {
     return _mm512_xor_epi32(a, b);
 }
-
 // vector operator ^= : bitwise xor
-static inline Vec16i & operator ^= (Vec16i & a, Vec16i const & b) {
+static inline Vec16i & operator ^= (Vec16i & a, Vec16i const b) {
     a = a ^ b;
     return a;
 }
 
 // vector operator ~ : bitwise not
-static inline Vec16i operator ~ (Vec16i const & a) {
+static inline Vec16i operator ~ (Vec16i const a) {
     return a ^ Vec16i(-1);
+    // This is potentially faster, but not on any current compiler:
+    //return _mm512_ternarylogic_epi32(_mm512_undefined_epi32(), _mm512_undefined_epi32(), a, 0x55);
 }
 
 // Functions for this class
 
 // Select between two operands. Corresponds to this pseudocode:
 // for (int i = 0; i < 16; i++) result[i] = s[i] ? a[i] : b[i];
-static inline Vec16i select (Vec16ib const & s, Vec16i const & a, Vec16i const & b) {
+static inline Vec16i select (Vec16ib const s, Vec16i const a, Vec16i const b) {
     return _mm512_mask_mov_epi32(b, s, a);  // conditional move may be optimized better by the compiler than blend
     // return _mm512_mask_blend_epi32(s, b, a);
 }
 
 // Conditional add: For all vector elements i: result[i] = f[i] ? (a[i] + b[i]) : a[i]
-static inline Vec16i if_add (Vec16ib const & f, Vec16i const & a, Vec16i const & b) {
+static inline Vec16i if_add (Vec16ib const f, Vec16i const a, Vec16i const b) {
     return _mm512_mask_add_epi32(a, f, a, b);
 }
 
-// Horizontal add: Calculates the sum of all vector elements.
-// Overflow will wrap around
-static inline int32_t horizontal_add (Vec16i const & a) {
+// Conditional subtract
+static inline Vec16i if_sub (Vec16ib const f, Vec16i const a, Vec16i const b) {
+    return _mm512_mask_sub_epi32(a, f, a, b);
+}
+
+// Conditional multiply
+static inline Vec16i if_mul (Vec16ib const f, Vec16i const a, Vec16i const b) {
+    return _mm512_mask_mullo_epi32(a, f, a, b);
+}
+
+// Horizontal add: Calculates the sum of all vector elements. Overflow will wrap around
+static inline int32_t horizontal_add (Vec16i const a) {
 #if defined(__INTEL_COMPILER)
     return _mm512_reduce_add_epi32(a);
 #else
@@ -988,56 +475,56 @@ static inline int32_t horizontal_add (Vec16i const & a) {
 
 // function add_saturated: add element by element, signed with saturation
 // (is it faster to up-convert to 64 bit integers, and then downconvert the sum with saturation?)
-static inline Vec16i add_saturated(Vec16i const & a, Vec16i const & b) {
-    __m512i sum    = _mm512_add_epi32(a, b);                  // a + b
-    __m512i axb    = _mm512_xor_epi32(a, b);                  // check if a and b have different sign
-    __m512i axs    = _mm512_xor_epi32(a, sum);                // check if a and sum have different sign
-    __m512i ovf1   = _mm512_andnot_epi32(axb,axs);            // check if sum has wrong sign
-    __m512i ovf2   = _mm512_srai_epi32(ovf1,31);              // -1 if overflow
+static inline Vec16i add_saturated(Vec16i const a, Vec16i const b) {
+    __m512i sum    = _mm512_add_epi32(a, b);               // a + b
+    __m512i axb    = _mm512_xor_epi32(a, b);               // check if a and b have different sign
+    __m512i axs    = _mm512_xor_epi32(a, sum);             // check if a and sum have different sign
+    __m512i ovf1   = _mm512_andnot_epi32(axb,axs);         // check if sum has wrong sign
+    __m512i ovf2   = _mm512_srai_epi32(ovf1,31);           // -1 if overflow
     __mmask16 ovf3 = _mm512_cmpneq_epi32_mask(ovf2, _mm512_setzero_epi32()); // same, as mask
-    __m512i asign  = _mm512_srli_epi32(a,31);                 // 1  if a < 0
-    __m512i sat1   = _mm512_srli_epi32(ovf2,1);               // 7FFFFFFF if overflow
-    __m512i sat2   = _mm512_add_epi32(sat1,asign);            // 7FFFFFFF if positive overflow 80000000 if negative overflow
-    return _mm512_mask_blend_epi32(ovf3, sum, sat2);          // sum if not overflow, else sat2
+    __m512i asign  = _mm512_srli_epi32(a,31);              // 1  if a < 0
+    __m512i sat1   = _mm512_srli_epi32(ovf2,1);            // 7FFFFFFF if overflow
+    __m512i sat2   = _mm512_add_epi32(sat1,asign);         // 7FFFFFFF if positive overflow 80000000 if negative overflow
+    return _mm512_mask_blend_epi32(ovf3, sum, sat2);       // sum if not overflow, else sat2
 }
 
 // function sub_saturated: subtract element by element, signed with saturation
-static inline Vec16i sub_saturated(Vec16i const & a, Vec16i const & b) {
-    __m512i diff   = _mm512_sub_epi32(a, b);                  // a + b
-    __m512i axb    = _mm512_xor_si512(a, b);                  // check if a and b have different sign
-    __m512i axs    = _mm512_xor_si512(a, diff);               // check if a and sum have different sign
-    __m512i ovf1   = _mm512_and_si512(axb,axs);               // check if sum has wrong sign
-    __m512i ovf2   = _mm512_srai_epi32(ovf1,31);              // -1 if overflow
+static inline Vec16i sub_saturated(Vec16i const a, Vec16i const b) {
+    __m512i diff   = _mm512_sub_epi32(a, b);               // a + b
+    __m512i axb    = _mm512_xor_si512(a, b);               // check if a and b have different sign
+    __m512i axs    = _mm512_xor_si512(a, diff);            // check if a and sum have different sign
+    __m512i ovf1   = _mm512_and_si512(axb,axs);            // check if sum has wrong sign
+    __m512i ovf2   = _mm512_srai_epi32(ovf1,31);           // -1 if overflow
     __mmask16 ovf3 = _mm512_cmpneq_epi32_mask(ovf2, _mm512_setzero_epi32()); // same, as mask
-    __m512i asign  = _mm512_srli_epi32(a,31);                 // 1  if a < 0
-    __m512i sat1   = _mm512_srli_epi32(ovf2,1);               // 7FFFFFFF if overflow
-    __m512i sat2   = _mm512_add_epi32(sat1,asign);            // 7FFFFFFF if positive overflow 80000000 if negative overflow
-    return _mm512_mask_blend_epi32(ovf3, diff, sat2);         // sum if not overflow, else sat2
+    __m512i asign  = _mm512_srli_epi32(a,31);              // 1  if a < 0
+    __m512i sat1   = _mm512_srli_epi32(ovf2,1);            // 7FFFFFFF if overflow
+    __m512i sat2   = _mm512_add_epi32(sat1,asign);         // 7FFFFFFF if positive overflow 80000000 if negative overflow
+    return _mm512_mask_blend_epi32(ovf3, diff, sat2);      // sum if not overflow, else sat2
 }
 
 // function max: a > b ? a : b
-static inline Vec16i max(Vec16i const & a, Vec16i const & b) {
+static inline Vec16i max(Vec16i const a, Vec16i const b) {
     return _mm512_max_epi32(a,b);
 }
 
 // function min: a < b ? a : b
-static inline Vec16i min(Vec16i const & a, Vec16i const & b) {
+static inline Vec16i min(Vec16i const a, Vec16i const b) {
     return _mm512_min_epi32(a,b);
 }
 
 // function abs: a >= 0 ? a : -a
-static inline Vec16i abs(Vec16i const & a) {
+static inline Vec16i abs(Vec16i const a) {
     return _mm512_abs_epi32(a);
 }
 
 // function abs_saturated: same as abs, saturate if overflow
-static inline Vec16i abs_saturated(Vec16i const & a) {
+static inline Vec16i abs_saturated(Vec16i const a) {
     return _mm512_min_epu32(abs(a), Vec16i(0x7FFFFFFF));
 }
 
 // function rotate_left all elements
 // Use negative count to rotate right
-static inline Vec16i rotate_left(Vec16i const & a, int b) {
+static inline Vec16i rotate_left(Vec16i const a, int b) {
     return _mm512_rolv_epi32(a, Vec16i(b));
 }
 
@@ -1048,34 +535,33 @@ static inline Vec16i rotate_left(Vec16i const & a, int b) {
 *
 *****************************************************************************/
 
-
 class Vec16ui : public Vec16i {
 public:
     // Default constructor:
-    Vec16ui() {
-    };
+    Vec16ui() = default;
     // Constructor to broadcast the same value into all elements:
     Vec16ui(uint32_t i) {
-        zmm = _mm512_set1_epi32(i);
-    };
+        zmm = _mm512_set1_epi32((int32_t)i);
+    }
     // Constructor to build from all elements:
-    Vec16ui(uint32_t i0, uint32_t i1, uint32_t i2, uint32_t i3, uint32_t i4, uint32_t i5, uint32_t i6, uint32_t i7, 
+    Vec16ui(uint32_t i0, uint32_t i1, uint32_t i2, uint32_t i3, uint32_t i4, uint32_t i5, uint32_t i6, uint32_t i7,
         uint32_t i8, uint32_t i9, uint32_t i10, uint32_t i11, uint32_t i12, uint32_t i13, uint32_t i14, uint32_t i15) {
-        zmm = _mm512_setr_epi32(i0, i1, i2, i3, i4, i5, i6, i7, i8, i9, i10, i11, i12, i13, i14, i15);
-    };
+        zmm = _mm512_setr_epi32((int32_t)i0, (int32_t)i1, (int32_t)i2, (int32_t)i3, (int32_t)i4, (int32_t)i5, (int32_t)i6, (int32_t)i7,
+            (int32_t)i8, (int32_t)i9, (int32_t)i10, (int32_t)i11, (int32_t)i12, (int32_t)i13, (int32_t)i14, (int32_t)i15);
+    }
     // Constructor to build from two Vec8ui:
-    Vec16ui(Vec8ui const & a0, Vec8ui const & a1) {
+    Vec16ui(Vec8ui const a0, Vec8ui const a1) {
         zmm = Vec16i(Vec8i(a0), Vec8i(a1));
     }
     // Constructor to convert from type __m512i used in intrinsics:
-    Vec16ui(__m512i const & x) {
+    Vec16ui(__m512i const x) {
         zmm = x;
-    };
+    }
     // Assignment operator to convert from type __m512i used in intrinsics:
-    Vec16ui & operator = (__m512i const & x) {
+    Vec16ui & operator = (__m512i const x) {
         zmm = x;
         return *this;
-    };
+    }
     // Member function to load from array (unaligned)
     Vec16ui & load(void const * p) {
         Vec16i::load(p);
@@ -1087,18 +573,17 @@ public:
         return *this;
     }
     // Member function to change a single element in vector
-    // Note: This function is inefficient. Use load function if changing more than one element
-    Vec16ui const & insert(uint32_t index, uint32_t value) {
-        Vec16i::insert(index, value);
+    Vec16ui const insert(int index, uint32_t value) {
+        Vec16i::insert(index, (int32_t)value);
         return *this;
     }
     // Member function extract a single element from vector
-    uint32_t extract(uint32_t index) const {
-        return Vec16i::extract(index);
+    uint32_t extract(int index) const {
+        return (uint32_t)Vec16i::extract(index);
     }
     // Extract a single element. Use store function if extracting more than one element.
     // Operator [] can only read an element, not write.
-    uint32_t operator [] (uint32_t index) const {
+    uint32_t operator [] (int index) const {
         return extract(index);
     }
     // Member functions to split into two Vec4ui:
@@ -1108,22 +593,25 @@ public:
     Vec8ui get_high() const {
         return Vec8ui(Vec16i::get_high());
     }
+    static constexpr int elementtype() {
+        return 9;
+    }
 };
 
 // Define operators for this class
 
 // vector operator + : add
-static inline Vec16ui operator + (Vec16ui const & a, Vec16ui const & b) {
+static inline Vec16ui operator + (Vec16ui const a, Vec16ui const b) {
     return Vec16ui (Vec16i(a) + Vec16i(b));
 }
 
 // vector operator - : subtract
-static inline Vec16ui operator - (Vec16ui const & a, Vec16ui const & b) {
+static inline Vec16ui operator - (Vec16ui const a, Vec16ui const b) {
     return Vec16ui (Vec16i(a) - Vec16i(b));
 }
 
 // vector operator * : multiply
-static inline Vec16ui operator * (Vec16ui const & a, Vec16ui const & b) {
+static inline Vec16ui operator * (Vec16ui const a, Vec16ui const b) {
     return Vec16ui (Vec16i(a) * Vec16i(b));
 }
 
@@ -1131,12 +619,10 @@ static inline Vec16ui operator * (Vec16ui const & a, Vec16ui const & b) {
 // See bottom of file
 
 // vector operator >> : shift right logical all elements
-static inline Vec16ui operator >> (Vec16ui const & a, uint32_t b) {
-    return _mm512_srl_epi32(a, _mm_cvtsi32_si128(b)); 
+static inline Vec16ui operator >> (Vec16ui const a, uint32_t b) {
+    return _mm512_srl_epi32(a, _mm_cvtsi32_si128((int32_t)b));
 }
-
-// vector operator >> : shift right logical all elements
-static inline Vec16ui operator >> (Vec16ui const & a, int32_t b) {
+static inline Vec16ui operator >> (Vec16ui const a, int32_t b) {
     return a >> (uint32_t)b;
 }
 
@@ -1144,7 +630,7 @@ static inline Vec16ui operator >> (Vec16ui const & a, int32_t b) {
 static inline Vec16ui & operator >>= (Vec16ui & a, uint32_t b) {
     a = a >> b;
     return a;
-} 
+}
 
 // vector operator >>= : shift right logical
 static inline Vec16ui & operator >>= (Vec16ui & a, int32_t b) {
@@ -1153,53 +639,52 @@ static inline Vec16ui & operator >>= (Vec16ui & a, int32_t b) {
 }
 
 // vector operator << : shift left all elements
-static inline Vec16ui operator << (Vec16ui const & a, uint32_t b) {
+static inline Vec16ui operator << (Vec16ui const a, uint32_t b) {
     return Vec16ui ((Vec16i)a << (int32_t)b);
 }
 
 // vector operator << : shift left all elements
-static inline Vec16ui operator << (Vec16ui const & a, int32_t b) {
+static inline Vec16ui operator << (Vec16ui const a, int32_t b) {
     return Vec16ui ((Vec16i)a << (int32_t)b);
 }
 
 // vector operator < : returns true for elements for which a < b (unsigned)
-static inline Vec16ib operator < (Vec16ui const & a, Vec16ui const & b) {
-    return _mm512_cmplt_epu32_mask(a, b);
+static inline Vec16ib operator < (Vec16ui const a, Vec16ui const b) {
+    return _mm512_cmp_epu32_mask(a, b, 1);
 }
 
 // vector operator > : returns true for elements for which a > b (unsigned)
-static inline Vec16ib operator > (Vec16ui const & a, Vec16ui const & b) {
-    return b < a;
+static inline Vec16ib operator > (Vec16ui const a, Vec16ui const b) {
+    return _mm512_cmp_epu32_mask(a, b, 6);
 }
 
-
 // vector operator >= : returns true for elements for which a >= b (unsigned)
-static inline Vec16ib operator >= (Vec16ui const & a, Vec16ui const & b) {
-    return  _mm512_cmpge_epu32_mask(a, b);
-}            
+static inline Vec16ib operator >= (Vec16ui const a, Vec16ui const b) {
+    return _mm512_cmp_epu32_mask(a, b, 5);
+}
 
 // vector operator <= : returns true for elements for which a <= b (unsigned)
-static inline Vec16ib operator <= (Vec16ui const & a, Vec16ui const & b) {
-    return b >= a;
+static inline Vec16ib operator <= (Vec16ui const a, Vec16ui const b) {
+    return _mm512_cmp_epu32_mask(a, b, 2);
 }
 
 // vector operator & : bitwise and
-static inline Vec16ui operator & (Vec16ui const & a, Vec16ui const & b) {
+static inline Vec16ui operator & (Vec16ui const a, Vec16ui const b) {
     return Vec16ui(Vec16i(a) & Vec16i(b));
 }
 
 // vector operator | : bitwise or
-static inline Vec16ui operator | (Vec16ui const & a, Vec16ui const & b) {
+static inline Vec16ui operator | (Vec16ui const a, Vec16ui const b) {
     return Vec16ui(Vec16i(a) | Vec16i(b));
 }
 
 // vector operator ^ : bitwise xor
-static inline Vec16ui operator ^ (Vec16ui const & a, Vec16ui const & b) {
+static inline Vec16ui operator ^ (Vec16ui const a, Vec16ui const b) {
     return Vec16ui(Vec16i(a) ^ Vec16i(b));
 }
 
 // vector operator ~ : bitwise not
-static inline Vec16ui operator ~ (Vec16ui const & a) {
+static inline Vec16ui operator ~ (Vec16ui const a) {
     return Vec16ui( ~ Vec16i(a));
 }
 
@@ -1207,43 +692,52 @@ static inline Vec16ui operator ~ (Vec16ui const & a) {
 
 // Select between two operands. Corresponds to this pseudocode:
 // for (int i = 0; i < 16; i++) result[i] = s[i] ? a[i] : b[i];
-static inline Vec16ui select (Vec16ib const & s, Vec16ui const & a, Vec16ui const & b) {
+static inline Vec16ui select (Vec16ib const s, Vec16ui const a, Vec16ui const b) {
     return Vec16ui(select(s, Vec16i(a), Vec16i(b)));
 }
 
 // Conditional add: For all vector elements i: result[i] = f[i] ? (a[i] + b[i]) : a[i]
-static inline Vec16ui if_add (Vec16ib const & f, Vec16ui const & a, Vec16ui const & b) {
+static inline Vec16ui if_add (Vec16ib const f, Vec16ui const a, Vec16ui const b) {
     return Vec16ui(if_add(f, Vec16i(a), Vec16i(b)));
 }
 
-// Horizontal add: Calculates the sum of all vector elements.
-// Overflow will wrap around
-static inline uint32_t horizontal_add (Vec16ui const & a) {
-    return horizontal_add((Vec16i)a);
+// Conditional subtract
+static inline Vec16ui if_sub (Vec16ib const f, Vec16ui const a, Vec16ui const b) {
+    return Vec16ui(if_sub(f, Vec16i(a), Vec16i(b)));
+}
+
+// Conditional multiply
+static inline Vec16ui if_mul (Vec16ib const f, Vec16ui const a, Vec16ui const b) {
+    return Vec16ui(if_mul(f, Vec16i(a), Vec16i(b)));
+}
+
+// Horizontal add: Calculates the sum of all vector elements. Overflow will wrap around
+static inline uint32_t horizontal_add (Vec16ui const a) {
+    return (uint32_t)horizontal_add((Vec16i)a);
 }
 
 // horizontal_add_x: Horizontal add extended: Calculates the sum of all vector elements. Defined later in this file
 
 // function add_saturated: add element by element, unsigned with saturation
-static inline Vec16ui add_saturated(Vec16ui const & a, Vec16ui const & b) {
+static inline Vec16ui add_saturated(Vec16ui const a, Vec16ui const b) {
     Vec16ui sum      = a + b;
-    Vec16ib overflow = sum < (a | b);                  // overflow if (a + b) < (a | b)
-    return _mm512_mask_set1_epi32(sum, overflow, -1);  // 0xFFFFFFFF if overflow
+    Vec16ib overflow = sum < (a | b);                      // overflow if (a + b) < (a | b)
+    return _mm512_mask_set1_epi32(sum, overflow, -1);      // 0xFFFFFFFF if overflow
 }
 
 // function sub_saturated: subtract element by element, unsigned with saturation
-static inline Vec16ui sub_saturated(Vec16ui const & a, Vec16ui const & b) {
+static inline Vec16ui sub_saturated(Vec16ui const a, Vec16ui const b) {
     Vec16ui diff      = a - b;
-    return _mm512_maskz_mov_epi32(diff <= a, diff);   // underflow if diff > a gives zero
+    return _mm512_maskz_mov_epi32(diff <= a, diff);        // underflow if diff > a gives zero
 }
 
 // function max: a > b ? a : b
-static inline Vec16ui max(Vec16ui const & a, Vec16ui const & b) {
+static inline Vec16ui max(Vec16ui const a, Vec16ui const b) {
     return _mm512_max_epu32(a,b);
 }
 
 // function min: a < b ? a : b
-static inline Vec16ui min(Vec16ui const & a, Vec16ui const & b) {
+static inline Vec16ui min(Vec16ui const a, Vec16ui const b) {
     return _mm512_min_epu32(a,b);
 }
 
@@ -1257,8 +751,7 @@ static inline Vec16ui min(Vec16ui const & a, Vec16ui const & b) {
 class Vec8q : public Vec512b {
 public:
     // Default constructor:
-    Vec8q() {
-    }
+    Vec8q() = default;
     // Constructor to broadcast the same value into all elements:
     Vec8q(int64_t i) {
         zmm = _mm512_set1_epi64(i);
@@ -1268,15 +761,15 @@ public:
         zmm = _mm512_setr_epi64(i0, i1, i2, i3, i4, i5, i6, i7);
     }
     // Constructor to build from two Vec4q:
-    Vec8q(Vec4q const & a0, Vec4q const & a1) {
+    Vec8q(Vec4q const a0, Vec4q const a1) {
         zmm = _mm512_inserti64x4(_mm512_castsi256_si512(a0), a1, 1);
     }
     // Constructor to convert from type __m512i used in intrinsics:
-    Vec8q(__m512i const & x) {
+    Vec8q(__m512i const x) {
         zmm = x;
     }
     // Assignment operator to convert from type __m512i used in intrinsics:
-    Vec8q & operator = (__m512i const & x) {
+    Vec8q & operator = (__m512i const x) {
         zmm = x;
         return *this;
     }
@@ -1309,21 +802,23 @@ public:
         return *this;
     }
     // Member function to change a single element in vector
-    // Note: This function is inefficient. Use load function if changing more than one element
-    Vec8q const & insert(uint32_t index, int64_t value) {
+    Vec8q const insert(int index, int64_t value) {
+#ifdef __x86_64__
         zmm = _mm512_mask_set1_epi64(zmm, __mmask16(1 << index), value);
-        // zmm = _mm512_mask_blend_epi64(__mmask16(1 << index), zmm, _mm512_set1_epi64(value));
+#else
+        __m512i v = Vec8q(value);
+        zmm = _mm512_mask_mov_epi64(zmm, __mmask16(1 << index), v);
+#endif
         return *this;
     }
     // Member function extract a single element from vector
-    int64_t extract(uint32_t index) const {
-        int64_t a[8];
-        store (a);
-        return a[index & 7];
+    int64_t extract(int index) const {
+        __m512i x = _mm512_maskz_compress_epi64(__mmask8(1u << index), zmm);
+        return _emulate_movq(_mm512_castsi512_si128(x));
     }
     // Extract a single element. Use store function if extracting more than one element.
     // Operator [] can only read an element, not write.
-    int64_t operator [] (uint32_t index) const {
+    int64_t operator [] (int index) const {
         return extract(index);
     }
     // Member functions to split into two Vec2q:
@@ -1333,21 +828,23 @@ public:
     Vec4q get_high() const {
         return _mm512_extracti64x4_epi64(zmm,1);
     }
-    static int size () {
+    static constexpr int size() {
         return 8;
     }
+    static constexpr int elementtype() {
+        return 10;
+    }
 };
 
 
 // Define operators for Vec8q
 
 // vector operator + : add element by element
-static inline Vec8q operator + (Vec8q const & a, Vec8q const & b) {
+static inline Vec8q operator + (Vec8q const a, Vec8q const b) {
     return _mm512_add_epi64(a, b);
 }
-
 // vector operator += : add
-static inline Vec8q & operator += (Vec8q & a, Vec8q const & b) {
+static inline Vec8q & operator += (Vec8q & a, Vec8q const b) {
     a = a + b;
     return a;
 }
@@ -1358,7 +855,6 @@ static inline Vec8q operator ++ (Vec8q & a, int) {
     a = a + 1;
     return a0;
 }
-
 // prefix operator ++
 static inline Vec8q & operator ++ (Vec8q & a) {
     a = a + 1;
@@ -1366,17 +862,15 @@ static inline Vec8q & operator ++ (Vec8q & a) {
 }
 
 // vector operator - : subtract element by element
-static inline Vec8q operator - (Vec8q const & a, Vec8q const & b) {
+static inline Vec8q operator - (Vec8q const a, Vec8q const b) {
     return _mm512_sub_epi64(a, b);
 }
-
 // vector operator - : unary minus
-static inline Vec8q operator - (Vec8q const & a) {
+static inline Vec8q operator - (Vec8q const a) {
     return _mm512_sub_epi64(_mm512_setzero_epi32(), a);
 }
-
 // vector operator -= : subtract
-static inline Vec8q & operator -= (Vec8q & a, Vec8q const & b) {
+static inline Vec8q & operator -= (Vec8q & a, Vec8q const b) {
     a = a - b;
     return a;
 }
@@ -1387,7 +881,6 @@ static inline Vec8q operator -- (Vec8q & a, int) {
     a = a - 1;
     return a0;
 }
-
 // prefix operator --
 static inline Vec8q & operator -- (Vec8q & a) {
     a = a - 1;
@@ -1395,13 +888,13 @@ static inline Vec8q & operator -- (Vec8q & a) {
 }
 
 // vector operator * : multiply element by element
-static inline Vec8q operator * (Vec8q const & a, Vec8q const & b) {
-#ifdef __AVX512DQ__
+static inline Vec8q operator * (Vec8q const a, Vec8q const b) {
+#if INSTRSET >= 10  // __AVX512DQ__
     return _mm512_mullo_epi64(a, b);
 #elif defined (__INTEL_COMPILER)
     return _mm512_mullox_epi64(a, b);                      // _mm512_mullox_epi64 missing in gcc
 #else
-    // instruction does not exist. Split into 32-bit multiplies
+    // instruction does not exist. Split into 32-bit multiplications
     //__m512i ahigh = _mm512_shuffle_epi32(a, 0xB1);       // swap H<->L
     __m512i ahigh   = _mm512_srli_epi64(a, 32);            // high 32 bits of each a
     __m512i bhigh   = _mm512_srli_epi64(b, 32);            // high 32 bits of each b
@@ -1416,16 +909,15 @@ static inline Vec8q operator * (Vec8q const & a, Vec8q const & b) {
 }
 
 // vector operator *= : multiply
-static inline Vec8q & operator *= (Vec8q & a, Vec8q const & b) {
+static inline Vec8q & operator *= (Vec8q & a, Vec8q const b) {
     a = a * b;
     return a;
 }
 
 // vector operator << : shift left
-static inline Vec8q operator << (Vec8q const & a, int32_t b) {
+static inline Vec8q operator << (Vec8q const a, int32_t b) {
     return _mm512_sll_epi64(a, _mm_cvtsi32_si128(b));
 }
-
 // vector operator <<= : shift left
 static inline Vec8q & operator <<= (Vec8q & a, int32_t b) {
     a = a << b;
@@ -1433,10 +925,9 @@ static inline Vec8q & operator <<= (Vec8q & a, int32_t b) {
 }
 
 // vector operator >> : shift right arithmetic
-static inline Vec8q operator >> (Vec8q const & a, int32_t b) {
+static inline Vec8q operator >> (Vec8q const a, int32_t b) {
     return _mm512_sra_epi64(a, _mm_cvtsi32_si128(b));
 }
-
 // vector operator >>= : shift right arithmetic
 static inline Vec8q & operator >>= (Vec8q & a, int32_t b) {
     a = a >> b;
@@ -1444,89 +935,101 @@ static inline Vec8q & operator >>= (Vec8q & a, int32_t b) {
 }
 
 // vector operator == : returns true for elements for which a == b
-static inline Vec8qb operator == (Vec8q const & a, Vec8q const & b) {
+static inline Vec8qb operator == (Vec8q const a, Vec8q const b) {
     return Vec8qb(_mm512_cmpeq_epi64_mask(a, b));
 }
 
 // vector operator != : returns true for elements for which a != b
-static inline Vec8qb operator != (Vec8q const & a, Vec8q const & b) {
+static inline Vec8qb operator != (Vec8q const a, Vec8q const b) {
     return Vec8qb(_mm512_cmpneq_epi64_mask(a, b));
 }
-  
+
 // vector operator < : returns true for elements for which a < b
-static inline Vec8qb operator < (Vec8q const & a, Vec8q const & b) {
-    return Vec8qb(_mm512_cmplt_epi64_mask(a, b));
+static inline Vec8qb operator < (Vec8q const a, Vec8q const b) {
+    return _mm512_cmp_epi64_mask(a, b, 1);
 }
 
 // vector operator > : returns true for elements for which a > b
-static inline Vec8qb operator > (Vec8q const & a, Vec8q const & b) {
-    return b < a;
+static inline Vec8qb operator > (Vec8q const a, Vec8q const b) {
+    return _mm512_cmp_epi64_mask(a, b, 6);
 }
 
 // vector operator >= : returns true for elements for which a >= b (signed)
-static inline Vec8qb operator >= (Vec8q const & a, Vec8q const & b) {
-    return Vec8qb(_mm512_cmpge_epi64_mask(a, b));
+static inline Vec8qb operator >= (Vec8q const a, Vec8q const b) {
+    return _mm512_cmp_epi64_mask(a, b, 5);
 }
 
 // vector operator <= : returns true for elements for which a <= b (signed)
-static inline Vec8qb operator <= (Vec8q const & a, Vec8q const & b) {
-    return b >= a;
+static inline Vec8qb operator <= (Vec8q const a, Vec8q const b) {
+    return _mm512_cmp_epi64_mask(a, b, 2);
 }
 
 // vector operator & : bitwise and
-static inline Vec8q operator & (Vec8q const & a, Vec8q const & b) {
+static inline Vec8q operator & (Vec8q const a, Vec8q const b) {
     return _mm512_and_epi32(a, b);
 }
-
 // vector operator &= : bitwise and
-static inline Vec8q & operator &= (Vec8q & a, Vec8q const & b) {
+static inline Vec8q & operator &= (Vec8q & a, Vec8q const b) {
     a = a & b;
     return a;
 }
 
 // vector operator | : bitwise or
-static inline Vec8q operator | (Vec8q const & a, Vec8q const & b) {
+static inline Vec8q operator | (Vec8q const a, Vec8q const b) {
     return _mm512_or_epi32(a, b);
 }
-
 // vector operator |= : bitwise or
-static inline Vec8q & operator |= (Vec8q & a, Vec8q const & b) {
+static inline Vec8q & operator |= (Vec8q & a, Vec8q const b) {
     a = a | b;
     return a;
 }
 
 // vector operator ^ : bitwise xor
-static inline Vec8q operator ^ (Vec8q const & a, Vec8q const & b) {
+static inline Vec8q operator ^ (Vec8q const a, Vec8q const b) {
     return _mm512_xor_epi32(a, b);
 }
 // vector operator ^= : bitwise xor
-static inline Vec8q & operator ^= (Vec8q & a, Vec8q const & b) {
+static inline Vec8q & operator ^= (Vec8q & a, Vec8q const b) {
     a = a ^ b;
     return a;
 }
 
 // vector operator ~ : bitwise not
-static inline Vec8q operator ~ (Vec8q const & a) {
+static inline Vec8q operator ~ (Vec8q const a) {
     return Vec8q(~ Vec16i(a));
+    //return _mm512_ternarylogic_epi64(_mm512_undefined_epi32(), _mm512_undefined_epi32(), a, 0x55);
 }
 
 // Functions for this class
 
 // Select between two operands. Corresponds to this pseudocode:
 // for (int i = 0; i < 4; i++) result[i] = s[i] ? a[i] : b[i];
-static inline Vec8q select (Vec8qb const & s, Vec8q const & a, Vec8q const & b) {
-    return _mm512_mask_mov_epi64(b, s, a);
-    //return _mm512_mask_blend_epi64(s, b, a);
+static inline Vec8q select (Vec8qb const s, Vec8q const a, Vec8q const b) {
+    // avoid warning in MS compiler if INSTRSET = 9 by casting mask to uint8_t, while __mmask8 is not supported in AVX512F
+    return _mm512_mask_mov_epi64(b, (uint8_t)s, a);
 }
 
 // Conditional add: For all vector elements i: result[i] = f[i] ? (a[i] + b[i]) : a[i]
-static inline Vec8q if_add (Vec8qb const & f, Vec8q const & a, Vec8q const & b) {
-    return _mm512_mask_add_epi64(a, f, a, b);
+static inline Vec8q if_add (Vec8qb const f, Vec8q const a, Vec8q const b) {
+    return _mm512_mask_add_epi64(a, (uint8_t)f, a, b);
 }
 
-// Horizontal add: Calculates the sum of all vector elements.
-// Overflow will wrap around
-static inline int64_t horizontal_add (Vec8q const & a) {
+// Conditional subtract
+static inline Vec8q if_sub (Vec8qb const f, Vec8q const a, Vec8q const b) {
+    return _mm512_mask_sub_epi64(a, (uint8_t)f, a, b);
+}
+
+// Conditional multiply
+static inline Vec8q if_mul (Vec8qb const f, Vec8q const a, Vec8q const b) {
+#if INSTRSET >= 10
+    return _mm512_mask_mullo_epi64(a, f, a, b);  // AVX512DQ
+#else
+    return select(f, a*b, a);
+#endif
+}
+
+// Horizontal add: Calculates the sum of all vector elements. Overflow will wrap around
+static inline int64_t horizontal_add (Vec8q const a) {
 #if defined(__INTEL_COMPILER)
     return _mm512_reduce_add_epi64(a);
 #else
@@ -1536,7 +1039,7 @@ static inline int64_t horizontal_add (Vec8q const & a) {
 
 // Horizontal add extended: Calculates the sum of all vector elements
 // Elements are sign extended before adding to avoid overflow
-static inline int64_t horizontal_add_x (Vec16i const & x) {
+static inline int64_t horizontal_add_x (Vec16i const x) {
     Vec8q a = _mm512_cvtepi32_epi64(x.get_low());
     Vec8q b = _mm512_cvtepi32_epi64(x.get_high());
     return horizontal_add(a+b);
@@ -1544,35 +1047,35 @@ static inline int64_t horizontal_add_x (Vec16i const & x) {
 
 // Horizontal add extended: Calculates the sum of all vector elements
 // Elements are zero extended before adding to avoid overflow
-static inline uint64_t horizontal_add_x (Vec16ui const & x) {
+static inline uint64_t horizontal_add_x (Vec16ui const x) {
     Vec8q a = _mm512_cvtepu32_epi64(x.get_low());
     Vec8q b = _mm512_cvtepu32_epi64(x.get_high());
-    return horizontal_add(a+b);
+    return (uint64_t)horizontal_add(a+b);
 }
 
 // function max: a > b ? a : b
-static inline Vec8q max(Vec8q const & a, Vec8q const & b) {
+static inline Vec8q max(Vec8q const a, Vec8q const b) {
     return _mm512_max_epi64(a, b);
 }
 
 // function min: a < b ? a : b
-static inline Vec8q min(Vec8q const & a, Vec8q const & b) {
+static inline Vec8q min(Vec8q const a, Vec8q const b) {
     return _mm512_min_epi64(a, b);
 }
 
 // function abs: a >= 0 ? a : -a
-static inline Vec8q abs(Vec8q const & a) {
+static inline Vec8q abs(Vec8q const a) {
     return _mm512_abs_epi64(a);
 }
 
 // function abs_saturated: same as abs, saturate if overflow
-static inline Vec8q abs_saturated(Vec8q const & a) {
+static inline Vec8q abs_saturated(Vec8q const a) {
     return _mm512_min_epu64(abs(a), Vec8q(0x7FFFFFFFFFFFFFFF));
 }
 
 // function rotate_left all elements
 // Use negative count to rotate right
-static inline Vec8q rotate_left(Vec8q const & a, int b) {
+static inline Vec8q rotate_left(Vec8q const a, int b) {
     return _mm512_rolv_epi64(a, Vec8q(b));
 }
 
@@ -1586,35 +1089,34 @@ static inline Vec8q rotate_left(Vec8q const & a, int b) {
 class Vec8uq : public Vec8q {
 public:
     // Default constructor:
-    Vec8uq() {
-    }
+    Vec8uq() = default;
     // Constructor to broadcast the same value into all elements:
     Vec8uq(uint64_t i) {
-        zmm = Vec8q(i);
+        zmm = Vec8q((int64_t)i);
     }
     // Constructor to convert from Vec8q:
-    Vec8uq(Vec8q const & x) {
+    Vec8uq(Vec8q const x) {
         zmm = x;
     }
     // Constructor to convert from type __m512i used in intrinsics:
-    Vec8uq(__m512i const & x) {
+    Vec8uq(__m512i const x) {
         zmm = x;
     }
     // Constructor to build from all elements:
     Vec8uq(uint64_t i0, uint64_t i1, uint64_t i2, uint64_t i3, uint64_t i4, uint64_t i5, uint64_t i6, uint64_t i7) {
-        zmm = Vec8q(i0, i1, i2, i3, i4, i5, i6, i7);
+        zmm = Vec8q((int64_t)i0, (int64_t)i1, (int64_t)i2, (int64_t)i3, (int64_t)i4, (int64_t)i5, (int64_t)i6, (int64_t)i7);
     }
     // Constructor to build from two Vec4uq:
-    Vec8uq(Vec4uq const & a0, Vec4uq const & a1) {
+    Vec8uq(Vec4uq const a0, Vec4uq const a1) {
         zmm = Vec8q(Vec4q(a0), Vec4q(a1));
     }
     // Assignment operator to convert from Vec8q:
-    Vec8uq  & operator = (Vec8q const & x) {
+    Vec8uq  & operator = (Vec8q const x) {
         zmm = x;
         return *this;
     }
     // Assignment operator to convert from type __m512i used in intrinsics:
-    Vec8uq & operator = (__m512i const & x) {
+    Vec8uq & operator = (__m512i const x) {
         zmm = x;
         return *this;
     }
@@ -1629,18 +1131,17 @@ public:
         return *this;
     }
     // Member function to change a single element in vector
-    // Note: This function is inefficient. Use load function if changing more than one element
-    Vec8uq const & insert(uint32_t index, uint64_t value) {
-        Vec8q::insert(index, value);
+    Vec8uq const insert(int index, uint64_t value) {
+        Vec8q::insert(index, (int64_t)value);
         return *this;
     }
     // Member function extract a single element from vector
-    uint64_t extract(uint32_t index) const {
-        return Vec8q::extract(index);
+    uint64_t extract(int index) const {
+        return (uint64_t)Vec8q::extract(index);
     }
     // Extract a single element. Use store function if extracting more than one element.
     // Operator [] can only read an element, not write.
-    uint64_t operator [] (uint32_t index) const {
+    uint64_t operator [] (int index) const {
         return extract(index);
     }
     // Member functions to split into two Vec2uq:
@@ -1650,41 +1151,40 @@ public:
     Vec4uq get_high() const {
         return Vec4uq(Vec8q::get_high());
     }
+    static constexpr int elementtype() {
+        return 11;
+    }
 };
 
 // Define operators for this class
 
 // vector operator + : add
-static inline Vec8uq operator + (Vec8uq const & a, Vec8uq const & b) {
+static inline Vec8uq operator + (Vec8uq const a, Vec8uq const b) {
     return Vec8uq (Vec8q(a) + Vec8q(b));
 }
 
 // vector operator - : subtract
-static inline Vec8uq operator - (Vec8uq const & a, Vec8uq const & b) {
+static inline Vec8uq operator - (Vec8uq const a, Vec8uq const b) {
     return Vec8uq (Vec8q(a) - Vec8q(b));
 }
 
 // vector operator * : multiply element by element
-static inline Vec8uq operator * (Vec8uq const & a, Vec8uq const & b) {
+static inline Vec8uq operator * (Vec8uq const a, Vec8uq const b) {
     return Vec8uq (Vec8q(a) * Vec8q(b));
 }
 
 // vector operator >> : shift right logical all elements
-static inline Vec8uq operator >> (Vec8uq const & a, uint32_t b) {
-    return _mm512_srl_epi64(a,_mm_cvtsi32_si128(b)); 
+static inline Vec8uq operator >> (Vec8uq const a, uint32_t b) {
+    return _mm512_srl_epi64(a,_mm_cvtsi32_si128((int32_t)b));
 }
-
-// vector operator >> : shift right logical all elements
-static inline Vec8uq operator >> (Vec8uq const & a, int32_t b) {
+static inline Vec8uq operator >> (Vec8uq const a, int32_t b) {
     return a >> (uint32_t)b;
 }
-
 // vector operator >>= : shift right artihmetic
 static inline Vec8uq & operator >>= (Vec8uq & a, uint32_t b) {
     a = a >> b;
     return a;
 }
-
 // vector operator >>= : shift right logical
 static inline Vec8uq & operator >>= (Vec8uq & a, int32_t b) {
     a = a >> uint32_t(b);
@@ -1692,47 +1192,46 @@ static inline Vec8uq & operator >>= (Vec8uq & a, int32_t b) {
 }
 
 // vector operator << : shift left all elements
-static inline Vec8uq operator << (Vec8uq const & a, uint32_t b) {
+static inline Vec8uq operator << (Vec8uq const a, uint32_t b) {
     return Vec8uq ((Vec8q)a << (int32_t)b);
 }
-
 // vector operator << : shift left all elements
-static inline Vec8uq operator << (Vec8uq const & a, int32_t b) {
+static inline Vec8uq operator << (Vec8uq const a, int32_t b) {
     return Vec8uq ((Vec8q)a << b);
 }
 
 // vector operator < : returns true for elements for which a < b (unsigned)
-static inline Vec8qb operator < (Vec8uq const & a, Vec8uq const & b) {
-    return _mm512_cmplt_epu64_mask(a, b);
+static inline Vec8qb operator < (Vec8uq const a, Vec8uq const b) {
+    return _mm512_cmp_epu64_mask(a, b, 1);
 }
 
 // vector operator > : returns true for elements for which a > b (unsigned)
-static inline Vec8qb operator > (Vec8uq const & a, Vec8uq const & b) {
-    return b < a;
+static inline Vec8qb operator > (Vec8uq const a, Vec8uq const b) {
+    return _mm512_cmp_epu64_mask(a, b, 6);
 }
 
 // vector operator >= : returns true for elements for which a >= b (unsigned)
-static inline Vec8qb operator >= (Vec8uq const & a, Vec8uq const & b) {
-    return _mm512_cmpge_epu64_mask(a, b);
+static inline Vec8qb operator >= (Vec8uq const a, Vec8uq const b) {
+    return _mm512_cmp_epu64_mask(a, b, 5);
 }
 
 // vector operator <= : returns true for elements for which a <= b (unsigned)
-static inline Vec8qb operator <= (Vec8uq const & a, Vec8uq const & b) {
-    return b >= a;
+static inline Vec8qb operator <= (Vec8uq const a, Vec8uq const b) {
+    return _mm512_cmp_epu64_mask(a, b, 2);
 }
 
 // vector operator & : bitwise and
-static inline Vec8uq operator & (Vec8uq const & a, Vec8uq const & b) {
+static inline Vec8uq operator & (Vec8uq const a, Vec8uq const b) {
     return Vec8uq(Vec8q(a) & Vec8q(b));
 }
 
 // vector operator | : bitwise or
-static inline Vec8uq operator | (Vec8uq const & a, Vec8uq const & b) {
+static inline Vec8uq operator | (Vec8uq const a, Vec8uq const b) {
     return Vec8uq(Vec8q(a) | Vec8q(b));
 }
 
 // vector operator ^ : bitwise xor
-static inline Vec8uq operator ^ (Vec8uq const & a, Vec8uq const & b) {
+static inline Vec8uq operator ^ (Vec8uq const a, Vec8uq const b) {
     return Vec8uq(Vec8q(a) ^ Vec8q(b));
 }
 
@@ -1740,28 +1239,41 @@ static inline Vec8uq operator ^ (Vec8uq const & a, Vec8uq const & b) {
 
 // Select between two operands. Corresponds to this pseudocode:
 // for (int i = 0; i < 4; i++) result[i] = s[i] ? a[i] : b[i];
-static inline Vec8uq select (Vec8qb const & s, Vec8uq const & a, Vec8uq const & b) {
+static inline Vec8uq select (Vec8qb const s, Vec8uq const a, Vec8uq const b) {
     return Vec8uq(select(s, Vec8q(a), Vec8q(b)));
 }
 
 // Conditional add: For all vector elements i: result[i] = f[i] ? (a[i] + b[i]) : a[i]
-static inline Vec8uq if_add (Vec8qb const & f, Vec8uq const & a, Vec8uq const & b) {
-    return _mm512_mask_add_epi64(a, f, a, b);
+static inline Vec8uq if_add (Vec8qb const f, Vec8uq const a, Vec8uq const b) {
+    return _mm512_mask_add_epi64(a, (uint8_t)f, a, b);
+}
+
+// Conditional subtract
+static inline Vec8uq if_sub (Vec8qb const f, Vec8uq const a, Vec8uq const b) {
+    return _mm512_mask_sub_epi64(a, (uint8_t)f, a, b);
 }
 
-// Horizontal add: Calculates the sum of all vector elements.
-// Overflow will wrap around
-static inline uint64_t horizontal_add (Vec8uq const & a) {
-    return horizontal_add(Vec8q(a));
+// Conditional multiply
+static inline Vec8uq if_mul (Vec8qb const f, Vec8uq const a, Vec8uq const b) {
+#if INSTRSET >= 10
+    return _mm512_mask_mullo_epi64(a, f, a, b);  // AVX512DQ
+#else
+    return select(f, a*b, a);
+#endif
+}
+
+// Horizontal add: Calculates the sum of all vector elements. Overflow will wrap around
+static inline uint64_t horizontal_add (Vec8uq const a) {
+    return (uint64_t)horizontal_add(Vec8q(a));
 }
 
 // function max: a > b ? a : b
-static inline Vec8uq max(Vec8uq const & a, Vec8uq const & b) {
+static inline Vec8uq max(Vec8uq const a, Vec8uq const b) {
     return _mm512_max_epu64(a, b);
 }
 
 // function min: a < b ? a : b
-static inline Vec8uq min(Vec8uq const & a, Vec8uq const & b) {
+static inline Vec8uq min(Vec8uq const a, Vec8uq const b) {
     return _mm512_min_epu64(a, b);
 }
 
@@ -1773,180 +1285,163 @@ static inline Vec8uq min(Vec8uq const & a, Vec8uq const & b) {
 ******************************************************************************
 *
 * These permute functions can reorder the elements of a vector and optionally
-* set some elements to zero. 
+* set some elements to zero. See Vectori128.h for description
 *
-* The indexes are inserted as template parameters in <>. These indexes must be
-* constants. Each template parameter is an index to the element you want to select.
-* An index of -1 will generate zero. An index of -256 means don't care.
-*
-* Example:
-* Vec8q a(10,11,12,13,14,15,16,17);      // a is (10,11,12,13,14,15,16,17)
-* Vec8q b;
-* b = permute8q<0,2,7,7,-1,-1,1,1>(a);   // b is (10,12,17,17, 0, 0,11,11)
-*
-* A lot of the code here is metaprogramming aiming to find the instructions
-* that best fit the template parameters and instruction set. The metacode
-* will be reduced out to leave only a few vector instructions in release
-* mode with optimization on.
 *****************************************************************************/
 
 // Permute vector of 8 64-bit integers.
-// Index -1 gives 0, index -256 means don't care.
+// Index -1 gives 0, index V_DC means don't care.
 template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7>
-static inline Vec8q permute8q(Vec8q const & a) {
-
-    // Combine indexes into a single bitfield, with 4 bits for each
-    const int m1 = (i0&7) | (i1&7)<<4 | (i2&7)<< 8 | (i3&7)<<12 | (i4&7)<<16 | (i5&7)<<20 | (i6&7)<<24 | (i7&7)<<28;
-
-    // Mask to zero out negative indexes
-    const int mz = (i0<0?0:0xF) | (i1<0?0:0xF0) | (i2<0?0:0xF00) | (i3<0?0:0xF000) | (i4<0?0:0xF0000) | (i5<0?0:0xF00000) | (i6<0?0:0xF000000) | (i7<0?0:0xF0000000);
-    const int m2 = m1 & mz;
+static inline Vec8q permute8(Vec8q const a) {
+    int constexpr indexs[8] = { i0, i1, i2, i3, i4, i5, i6, i7 }; // indexes as array
+    __m512i y = a;  // result
+    // get flags for possibilities that fit the permutation pattern
+    constexpr uint64_t flags = perm_flags<Vec8q>(indexs);
 
-    // zeroing needed
-    const bool dozero = ((i0|i1|i2|i3|i4|i5|i6|i7) & 0x80) != 0;
+    static_assert((flags & perm_outofrange) == 0, "Index out of range in permute function");
 
-    // special case: all zero
-    if (mz == 0) return  _mm512_setzero_epi32();
+    if constexpr ((flags & perm_allzero) != 0) return _mm512_setzero_si512();  // just return zero
 
-    // mask for elements not zeroed
-    const __mmask16  z = __mmask16((i0>=0)<<0 | (i1>=0)<<1 | (i2>=0)<<2 | (i3>=0)<<3 | (i4>=0)<<4 | (i5>=0)<<5 | (i6>=0)<<6 | (i7>=0)<<7);
-    // same with 2 bits for each element
-    const __mmask16 zz = __mmask16((i0>=0?3:0) | (i1>=0?0xC:0) | (i2>=0?0x30:0) | (i3>=0?0xC0:0) | (i4>=0?0x300:0) | (i5>=0?0xC00:0) | (i6>=0?0x3000:0) | (i7>=0?0xC000:0));
+    if constexpr ((flags & perm_perm) != 0) {              // permutation needed
 
-    if (((m1 ^ 0x76543210) & mz) == 0) {
-        // no shuffling
-        if (dozero) {
-            // zero some elements
-            return _mm512_maskz_mov_epi64(z, a);
+        if constexpr ((flags & perm_largeblock) != 0) {    // use larger permutation
+            constexpr EList<int, 4> L = largeblock_perm<8>(indexs); // permutation pattern
+            constexpr uint8_t  ppat = (L.a[0] & 3) | (L.a[1]<<2 & 0xC) | (L.a[2]<<4 & 0x30) | (L.a[3]<<6 & 0xC0);
+            y = _mm512_shuffle_i64x2(a, a, ppat);
         }
-        return a;                                 // do nothing
-    }
-
-    if (((m1 ^ 0x66442200) & 0x66666666 & mz) == 0) {
-        // no exchange of data between the four 128-bit lanes
-        const int pat = ((m2 | m2 >> 8 | m2 >> 16 | m2 >> 24) & 0x11) * 0x01010101;
-        const int pmask = ((pat & 1) * 10 + 4) | ((((pat >> 4) & 1) * 10 + 4) << 4);
-        if (((m1 ^ pat) & mz & 0x11111111) == 0) {
-            // same permute pattern in all lanes
-            if (dozero) {  // permute within lanes and zero
-                return _mm512_maskz_shuffle_epi32(zz, a, (_MM_PERM_ENUM)pmask);
+        else if constexpr ((flags & perm_same_pattern) != 0) {  // same pattern in all lanes
+            if constexpr ((flags & perm_punpckh) != 0) {   // fits punpckhi
+                y = _mm512_unpackhi_epi64(y, y);
             }
-            else {  // permute within lanes
-                return _mm512_shuffle_epi32(a, (_MM_PERM_ENUM)pmask);
+            else if constexpr ((flags & perm_punpckl)!=0){ // fits punpcklo
+                y = _mm512_unpacklo_epi64(y, y);
+            }
+            else { // general permute
+                y = _mm512_shuffle_epi32(a, (_MM_PERM_ENUM)uint8_t(flags >> perm_ipattern));
             }
         }
-        // different permute patterns in each lane. It's faster to do a full permute than four masked permutes within lanes
-    }
-    if ((((m1 ^ 0x10101010) & 0x11111111 & mz) == 0) 
-    &&  ((m1 ^ (m1 >> 4)) & 0x06060606 & mz & (mz >> 4)) == 0) {
-        // permute lanes only. no permutation within each lane
-        const int m3 = m2 | (m2 >> 4);
-        const int s = ((m3 >> 1) & 3) | (((m3 >> 9) & 3) << 2) | (((m3 >> 17) & 3) << 4) | (((m3 >> 25) & 3) << 6);
-        if (dozero) {
-            // permute lanes and zero some 64-bit elements
-            return  _mm512_maskz_shuffle_i64x2(z, a, a, (_MM_PERM_ENUM)s);
-        }
-        else {
-            // permute lanes
-            return _mm512_shuffle_i64x2(a, a, (_MM_PERM_ENUM)s);
+        else {  // different patterns in all lanes
+            if constexpr ((flags & perm_rotate_big) != 0) {// fits big rotate
+                constexpr uint8_t rot = uint8_t(flags >> perm_rot_count); // rotation count
+                y = _mm512_alignr_epi64 (y, y, rot);
+            }
+            else if constexpr ((flags & perm_broadcast) != 0) { // broadcast one element
+                constexpr int e = flags >> perm_rot_count;
+                if constexpr(e != 0) {
+                    y = _mm512_alignr_epi64(y, y, e);
+                }
+                y = _mm512_broadcastq_epi64(_mm512_castsi512_si128(y));
+            }
+            else if constexpr ((flags & perm_compress) != 0) {
+                y = _mm512_maskz_compress_epi64(__mmask8(compress_mask(indexs)), y); // compress
+                if constexpr ((flags & perm_addz2) == 0) return y;
+            }
+            else if constexpr ((flags & perm_expand) != 0) {
+                y = _mm512_maskz_expand_epi64(__mmask8(expand_mask(indexs)), y); // expand
+                if constexpr ((flags & perm_addz2) == 0) return y;
+            }
+            else if constexpr ((flags & perm_cross_lane) == 0) {  // no lane crossing. Use pshufb
+                constexpr EList <int8_t, 64> bm = pshufb_mask<Vec8q>(indexs);
+                return _mm512_shuffle_epi8(y, Vec8q().load(bm.a));
+            }
+            else {
+                // full permute needed
+                const __m512i pmask = constant16ui <
+                    i0 & 7, 0, i1 & 7, 0, i2 & 7, 0, i3 & 7, 0, i4 & 7, 0, i5 & 7, 0, i6 & 7, 0, i7 & 7, 0>();
+                y = _mm512_permutexvar_epi64(pmask, y);
+            }
         }
     }
-    // full permute needed
-    const __m512i pmask = constant16i<i0&7, 0, i1&7, 0, i2&7, 0, i3&7, 0, i4&7, 0, i5&7, 0, i6&7, 0, i7&7, 0>();
-    if (dozero) {
-        // full permute and zeroing
-        // Documentation is inconsistent. which order of the operands is correct?
-        return _mm512_maskz_permutexvar_epi64(z, pmask, a);
-    }
-    else {    
-        return _mm512_permutexvar_epi64(pmask, a);
+    if constexpr ((flags & perm_zeroing) != 0) {           // additional zeroing needed
+        y = _mm512_maskz_mov_epi64(zero_mask<8>(indexs), y);
     }
+    return y;
 }
 
 template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7>
-static inline Vec8uq permute8uq(Vec8uq const & a) {
-    return Vec8uq (permute8q<i0,i1,i2,i3,i4,i5,i6,i7> (a));
+static inline Vec8uq permute8(Vec8uq const a) {
+    return Vec8uq (permute8<i0,i1,i2,i3,i4,i5,i6,i7> (Vec8q(a)));
 }
 
 
 // Permute vector of 16 32-bit integers.
-// Index -1 gives 0, index -256 means don't care.
+// Index -1 gives 0, index V_DC means don't care.
 template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7, int i8, int i9, int i10, int i11, int i12, int i13, int i14, int i15>
-static inline Vec16i permute16i(Vec16i const & a) {
+static inline Vec16i permute16(Vec16i const a) {
+    int constexpr indexs[16] = {  // indexes as array
+        i0, i1, i2, i3, i4, i5, i6, i7, i8, i9, i10, i11, i12, i13, i14, i15 };
+    __m512i y = a;  // result
+    // get flags for possibilities that fit the permutation pattern
+    constexpr uint64_t flags = perm_flags<Vec16i>(indexs);
 
-    // Combine indexes into a single bitfield, with 4 bits for each
-    const uint64_t m1 = (i0&15) | (i1&15)<<4 | (i2&15)<< 8 | (i3&15)<<12 | (i4&15)<<16 | (i5&15)<<20 | (i6&15)<<24 | (i7&15LL)<<28   // 15LL avoids sign extension of (int32_t | int64_t)
-        | (i8&15LL)<<32 | (i9&15LL)<<36 | (i10&15LL)<<40 | (i11&15LL)<<44 | (i12&15LL)<<48 | (i13&15LL)<<52 | (i14&15LL)<<56 | (i15&15LL)<<60;
+    static_assert((flags & perm_outofrange) == 0, "Index out of range in permute function");
 
-    // Mask to zero out negative indexes
-    const uint64_t mz = (i0<0?0:0xF) | (i1<0?0:0xF0) | (i2<0?0:0xF00) | (i3<0?0:0xF000) | (i4<0?0:0xF0000) | (i5<0?0:0xF00000) | (i6<0?0:0xF000000) | (i7<0?0:0xF0000000ULL) | (i8<0?0:0xF00000000) 
-        | (i9<0?0:0xF000000000) | (i10<0?0:0xF0000000000) | (i11<0?0:0xF00000000000) | (i12<0?0:0xF000000000000) | (i13<0?0:0xF0000000000000) | (i14<0?0:0xF00000000000000) | (i15<0?0:0xF000000000000000);
+    if constexpr ((flags & perm_allzero) != 0) return _mm512_setzero_si512();  // just return zero
 
-    const uint64_t m2 = m1 & mz;
+    if constexpr ((flags & perm_perm) != 0) {              // permutation needed
 
-    // zeroing needed
-    const bool dozero = ((i0|i1|i2|i3|i4|i5|i6|i7|i8|i9|i10|i11|i12|i13|i14|i15) & 0x80) != 0;
-
-    // special case: all zero
-    if (mz == 0) return  _mm512_setzero_epi32();
-
-    // mask for elements not zeroed
-    const __mmask16 z = __mmask16((i0>=0)<<0 | (i1>=0)<<1 | (i2>=0)<<2 | (i3>=0)<<3 | (i4>=0)<<4 | (i5>=0)<<5 | (i6>=0)<<6 | (i7>=0)<<7
-        | (i8>=0)<<8 | (i9>=0)<<9 | (i10>=0)<<10 | (i11>=0)<<11 | (i12>=0)<<12 | (i13>=0)<<13 | (i14>=0)<<14 | (i15>=0)<<15);
-
-    if (((m1 ^ 0xFEDCBA9876543210) & mz) == 0) {
-        // no shuffling
-        if (dozero) {
-            // zero some elements
-            return _mm512_maskz_mov_epi32(z, a);
+        if constexpr ((flags & perm_largeblock) != 0) {    // use larger permutation
+            constexpr EList<int, 8> L = largeblock_perm<16>(indexs); // permutation pattern
+            y = permute8 <L.a[0], L.a[1], L.a[2], L.a[3], L.a[4], L.a[5], L.a[6], L.a[7]> (Vec8q(a));
+            if (!(flags & perm_addz)) return y;            // no remaining zeroing
         }
-        return a;                                 // do nothing
-    }
-
-    if (((m1 ^ 0xCCCC888844440000) & 0xCCCCCCCCCCCCCCCC & mz) == 0) {
-        // no exchange of data between the four 128-bit lanes
-        const uint64_t pat = ((m2 | (m2 >> 16) | (m2 >> 32) | (m2 >> 48)) & 0x3333) * 0x0001000100010001;
-        const int pmask = (pat & 3) | (((pat >> 4) & 3) << 2) | (((pat >> 8) & 3) << 4) | (((pat >> 12) & 3) << 6);
-        if (((m1 ^ pat) & 0x3333333333333333 & mz) == 0) {
-            // same permute pattern in all lanes
-            if (dozero) {  // permute within lanes and zero
-                return _mm512_maskz_shuffle_epi32(z, a, (_MM_PERM_ENUM)pmask);
+        else if constexpr ((flags & perm_same_pattern) != 0) {  // same pattern in all lanes
+            if constexpr ((flags & perm_punpckh) != 0) {   // fits punpckhi
+                y = _mm512_unpackhi_epi32(y, y);
             }
-            else {  // permute within lanes
-                return _mm512_shuffle_epi32(a, (_MM_PERM_ENUM)pmask);
+            else if constexpr ((flags & perm_punpckl)!=0){ // fits punpcklo
+                y = _mm512_unpacklo_epi32(y, y);
+            }
+            else { // general permute
+                y = _mm512_shuffle_epi32(a, (_MM_PERM_ENUM)uint8_t(flags >> perm_ipattern));
             }
         }
-        // different permute patterns in each lane. It's faster to do a full permute than four masked permutes within lanes
-    }
-    const uint64_t lane = (m2 | m2 >> 4 | m2 >> 8 | m2 >> 12) & 0x000C000C000C000C;
-    if ((((m1 ^ 0x3210321032103210) & 0x3333333333333333 & mz) == 0) 
-    &&  ((m1 ^ (lane * 0x1111)) & 0xCCCCCCCCCCCCCCCC & mz) == 0) {
-        // permute lanes only. no permutation within each lane
-        const uint64_t s = ((lane >> 2) & 3) | (((lane >> 18) & 3) << 2) | (((lane >> 34) & 3) << 4) | (((lane >> 50) & 3) << 6);
-        if (dozero) {
-            // permute lanes and zero some 64-bit elements
-            return  _mm512_maskz_shuffle_i32x4(z, a, a, (_MM_PERM_ENUM)s);
-        }
-        else {
-            // permute lanes
-            return _mm512_shuffle_i32x4(a, a, (_MM_PERM_ENUM)s);
+        else {  // different patterns in all lanes
+            if constexpr ((flags & perm_rotate_big) != 0) {// fits big rotate
+                constexpr uint8_t rot = uint8_t(flags >> perm_rot_count); // rotation count
+                return _mm512_maskz_alignr_epi32 (zero_mask<16>(indexs), y, y, rot);
+            }
+            else if constexpr ((flags & perm_broadcast) != 0) { // broadcast one element
+                constexpr int e = flags >> perm_rot_count; // element index
+                if constexpr(e != 0) {
+                    y = _mm512_alignr_epi32(y, y, e);
+                }
+                y = _mm512_broadcastd_epi32(_mm512_castsi512_si128(y));
+            }
+            else if constexpr ((flags & perm_zext) != 0) {
+                y = _mm512_cvtepu32_epi64(_mm512_castsi512_si256(y)); // zero extension
+                if constexpr ((flags & perm_addz2) == 0) return y;
+            }
+            else if constexpr ((flags & perm_compress) != 0) {
+                y = _mm512_maskz_compress_epi32(__mmask16(compress_mask(indexs)), y); // compress
+                if constexpr ((flags & perm_addz2) == 0) return y;
+            }
+            else if constexpr ((flags & perm_expand) != 0) {
+                y = _mm512_maskz_expand_epi32(__mmask16(expand_mask(indexs)), y); // expand
+                if constexpr ((flags & perm_addz2) == 0) return y;
+            }
+            else if constexpr ((flags & perm_cross_lane) == 0) { // no lane crossing. Use pshufb
+                constexpr EList <int8_t, 64> bm = pshufb_mask<Vec16i>(indexs);
+                return _mm512_shuffle_epi8(a, Vec16i().load(bm.a));
+            }
+            else {
+                // full permute needed
+                const __m512i pmask = constant16ui <
+                    i0 & 15, i1 & 15, i2 & 15, i3 & 15, i4 & 15, i5 & 15, i6 & 15, i7 & 15,
+                    i8 & 15, i9 & 15, i10 & 15, i11 & 15, i12 & 15, i13 & 15, i14 & 15, i15 & 15>();
+                return _mm512_maskz_permutexvar_epi32(zero_mask<16>(indexs), pmask, a);
+            }
         }
     }
-    // full permute needed
-    const __m512i pmask = constant16i<i0&15, i1&15, i2&15, i3&15, i4&15, i5&15, i6&15, i7&15, i8&15, i9&15, i10&15, i11&15, i12&15, i13&15, i14&15, i15&15>();
-    if (dozero) {
-        // full permute and zeroing
-        return _mm512_maskz_permutexvar_epi32(z, pmask, a);
-    }
-    else {    
-        return _mm512_permutexvar_epi32(pmask, a);
+    if constexpr ((flags & perm_zeroing) != 0) {           // additional zeroing needed
+        y = _mm512_maskz_mov_epi32(zero_mask<16>(indexs), y);
     }
+    return y;
 }
 
-
 template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7, int i8, int i9, int i10, int i11, int i12, int i13, int i14, int i15>
-static inline Vec16ui permute16ui(Vec16ui const & a) {
-    return Vec16ui (permute16i<i0,i1,i2,i3,i4,i5,i6,i7,i8,i9,i10,i11,i12,i13,i14,i15> (a));
+static inline Vec16ui permute16(Vec16ui const a) {
+    return Vec16ui (permute16<i0,i1,i2,i3,i4,i5,i6,i7,i8,i9,i10,i11,i12,i13,i14,i15> (Vec16i(a)));
 }
 
 
@@ -1954,205 +1449,191 @@ static inline Vec16ui permute16ui(Vec16ui const & a) {
 *
 *          Vector blend functions
 *
-******************************************************************************
-*
-* These blend functions can mix elements from two different vectors and
-* optionally set some elements to zero. 
-*
-* The indexes are inserted as template parameters in <>. These indexes must be
-* constants. Each template parameter is an index to the element you want to 
-* select, where higher indexes indicate an element from the second source
-* vector. For example, if each vector has 8 elements, then indexes 0 - 7
-* will select an element from the first vector and indexes 8 - 15 will select 
-* an element from the second vector. A negative index will generate zero.
-*
-* Example:
-* Vec8q a(100,101,102,103,104,105,106,107); // a is (100, 101, 102, 103, 104, 105, 106, 107)
-* Vec8q b(200,201,202,203,204,205,206,207); // b is (200, 201, 202, 203, 204, 205, 206, 207)
-* Vec8q c;
-* c = blend8q<1,0,9,8,7,-1,15,15> (a,b);    // c is (101, 100, 201, 200, 107,   0, 207, 207)
-*
-* A lot of the code here is metaprogramming aiming to find the instructions
-* that best fit the template parameters and instruction set. The metacode
-* will be reduced out to leave only a few vector instructions in release
-* mode with optimization on.
 *****************************************************************************/
 
+// permute and blend Vec8q
+template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7>
+static inline Vec8q blend8(Vec8q const a, Vec8q const b) {
+    int constexpr indexs[8] = { i0, i1, i2, i3, i4, i5, i6, i7 }; // indexes as array
+    __m512i y = a;                                         // result
+    constexpr uint64_t flags = blend_flags<Vec8q>(indexs); // get flags for possibilities that fit the index pattern
 
-template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7> 
-static inline Vec8q blend8q(Vec8q const & a, Vec8q const & b) {  
-
-    // Combine indexes into a single bitfield, with 4 bits for each
-    const int m1 = (i0&0xF) | (i1&0xF)<<4 | (i2&0xF)<< 8 | (i3&0xF)<<12 | (i4&0xF)<<16 | (i5&0xF)<<20 | (i6&0xF)<<24 | (i7&0xF)<<28;
-
-    // Mask to zero out negative indexes
-    const int mz = (i0<0?0:0xF) | (i1<0?0:0xF0) | (i2<0?0:0xF00) | (i3<0?0:0xF000) | (i4<0?0:0xF0000) | (i5<0?0:0xF00000) | (i6<0?0:0xF000000) | (i7<0?0:0xF0000000);
-    const int m2 = m1 & mz;
-
-    // zeroing needed
-    const bool dozero = ((i0|i1|i2|i3|i4|i5|i6|i7) & 0x80) != 0;
-
-    // mask for elements not zeroed
-    const __mmask16 z = __mmask16((i0>=0)<<0 | (i1>=0)<<1 | (i2>=0)<<2 | (i3>=0)<<3 | (i4>=0)<<4 | (i5>=0)<<5 | (i6>=0)<<6 | (i7>=0)<<7);
+    static_assert((flags & blend_outofrange) == 0, "Index out of range in blend function");
 
-    // special case: all zero
-    if (mz == 0) return  _mm512_setzero_epi32();
+    if constexpr ((flags & blend_allzero) != 0) return _mm512_setzero_si512(); // just return zero
 
-    // special case: all from a
-    if ((m1 & 0x88888888 & mz) == 0) {
-        return permute8q <i0, i1, i2, i3, i4, i5, i6, i7> (a);
+    if constexpr ((flags & blend_b) == 0) {                // nothing from b. just permute a
+        return permute8 <i0, i1, i2, i3, i4, i5, i6, i7> (a);
     }
-
-    // special case: all from b
-    if ((~m1 & 0x88888888 & mz) == 0) {
-        return permute8q <i0^8, i1^8, i2^8, i3^8, i4^8, i5^8, i6^8, i7^8> (b);
+    if constexpr ((flags & blend_a) == 0) {                // nothing from a. just permute b
+        constexpr EList<int, 16> L = blend_perm_indexes<8, 2>(indexs); // get permutation indexes
+        return permute8 < L.a[8], L.a[9], L.a[10], L.a[11], L.a[12], L.a[13], L.a[14], L.a[15] > (b);
     }
-
-    // special case: blend without permute
-    if (((m1 ^ 0x76543210) & 0x77777777 & mz) == 0) {
-        __mmask16 blendmask = __mmask16((i0&8)>>3 | (i1&8)>>2 | (i2&8)>>1 | (i3&8)>>0 | (i4&8)<<1 | (i5&8)<<2 | (i6&8)<<3 | (i7&8)<<4 );
-        __m512i t = _mm512_mask_blend_epi64(blendmask, a, b);
-        if (dozero) {
-            t = _mm512_maskz_mov_epi64(z, t);
+    if constexpr ((flags & (blend_perma | blend_permb)) == 0) { // no permutation, only blending
+        constexpr uint8_t mb = (uint8_t)make_bit_mask<8, 0x303>(indexs);  // blend mask
+        y = _mm512_mask_mov_epi64 (a, mb, b);
+    }
+    else if constexpr ((flags & blend_rotate_big) != 0) {  // full rotate
+        constexpr uint8_t rot = uint8_t(flags >> blend_rotpattern); // rotate count
+        if constexpr (rot < 8) {
+            y = _mm512_alignr_epi64(b, a, rot);
         }
-        return t;
-    }
-    // special case: all data stay within their lane
-    if (((m1 ^ 0x66442200) & 0x66666666 & mz) == 0) {
-        // mask for elements from a and b
-        const uint32_t mb = ((i0&8)?0xF:0) | ((i1&8)?0xF0:0) | ((i2&8)?0xF00:0) | ((i3&8)?0xF000:0) | ((i4&8)?0xF0000:0) | ((i5&8)?0xF00000:0) | ((i6&8)?0xF000000:0) | ((i7&8)?0xF0000000:0);
-        const uint32_t mbz = mb & mz;     // mask for nonzero elements from b
-        const uint32_t maz = ~mb & mz;    // mask for nonzero elements from a
-        const uint32_t m1a = m1 & maz;
-        const uint32_t m1b = m1 & mbz;
-        const uint32_t pata = ((m1a | m1a >> 8 | m1a >> 16 | m1a >> 24) & 0xFF) * 0x01010101;  // permute pattern for elements from a
-        const uint32_t patb = ((m1b | m1b >> 8 | m1b >> 16 | m1b >> 24) & 0xFF) * 0x01010101;  // permute pattern for elements from b
-        if (((m1 ^ pata) & 0x11111111 & maz) == 0 && ((m1 ^ patb) & 0x11111111 & mbz) == 0) {
-            // Same permute pattern in all lanes:
-            // This code generates two instructions instead of one, but we are avoiding the slow lane-crossing instruction,
-            // and we are saving 64 bytes of data cache.
-            // 1. Permute a, zero elements not from a (using _mm512_maskz_shuffle_epi32)
-            __m512i ta = permute8q< (maz&0xF)?i0&7:-1, (maz&0xF0)?i1&7:-1, (maz&0xF00)?i2&7:-1, (maz&0xF000)?i3&7:-1, 
-                (maz&0xF0000)?i4&7:-1, (maz&0xF00000)?i5&7:-1, (maz&0xF000000)?i6&7:-1, (maz&0xF0000000)?i7&7:-1> (a);
-            // write mask for elements from b
-            const __mmask16 sb = ((mbz&0xF)?3:0) | ((mbz&0xF0)?0xC:0) | ((mbz&0xF00)?0x30:0) | ((mbz&0xF000)?0xC0:0) | ((mbz&0xF0000)?0x300:0) | ((mbz&0xF00000)?0xC00:0) | ((mbz&0xF000000)?0x3000:0) | ((mbz&0xF0000000)?0xC000:0);
-            // permute index for elements from b
-            const int pi = ((patb & 1) * 10 + 4) | ((((patb >> 4) & 1) * 10 + 4) << 4);
-            // 2. Permute elements from b and combine with elements from a through write mask
-            return _mm512_mask_shuffle_epi32(ta, sb, b, (_MM_PERM_ENUM)pi);
+        else {
+            y = _mm512_alignr_epi64(a, b, rot & 7);
         }
-        // not same permute pattern in all lanes. use full permute
     }
-    // general case: full permute
-    const __m512i pmask = constant16i<i0&0xF, 0, i1&0xF, 0, i2&0xF, 0, i3&0xF, 0, i4&0xF, 0, i5&0xF, 0, i6&0xF, 0, i7&0xF, 0>();
-    if (dozero) {
-        return _mm512_maskz_permutex2var_epi64(z, a, pmask, b);
+    else if constexpr ((flags & blend_largeblock) != 0) {  // blend and permute 128-bit blocks
+        constexpr EList<int, 4> L = largeblock_perm<8>(indexs); // get 128-bit blend pattern
+        constexpr uint8_t shuf = (L.a[0] & 3) | (L.a[1] & 3) << 2 | (L.a[2] & 3) << 4 | (L.a[3] & 3) << 6;
+        if constexpr (make_bit_mask<8, 0x103>(indexs) == 0) {  // fits vshufi64x2 (a,b)
+            y = _mm512_shuffle_i64x2(a, b, shuf);
+        }
+        else if constexpr (make_bit_mask<8, 0x203>(indexs) == 0) { // fits vshufi64x2 (b,a)
+            y = _mm512_shuffle_i64x2(b, a, shuf);
+        }
+        else {
+            constexpr EList <int64_t, 8> bm = perm_mask_broad<Vec8q>(indexs);   // full permute
+            y = _mm512_permutex2var_epi64(a, Vec8q().load(bm.a), b);
+        }
     }
-    else {
-        return _mm512_permutex2var_epi64(a, pmask, b);
+    // check if pattern fits special cases
+    else if constexpr ((flags & blend_punpcklab) != 0) {
+        y = _mm512_unpacklo_epi64 (a, b);
     }
+    else if constexpr ((flags & blend_punpcklba) != 0) {
+        y = _mm512_unpacklo_epi64 (b, a);
+    }
+    else if constexpr ((flags & blend_punpckhab) != 0) {
+        y = _mm512_unpackhi_epi64 (a, b);
+    }
+    else if constexpr ((flags & blend_punpckhba) != 0) {
+        y = _mm512_unpackhi_epi64 (b, a);
+    }
+#if ALLOW_FP_PERMUTE  // allow floating point permute instructions on integer vectors
+    else if constexpr ((flags & blend_shufab) != 0) {      // use floating point instruction shufpd
+        y = _mm512_castpd_si512(_mm512_shuffle_pd(_mm512_castsi512_pd(a), _mm512_castsi512_pd(b), uint8_t(flags >> blend_shufpattern)));
+    }
+    else if constexpr ((flags & blend_shufba) != 0) {      // use floating point instruction shufpd
+        y = _mm512_castpd_si512(_mm512_shuffle_pd(_mm512_castsi512_pd(b), _mm512_castsi512_pd(a), uint8_t(flags >> blend_shufpattern)));
+    }
+#else
+    // we might use 2 x _mm512_mask(z)_shuffle_epi32 like in blend16 below
+#endif
+    else { // No special cases
+        constexpr EList <int64_t, 8> bm = perm_mask_broad<Vec8q>(indexs);   // full permute
+        y = _mm512_permutex2var_epi64(a, Vec8q().load(bm.a), b);
+    }
+    if constexpr ((flags & blend_zeroing) != 0) {          // additional zeroing needed
+        y = _mm512_maskz_mov_epi64(zero_mask<8>(indexs), y);
+    }
+    return y;
 }
 
-template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7> 
-static inline Vec8uq blend8uq(Vec8uq const & a, Vec8uq const & b) {
-    return Vec8uq( blend8q<i0,i1,i2,i3,i4,i5,i6,i7> (a,b));
+template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7>
+static inline Vec8uq blend8(Vec8uq const a, Vec8uq const b) {
+    return Vec8uq( blend8<i0,i1,i2,i3,i4,i5,i6,i7> (Vec8q(a),Vec8q(b)));
 }
 
 
-template <int i0,  int i1,  int i2,  int i3,  int i4,  int i5,  int i6,  int i7, 
-          int i8,  int i9,  int i10, int i11, int i12, int i13, int i14, int i15 > 
-static inline Vec16i blend16i(Vec16i const & a, Vec16i const & b) {  
-
-    // Combine indexes into a single bitfield, with 4 bits for each indicating shuffle, but not source
-    const uint64_t m1 = (i0&0xF) | (i1&0xF)<<4 | (i2&0xF)<<8 | (i3&0xF)<<12 | (i4&0xF)<<16 | (i5&0xF)<<20 | (i6&0xF)<<24 | (i7&0xFLL)<<28
-        | (i8&0xFLL)<<32 | (i9&0xFLL)<<36 | (i10&0xFLL)<<40 | (i11&0xFLL)<<44 | (i12&0xFLL)<<48 | (i13&0xFLL)<<52 | (i14&0xFLL)<<56 | (i15&0xFLL)<<60;
-
-    // Mask to zero out negative indexes
-    const uint64_t mz = (i0<0?0:0xF) | (i1<0?0:0xF0) | (i2<0?0:0xF00) | (i3<0?0:0xF000) | (i4<0?0:0xF0000) | (i5<0?0:0xF00000) | (i6<0?0:0xF000000) | (i7<0?0:0xF0000000ULL)
-        | (i8<0?0:0xF00000000) | (i9<0?0:0xF000000000) | (i10<0?0:0xF0000000000) | (i11<0?0:0xF00000000000) | (i12<0?0:0xF000000000000) | (i13<0?0:0xF0000000000000) | (i14<0?0:0xF00000000000000) | (i15<0?0:0xF000000000000000);
-    const uint64_t m2 = m1 & mz;
+// permute and blend Vec16i
+template <int i0,  int i1,  int i2,  int i3,  int i4,  int i5,  int i6,  int i7,
+          int i8,  int i9,  int i10, int i11, int i12, int i13, int i14, int i15 >
+static inline Vec16i blend16(Vec16i const a, Vec16i const b) {
+    int constexpr indexs[16] = { i0, i1, i2, i3, i4, i5, i6, i7, i8, i9, i10, i11, i12, i13, i14, i15}; // indexes as array
+    __m512i y = a;                                         // result
+    constexpr uint64_t flags = blend_flags<Vec16i>(indexs);// get flags for possibilities that fit the index pattern
 
-    // collect bit 4 of each index = select source
-    const uint64_t ms = ((i0&16)?0xF:0) | ((i1&16)?0xF0:0) | ((i2&16)?0xF00:0) | ((i3&16)?0xF000:0) | ((i4&16)?0xF0000:0) | ((i5&16)?0xF00000:0) | ((i6&16)?0xF000000:0) | ((i7&16)?0xF0000000ULL:0)
-        | ((i8&16)?0xF00000000:0) | ((i9&16)?0xF000000000:0) | ((i10&16)?0xF0000000000:0) | ((i11&16)?0xF00000000000:0) | ((i12&16)?0xF000000000000:0) | ((i13&16)?0xF0000000000000:0) | ((i14&16)?0xF00000000000000:0) | ((i15&16)?0xF000000000000000:0);
+    static_assert((flags & blend_outofrange) == 0, "Index out of range in blend function");
 
-    // zeroing needed
-    const bool dozero = ((i0|i1|i2|i3|i4|i5|i6|i7|i8|i9|i10|i11|i12|i13|i14|i15) & 0x80) != 0;
+    if constexpr ((flags & blend_allzero) != 0) return _mm512_setzero_si512();  // just return zero
 
-    // mask for elements not zeroed
-    const __mmask16 z = __mmask16((i0>=0)<<0 | (i1>=0)<<1 | (i2>=0)<<2 | (i3>=0)<<3 | (i4>=0)<<4 | (i5>=0)<<5 | (i6>=0)<<6 | (i7>=0)<<7 
-        | (i8>=0)<<8 | (i9>=0)<<9 | (i10>=0)<<10 | (i11>=0)<<11 | (i12>=0)<<12 | (i13>=0)<<13 | (i14>=0)<<14 | (i15>=0)<<15);
-
-    // special case: all zero
-    if (mz == 0) return  _mm512_setzero_epi32();
-
-    // special case: all from a
-    if ((ms & mz) == 0) {
-        return permute16i<i0,i1,i2,i3,i4,i5,i6,i7,i8,i9,i10,i11,i12,i13,i14,i15> (a);
+    if constexpr ((flags & blend_b) == 0) {                // nothing from b. just permute a
+        return permute16 <i0, i1, i2, i3, i4, i5, i6, i7, i8, i9, i10, i11, i12, i13, i14, i15> (a);
     }
-
-    // special case: all from b
-    if ((~ms & mz) == 0) {
-        return permute16i<i0^16,i1^16,i2^16,i3^16,i4^16,i5^16,i6^16,i7^16,i8^16,i9^16,i10^16,i11^16,i12^16,i13^16,i14^16,i15^16 > (b);
+    if constexpr ((flags & blend_a) == 0) {                // nothing from a. just permute b
+        constexpr EList<int, 32> L = blend_perm_indexes<16, 2>(indexs); // get permutation indexes
+        return permute16 <
+            L.a[16], L.a[17], L.a[18], L.a[19], L.a[20], L.a[21], L.a[22], L.a[23],
+            L.a[24], L.a[25], L.a[26], L.a[27], L.a[28], L.a[29], L.a[30], L.a[31] > (b);
     }
-
-    // special case: blend without permute
-    if (((m1 ^ 0xFEDCBA9876543210) & mz) == 0) {
-        __mmask16 blendmask = __mmask16((i0&16)>>4 | (i1&16)>>3 | (i2&16)>>2 | (i3&16)>>1 | (i4&16) | (i5&16)<<1 | (i6&16)<<2 | (i7&16)<<3
-            | (i8&16)<<4 | (i9&16)<<5 | (i10&16)<<6 | (i11&16)<<7 | (i12&16)<<8 | (i13&16)<<9 | (i14&16)<<10 | (i15&16)<<11);
-        __m512i t = _mm512_mask_blend_epi32(blendmask, a, b);
-        if (dozero) {
-            t = _mm512_maskz_mov_epi32(z, t);
+    if constexpr ((flags & (blend_perma | blend_permb)) == 0) { // no permutation, only blending
+        constexpr uint16_t mb = (uint16_t)make_bit_mask<16, 0x304>(indexs);  // blend mask
+        y = _mm512_mask_mov_epi32 (a, mb, b);
+    }
+    else if constexpr ((flags & blend_largeblock) != 0) {  // blend and permute 64-bit blocks
+        constexpr EList<int, 8> L = largeblock_perm<16>(indexs); // get 64-bit blend pattern
+        y = blend8<L.a[0], L.a[1], L.a[2], L.a[3], L.a[4], L.a[5], L.a[6], L.a[7] >
+            (Vec8q(a), Vec8q(b));
+        if (!(flags & blend_addz)) return y;               // no remaining zeroing
+    }
+    else if constexpr ((flags & blend_same_pattern) != 0) {
+        // same pattern in all 128-bit lanes. check if pattern fits special cases
+        if constexpr ((flags & blend_punpcklab) != 0) {
+            y = _mm512_unpacklo_epi32(a, b);
+        }
+        else if constexpr ((flags & blend_punpcklba) != 0) {
+            y = _mm512_unpacklo_epi32(b, a);
+        }
+        else if constexpr ((flags & blend_punpckhab) != 0) {
+            y = _mm512_unpackhi_epi32(a, b);
+        }
+        else if constexpr ((flags & blend_punpckhba) != 0) {
+            y = _mm512_unpackhi_epi32(b, a);
+        }
+#if ALLOW_FP_PERMUTE  // allow floating point permute instructions on integer vectors
+        else if constexpr ((flags & blend_shufab) != 0) {  // use floating point instruction shufpd
+            y = _mm512_castps_si512(_mm512_shuffle_ps(_mm512_castsi512_ps(a), _mm512_castsi512_ps(b), uint8_t(flags >> blend_shufpattern)));
+        }
+        else if constexpr ((flags & blend_shufba) != 0) {  // use floating point instruction shufpd
+            y = _mm512_castps_si512(_mm512_shuffle_ps(_mm512_castsi512_ps(b), _mm512_castsi512_ps(a), uint8_t(flags >> blend_shufpattern)));
+        }
+#endif
+        else {
+            // Use vpshufd twice. This generates two instructions in the dependency chain,
+            // but we are avoiding the slower lane-crossing instruction, and saving 64
+            // bytes of data cache.
+            auto shuf = [](int const (&a)[16]) constexpr { // get pattern for vpshufd
+                int pat[4] = {-1,-1,-1,-1};
+                for (int i = 0; i < 16; i++) {
+                    int ix = a[i];
+                    if (ix >= 0 && pat[i&3] < 0) {
+                        pat[i&3] = ix;
+                    }
+                }
+                return (pat[0] & 3) | (pat[1] & 3) << 2 | (pat[2] & 3) << 4 | (pat[3] & 3) << 6;
+            };
+            constexpr uint8_t  pattern = uint8_t(shuf(indexs));                    // permute pattern
+            constexpr uint16_t froma = (uint16_t)make_bit_mask<16, 0x004>(indexs); // elements from a
+            constexpr uint16_t fromb = (uint16_t)make_bit_mask<16, 0x304>(indexs); // elements from b
+            y = _mm512_maskz_shuffle_epi32(   froma, a, (_MM_PERM_ENUM) pattern);
+            y = _mm512_mask_shuffle_epi32 (y, fromb, b, (_MM_PERM_ENUM) pattern);
+            return y;  // we have already zeroed any unused elements
+        }
+    }
+    else if constexpr ((flags & blend_rotate_big) != 0) {  // full rotate
+        constexpr uint8_t rot = uint8_t(flags >> blend_rotpattern); // rotate count
+        if constexpr (rot < 16) {
+            y = _mm512_alignr_epi32(b, a, rot);
         }
-        return t;
-    }
-
-    // special case: all data stay within their lane
-    if (((m1 ^ 0xCCCC888844440000) & 0xCCCCCCCCCCCCCCCC & mz) == 0) {
-
-        // mask for elements from a and b
-        const uint64_t mb  = ms;
-        const uint64_t mbz = mb & mz;     // mask for nonzero elements from b
-        const uint64_t maz = ~mb & mz;    // mask for nonzero elements from a
-        const uint64_t m1a = m1 & maz;
-        const uint64_t m1b = m1 & mbz;
-        const uint64_t pata = ((m1a | m1a >> 16 | m1a >> 32 | m1a >> 48) & 0xFFFF) * 0x0001000100010001;  // permute pattern for elements from a
-        const uint64_t patb = ((m1b | m1b >> 16 | m1b >> 32 | m1b >> 48) & 0xFFFF) * 0x0001000100010001;  // permute pattern for elements from b
-        if (((m1 ^ pata) & 0x3333333333333333 & maz) == 0 && ((m1 ^ patb) & 0x3333333333333333 & mbz) == 0) {
-            // Same permute pattern in all lanes:
-            // This code generates two instructions instead of one, but we are avoiding the slow lane-crossing instruction,
-            // and we are saving 64 bytes of data cache.
-            // 1. Permute a, zero elements not from a (using _mm512_maskz_shuffle_epi32)
-            __m512i ta = permute16i< (maz&0xF)?i0&15:-1, (maz&0xF0)?i1&15:-1, (maz&0xF00)?i2&15:-1, (maz&0xF000)?i3&15:-1, 
-                (maz&0xF0000)?i4&15:-1, (maz&0xF00000)?i5&15:-1, (maz&0xF000000)?i6&15:-1, (maz&0xF0000000)?i7&15:-1,
-                (maz&0xF00000000)?i8&15:-1, (maz&0xF000000000)?i9&15:-1, (maz&0xF0000000000)?i10&15:-1, (maz&0xF00000000000)?i11&15:-1, 
-                (maz&0xF000000000000)?i12&15:-1, (maz&0xF0000000000000)?i13&15:-1, (maz&0xF00000000000000)?i14&15:-1, (maz&0xF000000000000000)?i15&15:-1> (a);
-            // write mask for elements from b
-            const __mmask16 sb = ((mbz&0xF)?1:0) | ((mbz&0xF0)?0x2:0) | ((mbz&0xF00)?0x4:0) | ((mbz&0xF000)?0x8:0) | ((mbz&0xF0000)?0x10:0) | ((mbz&0xF00000)?0x20:0) | ((mbz&0xF000000)?0x40:0) | ((mbz&0xF0000000)?0x80:0) 
-                | ((mbz&0xF00000000)?0x100:0) | ((mbz&0xF000000000)?0x200:0) | ((mbz&0xF0000000000)?0x400:0) | ((mbz&0xF00000000000)?0x800:0) | ((mbz&0xF000000000000)?0x1000:0) | ((mbz&0xF0000000000000)?0x2000:0) | ((mbz&0xF00000000000000)?0x4000:0) | ((mbz&0xF000000000000000)?0x8000:0);
-            // permute index for elements from b
-            const int pi = (patb & 3) | (((patb >> 4) & 3) << 2) | (((patb >> 8) & 3) << 4) | (((patb >> 12) & 3) << 6);
-            // 2. Permute elements from b and combine with elements from a through write mask
-            return _mm512_mask_shuffle_epi32(ta, sb, b, (_MM_PERM_ENUM)pi);
+        else {
+            y = _mm512_alignr_epi32(a, b, rot & 0x0F);
         }
-        // not same permute pattern in all lanes. use full permute
     }
 
-    // general case: full permute
-    const __m512i pmask = constant16i<i0&0x1F, i1&0x1F, i2&0x1F, i3&0x1F, i4&0x1F, i5&0x1F, i6&0x1F, i7&0x1F, 
-        i8&0x1F, i9&0x1F, i10&0x1F, i11&0x1F, i12&0x1F, i13&0x1F, i14&0x1F, i15&0x1F>();
-    if (dozero) {
-        return _mm512_maskz_permutex2var_epi32(z, a, pmask, b);        
+    else { // No special cases
+        constexpr EList <int32_t, 16> bm = perm_mask_broad<Vec16i>(indexs);   // full permute
+        y = _mm512_permutex2var_epi32(a, Vec16i().load(bm.a), b);
     }
-    else {
-        return _mm512_permutex2var_epi32(a, pmask, b);
+    if constexpr ((flags & blend_zeroing) != 0) {          // additional zeroing needed
+        y = _mm512_maskz_mov_epi32(zero_mask<16>(indexs), y);
     }
+    return y;
 }
 
-template <int i0,  int i1,  int i2,  int i3,  int i4,  int i5,  int i6,  int i7, 
-          int i8,  int i9,  int i10, int i11, int i12, int i13, int i14, int i15 > 
-static inline Vec16ui blend16ui(Vec16ui const & a, Vec16ui const & b) {
-    return Vec16ui( blend16i<i0,i1,i2,i3,i4,i5,i6,i7,i8,i9,i10,i11,i12,i13,i14,i15> (Vec16i(a),Vec16i(b)));
+template <int i0,  int i1,  int i2,  int i3,  int i4,  int i5,  int i6,  int i7,
+          int i8,  int i9,  int i10, int i11, int i12, int i13, int i14, int i15 >
+static inline Vec16ui blend16(Vec16ui const a, Vec16ui const b) {
+    return Vec16ui( blend16<i0,i1,i2,i3,i4,i5,i6,i7,i8,i9,i10,i11,i12,i13,i14,i15> (Vec16i(a),Vec16i(b)));
 }
 
 
@@ -2165,43 +1646,40 @@ static inline Vec16ui blend16ui(Vec16ui const & a, Vec16ui const & b) {
 * These functions use vector elements as indexes into a table.
 * The table is given as one or more vectors or as an array.
 *
-* This can be used for several purposes:
-*  - table lookup
-*  - permute or blend with variable indexes
-*  - blend from more than two sources
-*  - gather non-contiguous data
-*
-* An index out of range may produce any value - the actual value produced is
-* implementation dependent and may be different for different instruction
-* sets. An index out of range does not produce an error message or exception.
-*
-* Example:
-* Vec8q a(2,0,0,6,4,3,5,0);                 // index a is (  2,   0,   0,   6,   4,   3,   5,   0)
-* Vec8q b(100,101,102,103,104,105,106,107); // table b is (100, 101, 102, 103, 104, 105, 106, 107)
-* Vec8q c;
-* c = lookup8 (a,b);                        // c is       (102, 100, 100, 106, 104, 103, 105, 100)
-*
 *****************************************************************************/
 
-static inline Vec16i lookup16(Vec16i const & index, Vec16i const & table) {
+static inline Vec16i lookup16(Vec16i const index, Vec16i const table) {
     return _mm512_permutexvar_epi32(index, table);
 }
 
+static inline Vec16i lookup32(Vec16i const index, Vec16i const table1, Vec16i const table2) {
+    return _mm512_permutex2var_epi32(table1, index, table2);
+}
+
+static inline Vec16i lookup64(Vec16i const index, Vec16i const table1, Vec16i const table2, Vec16i const table3, Vec16i const table4) {
+    Vec16i d12 = _mm512_permutex2var_epi32(table1, index, table2);
+    Vec16i d34 = _mm512_permutex2var_epi32(table3, index, table4);
+    return select((index >> 5) != 0, d34, d12);
+}
+
 template <int n>
-static inline Vec16i lookup(Vec16i const & index, void const * table) {
-    if (n <= 0) return 0;
-    if (n <= 16) {
+static inline Vec16i lookup(Vec16i const index, void const * table) {
+    if constexpr (n <= 0) return 0;
+    if constexpr (n <= 16) {
         Vec16i table1 = Vec16i().load(table);
         return lookup16(index, table1);
     }
-    if (n <= 32) {
+    if constexpr (n <= 32) {
         Vec16i table1 = Vec16i().load(table);
         Vec16i table2 = Vec16i().load((int8_t*)table + 64);
         return _mm512_permutex2var_epi32(table1, index, table2);
     }
     // n > 32. Limit index
     Vec16ui index1;
-    if ((n & (n-1)) == 0) {
+    if constexpr (n == INT_MAX) {
+        index1 = index;
+    }
+    else if constexpr ((n & (n-1)) == 0) {
         // n is a power of 2, make index modulo n
         index1 = Vec16ui(index) & (n-1);
     }
@@ -2214,25 +1692,28 @@ static inline Vec16i lookup(Vec16i const & index, void const * table) {
 }
 
 
-static inline Vec8q lookup8(Vec8q const & index, Vec8q const & table) {
+static inline Vec8q lookup8(Vec8q const index, Vec8q const table) {
     return _mm512_permutexvar_epi64(index, table);
 }
 
 template <int n>
-static inline Vec8q lookup(Vec8q const & index, void const * table) {
-    if (n <= 0) return 0;
-    if (n <= 8) {
+static inline Vec8q lookup(Vec8q const index, void const * table) {
+    if constexpr (n <= 0) return 0;
+    if constexpr (n <= 8) {
         Vec8q table1 = Vec8q().load(table);
         return lookup8(index, table1);
     }
-    if (n <= 16) {
+    if constexpr (n <= 16) {
         Vec8q table1 = Vec8q().load(table);
         Vec8q table2 = Vec8q().load((int8_t*)table + 64);
         return _mm512_permutex2var_epi64(table1, index, table2);
     }
     // n > 16. Limit index
     Vec8uq index1;
-    if ((n & (n-1)) == 0) {
+    if constexpr (n == INT_MAX) {
+        index1 = index;
+    }
+    else if constexpr ((n & (n-1)) == 0) {
         // n is a power of 2, make index modulo n
         index1 = Vec8uq(index) & (n-1);
     }
@@ -2250,57 +1731,30 @@ static inline Vec8q lookup(Vec8q const & index, void const * table) {
 *
 *****************************************************************************/
 // Load elements from array a with indices i0,i1,i2,i3,i4,i5,i6,i7,i8,i9,i10,i11,i12,i13,i14,i15
-template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7, 
+template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7,
 int i8, int i9, int i10, int i11, int i12, int i13, int i14, int i15>
 static inline Vec16i gather16i(void const * a) {
-    Static_error_check<(i0|i1|i2|i3|i4|i5|i6|i7|i8|i9|i10|i11|i12|i13|i14|i15)>=0> Negative_array_index;  // Error message if index is negative
-    // find smallest and biggest index, using only compile-time constant expressions
-    const int i01min   = i0  < i1  ? i0  : i1;
-    const int i23min   = i2  < i3  ? i2  : i3;
-    const int i45min   = i4  < i5  ? i4  : i5;
-    const int i67min   = i6  < i7  ? i6  : i7;
-    const int i89min   = i8  < i9  ? i8  : i9;
-    const int i1011min = i10 < i11 ? i10 : i11;
-    const int i1213min = i12 < i13 ? i12 : i13;
-    const int i1415min = i14 < i15 ? i14 : i15;
-    const int i0_3min   = i01min   < i23min    ? i01min   : i23min;
-    const int i4_7min   = i45min   < i67min    ? i45min   : i67min;
-    const int i8_11min  = i89min   < i1011min  ? i89min   : i1011min;
-    const int i12_15min = i1213min < i1415min  ? i1213min : i1415min;
-    const int i0_7min   = i0_3min  < i4_7min   ? i0_3min  : i4_7min;
-    const int i8_15min  = i8_11min < i12_15min ? i8_11min : i12_15min;
-    const int imin      = i0_7min  < i8_15min  ? i0_7min  : i8_15min;
-    const int i01max   = i0  > i1  ? i0  : i1;
-    const int i23max   = i2  > i3  ? i2  : i3;
-    const int i45max   = i4  > i5  ? i4  : i5;
-    const int i67max   = i6  > i7  ? i6  : i7;
-    const int i89max   = i8  > i9  ? i8  : i9;
-    const int i1011max = i10 > i11 ? i10 : i11;
-    const int i1213max = i12 > i13 ? i12 : i13;
-    const int i1415max = i14 > i15 ? i14 : i15;
-    const int i0_3max   = i01max   > i23max    ? i01max   : i23max;
-    const int i4_7max   = i45max   > i67max    ? i45max   : i67max;
-    const int i8_11max  = i89max   > i1011max  ? i89max   : i1011max;
-    const int i12_15max = i1213max > i1415max  ? i1213max : i1415max;
-    const int i0_7max   = i0_3max  > i4_7max   ? i0_3max  : i4_7max;
-    const int i8_15max  = i8_11max > i12_15max ? i8_11max : i12_15max;
-    const int imax      = i0_7max  > i8_15max  ? i0_7max  : i8_15max;
-    if (imax - imin <= 15) {
+    int constexpr indexs[16] = { i0, i1, i2, i3, i4, i5, i6, i7, i8, i9, i10, i11, i12, i13, i14, i15 };
+    constexpr int imin = min_index(indexs);
+    constexpr int imax = max_index(indexs);
+    static_assert(imin >= 0, "Negative index in gather function");
+
+    if constexpr (imax - imin <= 15) {
         // load one contiguous block and permute
-        if (imax > 15) {
+        if constexpr (imax > 15) {
             // make sure we don't read past the end of the array
             Vec16i b = Vec16i().load((int32_t const *)a + imax-15);
-            return permute16i<i0-imax+15, i1-imax+15, i2-imax+15, i3-imax+15, i4-imax+15, i5-imax+15, i6-imax+15, i7-imax+15,
+            return permute16<i0-imax+15, i1-imax+15, i2-imax+15, i3-imax+15, i4-imax+15, i5-imax+15, i6-imax+15, i7-imax+15,
                 i8-imax+15, i9-imax+15, i10-imax+15, i11-imax+15, i12-imax+15, i13-imax+15, i14-imax+15, i15-imax+15> (b);
         }
         else {
             Vec16i b = Vec16i().load((int32_t const *)a + imin);
-            return permute16i<i0-imin, i1-imin, i2-imin, i3-imin, i4-imin, i5-imin, i6-imin, i7-imin,
+            return permute16<i0-imin, i1-imin, i2-imin, i3-imin, i4-imin, i5-imin, i6-imin, i7-imin,
                 i8-imin, i9-imin, i10-imin, i11-imin, i12-imin, i13-imin, i14-imin, i15-imin> (b);
         }
     }
-    if ((i0<imin+16  || i0>imax-16)  && (i1<imin+16  || i1>imax-16)  && (i2<imin+16  || i2>imax-16)  && (i3<imin+16  || i3>imax-16)
-    &&  (i4<imin+16  || i4>imax-16)  && (i5<imin+16  || i5>imax-16)  && (i6<imin+16  || i6>imax-16)  && (i7<imin+16  || i7>imax-16)    
+    if constexpr ((i0<imin+16  || i0>imax-16)  && (i1<imin+16  || i1>imax-16)  && (i2<imin+16  || i2>imax-16)  && (i3<imin+16  || i3>imax-16)
+    &&  (i4<imin+16  || i4>imax-16)  && (i5<imin+16  || i5>imax-16)  && (i6<imin+16  || i6>imax-16)  && (i7<imin+16  || i7>imax-16)
     &&  (i8<imin+16  || i8>imax-16)  && (i9<imin+16  || i9>imax-16)  && (i10<imin+16 || i10>imax-16) && (i11<imin+16 || i11>imax-16)
     &&  (i12<imin+16 || i12>imax-16) && (i13<imin+16 || i13>imax-16) && (i14<imin+16 || i14>imax-16) && (i15<imin+16 || i15>imax-16) ) {
         // load two contiguous blocks and blend
@@ -2322,7 +1776,7 @@ static inline Vec16i gather16i(void const * a) {
         const int j13 = i13<imin+16 ? i13-imin : 31-imax+i13;
         const int j14 = i14<imin+16 ? i14-imin : 31-imax+i14;
         const int j15 = i15<imin+16 ? i15-imin : 31-imax+i15;
-        return blend16i<j0,j1,j2,j3,j4,j5,j6,j7,j8,j9,j10,j11,j12,j13,j14,j15>(b, c);
+        return blend16<j0,j1,j2,j3,j4,j5,j6,j7,j8,j9,j10,j11,j12,j13,j14,j15>(b, c);
     }
     // use gather instruction
     return _mm512_i32gather_epi32(Vec16i(i0,i1,i2,i3,i4,i5,i6,i7,i8,i9,i10,i11,i12,i13,i14,i15), (const int *)a, 4);
@@ -2331,35 +1785,24 @@ static inline Vec16i gather16i(void const * a) {
 
 template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7>
 static inline Vec8q gather8q(void const * a) {
-    Static_error_check<(i0|i1|i2|i3|i4|i5|i6|i7)>=0> Negative_array_index;  // Error message if index is negative
-
-    const int i01min = i0 < i1 ? i0 : i1;
-    const int i23min = i2 < i3 ? i2 : i3;
-    const int i45min = i4 < i5 ? i4 : i5;
-    const int i67min = i6 < i7 ? i6 : i7;
-    const int i0123min = i01min < i23min ? i01min : i23min;
-    const int i4567min = i45min < i67min ? i45min : i67min;
-    const int imin = i0123min < i4567min ? i0123min : i4567min;
-    const int i01max = i0 > i1 ? i0 : i1;
-    const int i23max = i2 > i3 ? i2 : i3;
-    const int i45max = i4 > i5 ? i4 : i5;
-    const int i67max = i6 > i7 ? i6 : i7;
-    const int i0123max = i01max > i23max ? i01max : i23max;
-    const int i4567max = i45max > i67max ? i45max : i67max;
-    const int imax = i0123max > i4567max ? i0123max : i4567max;
-    if (imax - imin <= 7) {
+    int constexpr indexs[8] = { i0, i1, i2, i3, i4, i5, i6, i7 }; // indexes as array
+    constexpr int imin = min_index(indexs);
+    constexpr int imax = max_index(indexs);
+    static_assert(imin >= 0, "Negative index in gather function");
+
+    if constexpr (imax - imin <= 7) {
         // load one contiguous block and permute
-        if (imax > 7) {
+        if constexpr (imax > 7) {
             // make sure we don't read past the end of the array
             Vec8q b = Vec8q().load((int64_t const *)a + imax-7);
-            return permute8q<i0-imax+7, i1-imax+7, i2-imax+7, i3-imax+7, i4-imax+7, i5-imax+7, i6-imax+7, i7-imax+7> (b);
+            return permute8<i0-imax+7, i1-imax+7, i2-imax+7, i3-imax+7, i4-imax+7, i5-imax+7, i6-imax+7, i7-imax+7> (b);
         }
         else {
             Vec8q b = Vec8q().load((int64_t const *)a + imin);
-            return permute8q<i0-imin, i1-imin, i2-imin, i3-imin, i4-imin, i5-imin, i6-imin, i7-imin> (b);
+            return permute8<i0-imin, i1-imin, i2-imin, i3-imin, i4-imin, i5-imin, i6-imin, i7-imin> (b);
         }
     }
-    if ((i0<imin+8 || i0>imax-8) && (i1<imin+8 || i1>imax-8) && (i2<imin+8 || i2>imax-8) && (i3<imin+8 || i3>imax-8)
+    if constexpr ((i0<imin+8 || i0>imax-8) && (i1<imin+8 || i1>imax-8) && (i2<imin+8 || i2>imax-8) && (i3<imin+8 || i3>imax-8)
     &&  (i4<imin+8 || i4>imax-8) && (i5<imin+8 || i5>imax-8) && (i6<imin+8 || i6>imax-8) && (i7<imin+8 || i7>imax-8)) {
         // load two contiguous blocks and blend
         Vec8q b = Vec8q().load((int64_t const *)a + imin);
@@ -2372,7 +1815,7 @@ static inline Vec8q gather8q(void const * a) {
         const int j5 = i5<imin+8 ? i5-imin : 15-imax+i5;
         const int j6 = i6<imin+8 ? i6-imin : 15-imax+i6;
         const int j7 = i7<imin+8 ? i7-imin : 15-imax+i7;
-        return blend8q<j0, j1, j2, j3, j4, j5, j6, j7>(b, c);
+        return blend8<j0, j1, j2, j3, j4, j5, j6, j7>(b, c);
     }
     // use gather instruction
     return _mm512_i64gather_epi64(Vec8q(i0,i1,i2,i3,i4,i5,i6,i7), (const long long *)a, 8);
@@ -2385,155 +1828,101 @@ static inline Vec8q gather8q(void const * a) {
 ******************************************************************************
 *
 * These functions write the elements of a vector to arbitrary positions in an
-* array in memory. Each vector element is written to an array position 
+* array in memory. Each vector element is written to an array position
 * determined by an index. An element is not written if the corresponding
 * index is out of range.
 * The indexes can be specified as constant template parameters or as an
 * integer vector.
-* 
-* The scatter functions are useful if the data are distributed in a sparce
-* manner into the array. If the array is dense then it is more efficient
-* to permute the data into the right positions and then write the whole
-* permuted vector into the array.
-*
-* Example:
-* Vec8q a(10,11,12,13,14,15,16,17);
-* int64_t b[16] = {0};
-* scatter<0,2,14,10,1,-1,5,9>(a,b); 
-* // Now, b = {10,14,11,0,0,16,0,0,0,17,13,0,0,0,12,0}
 *
 *****************************************************************************/
 
 template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7,
 int i8, int i9, int i10, int i11, int i12, int i13, int i14, int i15>
-    static inline void scatter(Vec16i const & data, void * array) {
-    __m512i indx = constant16i<i0,i1,i2,i3,i4,i5,i6,i7,i8,i9,i10,i11,i12,i13,i14,i15>();
+    static inline void scatter(Vec16i const data, void * array) {
+    __m512i indx = constant16ui<i0,i1,i2,i3,i4,i5,i6,i7,i8,i9,i10,i11,i12,i13,i14,i15>();
     Vec16ib mask(i0>=0, i1>=0, i2>=0, i3>=0, i4>=0, i5>=0, i6>=0, i7>=0,
         i8>=0, i9>=0, i10>=0, i11>=0, i12>=0, i13>=0, i14>=0, i15>=0);
     _mm512_mask_i32scatter_epi32((int*)array, mask, indx, data, 4);
 }
 
 template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7>
-static inline void scatter(Vec8q const & data, void * array) {
-    __m256i indx = constant8i<i0,i1,i2,i3,i4,i5,i6,i7>();
+static inline void scatter(Vec8q const data, void * array) {
+    __m256i indx = constant8ui<i0,i1,i2,i3,i4,i5,i6,i7>();
     Vec8qb mask(i0>=0, i1>=0, i2>=0, i3>=0, i4>=0, i5>=0, i6>=0, i7>=0);
     _mm512_mask_i32scatter_epi64((long long *)array, mask, indx, data, 8);
 }
 
-static inline void scatter(Vec16i const & index, uint32_t limit, Vec16i const & data, void * array) {
+
+/*****************************************************************************
+*
+*          Scatter functions with variable indexes
+*
+*****************************************************************************/
+
+static inline void scatter(Vec16i const index, uint32_t limit, Vec16i const data, void * destination) {
     Vec16ib mask = Vec16ui(index) < limit;
-    _mm512_mask_i32scatter_epi32((int*)array, mask, index, data, 4);
+    _mm512_mask_i32scatter_epi32((int*)destination, mask, index, data, 4);
 }
 
-static inline void scatter(Vec8q const & index, uint32_t limit, Vec8q const & data, void * array) {
+static inline void scatter(Vec8q const index, uint32_t limit, Vec8q const data, void * destination) {
     Vec8qb mask = Vec8uq(index) < uint64_t(limit);
-    _mm512_mask_i64scatter_epi64((long long *)array, mask, index, data, 8);
+    _mm512_mask_i64scatter_epi64((long long *)destination, (uint8_t)mask, index, data, 8);
 }
 
-static inline void scatter(Vec8i const & index, uint32_t limit, Vec8q const & data, void * array) {
-#if defined (__AVX512VL__)
+static inline void scatter(Vec8i const index, uint32_t limit, Vec8q const data, void * destination) {
+#if INSTRSET >= 10 //  __AVX512VL__
     __mmask16 mask = _mm256_cmplt_epu32_mask(index, Vec8ui(limit));
 #else
-    __mmask16 mask = _mm512_cmplt_epu32_mask(_mm512_castsi256_si512(index), _mm512_castsi256_si512(Vec8ui(limit)));
+    __mmask16 mask = _mm512_mask_cmplt_epu32_mask(0xFFu, _mm512_castsi256_si512(index), _mm512_castsi256_si512(Vec8ui(limit)));
 #endif
-    _mm512_mask_i32scatter_epi64((long long *)array, mask, index, data, 8);
+    _mm512_mask_i32scatter_epi64((long long *)destination, (uint8_t)mask, index, data, 8);
 }
 
+
 /*****************************************************************************
 *
-*          Functions for conversion between integer sizes
+*          Functions for conversion between integer sizes and vector types
 *
 *****************************************************************************/
 
-// Extend 16-bit integers to 32-bit integers, signed and unsigned
-
-// Function extend_to_int : extends Vec16s to Vec16i with sign extension
-static inline Vec16i extend_to_int (Vec16s const & a) {
-    return _mm512_cvtepi16_epi32(a);
-}
-
-// Function extend_to_int : extends Vec16us to Vec16ui with zero extension
-static inline Vec16ui extend_to_int (Vec16us const & a) {
-    return _mm512_cvtepu16_epi32(a);
-}
-
-// Function extend_to_int : extends Vec16c to Vec16i with sign extension
-static inline Vec16i extend_to_int (Vec16c const & a) {
-    return _mm512_cvtepi8_epi32(a);
-}
-
-// Function extend_to_int : extends Vec16uc to Vec16ui with zero extension
-static inline Vec16ui extend_to_int (Vec16uc const & a) {
-    return _mm512_cvtepu8_epi32(a);
-}
-
-
 // Extend 32-bit integers to 64-bit integers, signed and unsigned
 
 // Function extend_low : extends the low 8 elements to 64 bits with sign extension
-static inline Vec8q extend_low (Vec16i const & a) {
+static inline Vec8q extend_low (Vec16i const a) {
     return _mm512_cvtepi32_epi64(a.get_low());
 }
 
 // Function extend_high : extends the high 8 elements to 64 bits with sign extension
-static inline Vec8q extend_high (Vec16i const & a) {
+static inline Vec8q extend_high (Vec16i const a) {
     return _mm512_cvtepi32_epi64(a.get_high());
 }
 
 // Function extend_low : extends the low 8 elements to 64 bits with zero extension
-static inline Vec8uq extend_low (Vec16ui const & a) {
+static inline Vec8uq extend_low (Vec16ui const a) {
     return _mm512_cvtepu32_epi64(a.get_low());
 }
 
 // Function extend_high : extends the high 8 elements to 64 bits with zero extension
-static inline Vec8uq extend_high (Vec16ui const & a) {
+static inline Vec8uq extend_high (Vec16ui const a) {
     return _mm512_cvtepu32_epi64(a.get_high());
 }
 
-
-// Compress 32-bit integers to 8-bit integers, signed and unsigned, with and without saturation
-
-// Function compress : packs two vectors of 16-bit integers into one vector of 8-bit integers
-// Overflow wraps around
-static inline Vec16c compress_to_int8 (Vec16i const & a) {
-    return _mm512_cvtepi32_epi8(a);
-}
-
-static inline Vec16s compress_to_int16 (Vec16i const & a) {
-    return _mm512_cvtepi32_epi16(a);
-}
-
-// with signed saturation
-static inline Vec16c compress_to_int8_saturated (Vec16i const & a) {
-    return _mm512_cvtsepi32_epi8(a);
-}
-
-static inline Vec16s compress_to_int16_saturated (Vec16i const & a) {
-    return _mm512_cvtsepi32_epi16(a);
-}
-
-// with unsigned saturation
-static inline Vec16uc compress_to_int8_saturated (Vec16ui const & a) {
-    return _mm512_cvtusepi32_epi8(a);
-}
-
-static inline Vec16us compress_to_int16_saturated (Vec16ui const & a) {
-    return _mm512_cvtusepi32_epi16(a);
-}
-
 // Compress 64-bit integers to 32-bit integers, signed and unsigned, with and without saturation
 
 // Function compress : packs two vectors of 64-bit integers into one vector of 32-bit integers
 // Overflow wraps around
-static inline Vec16i compress (Vec8q const & low, Vec8q const & high) {
+static inline Vec16i compress (Vec8q const low, Vec8q const high) {
     Vec8i low2   = _mm512_cvtepi64_epi32(low);
     Vec8i high2  = _mm512_cvtepi64_epi32(high);
     return Vec16i(low2, high2);
 }
+static inline Vec16ui compress (Vec8uq const low, Vec8uq const high) {
+    return Vec16ui(compress(Vec8q(low), Vec8q(high)));
+}
 
 // Function compress_saturated : packs two vectors of 64-bit integers into one vector of 32-bit integers
 // Signed, with saturation
-static inline Vec16i compress_saturated (Vec8q const & low, Vec8q const & high) {
+static inline Vec16i compress_saturated (Vec8q const low, Vec8q const high) {
     Vec8i low2   = _mm512_cvtsepi64_epi32(low);
     Vec8i high2  = _mm512_cvtsepi64_epi32(high);
     return Vec16i(low2, high2);
@@ -2541,12 +1930,48 @@ static inline Vec16i compress_saturated (Vec8q const & low, Vec8q const & high)
 
 // Function compress_saturated : packs two vectors of 64-bit integers into one vector of 32-bit integers
 // Unsigned, with saturation
-static inline Vec16ui compress_saturated (Vec8uq const & low, Vec8uq const & high) {
+static inline Vec16ui compress_saturated (Vec8uq const low, Vec8uq const high) {
     Vec8ui low2   = _mm512_cvtusepi64_epi32(low);
     Vec8ui high2  = _mm512_cvtusepi64_epi32(high);
     return Vec16ui(low2, high2);
 }
 
+#ifdef ZEXT_MISSING
+// GCC v. 9 and earlier are missing the _mm512_zextsi256_si512 intrinsic
+// extend vectors to double size by adding zeroes
+static inline Vec16i extend_z(Vec8i a) {
+    return Vec16i(a, Vec8i(0));
+}
+static inline Vec16ui extend_z(Vec8ui a) {
+    return Vec16ui(a, Vec8ui(0));
+}
+static inline Vec8q extend_z(Vec4q a) {
+    return Vec8q(a, Vec4q(0));
+}
+static inline Vec8uq extend_z(Vec4uq a) {
+    return Vec8uq(a, Vec4uq(0));
+}
+#else
+// extend vectors to double size by adding zeroes
+static inline Vec16i extend_z(Vec8i a) {
+    return _mm512_zextsi256_si512(a);
+}
+static inline Vec16ui extend_z(Vec8ui a) {
+    return _mm512_zextsi256_si512(a);
+}
+static inline Vec8q extend_z(Vec4q a) {
+    return _mm512_zextsi256_si512(a);
+}
+static inline Vec8uq extend_z(Vec4uq a) {
+    return _mm512_zextsi256_si512(a);
+}
+#endif
+
+// compact boolean vectors
+
+//static inline Vec16ib extend_z(Vec8ib a); // same as Vec16is extend_z(Vec8is)
+//static inline Vec8qb extend_z(Vec4qb a);  // same as Vec8ib extend_z(Vec4ib)
+
 
 /*****************************************************************************
 *
@@ -2559,14 +1984,14 @@ static inline Vec16ui compress_saturated (Vec8uq const & low, Vec8uq const & hig
 // vector operator / : divide each element by divisor
 
 // vector of 16 32-bit signed integers
-static inline Vec16i operator / (Vec16i const & a, Divisor_i const & d) {
+static inline Vec16i operator / (Vec16i const a, Divisor_i const d) {
     __m512i m   = _mm512_broadcast_i32x4(d.getm());        // broadcast multiplier
     __m512i sgn = _mm512_broadcast_i32x4(d.getsign());     // broadcast sign of d
     __m512i t1  = _mm512_mul_epi32(a,m);                   // 32x32->64 bit signed multiplication of even elements of a
     __m512i t3  = _mm512_srli_epi64(a,32);                 // get odd elements of a into position for multiplication
     __m512i t4  = _mm512_mul_epi32(t3,m);                  // 32x32->64 bit signed multiplication of odd elements
     __m512i t2  = _mm512_srli_epi64(t1,32);                // dword of even index results
-    __m512i t7  = _mm512_mask_mov_epi32(t2, __mmask16(0xAAAA), t4);  // blend two results
+    __m512i t7  = _mm512_mask_mov_epi32(t2, 0xAAAA, t4);   // blend two results
     __m512i t8  = _mm512_add_epi32(t7,a);                  // add
     __m512i t9  = _mm512_sra_epi32(t8,d.gets1());          // shift right artihmetic
     __m512i t10 = _mm512_srai_epi32(a,31);                 // sign of a
@@ -2576,27 +2001,27 @@ static inline Vec16i operator / (Vec16i const & a, Divisor_i const & d) {
 }
 
 // vector of 16 32-bit unsigned integers
-static inline Vec16ui operator / (Vec16ui const & a, Divisor_ui const & d) {
-    __m512i m   = _mm512_broadcast_i32x4(d.getm());       // broadcast multiplier
+static inline Vec16ui operator / (Vec16ui const a, Divisor_ui const d) {
+    __m512i m   = _mm512_broadcast_i32x4(d.getm());        // broadcast multiplier
     __m512i t1  = _mm512_mul_epu32(a,m);                   // 32x32->64 bit unsigned multiplication of even elements of a
     __m512i t3  = _mm512_srli_epi64(a,32);                 // get odd elements of a into position for multiplication
     __m512i t4  = _mm512_mul_epu32(t3,m);                  // 32x32->64 bit unsigned multiplication of odd elements
     __m512i t2  = _mm512_srli_epi64(t1,32);                // high dword of even index results
-    __m512i t7  = _mm512_mask_mov_epi32(t2, __mmask16(0xAAAA), t4);  // blend two results
+    __m512i t7  = _mm512_mask_mov_epi32(t2, 0xAAAA, t4);   // blend two results
     __m512i t8  = _mm512_sub_epi32(a,t7);                  // subtract
     __m512i t9  = _mm512_srl_epi32(t8,d.gets1());          // shift right logical
     __m512i t10 = _mm512_add_epi32(t7,t9);                 // add
-    return        _mm512_srl_epi32(t10,d.gets2());         // shift right logical 
+    return        _mm512_srl_epi32(t10,d.gets2());         // shift right logical
 }
 
 // vector operator /= : divide
-static inline Vec16i & operator /= (Vec16i & a, Divisor_i const & d) {
+static inline Vec16i & operator /= (Vec16i & a, Divisor_i const d) {
     a = a / d;
     return a;
 }
 
 // vector operator /= : divide
-static inline Vec16ui & operator /= (Vec16ui & a, Divisor_ui const & d) {
+static inline Vec16ui & operator /= (Vec16ui & a, Divisor_ui const d) {
     a = a / d;
     return a;
 }
@@ -2610,44 +2035,43 @@ static inline Vec16ui & operator /= (Vec16ui & a, Divisor_ui const & d) {
 
 // Divide Vec16i by compile-time constant
 template <int32_t d>
-static inline Vec16i divide_by_i(Vec16i const & x) {
-    Static_error_check<(d!=0)> Dividing_by_zero;                     // Error message if dividing by zero
-    if (d ==  1) return  x;
-    if (d == -1) return -x;
-    if (uint32_t(d) == 0x80000000u) {
+static inline Vec16i divide_by_i(Vec16i const x) {
+    static_assert(d != 0, "Integer division by zero");
+    if constexpr (d ==  1) return  x;
+    if constexpr (d == -1) return -x;
+    if constexpr (uint32_t(d) == 0x80000000u) {
         return _mm512_maskz_set1_epi32(x == Vec16i(0x80000000), 1);  // avoid overflow of abs(d). return (x == 0x80000000) ? 1 : 0;
     }
-    const uint32_t d1 = d > 0 ? uint32_t(d) : -uint32_t(d);          // compile-time abs(d). (force GCC compiler to treat d as 32 bits, not 64 bits)
-    if ((d1 & (d1-1)) == 0) {
+    constexpr uint32_t d1 = d > 0 ? uint32_t(d) : uint32_t(-d);      // compile-time abs(d). (force compiler to treat d as 32 bits, not 64 bits)
+    if constexpr ((d1 & (d1-1)) == 0) {
         // d1 is a power of 2. use shift
-        const int k = bit_scan_reverse_const(d1);
+        constexpr int k = bit_scan_reverse_const(d1);
         __m512i sign;
-        if (k > 1) sign = _mm512_srai_epi32(x, k-1); else sign = x;  // k copies of sign bit
+        if constexpr (k > 1) sign = _mm512_srai_epi32(x, k-1); else sign = x;  // k copies of sign bit
         __m512i bias    = _mm512_srli_epi32(sign, 32-k);             // bias = x >= 0 ? 0 : k-1
         __m512i xpbias  = _mm512_add_epi32 (x, bias);                // x + bias
         __m512i q       = _mm512_srai_epi32(xpbias, k);              // (x + bias) >> k
         if (d > 0)      return q;                                    // d > 0: return  q
         return _mm512_sub_epi32(_mm512_setzero_epi32(), q);          // d < 0: return -q
-
     }
     // general case
-    const int32_t sh = bit_scan_reverse_const(uint32_t(d1)-1);       // ceil(log2(d1)) - 1. (d1 < 2 handled by power of 2 case)
-    const int32_t mult = int(1 + (uint64_t(1) << (32+sh)) / uint32_t(d1) - (int64_t(1) << 32));   // multiplier
+    constexpr int32_t sh = bit_scan_reverse_const(uint32_t(d1)-1);   // ceil(log2(d1)) - 1. (d1 < 2 handled by power of 2 case)
+    constexpr int32_t mult = int(1 + (uint64_t(1) << (32+sh)) / uint32_t(d1) - (int64_t(1) << 32));   // multiplier
     const Divisor_i div(mult, sh, d < 0 ? -1 : 0);
     return x / div;
 }
 
 // define Vec8i a / const_int(d)
 template <int32_t d>
-static inline Vec16i operator / (Vec16i const & a, Const_int_t<d>) {
+static inline Vec16i operator / (Vec16i const a, Const_int_t<d>) {
     return divide_by_i<d>(a);
 }
 
 // define Vec16i a / const_uint(d)
 template <uint32_t d>
-static inline Vec16i operator / (Vec16i const & a, Const_uint_t<d>) {
-    Static_error_check< (d<0x80000000u) > Error_overflow_dividing_signed_by_unsigned; // Error: dividing signed by overflowing unsigned
-    return divide_by_i<int32_t(d)>(a);                               // signed divide
+static inline Vec16i operator / (Vec16i const a, Const_uint_t<d>) {
+    static_assert(d < 0x80000000u, "Dividing signed integer by overflowing unsigned");
+    return divide_by_i<int32_t(d)>(a);                     // signed divide
 }
 
 // vector operator /= : divide
@@ -2667,49 +2091,48 @@ static inline Vec16i & operator /= (Vec16i & a, Const_uint_t<d> b) {
 
 // Divide Vec16ui by compile-time constant
 template <uint32_t d>
-static inline Vec16ui divide_by_ui(Vec16ui const & x) {
-    Static_error_check<(d!=0)> Dividing_by_zero;                     // Error message if dividing by zero
-    if (d == 1) return x;                                            // divide by 1
-    const int b = bit_scan_reverse_const(d);                         // floor(log2(d))
-    if ((uint32_t(d) & (uint32_t(d)-1)) == 0) {
+static inline Vec16ui divide_by_ui(Vec16ui const x) {
+    static_assert(d != 0, "Integer division by zero");
+    if constexpr (d == 1) return x;                        // divide by 1
+    constexpr  int b = bit_scan_reverse_const(d);          // floor(log2(d))
+    if constexpr ((uint32_t(d) & (uint32_t(d)-1)) == 0) {
         // d is a power of 2. use shift
-        return  _mm512_srli_epi32(x, b);                             // x >> b
+        return  _mm512_srli_epi32(x, b);                   // x >> b
     }
     // general case (d > 2)
-    uint32_t mult = uint32_t((uint64_t(1) << (b+32)) / d);           // multiplier = 2^(32+b) / d
-    const uint64_t rem = (uint64_t(1) << (b+32)) - uint64_t(d)*mult; // remainder 2^(32+b) % d
-    const bool round_down = (2*rem < d);                             // check if fraction is less than 0.5
-    if (!round_down) {
-        mult = mult + 1;                                             // round up mult
-    }
+    constexpr uint32_t mult = uint32_t((uint64_t(1) << (b+32)) / d); // multiplier = 2^(32+b) / d
+    constexpr  uint64_t rem = (uint64_t(1) << (b+32)) - uint64_t(d)*mult; // remainder 2^(32+b) % d
+    constexpr  bool round_down = (2*rem < d);                        // check if fraction is less than 0.5
+    constexpr uint32_t mult1 = round_down ? mult : mult + 1;
+
     // do 32*32->64 bit unsigned multiplication and get high part of result
-    const __m512i multv = Vec16ui(uint64_t(mult));                   // zero-extend mult and broadcast
-    __m512i t1 = _mm512_mul_epu32(x,multv);                          // 32x32->64 bit unsigned multiplication of even elements
-    if (round_down) {
-        t1      = _mm512_add_epi64(t1,multv);                        // compensate for rounding error. (x+1)*m replaced by x*m+m to avoid overflow
+    const __m512i multv = _mm512_maskz_set1_epi32(0x5555, mult1); // zero-extend mult and broadcast
+    __m512i t1 = _mm512_mul_epu32(x,multv);                // 32x32->64 bit unsigned multiplication of even elements
+    if constexpr (round_down) {
+        t1     = _mm512_add_epi64(t1,multv);               // compensate for rounding error. (x+1)*m replaced by x*m+m to avoid overflow
     }
-    __m512i t2 = _mm512_srli_epi64(t1,32);                           // high dword of result 0 and 2
-    __m512i t3 = _mm512_srli_epi64(x,32);                            // get odd elements into position for multiplication
-    __m512i t4 = _mm512_mul_epu32(t3,multv);                         // 32x32->64 bit unsigned multiplication of x[1] and x[3]
-    if (round_down) {
-        t4      = _mm512_add_epi64(t4,multv);                        // compensate for rounding error. (x+1)*m replaced by x*m+m to avoid overflow
+    __m512i t2 = _mm512_srli_epi64(t1,32);                 // high dword of result 0 and 2
+    __m512i t3 = _mm512_srli_epi64(x,32);                  // get odd elements into position for multiplication
+    __m512i t4 = _mm512_mul_epu32(t3,multv);               // 32x32->64 bit unsigned multiplication of x[1] and x[3]
+    if constexpr (round_down) {
+        t4     = _mm512_add_epi64(t4,multv);               // compensate for rounding error. (x+1)*m replaced by x*m+m to avoid overflow
     }
-    __m512i t7 = _mm512_mask_mov_epi32(t2, __mmask16(0xAA), t4);     // blend two results
-    Vec16ui q  = _mm512_srli_epi32(t7, b);                           // shift right by b
-    return q;                                                        // no overflow possible
+    __m512i t7 = _mm512_mask_mov_epi32(t2, 0xAAAA, t4);    // blend two results
+    Vec16ui q  = _mm512_srli_epi32(t7, b);                 // shift right by b
+    return q;                                              // no overflow possible
 }
 
 // define Vec8ui a / const_uint(d)
 template <uint32_t d>
-static inline Vec16ui operator / (Vec16ui const & a, Const_uint_t<d>) {
+static inline Vec16ui operator / (Vec16ui const a, Const_uint_t<d>) {
     return divide_by_ui<d>(a);
 }
 
 // define Vec8ui a / const_int(d)
 template <int32_t d>
-static inline Vec16ui operator / (Vec16ui const & a, Const_int_t<d>) {
-    Static_error_check< (d>=0) > Error_dividing_unsigned_by_negative;// Error: dividing unsigned by negative is ambiguous
-    return divide_by_ui<d>(a);                                       // unsigned divide
+static inline Vec16ui operator / (Vec16ui const a, Const_int_t<d>) {
+    static_assert(d >= 0, "Dividing unsigned integer by negative is ambiguous");
+    return divide_by_ui<d>(a);                             // unsigned divide
 }
 
 // vector operator /= : divide
@@ -2726,118 +2149,6 @@ static inline Vec16ui & operator /= (Vec16ui & a, Const_int_t<d> b) {
     return a;
 }
 
-/*****************************************************************************
-*
-*          Horizontal scan functions
-*
-*****************************************************************************/
-
-// Get index to the first element that is true. Return -1 if all are false
-
-static inline int horizontal_find_first(Vec16ib const & x) {
-    uint32_t b = uint16_t(__mmask16(x));
-    if (b) {
-        return bit_scan_forward(b);
-    }
-    else {
-        return -1;
-    }
-}
-
-static inline int horizontal_find_first(Vec8qb const & x) {
-    uint32_t b = uint8_t(__mmask16(x));
-    if (b) {
-        return bit_scan_forward(b);
-    }
-    else {
-        return -1;
-    }
-}
-
-static inline uint32_t horizontal_count(Vec16ib const & x) {
-    return vml_popcnt(uint32_t(uint16_t(__mmask16(x))));
-}
-
-static inline uint32_t horizontal_count(Vec8qb const & x) {
-    return vml_popcnt(uint32_t(uint16_t(__mmask16(x))));
-}
-
-
-/*****************************************************************************
-*
-*          Boolean <-> bitfield conversion functions
-*
-*****************************************************************************/
-
-// to_bits: convert boolean vector to integer bitfield
-static inline uint8_t to_bits(Vec4ib x) {
-    __m512i a = _mm512_castsi128_si512(x);
-    __mmask16 b = _mm512_mask_testn_epi32_mask(0xF, a, a);
-    return uint8_t(b) ^ 0xF;
-}
-
-// to_Vec16c: convert integer bitfield to boolean vector
-static inline Vec4ib to_Vec4ib(uint8_t x) {
-    return _mm512_castsi512_si128(_mm512_maskz_set1_epi32(__mmask16(x), -1));
-}
-
-// to_bits: convert boolean vector to integer bitfield
-static inline uint8_t to_bits(Vec2qb x) {
-    __m512i a = _mm512_castsi128_si512(x);
-    __mmask16 b = _mm512_mask_testn_epi64_mask(0x3, a, a);
-    return uint8_t(b) ^ 0x3;
-}
-
-// to_Vec16c: convert integer bitfield to boolean vector
-static inline Vec2qb to_Vec2qb(uint8_t x) {
-    return _mm512_castsi512_si128(_mm512_maskz_set1_epi64(__mmask16(x), -1LL));
-}
-
-// to_bits: convert boolean vector to integer bitfield
-static inline uint8_t to_bits(Vec8ib x) {
-    __m512i a = _mm512_castsi256_si512(x);
-    __mmask16 b = _mm512_mask_testn_epi32_mask(0xFF, a, a);
-    return ~ uint8_t(b);
-}
-
-// to_Vec16c: convert integer bitfield to boolean vector
-static inline Vec8ib to_Vec8ib(uint8_t x) {
-    return _mm512_castsi512_si256(_mm512_maskz_set1_epi32(__mmask16(x), -1));
-}
-
-// to_bits: convert boolean vector to integer bitfield
-static inline uint8_t to_bits(Vec4qb x) {
-    __m512i a = _mm512_castsi256_si512(x);
-    __mmask16 b = _mm512_mask_testn_epi64_mask(0xF, a, a);
-    return uint8_t(b) ^ 0xF;
-}
-
-// to_Vec16c: convert integer bitfield to boolean vector
-static inline Vec4qb to_Vec4qb(uint8_t x) {
-    return _mm512_castsi512_si256(_mm512_maskz_set1_epi64(__mmask16(x), -1LL));
-}
-
-
-// to_bits: convert to integer bitfield
-static inline uint16_t to_bits(Vec16b a) {
-    return (uint16_t)(__mmask16)a;
-}
-
-// to_Vec16b: convert integer bitfield to boolean vector
-static inline Vec16b to_Vec16b(uint16_t x) {
-    return (__mmask16)x;
-}
-
-// to_Vec16ib: convert integer bitfield to boolean vector
-static inline Vec16ib to_Vec16ib(uint16_t x) {
-    return to_Vec16b(x);
-}
-
-// to_Vec8b: convert integer bitfield to boolean vector
-static inline Vec8qb to_Vec8qb(uint8_t x) {
-    return (__mmask16)x;
-}
-
 #ifdef VCL_NAMESPACE
 }
 #endif
diff --git a/EEDI3/vectorclass/vectori512e.h b/EEDI3/vectorclass/vectori512e.h
index ba7ce80..5991d23 100644
--- a/EEDI3/vectorclass/vectori512e.h
+++ b/EEDI3/vectorclass/vectori512e.h
@@ -1,16 +1,14 @@
 /****************************  vectori512e.h   *******************************
 * Author:        Agner Fog
 * Date created:  2014-07-23
-* Last modified: 2017-02-19
-* Version:       1.27
+* Last modified: 2023-06-03
+* Version:       2.02.01
 * Project:       vector classes
 * Description:
-* Header file defining integer vector classes as interface to intrinsic 
-* functions in x86 microprocessors with AVX512 and later instruction sets.
+* Header file defining 512-bit integer vector classes for 32 and 64 bit integers.
+* Emulated for processors without AVX512 instruction set.
 *
-* Instructions:
-* Use Gnu, Intel or Microsoft C++ compiler. Compile for the desired 
-* instruction set, which must be at least AVX512. 
+* Instructions: see vcl_manual.pdf
 *
 * The following vector classes are defined here:
 * Vec16i    Vector of  16  32-bit signed   integers
@@ -20,73 +18,52 @@
 * Vec8uq    Vector of   8  64-bit unsigned integers
 * Vec8qb    Vector of   8  Booleans for use with Vec8q and Vec8uq
 *
-* Each vector object is represented internally in the CPU as a 512-bit register.
+* Each vector object is represented internally in the CPU as two 256-bit registers.
 * This header file defines operators and functions for these vectors.
 *
-* For detailed instructions, see VectorClass.pdf
-*
-* (c) Copyright 2014-2017 GNU General Public License http://www.gnu.org/licenses
+* (c) Copyright 2012-2023 Agner Fog.
+* Apache License version 2.0 or later.
 *****************************************************************************/
 
+#ifndef VECTORI512E_H
+#define VECTORI512E_H
+
+#ifndef VECTORCLASS_H
+#include "vectorclass.h"
+#endif
+
+#if VECTORCLASS_H < 20200
+#error Incompatible versions of vector class library mixed
+#endif
+
 // check combination of header files
 #if defined (VECTORI512_H)
-#if    VECTORI512_H != 1
 #error Two different versions of vectori512.h included
 #endif
-#else
-#define VECTORI512_H  1
+
 
 #ifdef VCL_NAMESPACE
 namespace VCL_NAMESPACE {
 #endif
 
+
 /*****************************************************************************
 *
-*          base class Vec512ie
+*          Vector of 512 bits
 *
 *****************************************************************************/
-// base class to replace _mm512i when AVX512 is not supported
-class Vec512ie {
+
+class Vec512b {
 protected:
     Vec256b z0;                         // low half
     Vec256b z1;                         // high half
-public:
-    Vec512ie(void) {};                  // default constructor
-    Vec512ie(Vec8i const & x0, Vec8i const & x1) {      // constructor to build from two Vec8i
-        z0 = x0;  z1 = x1;
-    }
-    Vec8i get_low() const {            // get low half
-        return Vec8i(z0);
-    }
-    Vec8i get_high() const {           // get high half
-        return Vec8i(z1);
-    }
-};
-
-
-/*****************************************************************************
-*
-*          Vector of 512 1-bit unsigned integers or Booleans
-*
-*****************************************************************************/
-class Vec512b : public Vec512ie {
 public:
     // Default constructor:
-    Vec512b() {
-    }
+    Vec512b() = default;
     // Constructor to build from two Vec256b:
-    Vec512b(Vec256b const & a0, Vec256b const & a1) {
+    Vec512b(Vec256b const a0, Vec256b const a1) {
         z0 = a0;  z1 = a1;
     }
-    // Constructor to convert from type Vec512ie
-    Vec512b(Vec512ie const & x) {
-        z0 = x.get_low();  z1 = x.get_high();
-    }
-    // Assignment operator to convert from type Vec512ie
-    Vec512b & operator = (Vec512ie const & x) {
-        z0 = x.get_low();  z1 = x.get_high();
-        return *this;
-    }
     // Member function to load from array (unaligned)
     Vec512b & load(void const * p) {
         z0 = Vec8i().load(p);
@@ -109,40 +86,21 @@ public:
         Vec8i(z0).store_a(p);
         Vec8i(z1).store_a((int32_t*)p+8);
     }
-    // Member function to change a single bit
-    // Note: This function is inefficient. Use load function if changing more than one bit
-    Vec512b const & set_bit(uint32_t index, int value) {
-        if (index < 256) {
-            z0 = Vec8i(z0).set_bit(index, value);
-        }
-        else {
-            z1 = Vec8i(z1).set_bit(index-256, value);
-        }
-        return *this;
-    }
-    // Member function to get a single bit
-    // Note: This function is inefficient. Use store function if reading more than one bit
-    int get_bit(uint32_t index) const {
-        if (index < 256) {
-            return Vec8i(z0).get_bit(index);
-        }
-        else {
-            return Vec8i(z1).get_bit(index-256);
-        }
-    }
-    // Extract a single element. Use store function if extracting more than one element.
-    // Operator [] can only read an element, not write.
-    bool operator [] (uint32_t index) const {
-        return get_bit(index) != 0;
+    // Member function storing to aligned uncached memory (non-temporal store).
+    // This may be more efficient than store_a when storing large blocks of memory if it 
+    // is unlikely that the data will stay in the cache until it is read again.
+    // Note: Will generate runtime error if p is not aligned by 64
+    void store_nt(void * p) const {
+        Vec8i(z0).store_nt(p);
+        Vec8i(z1).store_nt((int32_t*)p+8);
     }
-    // Member functions to split into two Vec128b:
-    Vec256b get_low() const {
+    Vec256b get_low() const {            // get low half
         return z0;
     }
-    Vec256b get_high() const {
+    Vec256b get_high() const {           // get high half
         return z1;
     }
-    static int size () {
+    static constexpr int size() {
         return 512;
     }
 };
@@ -150,45 +108,45 @@ public:
 // Define operators for this class
 
 // vector operator & : bitwise and
-static inline Vec512b operator & (Vec512b const & a, Vec512b const & b) {
+static inline Vec512b operator & (Vec512b const a, Vec512b const b) {
     return Vec512b(a.get_low() & b.get_low(), a.get_high() & b.get_high());
 }
-static inline Vec512b operator && (Vec512b const & a, Vec512b const & b) {
+static inline Vec512b operator && (Vec512b const a, Vec512b const b) {
     return a & b;
 }
 
 // vector operator | : bitwise or
-static inline Vec512b operator | (Vec512b const & a, Vec512b const & b) {
+static inline Vec512b operator | (Vec512b const a, Vec512b const b) {
     return Vec512b(a.get_low() | b.get_low(), a.get_high() | b.get_high());
 }
-static inline Vec512b operator || (Vec512b const & a, Vec512b const & b) {
+static inline Vec512b operator || (Vec512b const a, Vec512b const b) {
     return a | b;
 }
 
 // vector operator ^ : bitwise xor
-static inline Vec512b operator ^ (Vec512b const & a, Vec512b const & b) {
+static inline Vec512b operator ^ (Vec512b const a, Vec512b const b) {
     return Vec512b(a.get_low() ^ b.get_low(), a.get_high() ^ b.get_high());
 }
 
 // vector operator ~ : bitwise not
-static inline Vec512b operator ~ (Vec512b const & a) {
+static inline Vec512b operator ~ (Vec512b const a) {
     return Vec512b(~a.get_low(), ~a.get_high());
 }
 
 // vector operator &= : bitwise and
-static inline Vec512b & operator &= (Vec512b & a, Vec512b const & b) {
+static inline Vec512b & operator &= (Vec512b & a, Vec512b const b) {
     a = a & b;
     return a;
 }
 
 // vector operator |= : bitwise or
-static inline Vec512b & operator |= (Vec512b & a, Vec512b const & b) {
+static inline Vec512b & operator |= (Vec512b & a, Vec512b const b) {
     a = a | b;
     return a;
 }
 
 // vector operator ^= : bitwise xor
-static inline Vec512b & operator ^= (Vec512b & a, Vec512b const & b) {
+static inline Vec512b & operator ^= (Vec512b & a, Vec512b const b) {
     a = a ^ b;
     return a;
 }
@@ -196,60 +154,41 @@ static inline Vec512b & operator ^= (Vec512b & a, Vec512b const & b) {
 // Define functions for this class
 
 // function andnot: a & ~ b
-static inline Vec512b andnot (Vec512b const & a, Vec512b const & b) {
+static inline Vec512b andnot (Vec512b const a, Vec512b const b) {
     return Vec512b(andnot(a.get_low(), b.get_low()), andnot(a.get_high(), b.get_high()));
 }
 
 
-
 /*****************************************************************************
 *
-*          Generate compile-time constant vector
-*
-*****************************************************************************/
-// Generate a constant vector of 8 integers stored in memory.
-// Can be converted to any integer vector type
-template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7, int i8, int i9, int i10, int i11, int i12, int i13, int i14, int i15>
-static inline Vec512ie constant16i() {
-    static const union {
-        int32_t i[16];
-        Vec256b y[2];  // note: requires C++0x or later. Use option -std=c++0x
-    } u = {{i0,i1,i2,i3,i4,i5,i6,i7,i8,i9,i10,i11,i12,i13,i14,i15}};
-    return Vec512ie(u.y[0], u.y[1]);
-}
-
-
-/*****************************************************************************
-*
-*          Boolean vector base classes for AVX512
+*          Boolean vector (broad) base classes
 *
 *****************************************************************************/
 
 class Vec16b : public Vec512b {
 public:
     // Default constructor:
-    Vec16b () {
-    }
+    Vec16b() = default;
     // Constructor to build from all elements:
-    Vec16b(bool b0, bool b1, bool b2, bool b3, bool b4, bool b5, bool b6, bool b7, 
+    Vec16b(bool b0, bool b1, bool b2, bool b3, bool b4, bool b5, bool b6, bool b7,
     bool b8, bool b9, bool b10, bool b11, bool b12, bool b13, bool b14, bool b15) {
         *this = Vec512b(Vec8i(-(int)b0, -(int)b1, -(int)b2, -(int)b3, -(int)b4, -(int)b5, -(int)b6, -(int)b7), Vec8i(-(int)b8, -(int)b9, -(int)b10, -(int)b11, -(int)b12, -(int)b13, -(int)b14, -(int)b15));
     }
     // Constructor to convert from type Vec512b
-    Vec16b (Vec512b const & x) {
+    Vec16b (Vec512b const & x) {  // gcc requires const & here
         z0 = x.get_low();
         z1 = x.get_high();
     }
     // Constructor to make from two halves
-    Vec16b (Vec8ib const & x0, Vec8ib const & x1) {
+    Vec16b (Vec8ib const x0, Vec8ib const x1) {
         z0 = x0;
         z1 = x1;
-    }        
+    }
     // Constructor to make from two halves
-    Vec16b (Vec8i const & x0, Vec8i const & x1) {
+    Vec16b (Vec8i const x0, Vec8i const x1) {
         z0 = x0;
         z1 = x1;
-    }        
+    }
     // Constructor to broadcast single value:
     Vec16b(bool b) {
         z0 = z1 = Vec8i(-int32_t(b));
@@ -259,12 +198,6 @@ public:
         z0 = z1 = Vec8i(-int32_t(b));
         return *this;
     }
-private: 
-    // Prevent constructing from int, etc. because of ambiguity
-    Vec16b(int b);
-    // Prevent assigning int because of ambiguity
-    Vec16b & operator = (int x);
-public:
     // split into two halves
     Vec8ib get_low() const {
         return Vec8ib(z0);
@@ -272,16 +205,17 @@ public:
     Vec8ib get_high() const {
         return Vec8ib(z1);
     }
+    /*
     // Assignment operator to convert from type Vec512b
-    Vec16b & operator = (Vec512b const & x) {
+    Vec16b & operator = (Vec512b const x) {
         z0 = x.get_low();
         z1 = x.get_high();
         return *this;
-    }
+    } */
     // Member function to change a single element in vector
     // Note: This function is inefficient. Use load function if changing more than one element
-    Vec16b const & insert(uint32_t index, bool value) {
-        if (index < 8) {
+    Vec16b const insert(int index, bool value) {
+        if ((uint32_t)index < 8) {
             z0 = Vec8ib(z0).insert(index, value);
         }
         else {
@@ -290,8 +224,8 @@ public:
         return *this;
     }
     // Member function extract a single element from vector
-    bool extract(uint32_t index) const {
-        if (index < 8) {
+    bool extract(int index) const {
+        if ((uint32_t)index < 8) {
             return Vec8ib(z0).extract(index);
         }
         else {
@@ -299,61 +233,68 @@ public:
         }
     }
     // Extract a single element. Operator [] can only read an element, not write.
-    bool operator [] (uint32_t index) const {
+    bool operator [] (int index) const {
         return extract(index);
     }
-    static int size () {
+    static constexpr int size() {
         return 16;
     }
+    static constexpr int elementtype() {
+        return 3;
+    }
+    // Prevent constructing from int, etc. because of ambiguity
+    Vec16b(int b) = delete;
+    // Prevent assigning int because of ambiguity
+    Vec16b & operator = (int x) = delete;
 };
 
 // Define operators for this class
 
 // vector operator & : bitwise and
-static inline Vec16b operator & (Vec16b const & a, Vec16b const & b) {
+static inline Vec16b operator & (Vec16b const a, Vec16b const b) {
     return Vec16b(a.get_low() & b.get_low(), a.get_high() & b.get_high());
 }
-static inline Vec16b operator && (Vec16b const & a, Vec16b const & b) {
+static inline Vec16b operator && (Vec16b const a, Vec16b const b) {
     return a & b;
 }
 
 // vector operator | : bitwise or
-static inline Vec16b operator | (Vec16b const & a, Vec16b const & b) {
+static inline Vec16b operator | (Vec16b const a, Vec16b const b) {
     return Vec16b(a.get_low() | b.get_low(), a.get_high() | b.get_high());
 }
-static inline Vec16b operator || (Vec16b const & a, Vec16b const & b) {
+static inline Vec16b operator || (Vec16b const a, Vec16b const b) {
     return a | b;
 }
 
 // vector operator ^ : bitwise xor
-static inline Vec16b operator ^ (Vec16b const & a, Vec16b const & b) {
+static inline Vec16b operator ^ (Vec16b const a, Vec16b const b) {
     return Vec16b(a.get_low() ^ b.get_low(), a.get_high() ^ b.get_high());
 }
 
 // vector operator ~ : bitwise not
-static inline Vec16b operator ~ (Vec16b const & a) {
+static inline Vec16b operator ~ (Vec16b const a) {
     return Vec16b(~(a.get_low()), ~(a.get_high()));
 }
 
 // vector operator ! : element not
-static inline Vec16b operator ! (Vec16b const & a) {
+static inline Vec16b operator ! (Vec16b const a) {
     return ~a;
 }
 
 // vector operator &= : bitwise and
-static inline Vec16b & operator &= (Vec16b & a, Vec16b const & b) {
+static inline Vec16b & operator &= (Vec16b & a, Vec16b const b) {
     a = a & b;
     return a;
 }
 
 // vector operator |= : bitwise or
-static inline Vec16b & operator |= (Vec16b & a, Vec16b const & b) {
+static inline Vec16b & operator |= (Vec16b & a, Vec16b const b) {
     a = a | b;
     return a;
 }
 
 // vector operator ^= : bitwise xor
-static inline Vec16b & operator ^= (Vec16b & a, Vec16b const & b) {
+static inline Vec16b & operator ^= (Vec16b & a, Vec16b const b) {
     a = a ^ b;
     return a;
 }
@@ -365,17 +306,17 @@ static inline Vec16b & operator ^= (Vec16b & a, Vec16b const & b) {
 *****************************************************************************/
 
 // function andnot: a & ~ b
-static inline Vec16b andnot (Vec16b const & a, Vec16b const & b) {
+static inline Vec16b andnot (Vec16b const a, Vec16b const b) {
     return Vec16b(Vec8ib(andnot(a.get_low(),b.get_low())), Vec8ib(andnot(a.get_high(),b.get_high())));
 }
 
 // horizontal_and. Returns true if all bits are 1
-static inline bool horizontal_and (Vec16b const & a) {
+static inline bool horizontal_and (Vec16b const a) {
     return  horizontal_and(a.get_low() & a.get_high());
 }
 
 // horizontal_or. Returns true if at least one bit is 1
-static inline bool horizontal_or (Vec16b const & a) {
+static inline bool horizontal_or (Vec16b const a) {
     return  horizontal_or(a.get_low() | a.get_high());
 }
 
@@ -389,12 +330,12 @@ static inline bool horizontal_or (Vec16b const & a) {
 class Vec16ib : public Vec16b {
 public:
     // Default constructor:
-    Vec16ib () {
-    }
+    Vec16ib () = default;
+    /*
     Vec16ib (Vec16b const & x) {
         z0 = x.get_low();
         z1 = x.get_high();
-    }
+    } */
     // Constructor to build from all elements:
     Vec16ib(bool x0, bool x1, bool x2, bool x3, bool x4, bool x5, bool x6, bool x7,
         bool x8, bool x9, bool x10, bool x11, bool x12, bool x13, bool x14, bool x15) {
@@ -407,12 +348,12 @@ public:
         z1 = x.get_high();
     }
     // Construct from two halves
-    Vec16ib (Vec8ib const & x0, Vec8ib const & x1) {
+    Vec16ib (Vec8ib const x0, Vec8ib const x1) {
         z0 = x0;
         z1 = x1;
     }
     // Assignment operator to convert from type Vec512b
-    Vec16ib & operator = (Vec512b const & x) {
+    Vec16ib & operator = (Vec512b const x) {
         z0 = x.get_low();
         z1 = x.get_high();
         return *this;
@@ -425,65 +366,80 @@ public:
         *this = Vec16b(b);
         return *this;
     }
-private: // Prevent constructing from int, etc.
-    Vec16ib(int b);
-    Vec16ib & operator = (int x);
-public:
+    // Member function to change a bitfield to a boolean vector
+    Vec16ib & load_bits(uint16_t a) {
+        z0 = Vec8ib().load_bits(uint8_t(a));
+        z1 = Vec8ib().load_bits(uint8_t(a>>8));
+        return *this;
+    }
+    // Prevent constructing from int, etc.
+    Vec16ib(int b) = delete;
+    Vec16ib & operator = (int x) = delete;
 };
 
 // Define operators for Vec16ib
 
 // vector operator & : bitwise and
-static inline Vec16ib operator & (Vec16ib const & a, Vec16ib const & b) {
+static inline Vec16ib operator & (Vec16ib const a, Vec16ib const b) {
     return Vec16b(a) & Vec16b(b);
 }
-static inline Vec16ib operator && (Vec16ib const & a, Vec16ib const & b) {
+static inline Vec16ib operator && (Vec16ib const a, Vec16ib const b) {
     return a & b;
 }
 
 // vector operator | : bitwise or
-static inline Vec16ib operator | (Vec16ib const & a, Vec16ib const & b) {
+static inline Vec16ib operator | (Vec16ib const a, Vec16ib const b) {
     return Vec16b(a) | Vec16b(b);
 }
-static inline Vec16ib operator || (Vec16ib const & a, Vec16ib const & b) {
+static inline Vec16ib operator || (Vec16ib const a, Vec16ib const b) {
     return a | b;
 }
 
 // vector operator ^ : bitwise xor
-static inline Vec16ib operator ^ (Vec16ib const & a, Vec16ib const & b) {
+static inline Vec16ib operator ^ (Vec16ib const a, Vec16ib const b) {
     return Vec16b(a) ^ Vec16b(b);
 }
 
+// vector operator == : xnor
+static inline Vec16ib operator == (Vec16ib const a, Vec16ib const b) {
+    return Vec16ib(Vec16b(a) ^ Vec16b(~b));
+}
+
+// vector operator != : xor
+static inline Vec16ib operator != (Vec16ib const a, Vec16ib const b) {
+    return Vec16ib(a ^ b);
+}
+
 // vector operator ~ : bitwise not
-static inline Vec16ib operator ~ (Vec16ib const & a) {
+static inline Vec16ib operator ~ (Vec16ib const a) {
     return ~Vec16b(a);
 }
 
 // vector operator ! : element not
-static inline Vec16ib operator ! (Vec16ib const & a) {
+static inline Vec16ib operator ! (Vec16ib const a) {
     return ~a;
 }
 
 // vector operator &= : bitwise and
-static inline Vec16ib & operator &= (Vec16ib & a, Vec16ib const & b) {
+static inline Vec16ib & operator &= (Vec16ib & a, Vec16ib const b) {
     a = a & b;
     return a;
 }
 
 // vector operator |= : bitwise or
-static inline Vec16ib & operator |= (Vec16ib & a, Vec16ib const & b) {
+static inline Vec16ib & operator |= (Vec16ib & a, Vec16ib const b) {
     a = a | b;
     return a;
 }
 
 // vector operator ^= : bitwise xor
-static inline Vec16ib & operator ^= (Vec16ib & a, Vec16ib const & b) {
+static inline Vec16ib & operator ^= (Vec16ib & a, Vec16ib const b) {
     a = a ^ b;
     return a;
 }
 
 // vector function andnot
-static inline Vec16ib andnot (Vec16ib const & a, Vec16ib const & b) {
+static inline Vec16ib andnot (Vec16ib const a, Vec16ib const b) {
     return Vec16ib(andnot(Vec16b(a), Vec16b(b)));
 }
 
@@ -497,19 +453,19 @@ static inline Vec16ib andnot (Vec16ib const & a, Vec16ib const & b) {
 class Vec8b : public Vec16b {
 public:
     // Default constructor:
-    Vec8b () {
-    }
+    Vec8b () = default;
+    /*
     Vec8b (Vec16b const & x) {
         z0 = x.get_low();
         z1 = x.get_high();
-    }
+    } */
     // Constructor to convert from type Vec512b
     Vec8b (Vec512b const & x) {
         z0 = x.get_low();
         z1 = x.get_high();
     }
     // construct from two halves
-    Vec8b (Vec4qb const & x0, Vec4qb const & x1) {
+    Vec8b (Vec4qb const x0, Vec4qb const x1) {
         z0 = x0;
         z1 = x1;
     }
@@ -522,12 +478,6 @@ public:
         z0 = z1 = Vec8i(-int32_t(b));
         return *this;
     }
-private: 
-    // Prevent constructing from int, etc. because of ambiguity
-    Vec8b(int b);
-    // Prevent assigning int because of ambiguity
-    Vec8b & operator = (int x);
-public:
     // split into two halves
     Vec4qb get_low() const {
         return Vec4qb(z0);
@@ -535,16 +485,16 @@ public:
     Vec4qb get_high() const {
         return Vec4qb(z1);
     }
+    /*
     // Assignment operator to convert from type Vec512b
-    Vec8b & operator = (Vec512b const & x) {
+    Vec8b & operator = (Vec512b const x) {
         z0 = x.get_low();
         z1 = x.get_high();
         return *this;
-    }
+    } */
     // Member function to change a single element in vector
-    // Note: This function is inefficient. Use load function if changing more than one element
-    Vec8b const & insert(uint32_t index, bool value) {
-        if (index < 4) {
+    Vec8b const insert(int index, bool value) {
+        if ((uint32_t)index < 4) {
             z0 = Vec4qb(z0).insert(index, value);
         }
         else {
@@ -552,20 +502,24 @@ public:
         }
         return *this;
     }
-    bool extract(uint32_t index) const {
-        if (index < 4) {
+    bool extract(int index) const {
+        if ((uint32_t)index < 4) {
             return Vec4qb(Vec4q(z0)).extract(index);
         }
         else {
             return Vec4qb(Vec4q(z1)).extract(index-4);
         }
     }
-    bool operator [] (uint32_t index) const {
+    bool operator [] (int index) const {
         return extract(index);
     }
-    static int size () {
+    static constexpr int size() {
         return 8;
     }
+    // Prevent constructing from int, etc. because of ambiguity
+    Vec8b(int b) = delete;
+    // Prevent assigning int because of ambiguity
+    Vec8b & operator = (int x) = delete;
 };
 
 
@@ -578,9 +532,8 @@ public:
 class Vec8qb : public Vec8b {
 public:
     // Default constructor:
-    Vec8qb () {
-    }
-    Vec8qb (Vec16b const & x) {
+    Vec8qb() = default;
+    Vec8qb (Vec16b const x) {
         z0 = x.get_low();
         z1 = x.get_high();
     }
@@ -595,12 +548,12 @@ public:
         z1 = x.get_high();
     }
     // construct from two halves
-    Vec8qb (Vec4qb const & x0, Vec4qb const & x1) {
+    Vec8qb (Vec4qb const x0, Vec4qb const x1) {
         z0 = x0;
         z1 = x1;
     }
     // Assignment operator to convert from type Vec512b
-    Vec8qb & operator = (Vec512b const & x) {
+    Vec8qb & operator = (Vec512b const x) {
         z0 = x.get_low();
         z1 = x.get_high();
         return *this;
@@ -613,67 +566,81 @@ public:
         *this = Vec8b(b);
         return *this;
     }
-private: 
+    // Member function to change a bitfield to a boolean vector
+    Vec8qb & load_bits(uint8_t a) {
+        z0 = Vec4qb().load_bits(a);
+        z1 = Vec4qb().load_bits(uint8_t(a>>4u));
+        return *this;
+    }
     // Prevent constructing from int, etc. because of ambiguity
-    Vec8qb(int b);
+    Vec8qb(int b) = delete;
     // Prevent assigning int because of ambiguity
-    Vec8qb & operator = (int x);
-public:
+    Vec8qb & operator = (int x) = delete;
 };
 
 // Define operators for Vec8qb
 
 // vector operator & : bitwise and
-static inline Vec8qb operator & (Vec8qb const & a, Vec8qb const & b) {
+static inline Vec8qb operator & (Vec8qb const a, Vec8qb const b) {
     return Vec16b(a) & Vec16b(b);
 }
-static inline Vec8qb operator && (Vec8qb const & a, Vec8qb const & b) {
+static inline Vec8qb operator && (Vec8qb const a, Vec8qb const b) {
     return a & b;
 }
 
 // vector operator | : bitwise or
-static inline Vec8qb operator | (Vec8qb const & a, Vec8qb const & b) {
+static inline Vec8qb operator | (Vec8qb const a, Vec8qb const b) {
     return Vec16b(a) | Vec16b(b);
 }
-static inline Vec8qb operator || (Vec8qb const & a, Vec8qb const & b) {
+static inline Vec8qb operator || (Vec8qb const a, Vec8qb const b) {
     return a | b;
 }
 
 // vector operator ^ : bitwise xor
-static inline Vec8qb operator ^ (Vec8qb const & a, Vec8qb const & b) {
+static inline Vec8qb operator ^ (Vec8qb const a, Vec8qb const b) {
     return Vec16b(a) ^ Vec16b(b);
 }
 
+// vector operator == : xnor
+static inline Vec8qb operator == (Vec8qb const a, Vec8qb const b) {
+    return Vec8qb(Vec16b(a) ^ Vec16b(~b));
+}
+
+// vector operator != : xor
+static inline Vec8qb operator != (Vec8qb const a, Vec8qb const b) {
+    return Vec8qb(a ^ b);
+}
+
 // vector operator ~ : bitwise not
-static inline Vec8qb operator ~ (Vec8qb const & a) {
+static inline Vec8qb operator ~ (Vec8qb const a) {
     return ~Vec16b(a);
 }
 
 // vector operator ! : element not
-static inline Vec8qb operator ! (Vec8qb const & a) {
+static inline Vec8qb operator ! (Vec8qb const a) {
     return ~a;
 }
 
 // vector operator &= : bitwise and
-static inline Vec8qb & operator &= (Vec8qb & a, Vec8qb const & b) {
+static inline Vec8qb & operator &= (Vec8qb & a, Vec8qb const b) {
     a = a & b;
     return a;
 }
 
 // vector operator |= : bitwise or
-static inline Vec8qb & operator |= (Vec8qb & a, Vec8qb const & b) {
+static inline Vec8qb & operator |= (Vec8qb & a, Vec8qb const b) {
     a = a | b;
     return a;
 }
 
 // vector operator ^= : bitwise xor
-static inline Vec8qb & operator ^= (Vec8qb & a, Vec8qb const & b) {
+static inline Vec8qb & operator ^= (Vec8qb & a, Vec8qb const b) {
     a = a ^ b;
     return a;
 }
 
 // vector function andnot
-static inline Vec8qb andnot (Vec8qb const & a, Vec8qb const & b) {
+static inline Vec8qb andnot (Vec8qb const a, Vec8qb const b) {
     return Vec8qb(andnot(Vec16b(a), Vec16b(b)));
 }
 
@@ -687,8 +654,7 @@ static inline Vec8qb andnot (Vec8qb const & a, Vec8qb const & b) {
 class Vec16i: public Vec512b {
 public:
     // Default constructor:
-    Vec16i() {
-    }
+    Vec16i() = default;
     // Constructor to broadcast the same value into all elements:
     Vec16i(int i) {
         z0 = z1 = Vec8i(i);
@@ -700,7 +666,7 @@ public:
         z1 = Vec8i(i8, i9, i10, i11, i12, i13, i14, i15);
     }
     // Constructor to build from two Vec8i:
-    Vec16i(Vec8i const & a0, Vec8i const & a1) {
+    Vec16i(Vec8i const a0, Vec8i const a1) {
         *this = Vec512b(a0, a1);
     }
     // Constructor to convert from type Vec512b
@@ -709,7 +675,7 @@ public:
         z1 = x.get_high();
     }
     // Assignment operator to convert from type Vec512b
-    Vec16i & operator = (Vec512b const & x) {
+    Vec16i & operator = (Vec512b const x) {
         z0 = x.get_low();
         z1 = x.get_high();
         return *this;
@@ -758,8 +724,8 @@ public:
         return *this;
     }
     // Member function to change a single element in vector
-    Vec16i const & insert(uint32_t index, int32_t value) {
-        if (index < 8) {
+    Vec16i const insert(int index, int32_t value) {
+        if ((uint32_t)index < 8) {
             z0 = Vec8i(z0).insert(index, value);
         }
         else {
@@ -768,8 +734,8 @@ public:
         return *this;
     }
     // Member function extract a single element from vector
-    int32_t extract(uint32_t index) const {
-        if (index < 8) {
+    int32_t extract(int index) const {
+        if ((uint32_t)index < 8) {
             return Vec8i(z0).extract(index);
         }
         else {
@@ -778,7 +744,7 @@ public:
     }
     // Extract a single element. Use store function if extracting more than one element.
     // Operator [] can only read an element, not write.
-    int32_t operator [] (uint32_t index) const {
+    int32_t operator [] (int index) const {
         return extract(index);
     }
     // Member functions to split into two Vec8i:
@@ -788,21 +754,24 @@ public:
     Vec8i get_high() const {
         return Vec8i(z1);
     }
-    static int size () {
+    static constexpr int size() {
         return 16;
     }
+    static constexpr int elementtype() {
+        return 8;
+    }
 };
 
 
 // Define operators for Vec16i
 
 // vector operator + : add element by element
-static inline Vec16i operator + (Vec16i const & a, Vec16i const & b) {
+static inline Vec16i operator + (Vec16i const a, Vec16i const b) {
     return Vec16i(a.get_low() + b.get_low(), a.get_high() + b.get_high());
 }
 
 // vector operator += : add
-static inline Vec16i & operator += (Vec16i & a, Vec16i const & b) {
+static inline Vec16i & operator += (Vec16i & a, Vec16i const b) {
     a = a + b;
     return a;
 }
@@ -821,17 +790,17 @@ static inline Vec16i & operator ++ (Vec16i & a) {
 }
 
 // vector operator - : subtract element by element
-static inline Vec16i operator - (Vec16i const & a, Vec16i const & b) {
+static inline Vec16i operator - (Vec16i const a, Vec16i const b) {
     return Vec16i(a.get_low() - b.get_low(), a.get_high() - b.get_high());
 }
 
 // vector operator - : unary minus
-static inline Vec16i operator - (Vec16i const & a) {
+static inline Vec16i operator - (Vec16i const a) {
     return Vec16i(-a.get_low(), -a.get_high());
 }
 
 // vector operator -= : subtract
-static inline Vec16i & operator -= (Vec16i & a, Vec16i const & b) {
+static inline Vec16i & operator -= (Vec16i & a, Vec16i const b) {
     a = a - b;
     return a;
 }
@@ -850,22 +819,20 @@ static inline Vec16i & operator -- (Vec16i & a) {
 }
 
 // vector operator * : multiply element by element
-static inline Vec16i operator * (Vec16i const & a, Vec16i const & b) {
+static inline Vec16i operator * (Vec16i const a, Vec16i const b) {
     return Vec16i(a.get_low() * b.get_low(), a.get_high() * b.get_high());
 }
 
 // vector operator *= : multiply
-static inline Vec16i & operator *= (Vec16i & a, Vec16i const & b) {
+static inline Vec16i & operator *= (Vec16i & a, Vec16i const b) {
     a = a * b;
     return a;
 }
 
-// vector operator / : divide all elements by same integer
-// See bottom of file
-
+// vector operator / : divide all elements by same integer. See bottom of file
 
 // vector operator << : shift left
-static inline Vec16i operator << (Vec16i const & a, int32_t b) {
+static inline Vec16i operator << (Vec16i const a, int32_t b) {
     return Vec16i(a.get_low() << b, a.get_high() << b);
 }
 
@@ -876,7 +843,7 @@ static inline Vec16i & operator <<= (Vec16i & a, int32_t b) {
 }
 
 // vector operator >> : shift right arithmetic
-static inline Vec16i operator >> (Vec16i const & a, int32_t b) {
+static inline Vec16i operator >> (Vec16i const a, int32_t b) {
     return Vec16i(a.get_low() >> b, a.get_high() >> b);
 }
 
@@ -887,70 +854,70 @@ static inline Vec16i & operator >>= (Vec16i & a, int32_t b) {
 }
 
 // vector operator == : returns true for elements for which a == b
-static inline Vec16ib operator == (Vec16i const & a, Vec16i const & b) {
+static inline Vec16ib operator == (Vec16i const a, Vec16i const b) {
     return Vec16ib(a.get_low() == b.get_low(), a.get_high() == b.get_high());
 }
 
 // vector operator != : returns true for elements for which a != b
-static inline Vec16ib operator != (Vec16i const & a, Vec16i const & b) {
+static inline Vec16ib operator != (Vec16i const a, Vec16i const b) {
     return Vec16ib(a.get_low() != b.get_low(), a.get_high() != b.get_high());
 }
-  
+
 // vector operator > : returns true for elements for which a > b
-static inline Vec16ib operator > (Vec16i const & a, Vec16i const & b) {
+static inline Vec16ib operator > (Vec16i const a, Vec16i const b) {
     return Vec16ib(a.get_low() > b.get_low(), a.get_high() > b.get_high());
 }
 
 // vector operator < : returns true for elements for which a < b
-static inline Vec16ib operator < (Vec16i const & a, Vec16i const & b) {
+static inline Vec16ib operator < (Vec16i const a, Vec16i const b) {
     return b > a;
 }
 
 // vector operator >= : returns true for elements for which a >= b (signed)
-static inline Vec16ib operator >= (Vec16i const & a, Vec16i const & b) {
+static inline Vec16ib operator >= (Vec16i const a, Vec16i const b) {
     return Vec16ib(a.get_low() >= b.get_low(), a.get_high() >= b.get_high());
 }
 
 // vector operator <= : returns true for elements for which a <= b (signed)
-static inline Vec16ib operator <= (Vec16i const & a, Vec16i const & b) {
+static inline Vec16ib operator <= (Vec16i const a, Vec16i const b) {
     return b >= a;
 }
 
 // vector operator & : bitwise and
-static inline Vec16i operator & (Vec16i const & a, Vec16i const & b) {
+static inline Vec16i operator & (Vec16i const a, Vec16i const b) {
     return Vec16i(a.get_low() & b.get_low(), a.get_high() & b.get_high());
 }
 
 // vector operator &= : bitwise and
-static inline Vec16i & operator &= (Vec16i & a, Vec16i const & b) {
+static inline Vec16i & operator &= (Vec16i & a, Vec16i const b) {
     a = a & b;
     return a;
 }
 
 // vector operator | : bitwise or
-static inline Vec16i operator | (Vec16i const & a, Vec16i const & b) {
+static inline Vec16i operator | (Vec16i const a, Vec16i const b) {
     return Vec16i(a.get_low() | b.get_low(), a.get_high() | b.get_high());
 }
 
 // vector operator |= : bitwise or
-static inline Vec16i & operator |= (Vec16i & a, Vec16i const & b) {
+static inline Vec16i & operator |= (Vec16i & a, Vec16i const b) {
     a = a | b;
     return a;
 }
 
 // vector operator ^ : bitwise xor
-static inline Vec16i operator ^ (Vec16i const & a, Vec16i const & b) {
+static inline Vec16i operator ^ (Vec16i const a, Vec16i const b) {
     return Vec16i(a.get_low() ^ b.get_low(), a.get_high() ^ b.get_high());
 }
 
 // vector operator ^= : bitwise xor
-static inline Vec16i & operator ^= (Vec16i & a, Vec16i const & b) {
+static inline Vec16i & operator ^= (Vec16i & a, Vec16i const b) {
     a = a ^ b;
     return a;
 }
 
 // vector operator ~ : bitwise not
-static inline Vec16i operator ~ (Vec16i const & a) {
+static inline Vec16i operator ~ (Vec16i const a) {
     return Vec16i(~(a.get_low()), ~(a.get_high()));
 }
 
@@ -958,54 +925,63 @@ static inline Vec16i operator ~ (Vec16i const & a) {
 
 // Select between two operands. Corresponds to this pseudocode:
 // for (int i = 0; i < 16; i++) result[i] = s[i] ? a[i] : b[i];
-static inline Vec16i select (Vec16ib const & s, Vec16i const & a, Vec16i const & b) {
+static inline Vec16i select (Vec16ib const s, Vec16i const a, Vec16i const b) {
     return Vec16i(select(s.get_low(), a.get_low(), b.get_low()), select(s.get_high(), a.get_high(), b.get_high()));
 }
 
 // Conditional add: For all vector elements i: result[i] = f[i] ? (a[i] + b[i]) : a[i]
-static inline Vec16i if_add (Vec16ib const & f, Vec16i const & a, Vec16i const & b) {
+static inline Vec16i if_add (Vec16ib const f, Vec16i const a, Vec16i const b) {
     return Vec16i(if_add(f.get_low(), a.get_low(), b.get_low()), if_add(f.get_high(), a.get_high(), b.get_high()));
 }
 
-// Horizontal add: Calculates the sum of all vector elements.
-// Overflow will wrap around
-static inline int32_t horizontal_add (Vec16i const & a) {
+// Conditional subtract
+static inline Vec16i if_sub (Vec16ib const f, Vec16i const a, Vec16i const b) {
+    return Vec16i(if_sub(f.get_low(), a.get_low(), b.get_low()), if_sub(f.get_high(), a.get_high(), b.get_high()));
+}
+
+// Conditional multiply
+static inline Vec16i if_mul (Vec16ib const f, Vec16i const a, Vec16i const b) {
+    return Vec16i(if_mul(f.get_low(), a.get_low(), b.get_low()), if_mul(f.get_high(), a.get_high(), b.get_high()));
+}
+
+// Horizontal add: Calculates the sum of all vector elements. Overflow will wrap around
+static inline int32_t horizontal_add (Vec16i const a) {
     return horizontal_add(a.get_low() + a.get_high());
 }
 
 // function add_saturated: add element by element, signed with saturation
-static inline Vec16i add_saturated(Vec16i const & a, Vec16i const & b) {
+static inline Vec16i add_saturated(Vec16i const a, Vec16i const b) {
     return Vec16i(add_saturated(a.get_low(), b.get_low()), add_saturated(a.get_high(), b.get_high()));
 }
 
 // function sub_saturated: subtract element by element, signed with saturation
-static inline Vec16i sub_saturated(Vec16i const & a, Vec16i const & b) {
+static inline Vec16i sub_saturated(Vec16i const a, Vec16i const b) {
     return Vec16i(sub_saturated(a.get_low(), b.get_low()), sub_saturated(a.get_high(), b.get_high()));
 }
 
 // function max: a > b ? a : b
-static inline Vec16i max(Vec16i const & a, Vec16i const & b) {
+static inline Vec16i max(Vec16i const a, Vec16i const b) {
     return Vec16i(max(a.get_low(), b.get_low()), max(a.get_high(), b.get_high()));
 }
 
 // function min: a < b ? a : b
-static inline Vec16i min(Vec16i const & a, Vec16i const & b) {
+static inline Vec16i min(Vec16i const a, Vec16i const b) {
     return Vec16i(min(a.get_low(), b.get_low()), min(a.get_high(), b.get_high()));
 }
 
 // function abs: a >= 0 ? a : -a
-static inline Vec16i abs(Vec16i const & a) {
+static inline Vec16i abs(Vec16i const a) {
     return Vec16i(abs(a.get_low()), abs(a.get_high()));
 }
 
 // function abs_saturated: same as abs, saturate if overflow
-static inline Vec16i abs_saturated(Vec16i const & a) {
+static inline Vec16i abs_saturated(Vec16i const a) {
     return Vec16i(abs_saturated(a.get_low()), abs_saturated(a.get_high()));
 }
 
 // function rotate_left all elements
 // Use negative count to rotate right
-static inline Vec16i rotate_left(Vec16i const & a, int b) {
+static inline Vec16i rotate_left(Vec16i const a, int b) {
     return Vec16i(rotate_left(a.get_low(), b), rotate_left(a.get_high(), b));
 }
 
@@ -1019,33 +995,32 @@ static inline Vec16i rotate_left(Vec16i const & a, int b) {
 class Vec16ui : public Vec16i {
 public:
     // Default constructor:
-    Vec16ui() {
-    };
+    Vec16ui() = default;
     // Constructor to broadcast the same value into all elements:
     Vec16ui(uint32_t i) {
         z0 = z1 = Vec8ui(i);
-    };
+    }
     // Constructor to build from all elements:
     Vec16ui(uint32_t i0, uint32_t i1, uint32_t i2, uint32_t i3, uint32_t i4, uint32_t i5, uint32_t i6, uint32_t i7,
     uint32_t i8, uint32_t i9, uint32_t i10, uint32_t i11, uint32_t i12, uint32_t i13, uint32_t i14, uint32_t i15) {
         z0 = Vec8ui(i0, i1, i2, i3, i4, i5, i6, i7);
         z1 = Vec8ui(i8, i9, i10, i11, i12, i13, i14, i15);
-    };
+    }
     // Constructor to build from two Vec8ui:
-    Vec16ui(Vec8ui const & a0, Vec8ui const & a1) {
+    Vec16ui(Vec8ui const a0, Vec8ui const a1) {
         z0 = a0;
         z1 = a1;
     }
     // Constructor to convert from type Vec512b
     Vec16ui(Vec512b const & x) {
         *this = x;
-    };
+    }
     // Assignment operator to convert from type Vec512b
-    Vec16ui & operator = (Vec512b const & x) {
+    Vec16ui & operator = (Vec512b const x) {
         z0 = x.get_low();
         z1 = x.get_high();
         return *this;
-    };
+    }
     // Member function to load from array (unaligned)
     Vec16ui & load(void const * p) {
         Vec16i::load(p);
@@ -1057,18 +1032,17 @@ public:
         return *this;
     }
     // Member function to change a single element in vector
-    // Note: This function is inefficient. Use load function if changing more than one element
-    Vec16ui const & insert(uint32_t index, uint32_t value) {
-        Vec16i::insert(index, value);
+    Vec16ui const insert(int index, uint32_t value) {
+        Vec16i::insert(index, (int32_t)value);
         return *this;
     }
     // Member function extract a single element from vector
-    uint32_t extract(uint32_t index) const {
-        return Vec16i::extract(index);
+    uint32_t extract(int index) const {
+        return (uint32_t)Vec16i::extract(index);
     }
     // Extract a single element. Use store function if extracting more than one element.
     // Operator [] can only read an element, not write.
-    uint32_t operator [] (uint32_t index) const {
+    uint32_t operator [] (int index) const {
         return extract(index);
     }
     // Member functions to split into two Vec4ui:
@@ -1078,35 +1052,37 @@ public:
     Vec8ui get_high() const {
         return Vec8ui(Vec16i::get_high());
     }
+    static constexpr int elementtype() {
+        return 9;
+    }
 };
 
 // Define operators for this class
 
 // vector operator + : add
-static inline Vec16ui operator + (Vec16ui const & a, Vec16ui const & b) {
+static inline Vec16ui operator + (Vec16ui const a, Vec16ui const b) {
     return Vec16ui (Vec16i(a) + Vec16i(b));
 }
 
 // vector operator - : subtract
-static inline Vec16ui operator - (Vec16ui const & a, Vec16ui const & b) {
+static inline Vec16ui operator - (Vec16ui const a, Vec16ui const b) {
     return Vec16ui (Vec16i(a) - Vec16i(b));
 }
 
 // vector operator * : multiply
-static inline Vec16ui operator * (Vec16ui const & a, Vec16ui const & b) {
+static inline Vec16ui operator * (Vec16ui const a, Vec16ui const b) {
     return Vec16ui (Vec16i(a) * Vec16i(b));
 }
 
-// vector operator / : divide
-// See bottom of file
+// vector operator / : divide. See bottom of file
 
 // vector operator >> : shift right logical all elements
-static inline Vec16ui operator >> (Vec16ui const & a, uint32_t b) {
+static inline Vec16ui operator >> (Vec16ui const a, uint32_t b) {
     return Vec16ui(a.get_low() >> b, a.get_high() >> b);
 }
 
 // vector operator >> : shift right logical all elements
-static inline Vec16ui operator >> (Vec16ui const & a, int32_t b) {
+static inline Vec16ui operator >> (Vec16ui const a, int32_t b) {
     return a >> (uint32_t)b;
 }
 
@@ -1120,55 +1096,55 @@ static inline Vec16ui & operator >>= (Vec16ui & a, uint32_t b) {
 static inline Vec16ui & operator >>= (Vec16ui & a, int32_t b) {
     a = a >> uint32_t(b);
     return a;
-} 
+}
 
 // vector operator << : shift left all elements
-static inline Vec16ui operator << (Vec16ui const & a, uint32_t b) {
+static inline Vec16ui operator << (Vec16ui const a, uint32_t b) {
     return Vec16ui ((Vec16i)a << (int32_t)b);
 }
 
 // vector operator << : shift left all elements
-static inline Vec16ui operator << (Vec16ui const & a, int32_t b) {
+static inline Vec16ui operator << (Vec16ui const a, int32_t b) {
     return Vec16ui ((Vec16i)a << (int32_t)b);
 }
 
 // vector operator < : returns true for elements for which a < b (unsigned)
-static inline Vec16ib operator < (Vec16ui const & a, Vec16ui const & b) {
+static inline Vec16ib operator < (Vec16ui const a, Vec16ui const b) {
     return Vec16ib(a.get_low() < b.get_low(), a.get_high() < b.get_high());
 }
 
 // vector operator > : returns true for elements for which a > b (unsigned)
-static inline Vec16ib operator > (Vec16ui const & a, Vec16ui const & b) {
+static inline Vec16ib operator > (Vec16ui const a, Vec16ui const b) {
     return b < a;
 }
 
 // vector operator >= : returns true for elements for which a >= b (unsigned)
-static inline Vec16ib operator >= (Vec16ui const & a, Vec16ui const & b) {
+static inline Vec16ib operator >= (Vec16ui const a, Vec16ui const b) {
     return Vec16ib(a.get_low() >= b.get_low(), a.get_high() >= b.get_high());
-}            
+}
 
 // vector operator <= : returns true for elements for which a <= b (unsigned)
-static inline Vec16ib operator <= (Vec16ui const & a, Vec16ui const & b) {
+static inline Vec16ib operator <= (Vec16ui const a, Vec16ui const b) {
     return b >= a;
 }
 
 // vector operator & : bitwise and
-static inline Vec16ui operator & (Vec16ui const & a, Vec16ui const & b) {
+static inline Vec16ui operator & (Vec16ui const a, Vec16ui const b) {
     return Vec16ui(Vec16i(a) & Vec16i(b));
 }
 
 // vector operator | : bitwise or
-static inline Vec16ui operator | (Vec16ui const & a, Vec16ui const & b) {
+static inline Vec16ui operator | (Vec16ui const a, Vec16ui const b) {
     return Vec16ui(Vec16i(a) | Vec16i(b));
 }
 
 // vector operator ^ : bitwise xor
-static inline Vec16ui operator ^ (Vec16ui const & a, Vec16ui const & b) {
+static inline Vec16ui operator ^ (Vec16ui const a, Vec16ui const b) {
     return Vec16ui(Vec16i(a) ^ Vec16i(b));
 }
 
 // vector operator ~ : bitwise not
-static inline Vec16ui operator ~ (Vec16ui const & a) {
+static inline Vec16ui operator ~ (Vec16ui const a) {
     return Vec16ui( ~ Vec16i(a));
 }
 
@@ -1176,40 +1152,49 @@ static inline Vec16ui operator ~ (Vec16ui const & a) {
 
 // Select between two operands. Corresponds to this pseudocode:
 // for (int i = 0; i < 16; i++) result[i] = s[i] ? a[i] : b[i];
-static inline Vec16ui select (Vec16ib const & s, Vec16ui const & a, Vec16ui const & b) {
+static inline Vec16ui select (Vec16ib const s, Vec16ui const a, Vec16ui const b) {
     return Vec16ui(select(s, Vec16i(a), Vec16i(b)));
 }
 
 // Conditional add: For all vector elements i: result[i] = f[i] ? (a[i] + b[i]) : a[i]
-static inline Vec16ui if_add (Vec16ib const & f, Vec16ui const & a, Vec16ui const & b) {
+static inline Vec16ui if_add (Vec16ib const f, Vec16ui const a, Vec16ui const b) {
     return Vec16ui(if_add(f, Vec16i(a), Vec16i(b)));
 }
 
-// Horizontal add: Calculates the sum of all vector elements.
-// Overflow will wrap around
-static inline uint32_t horizontal_add (Vec16ui const & a) {
-    return horizontal_add((Vec16i)a);
+// Conditional subtract
+static inline Vec16ui if_sub (Vec16ib const f, Vec16ui const a, Vec16ui const b) {
+    return Vec16ui(if_sub(f, Vec16i(a), Vec16i(b)));
+}
+
+// Conditional multiply
+static inline Vec16ui if_mul (Vec16ib const f, Vec16ui const a, Vec16ui const b) {
+    return Vec16ui(if_mul(f, Vec16i(a), Vec16i(b)));
+}
+
+// Horizontal add: Calculates the sum of all vector elements. Overflow will wrap around
+static inline uint32_t horizontal_add (Vec16ui const a) {
+    return (uint32_t)horizontal_add((Vec16i)a);
 }
 
 // horizontal_add_x: Horizontal add extended: Calculates the sum of all vector elements. Defined later in this file
 
 // function add_saturated: add element by element, unsigned with saturation
-static inline Vec16ui add_saturated(Vec16ui const & a, Vec16ui const & b) {
+static inline Vec16ui add_saturated(Vec16ui const a, Vec16ui const b) {
     return Vec16ui(add_saturated(a.get_low(), b.get_low()), add_saturated(a.get_high(), b.get_high()));
 }
 
 // function sub_saturated: subtract element by element, unsigned with saturation
-static inline Vec16ui sub_saturated(Vec16ui const & a, Vec16ui const & b) {
+static inline Vec16ui sub_saturated(Vec16ui const a, Vec16ui const b) {
     return Vec16ui(sub_saturated(a.get_low(), b.get_low()), sub_saturated(a.get_high(), b.get_high()));
 }
 
 // function max: a > b ? a : b
-static inline Vec16ui max(Vec16ui const & a, Vec16ui const & b) {
+static inline Vec16ui max(Vec16ui const a, Vec16ui const b) {
     return Vec16ui(max(a.get_low(), b.get_low()), max(a.get_high(), b.get_high()));
 }
 
 // function min: a < b ? a : b
-static inline Vec16ui min(Vec16ui const & a, Vec16ui const & b) {
+static inline Vec16ui min(Vec16ui const a, Vec16ui const b) {
     return Vec16ui(min(a.get_low(), b.get_low()), min(a.get_high(), b.get_high()));
 }
 
@@ -1223,8 +1208,7 @@ static inline Vec16ui min(Vec16ui const & a, Vec16ui const & b) {
 class Vec8q : public Vec512b {
 public:
     // Default constructor:
-    Vec8q() {
-    }
+    Vec8q() = default;
     // Constructor to broadcast the same value into all elements:
     Vec8q(int64_t i) {
         z0 = z1 = Vec4q(i);
@@ -1235,7 +1219,7 @@ public:
         z1 = Vec4q(i4, i5, i6, i7);
     }
     // Constructor to build from two Vec4q:
-    Vec8q(Vec4q const & a0, Vec4q const & a1) {
+    Vec8q(Vec4q const a0, Vec4q const a1) {
         z0 = a0;
         z1 = a1;
     }
@@ -1245,7 +1229,7 @@ public:
         z1 = x.get_high();
     }
     // Assignment operator to convert from type Vec512b
-    Vec8q & operator = (Vec512b const & x) {
+    Vec8q & operator = (Vec512b const x) {
         z0 = x.get_low();
         z1 = x.get_high();
         return *this;
@@ -1296,9 +1280,8 @@ public:
         return *this;
     }
     // Member function to change a single element in vector
-    // Note: This function is inefficient. Use load function if changing more than one element
-    Vec8q const & insert(uint32_t index, int64_t value) {
-        if (index < 4) {
+    Vec8q const insert(int index, int64_t value) {
+        if ((uint32_t)index < 4) {
             z0 = Vec4q(z0).insert(index, value);
         }
         else {
@@ -1307,8 +1290,8 @@ public:
         return *this;
     }
     // Member function extract a single element from vector
-    int64_t extract(uint32_t index) const {
-        if (index < 4) {
+    int64_t extract(int index) const {
+        if ((uint32_t)index < 4) {
             return Vec4q(z0).extract(index);
         }
         else {
@@ -1317,7 +1300,7 @@ public:
     }
     // Extract a single element. Use store function if extracting more than one element.
     // Operator [] can only read an element, not write.
-    int64_t operator [] (uint32_t index) const {
+    int64_t operator [] (int index) const {
         return extract(index);
     }
     // Member functions to split into two Vec2q:
@@ -1327,21 +1310,23 @@ public:
     Vec4q get_high() const {
         return Vec4q(z1);
     }
-    static int size () {
+    static constexpr int size() {
         return 8;
     }
+    static constexpr int elementtype() {
+        return 10;
+    }
 };
 
-
 // Define operators for Vec8q
 
 // vector operator + : add element by element
-static inline Vec8q operator + (Vec8q const & a, Vec8q const & b) {
+static inline Vec8q operator + (Vec8q const a, Vec8q const b) {
     return Vec8q(a.get_low() + b.get_low(), a.get_high() + b.get_high());
 }
 
 // vector operator += : add
-static inline Vec8q & operator += (Vec8q & a, Vec8q const & b) {
+static inline Vec8q & operator += (Vec8q & a, Vec8q const b) {
     a = a + b;
     return a;
 }
@@ -1360,17 +1345,17 @@ static inline Vec8q & operator ++ (Vec8q & a) {
 }
 
 // vector operator - : subtract element by element
-static inline Vec8q operator - (Vec8q const & a, Vec8q const & b) {
+static inline Vec8q operator - (Vec8q const a, Vec8q const b) {
     return Vec8q(a.get_low() - b.get_low(), a.get_high() - b.get_high());
 }
 
 // vector operator - : unary minus
-static inline Vec8q operator - (Vec8q const & a) {
+static inline Vec8q operator - (Vec8q const a) {
     return Vec8q(- a.get_low(), - a.get_high());
 }
 
 // vector operator -= : subtract
-static inline Vec8q & operator -= (Vec8q & a, Vec8q const & b) {
+static inline Vec8q & operator -= (Vec8q & a, Vec8q const b) {
     a = a - b;
     return a;
 }
@@ -1389,18 +1374,18 @@ static inline Vec8q & operator -- (Vec8q & a) {
 }
 
 // vector operator * : multiply element by element
-static inline Vec8q operator * (Vec8q const & a, Vec8q const & b) {
+static inline Vec8q operator * (Vec8q const a, Vec8q const b) {
     return Vec8q(a.get_low() * b.get_low(), a.get_high() * b.get_high());
 }
 
 // vector operator *= : multiply
-static inline Vec8q & operator *= (Vec8q & a, Vec8q const & b) {
+static inline Vec8q & operator *= (Vec8q & a, Vec8q const b) {
     a = a * b;
     return a;
 }
 
 // vector operator << : shift left
-static inline Vec8q operator << (Vec8q const & a, int32_t b) {
+static inline Vec8q operator << (Vec8q const a, int32_t b) {
     return Vec8q(a.get_low() << b, a.get_high() << b);
 }
 
@@ -1411,7 +1396,7 @@ static inline Vec8q & operator <<= (Vec8q & a, int32_t b) {
 }
 
 // vector operator >> : shift right arithmetic
-static inline Vec8q operator >> (Vec8q const & a, int32_t b) {
+static inline Vec8q operator >> (Vec8q const a, int32_t b) {
     return Vec8q(a.get_low() >> b, a.get_high() >> b);
 }
 
@@ -1422,69 +1407,69 @@ static inline Vec8q & operator >>= (Vec8q & a, int32_t b) {
 }
 
 // vector operator == : returns true for elements for which a == b
-static inline Vec8qb operator == (Vec8q const & a, Vec8q const & b) {
+static inline Vec8qb operator == (Vec8q const a, Vec8q const b) {
     return Vec8qb(a.get_low() == b.get_low(), a.get_high() == b.get_high());
 }
 
 // vector operator != : returns true for elements for which a != b
-static inline Vec8qb operator != (Vec8q const & a, Vec8q const & b) {
+static inline Vec8qb operator != (Vec8q const a, Vec8q const b) {
     return Vec8qb(a.get_low() != b.get_low(), a.get_high() != b.get_high());
 }
-  
+
 // vector operator < : returns true for elements for which a < b
-static inline Vec8qb operator < (Vec8q const & a, Vec8q const & b) {
+static inline Vec8qb operator < (Vec8q const a, Vec8q const b) {
     return Vec8qb(a.get_low() < b.get_low(), a.get_high() < b.get_high());
 }
 
 // vector operator > : returns true for elements for which a > b
-static inline Vec8qb operator > (Vec8q const & a, Vec8q const & b) {
+static inline Vec8qb operator > (Vec8q const a, Vec8q const b) {
     return b < a;
 }
 
 // vector operator >= : returns true for elements for which a >= b (signed)
-static inline Vec8qb operator >= (Vec8q const & a, Vec8q const & b) {
+static inline Vec8qb operator >= (Vec8q const a, Vec8q const b) {
     return Vec8qb(a.get_low() >= b.get_low(), a.get_high() >= b.get_high());
 }
 
 // vector operator <= : returns true for elements for which a <= b (signed)
-static inline Vec8qb operator <= (Vec8q const & a, Vec8q const & b) {
+static inline Vec8qb operator <= (Vec8q const a, Vec8q const b) {
     return b >= a;
 }
 
 // vector operator & : bitwise and
-static inline Vec8q operator & (Vec8q const & a, Vec8q const & b) {
+static inline Vec8q operator & (Vec8q const a, Vec8q const b) {
     return Vec8q(a.get_low() & b.get_low(), a.get_high() & b.get_high());
 }
 
 // vector operator &= : bitwise and
-static inline Vec8q & operator &= (Vec8q & a, Vec8q const & b) {
+static inline Vec8q & operator &= (Vec8q & a, Vec8q const b) {
     a = a & b;
     return a;
 }
 
 // vector operator | : bitwise or
-static inline Vec8q operator | (Vec8q const & a, Vec8q const & b) {
+static inline Vec8q operator | (Vec8q const a, Vec8q const b) {
     return Vec8q(a.get_low() | b.get_low(), a.get_high() | b.get_high());
 }
 
 // vector operator |= : bitwise or
-static inline Vec8q & operator |= (Vec8q & a, Vec8q const & b) {
+static inline Vec8q & operator |= (Vec8q & a, Vec8q const b) {
     a = a | b;
     return a;
 }
 
 // vector operator ^ : bitwise xor
-static inline Vec8q operator ^ (Vec8q const & a, Vec8q const & b) {
+static inline Vec8q operator ^ (Vec8q const a, Vec8q const b) {
     return Vec8q(a.get_low() ^ b.get_low(), a.get_high() ^ b.get_high());
 }
 // vector operator ^= : bitwise xor
-static inline Vec8q & operator ^= (Vec8q & a, Vec8q const & b) {
+static inline Vec8q & operator ^= (Vec8q & a, Vec8q const b) {
     a = a ^ b;
     return a;
 }
 
 // vector operator ~ : bitwise not
-static inline Vec8q operator ~ (Vec8q const & a) {
+static inline Vec8q operator ~ (Vec8q const a) {
     return Vec8q(~(a.get_low()), ~(a.get_high()));
 }
 
@@ -1492,56 +1477,65 @@ static inline Vec8q operator ~ (Vec8q const & a) {
 
 // Select between two operands. Corresponds to this pseudocode:
 // for (int i = 0; i < 4; i++) result[i] = s[i] ? a[i] : b[i];
-static inline Vec8q select (Vec8qb const & s, Vec8q const & a, Vec8q const & b) {
+static inline Vec8q select (Vec8qb const s, Vec8q const a, Vec8q const b) {
     return Vec8q(select(s.get_low(), a.get_low(), b.get_low()), select(s.get_high(), a.get_high(), b.get_high()));
 }
 
 // Conditional add: For all vector elements i: result[i] = f[i] ? (a[i] + b[i]) : a[i]
-static inline Vec8q if_add (Vec8qb const & f, Vec8q const & a, Vec8q const & b) {
+static inline Vec8q if_add (Vec8qb const f, Vec8q const a, Vec8q const b) {
     return Vec8q(if_add(f.get_low(), a.get_low(), b.get_low()), if_add(f.get_high(), a.get_high(), b.get_high()));
 }
 
-// Horizontal add: Calculates the sum of all vector elements.
-// Overflow will wrap around
-static inline int64_t horizontal_add (Vec8q const & a) {
+// Conditional subtract
+static inline Vec8q if_sub (Vec8qb const f, Vec8q const a, Vec8q const b) {
+    return Vec8q(if_sub(f.get_low(), a.get_low(), b.get_low()), if_sub(f.get_high(), a.get_high(), b.get_high()));
+}
+
+// Conditional multiply
+static inline Vec8q if_mul (Vec8qb const f, Vec8q const a, Vec8q const b) {
+    return Vec8q(if_mul(f.get_low(), a.get_low(), b.get_low()), if_mul(f.get_high(), a.get_high(), b.get_high()));
+}
+
+// Horizontal add: Calculates the sum of all vector elements. Overflow will wrap around
+static inline int64_t horizontal_add (Vec8q const a) {
     return horizontal_add(a.get_low() + a.get_high());
 }
 
 // Horizontal add extended: Calculates the sum of all vector elements
 // Elements are sign extended before adding to avoid overflow
-static inline int64_t horizontal_add_x (Vec16i const & x) {
+static inline int64_t horizontal_add_x (Vec16i const x) {
     return horizontal_add_x(x.get_low()) + horizontal_add_x(x.get_high());
 }
 
 // Horizontal add extended: Calculates the sum of all vector elements
 // Elements are zero extended before adding to avoid overflow
-static inline uint64_t horizontal_add_x (Vec16ui const & x) {
+static inline uint64_t horizontal_add_x (Vec16ui const x) {
     return horizontal_add_x(x.get_low()) + horizontal_add_x(x.get_high());
 }
 
 // function max: a > b ? a : b
-static inline Vec8q max(Vec8q const & a, Vec8q const & b) {
+static inline Vec8q max(Vec8q const a, Vec8q const b) {
     return Vec8q(max(a.get_low(), b.get_low()), max(a.get_high(), b.get_high()));
 }
 
 // function min: a < b ? a : b
-static inline Vec8q min(Vec8q const & a, Vec8q const & b) {
+static inline Vec8q min(Vec8q const a, Vec8q const b) {
     return Vec8q(min(a.get_low(), b.get_low()), min(a.get_high(), b.get_high()));
 }
 
 // function abs: a >= 0 ? a : -a
-static inline Vec8q abs(Vec8q const & a) {
+static inline Vec8q abs(Vec8q const a) {
     return Vec8q(abs(a.get_low()), abs(a.get_high()));
 }
 
 // function abs_saturated: same as abs, saturate if overflow
-static inline Vec8q abs_saturated(Vec8q const & a) {
+static inline Vec8q abs_saturated(Vec8q const a) {
     return Vec8q(abs_saturated(a.get_low()), abs_saturated(a.get_high()));
 }
 
 // function rotate_left all elements
 // Use negative count to rotate right
-static inline Vec8q rotate_left(Vec8q const & a, int b) {
+static inline Vec8q rotate_left(Vec8q const a, int b) {
     return Vec8q(rotate_left(a.get_low(), b), rotate_left(a.get_high(), b));
 }
 
@@ -1555,14 +1549,13 @@ static inline Vec8q rotate_left(Vec8q const & a, int b) {
 class Vec8uq : public Vec8q {
 public:
     // Default constructor:
-    Vec8uq() {
-    }
+    Vec8uq() = default;
     // Constructor to broadcast the same value into all elements:
     Vec8uq(uint64_t i) {
         z0 = z1 = Vec4uq(i);
     }
     // Constructor to convert from Vec8q:
-    Vec8uq(Vec8q const & x) {
+    Vec8uq(Vec8q const x) {
         z0 = x.get_low();
         z1 = x.get_high();
     }
@@ -1573,22 +1566,22 @@ public:
     }
     // Constructor to build from all elements:
     Vec8uq(uint64_t i0, uint64_t i1, uint64_t i2, uint64_t i3, uint64_t i4, uint64_t i5, uint64_t i6, uint64_t i7) {
-        z0 = Vec4q(i0, i1, i2, i3);
-        z1 = Vec4q(i4, i5, i6, i7);
+        z0 = Vec4q((int64_t)i0, (int64_t)i1, (int64_t)i2, (int64_t)i3);
+        z1 = Vec4q((int64_t)i4, (int64_t)i5, (int64_t)i6, (int64_t)i7);
     }
     // Constructor to build from two Vec4uq:
-    Vec8uq(Vec4uq const & a0, Vec4uq const & a1) {
+    Vec8uq(Vec4uq const a0, Vec4uq const a1) {
         z0 = a0;
         z1 = a1;
     }
     // Assignment operator to convert from Vec8q:
-    Vec8uq  & operator = (Vec8q const & x) {
+    Vec8uq & operator = (Vec8q const x) {
         z0 = x.get_low();
         z1 = x.get_high();
         return *this;
     }
     // Assignment operator to convert from type Vec512b
-    Vec8uq & operator = (Vec512b const & x) {
+    Vec8uq & operator = (Vec512b const x) {
         z0 = x.get_low();
         z1 = x.get_high();
         return *this;
@@ -1604,18 +1597,17 @@ public:
         return *this;
     }
     // Member function to change a single element in vector
-    // Note: This function is inefficient. Use load function if changing more than one element
-    Vec8uq const & insert(uint32_t index, uint64_t value) {
-        Vec8q::insert(index, value);
+    Vec8uq const insert(int index, uint64_t value) {
+        Vec8q::insert(index, (int64_t)value);
         return *this;
     }
     // Member function extract a single element from vector
-    uint64_t extract(uint32_t index) const {
-        return Vec8q::extract(index);
+    uint64_t extract(int index) const {
+        return (uint64_t)Vec8q::extract(index);
     }
     // Extract a single element. Use store function if extracting more than one element.
     // Operator [] can only read an element, not write.
-    uint64_t operator [] (uint32_t index) const {
+    uint64_t operator [] (int index) const {
         return extract(index);
     }
     // Member functions to split into two Vec2uq:
@@ -1625,32 +1617,35 @@ public:
     Vec4uq get_high() const {
         return Vec4uq(Vec8q::get_high());
     }
+    static constexpr int elementtype() {
+        return 11;
+    }
 };
 
 // Define operators for this class
 
 // vector operator + : add
-static inline Vec8uq operator + (Vec8uq const & a, Vec8uq const & b) {
+static inline Vec8uq operator + (Vec8uq const a, Vec8uq const b) {
     return Vec8uq (Vec8q(a) + Vec8q(b));
 }
 
 // vector operator - : subtract
-static inline Vec8uq operator - (Vec8uq const & a, Vec8uq const & b) {
+static inline Vec8uq operator - (Vec8uq const a, Vec8uq const b) {
     return Vec8uq (Vec8q(a) - Vec8q(b));
 }
 
 // vector operator * : multiply element by element
-static inline Vec8uq operator * (Vec8uq const & a, Vec8uq const & b) {
+static inline Vec8uq operator * (Vec8uq const a, Vec8uq const b) {
     return Vec8uq (Vec8q(a) * Vec8q(b));
 }
 
 // vector operator >> : shift right logical all elements
-static inline Vec8uq operator >> (Vec8uq const & a, uint32_t b) {
+static inline Vec8uq operator >> (Vec8uq const a, uint32_t b) {
     return Vec8uq(a.get_low() >> b, a.get_high() >> b);
 }
 
 // vector operator >> : shift right logical all elements
-static inline Vec8uq operator >> (Vec8uq const & a, int32_t b) {
+static inline Vec8uq operator >> (Vec8uq const a, int32_t b) {
     return a >> (uint32_t)b;
 }
 
@@ -1664,50 +1659,50 @@ static inline Vec8uq & operator >>= (Vec8uq & a, uint32_t b) {
 static inline Vec8uq & operator >>= (Vec8uq & a, int32_t b) {
     a = a >> uint32_t(b);
     return a;
-} 
+}
 
 // vector operator << : shift left all elements
-static inline Vec8uq operator << (Vec8uq const & a, uint32_t b) {
+static inline Vec8uq operator << (Vec8uq const a, uint32_t b) {
     return Vec8uq ((Vec8q)a << (int32_t)b);
 }
 
 // vector operator << : shift left all elements
-static inline Vec8uq operator << (Vec8uq const & a, int32_t b) {
+static inline Vec8uq operator << (Vec8uq const a, int32_t b) {
     return Vec8uq ((Vec8q)a << b);
 }
 
 // vector operator < : returns true for elements for which a < b (unsigned)
-static inline Vec8qb operator < (Vec8uq const & a, Vec8uq const & b) {
+static inline Vec8qb operator < (Vec8uq const a, Vec8uq const b) {
     return Vec8qb(a.get_low() < b.get_low(), a.get_high() < b.get_high());
 }
 
 // vector operator > : returns true for elements for which a > b (unsigned)
-static inline Vec8qb operator > (Vec8uq const & a, Vec8uq const & b) {
+static inline Vec8qb operator > (Vec8uq const a, Vec8uq const b) {
     return b < a;
 }
 
 // vector operator >= : returns true for elements for which a >= b (unsigned)
-static inline Vec8qb operator >= (Vec8uq const & a, Vec8uq const & b) {
+static inline Vec8qb operator >= (Vec8uq const a, Vec8uq const b) {
     return Vec8qb(a.get_low() >= b.get_low(), a.get_high() >= b.get_high());
 }
 
 // vector operator <= : returns true for elements for which a <= b (unsigned)
-static inline Vec8qb operator <= (Vec8uq const & a, Vec8uq const & b) {
+static inline Vec8qb operator <= (Vec8uq const a, Vec8uq const b) {
     return b >= a;
 }
 
 // vector operator & : bitwise and
-static inline Vec8uq operator & (Vec8uq const & a, Vec8uq const & b) {
+static inline Vec8uq operator & (Vec8uq const a, Vec8uq const b) {
     return Vec8uq(Vec8q(a) & Vec8q(b));
 }
 
 // vector operator | : bitwise or
-static inline Vec8uq operator | (Vec8uq const & a, Vec8uq const & b) {
+static inline Vec8uq operator | (Vec8uq const a, Vec8uq const b) {
     return Vec8uq(Vec8q(a) | Vec8q(b));
 }
 
 // vector operator ^ : bitwise xor
-static inline Vec8uq operator ^ (Vec8uq const & a, Vec8uq const & b) {
+static inline Vec8uq operator ^ (Vec8uq const a, Vec8uq const b) {
     return Vec8uq(Vec8q(a) ^ Vec8q(b));
 }
 
@@ -1715,28 +1710,37 @@ static inline Vec8uq operator ^ (Vec8uq const & a, Vec8uq const & b) {
 
 // Select between two operands. Corresponds to this pseudocode:
 // for (int i = 0; i < 4; i++) result[i] = s[i] ? a[i] : b[i];
-static inline Vec8uq select (Vec8qb const & s, Vec8uq const & a, Vec8uq const & b) {
+static inline Vec8uq select (Vec8qb const s, Vec8uq const a, Vec8uq const b) {
     return Vec8uq(select(s, Vec8q(a), Vec8q(b)));
 }
 
 // Conditional add: For all vector elements i: result[i] = f[i] ? (a[i] + b[i]) : a[i]
-static inline Vec8uq if_add (Vec8qb const & f, Vec8uq const & a, Vec8uq const & b) {
+static inline Vec8uq if_add (Vec8qb const f, Vec8uq const a, Vec8uq const b) {
     return Vec8uq(if_add(f.get_low(), a.get_low(), b.get_low()), if_add(f.get_high(), a.get_high(), b.get_high()));
 }
 
-// Horizontal add: Calculates the sum of all vector elements.
-// Overflow will wrap around
-static inline uint64_t horizontal_add (Vec8uq const & a) {
-    return horizontal_add(Vec8q(a));
+// Conditional subtract
+static inline Vec8uq if_sub (Vec8qb const f, Vec8uq const a, Vec8uq const b) {
+    return Vec8uq(if_sub(f.get_low(), a.get_low(), b.get_low()), if_sub(f.get_high(), a.get_high(), b.get_high()));
+}
+
+// Conditional multiply
+static inline Vec8uq if_mul (Vec8qb const f, Vec8uq const a, Vec8uq const b) {
+    return Vec8uq(if_mul(f.get_low(), a.get_low(), b.get_low()), if_mul(f.get_high(), a.get_high(), b.get_high()));
+}
+
+// Horizontal add: Calculates the sum of all vector elements. Overflow will wrap around
+static inline uint64_t horizontal_add (Vec8uq const a) {
+    return (uint64_t)horizontal_add(Vec8q(a));
 }
 
 // function max: a > b ? a : b
-static inline Vec8uq max(Vec8uq const & a, Vec8uq const & b) {
+static inline Vec8uq max(Vec8uq const a, Vec8uq const b) {
     return Vec8uq(max(a.get_low(), b.get_low()), max(a.get_high(), b.get_high()));
 }
 
 // function min: a < b ? a : b
-static inline Vec8uq min(Vec8uq const & a, Vec8uq const & b) {
+static inline Vec8uq min(Vec8uq const a, Vec8uq const b) {
     return Vec8uq(min(a.get_low(), b.get_low()), min(a.get_high(), b.get_high()));
 }
 
@@ -1748,48 +1752,34 @@ static inline Vec8uq min(Vec8uq const & a, Vec8uq const & b) {
 ******************************************************************************
 *
 * These permute functions can reorder the elements of a vector and optionally
-* set some elements to zero. 
-*
-* The indexes are inserted as template parameters in <>. These indexes must be
-* constants. Each template parameter is an index to the element you want to select.
-* An index of -1 will generate zero. An index of -256 means don't care.
+* set some elements to zero. See Vectori128.h for description
 *
-* Example:
-* Vec8q a(10,11,12,13,14,15,16,17);      // a is (10,11,12,13,14,15,16,17)
-* Vec8q b;
-* b = permute8q<0,2,7,7,-1,-1,1,1>(a);   // b is (10,12,17,17, 0, 0,11,11)
-*
-* A lot of the code here is metaprogramming aiming to find the instructions
-* that best fit the template parameters and instruction set. The metacode
-* will be reduced out to leave only a few vector instructions in release
-* mode with optimization on.
 *****************************************************************************/
 
 // Permute vector of 8 64-bit integers.
-// Index -1 gives 0, index -256 means don't care.
+// Index -1 gives 0, index V_DC means don't care.
 template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7>
-static inline Vec8q permute8q(Vec8q const & a) {
-    return Vec8q(blend4q<i0,i1,i2,i3> (a.get_low(), a.get_high()),
-                 blend4q<i4,i5,i6,i7> (a.get_low(), a.get_high()));
+static inline Vec8q permute8(Vec8q const a) {
+    return Vec8q(blend4<i0,i1,i2,i3> (a.get_low(), a.get_high()),
+                 blend4<i4,i5,i6,i7> (a.get_low(), a.get_high()));
 }
 
 template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7>
-static inline Vec8uq permute8uq(Vec8uq const & a) {
-    return Vec8uq (permute8q<i0,i1,i2,i3,i4,i5,i6,i7> (a));
+static inline Vec8uq permute8(Vec8uq const& a) {
+    return Vec8uq(permute8<i0, i1, i2, i3, i4, i5, i6, i7>(Vec8q(a)));
 }
 
-
 // Permute vector of 16 32-bit integers.
-// Index -1 gives 0, index -256 means don't care.
+// Index -1 gives 0, index V_DC means don't care.
 template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7, int i8, int i9, int i10, int i11, int i12, int i13, int i14, int i15>
-static inline Vec16i permute16i(Vec16i const & a) {
-    return Vec16i(blend8i<i0,i1,i2 ,i3 ,i4 ,i5 ,i6 ,i7 > (a.get_low(), a.get_high()),
-                  blend8i<i8,i9,i10,i11,i12,i13,i14,i15> (a.get_low(), a.get_high()));
+static inline Vec16i permute16(Vec16i const a) {
+    return Vec16i(blend8<i0,i1,i2 ,i3 ,i4 ,i5 ,i6 ,i7 > (a.get_low(), a.get_high()),
+                  blend8<i8,i9,i10,i11,i12,i13,i14,i15> (a.get_low(), a.get_high()));
 }
 
 template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7, int i8, int i9, int i10, int i11, int i12, int i13, int i14, int i15>
-static inline Vec16ui permute16ui(Vec16ui const & a) {
-    return Vec16ui (permute16i<i0,i1,i2,i3,i4,i5,i6,i7,i8,i9,i10,i11,i12,i13,i14,i15> (a));
+static inline Vec16ui permute16(Vec16ui const a) {
+    return Vec16ui (permute16<i0,i1,i2,i3,i4,i5,i6,i7,i8,i9,i10,i11,i12,i13,i14,i15> (Vec16i(a)));
 }
 
 
@@ -1797,242 +1787,33 @@ static inline Vec16ui permute16ui(Vec16ui const & a) {
 *
 *          Vector blend functions
 *
-******************************************************************************
-*
-* These blend functions can mix elements from two different vectors and
-* optionally set some elements to zero. 
-*
-* The indexes are inserted as template parameters in <>. These indexes must be
-* constants. Each template parameter is an index to the element you want to 
-* select, where higher indexes indicate an element from the second source
-* vector. For example, if each vector has 8 elements, then indexes 0 - 7
-* will select an element from the first vector and indexes 8 - 15 will select 
-* an element from the second vector. A negative index will generate zero.
-*
-* Example:
-* Vec8q a(100,101,102,103,104,105,106,107); // a is (100, 101, 102, 103, 104, 105, 106, 107)
-* Vec8q b(200,201,202,203,204,205,206,207); // b is (200, 201, 202, 203, 204, 205, 206, 207)
-* Vec8q c;
-* c = blend8q<1,0,9,8,7,-1,15,15> (a,b);    // c is (101, 100, 201, 200, 107,   0, 207, 207)
-*
-* A lot of the code here is metaprogramming aiming to find the instructions
-* that best fit the template parameters and instruction set. The metacode
-* will be reduced out to leave only a few vector instructions in release
-* mode with optimization on.
 *****************************************************************************/
 
-
-// helper function used below
-template <int n>
-static inline Vec4q select4(Vec8q const & a, Vec8q const & b) {
-    switch (n) {
-    case 0:
-        return a.get_low();
-    case 1:
-        return a.get_high();
-    case 2:
-        return b.get_low();
-    case 3:
-        return b.get_high();
-    }
-    return Vec4q(0);
-}
-
 // blend vectors Vec8q
-template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7> 
-static inline Vec8q blend8q(Vec8q const & a, Vec8q const & b) {  
-    const int j0 = i0 >= 0 ? i0/4 : i0;
-    const int j1 = i1 >= 0 ? i1/4 : i1;
-    const int j2 = i2 >= 0 ? i2/4 : i2;
-    const int j3 = i3 >= 0 ? i3/4 : i3;
-    const int j4 = i4 >= 0 ? i4/4 : i4;
-    const int j5 = i5 >= 0 ? i5/4 : i5;
-    const int j6 = i6 >= 0 ? i6/4 : i6;
-    const int j7 = i7 >= 0 ? i7/4 : i7;
-    Vec4q x0, x1;
-
-    const int r0 = j0 >= 0 ? j0 : j1 >= 0 ? j1 : j2 >= 0 ? j2 : j3;
-    const int r1 = j4 >= 0 ? j4 : j5 >= 0 ? j5 : j6 >= 0 ? j6 : j7;
-    const int s0 = (j1 >= 0 && j1 != r0) ? j1 : (j2 >= 0 && j2 != r0) ? j2 : j3;
-    const int s1 = (j5 >= 0 && j5 != r1) ? j5 : (j6 >= 0 && j6 != r1) ? j6 : j7;
-
-    // Combine all the indexes into a single bitfield, with 4 bits for each
-    const int m1 = (i0&0xF) | (i1&0xF)<<4 | (i2&0xF)<<8 | (i3&0xF)<<12 | (i4&0xF)<<16 | (i5&0xF)<<20 | (i6&0xF)<<24 | (i7&0xF)<<28;
-
-    // Mask to zero out negative indexes
-    const int mz = (i0<0?0:0xF) | (i1<0?0:0xF)<<4 | (i2<0?0:0xF)<<8 | (i3<0?0:0xF)<<12 | (i4<0?0:0xF)<<16 | (i5<0?0:0xF)<<20 | (i6<0?0:0xF)<<24 | (i7<0?0:0xF)<<28;
-
-    if (r0 < 0) {
-        x0 =  Vec4q(0);
-    }
-    else if (((m1 ^ r0*0x4444) & 0xCCCC & mz) == 0) { 
-        // i0 - i3 all from same source
-        x0 = permute4q<i0 & -13, i1 & -13, i2 & -13, i3 & -13> (select4<r0> (a,b));
-    }
-    else if ((j2 < 0 || j2 == r0 || j2 == s0) && (j3 < 0 || j3 == r0 || j3 == s0)) { 
-        // i0 - i3 all from two sources
-        const int k0 =  i0 >= 0 ? i0 & 3 : i0;
-        const int k1 = (i1 >= 0 ? i1 & 3 : i1) | (j1 == s0 ? 4 : 0);
-        const int k2 = (i2 >= 0 ? i2 & 3 : i2) | (j2 == s0 ? 4 : 0);
-        const int k3 = (i3 >= 0 ? i3 & 3 : i3) | (j3 == s0 ? 4 : 0);
-        x0 = blend4q<k0,k1,k2,k3> (select4<r0>(a,b), select4<s0>(a,b));
-    }
-    else {
-        // i0 - i3 from three or four different sources
-        x0 = blend4q<0,1,6,7> (
-             blend4q<i0 & -13, (i1 & -13) | 4, -0x100, -0x100> (select4<j0>(a,b), select4<j1>(a,b)),
-             blend4q<-0x100, -0x100, i2 & -13, (i3 & -13) | 4> (select4<j2>(a,b), select4<j3>(a,b)));
-    }
-
-    if (r1 < 0) {
-        x1 =  Vec4q(0);
-    }
-    else if (((m1 ^ uint32_t(r1)*0x44440000u) & 0xCCCC0000 & mz) == 0) { 
-        // i4 - i7 all from same source
-        x1 = permute4q<i4 & -13, i5 & -13, i6 & -13, i7 & -13> (select4<r1> (a,b));
-    }
-    else if ((j6 < 0 || j6 == r1 || j6 == s1) && (j7 < 0 || j7 == r1 || j7 == s1)) { 
-        // i4 - i7 all from two sources
-        const int k4 =  i4 >= 0 ? i4 & 3 : i4;
-        const int k5 = (i5 >= 0 ? i5 & 3 : i5) | (j5 == s1 ? 4 : 0);
-        const int k6 = (i6 >= 0 ? i6 & 3 : i6) | (j6 == s1 ? 4 : 0);
-        const int k7 = (i7 >= 0 ? i7 & 3 : i7) | (j7 == s1 ? 4 : 0);
-        x1 = blend4q<k4,k5,k6,k7> (select4<r1>(a,b), select4<s1>(a,b));
-    }
-    else {
-        // i4 - i7 from three or four different sources
-        x1 = blend4q<0,1,6,7> (
-             blend4q<i4 & -13, (i5 & -13) | 4, -0x100, -0x100> (select4<j4>(a,b), select4<j5>(a,b)),
-             blend4q<-0x100, -0x100, i6 & -13, (i7 & -13) | 4> (select4<j6>(a,b), select4<j7>(a,b)));
-    }
-
-    return Vec8q(x0,x1);
+template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7>
+static inline Vec8q blend8(Vec8q const a, Vec8q const b) {
+    Vec4q x0 = blend_half<Vec8q, i0, i1, i2, i3>(a, b);
+    Vec4q x1 = blend_half<Vec8q, i4, i5, i6, i7>(a, b);
+    return Vec8q(x0, x1);
 }
 
-template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7> 
-static inline Vec8uq blend8uq(Vec8uq const & a, Vec8uq const & b) {
-    return Vec8uq( blend8q<i0,i1,i2,i3,i4,i5,i6,i7> (a,b));
+template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7>
+static inline Vec8uq blend8(Vec8uq const a, Vec8uq const b) {
+    return Vec8uq( blend8<i0,i1,i2,i3,i4,i5,i6,i7> (Vec8q(a),Vec8q(b)));
 }
 
-
-// helper function used below
-template <int n>
-static inline Vec8i select4(Vec16i const & a, Vec16i const & b) {
-    switch (n) {
-    case 0:
-        return a.get_low();
-    case 1:
-        return a.get_high();
-    case 2:
-        return b.get_low();
-    case 3:
-        return b.get_high();
-    }
-    return  Vec8i(0);
-}
-
-template <int i0,  int i1,  int i2,  int i3,  int i4,  int i5,  int i6,  int i7, 
-          int i8,  int i9,  int i10, int i11, int i12, int i13, int i14, int i15 > 
-static inline Vec16i blend16i(Vec16i const & a, Vec16i const & b) {
-
-    const int j0  = i0  >= 0 ? i0 /8 : i0;
-    const int j1  = i1  >= 0 ? i1 /8 : i1;
-    const int j2  = i2  >= 0 ? i2 /8 : i2;
-    const int j3  = i3  >= 0 ? i3 /8 : i3;
-    const int j4  = i4  >= 0 ? i4 /8 : i4;
-    const int j5  = i5  >= 0 ? i5 /8 : i5;
-    const int j6  = i6  >= 0 ? i6 /8 : i6;
-    const int j7  = i7  >= 0 ? i7 /8 : i7;
-    const int j8  = i8  >= 0 ? i8 /8 : i8;
-    const int j9  = i9  >= 0 ? i9 /8 : i9;
-    const int j10 = i10 >= 0 ? i10/8 : i10;
-    const int j11 = i11 >= 0 ? i11/8 : i11;
-    const int j12 = i12 >= 0 ? i12/8 : i12;
-    const int j13 = i13 >= 0 ? i13/8 : i13;
-    const int j14 = i14 >= 0 ? i14/8 : i14;
-    const int j15 = i15 >= 0 ? i15/8 : i15;
-
-    Vec8i x0, x1;
-
-    const int r0 = j0 >= 0 ? j0 : j1 >= 0 ? j1 : j2  >= 0 ? j2  : j3  >= 0 ? j3  : j4  >= 0 ? j4  : j5  >= 0 ? j5  : j6  >= 0 ? j6  : j7;
-    const int r1 = j8 >= 0 ? j8 : j9 >= 0 ? j9 : j10 >= 0 ? j10 : j11 >= 0 ? j11 : j12 >= 0 ? j12 : j13 >= 0 ? j13 : j14 >= 0 ? j14 : j15;
-    const int s0 = (j1 >= 0 && j1 != r0) ? j1 : (j2 >= 0 && j2 != r0) ? j2  : (j3 >= 0 && j3 != r0) ? j3 : (j4 >= 0 && j4 != r0) ? j4 : (j5 >= 0 && j5 != r0) ? j5 : (j6 >= 0 && j6 != r0) ? j6 : j7;
-    const int s1 = (j9 >= 0 && j9 != r1) ? j9 : (j10>= 0 && j10!= r1) ? j10 : (j11>= 0 && j11!= r1) ? j11: (j12>= 0 && j12!= r1) ? j12: (j13>= 0 && j13!= r1) ? j13: (j14>= 0 && j14!= r1) ? j14: j15;
-
-    if (r0 < 0) {
-        x0 = Vec8i(0);
-    }
-    else if (r0 == s0) {
-        // i0 - i7 all from same source
-        x0 = permute8i<i0&-25, i1&-25, i2&-25, i3&-25, i4&-25, i5&-25, i6&-25, i7&-25> (select4<r0> (a,b));
-    }
-    else if ((j2<0||j2==r0||j2==s0) && (j3<0||j3==r0||j3==s0) && (j4<0||j4==r0||j4==s0) && (j5<0||j5==r0||j5==s0) && (j6<0||j6==r0||j6==s0) && (j7<0||j7==r0||j7==s0)) {
-        // i0 - i7 all from two sources
-        const int k0 =  i0 >= 0 ? (i0 & 7) : i0;
-        const int k1 = (i1 >= 0 ? (i1 & 7) : i1) | (j1 == s0 ? 8 : 0);
-        const int k2 = (i2 >= 0 ? (i2 & 7) : i2) | (j2 == s0 ? 8 : 0);
-        const int k3 = (i3 >= 0 ? (i3 & 7) : i3) | (j3 == s0 ? 8 : 0);
-        const int k4 = (i4 >= 0 ? (i4 & 7) : i4) | (j4 == s0 ? 8 : 0);
-        const int k5 = (i5 >= 0 ? (i5 & 7) : i5) | (j5 == s0 ? 8 : 0);
-        const int k6 = (i6 >= 0 ? (i6 & 7) : i6) | (j6 == s0 ? 8 : 0);
-        const int k7 = (i7 >= 0 ? (i7 & 7) : i7) | (j7 == s0 ? 8 : 0);
-        x0 = blend8i<k0,k1,k2,k3,k4,k5,k6,k7> (select4<r0>(a,b), select4<s0>(a,b));
-    }
-    else {
-        // i0 - i7 from three or four different sources
-        const int n0 = j0 >= 0 ? j0 /2*8 + 0 : j0;
-        const int n1 = j1 >= 0 ? j1 /2*8 + 1 : j1;
-        const int n2 = j2 >= 0 ? j2 /2*8 + 2 : j2;
-        const int n3 = j3 >= 0 ? j3 /2*8 + 3 : j3;
-        const int n4 = j4 >= 0 ? j4 /2*8 + 4 : j4;
-        const int n5 = j5 >= 0 ? j5 /2*8 + 5 : j5;
-        const int n6 = j6 >= 0 ? j6 /2*8 + 6 : j6;
-        const int n7 = j7 >= 0 ? j7 /2*8 + 7 : j7;
-        x0 = blend8i<n0, n1, n2, n3, n4, n5, n6, n7> (
-             blend8i< j0   & 2 ? -256 : i0 &15,  j1   & 2 ? -256 : i1 &15,  j2   & 2 ? -256 : i2 &15,  j3   & 2 ? -256 : i3 &15,  j4   & 2 ? -256 : i4 &15,  j5   & 2 ? -256 : i5 &15,  j6   & 2 ? -256 : i6 &15,  j7   & 2 ? -256 : i7 &15> (a.get_low(),a.get_high()),
-             blend8i<(j0^2)& 6 ? -256 : i0 &15, (j1^2)& 6 ? -256 : i1 &15, (j2^2)& 6 ? -256 : i2 &15, (j3^2)& 6 ? -256 : i3 &15, (j4^2)& 6 ? -256 : i4 &15, (j5^2)& 6 ? -256 : i5 &15, (j6^2)& 6 ? -256 : i6 &15, (j7^2)& 6 ? -256 : i7 &15> (b.get_low(),b.get_high()));
-    }
-
-    if (r1 < 0) {
-        x1 = Vec8i(0);
-    }
-    else if (r1 == s1) {
-        // i8 - i15 all from same source
-        x1 = permute8i<i8&-25, i9&-25, i10&-25, i11&-25, i12&-25, i13&-25, i14&-25, i15&-25> (select4<r1> (a,b));
-    }
-    else if ((j10<0||j10==r1||j10==s1) && (j11<0||j11==r1||j11==s1) && (j12<0||j12==r1||j12==s1) && (j13<0||j13==r1||j13==s1) && (j14<0||j14==r1||j14==s1) && (j15<0||j15==r1||j15==s1)) {
-        // i8 - i15 all from two sources
-        const int k8 =  i8 >= 0 ? (i8 & 7) : i8;
-        const int k9 = (i9 >= 0 ? (i9 & 7) : i9 ) | (j9 == s1 ? 8 : 0);
-        const int k10= (i10>= 0 ? (i10& 7) : i10) | (j10== s1 ? 8 : 0);
-        const int k11= (i11>= 0 ? (i11& 7) : i11) | (j11== s1 ? 8 : 0);
-        const int k12= (i12>= 0 ? (i12& 7) : i12) | (j12== s1 ? 8 : 0);
-        const int k13= (i13>= 0 ? (i13& 7) : i13) | (j13== s1 ? 8 : 0);
-        const int k14= (i14>= 0 ? (i14& 7) : i14) | (j14== s1 ? 8 : 0);
-        const int k15= (i15>= 0 ? (i15& 7) : i15) | (j15== s1 ? 8 : 0);
-        x1 = blend8i<k8,k9,k10,k11,k12,k13,k14,k15> (select4<r1>(a,b), select4<s1>(a,b));
-    }
-    else {
-        // i8 - i15 from three or four different sources
-        const int n8 = j8 >= 0 ? j8 /2*8 + 0 : j8 ;
-        const int n9 = j9 >= 0 ? j9 /2*8 + 1 : j9 ;
-        const int n10= j10>= 0 ? j10/2*8 + 2 : j10;
-        const int n11= j11>= 0 ? j11/2*8 + 3 : j11;
-        const int n12= j12>= 0 ? j12/2*8 + 4 : j12;
-        const int n13= j13>= 0 ? j13/2*8 + 5 : j13;
-        const int n14= j14>= 0 ? j14/2*8 + 6 : j14;
-        const int n15= j15>= 0 ? j15/2*8 + 7 : j15;
-        x1 = blend8i<n8, n9, n10, n11, n12, n13, n14, n15> (
-             blend8i< j8   & 2 ? -256 : i8 &15,  j9   & 2 ? -256 : i9 &15,  j10   & 2 ? -256 : i10 &15,  j11   & 2 ? -256 : i11 &15,  j12   & 2 ? -256 : i12 &15,  j13   & 2 ? -256 : i13 &15,  j14   & 2 ? -256 : i14 &15,  j15   & 2 ? -256 : i15 &15> (a.get_low(),a.get_high()),
-             blend8i<(j8^2)& 6 ? -256 : i8 &15, (j9^2)& 6 ? -256 : i9 &15, (j10^2)& 6 ? -256 : i10 &15, (j11^2)& 6 ? -256 : i11 &15, (j12^2)& 6 ? -256 : i12 &15, (j13^2)& 6 ? -256 : i13 &15, (j14^2)& 6 ? -256 : i14 &15, (j15^2)& 6 ? -256 : i15 &15> (b.get_low(),b.get_high()));
-    }
-    return Vec16i(x0,x1);
+template <int i0,  int i1,  int i2,  int i3,  int i4,  int i5,  int i6,  int i7,
+          int i8,  int i9,  int i10, int i11, int i12, int i13, int i14, int i15 >
+static inline Vec16i blend16(Vec16i const a, Vec16i const b) {
+    Vec8i x0 = blend_half<Vec16i, i0, i1, i2, i3, i4, i5, i6, i7>(a, b);
+    Vec8i x1 = blend_half<Vec16i, i8, i9, i10, i11, i12, i13, i14, i15>(a, b);
+    return Vec16i(x0, x1);
 }
 
-template <int i0,  int i1,  int i2,  int i3,  int i4,  int i5,  int i6,  int i7, 
-          int i8,  int i9,  int i10, int i11, int i12, int i13, int i14, int i15 > 
-static inline Vec16ui blend16ui(Vec16ui const & a, Vec16ui const & b) {
-    return Vec16ui( blend16i<i0,i1,i2,i3,i4,i5,i6,i7,i8,i9,i10,i11,i12,i13,i14,i15> (Vec16i(a),Vec16i(b)));
+template <int i0,  int i1,  int i2,  int i3,  int i4,  int i5,  int i6,  int i7,
+          int i8,  int i9,  int i10, int i11, int i12, int i13, int i14, int i15 >
+static inline Vec16ui blend16(Vec16ui const a, Vec16ui const b) {
+    return Vec16ui( blend16<i0,i1,i2,i3,i4,i5,i6,i7,i8,i9,i10,i11,i12,i13,i14,i15> (Vec16i(a),Vec16i(b)));
 }
 
 
@@ -2045,58 +1826,61 @@ static inline Vec16ui blend16ui(Vec16ui const & a, Vec16ui const & b) {
 * These functions use vector elements as indexes into a table.
 * The table is given as one or more vectors or as an array.
 *
-* This can be used for several purposes:
-*  - table lookup
-*  - permute or blend with variable indexes
-*  - blend from more than two sources
-*  - gather non-contiguous data
-*
-* An index out of range may produce any value - the actual value produced is
-* implementation dependent and may be different for different instruction
-* sets. An index out of range does not produce an error message or exception.
-*
-* Example:
-* Vec8q a(2,0,0,6,4,3,5,0);                 // index a is (  2,   0,   0,   6,   4,   3,   5,   0)
-* Vec8q b(100,101,102,103,104,105,106,107); // table b is (100, 101, 102, 103, 104, 105, 106, 107)
-* Vec8q c;
-* c = lookup8 (a,b);                        // c is       (102, 100, 100, 106, 104, 103, 105, 100)
-*
 *****************************************************************************/
 
-static inline Vec16i lookup16(Vec16i const & index, Vec16i const & table) {
-    int32_t tab[16];
-    table.store(tab);
-    Vec8i t0 = lookup<16>(index.get_low(), tab);
-    Vec8i t1 = lookup<16>(index.get_high(), tab);
-    return Vec16i(t0, t1);
+static inline Vec16i lookup16(Vec16i const i1, Vec16i const table) {
+    int32_t t[16];
+    table.store(t);
+    return Vec16i(t[i1[0]], t[i1[1]], t[i1[2]], t[i1[3]], t[i1[4]], t[i1[5]], t[i1[6]], t[i1[7]],
+        t[i1[8]], t[i1[9]], t[i1[10]], t[i1[11]], t[i1[12]], t[i1[13]], t[i1[14]], t[i1[15]]);
 }
 
 template <int n>
-static inline Vec16i lookup(Vec16i const & index, void const * table) {
-    if (n <=  0) return 0;
-    if (n <=  8) {
-        Vec8i table1 = Vec8i().load(table);        
-        return Vec16i(       
-            lookup8 (index.get_low(),  table1),
-            lookup8 (index.get_high(), table1));
-    }
-    if (n <= 16) return lookup16(index, Vec16i().load(table));
+static inline Vec16i lookup(Vec16i const index, void const * table) {
+    if constexpr (n <= 0) return 0;
+    if constexpr (n <= 8) {
+        Vec8i table1 = Vec8i().load(table);
+        return Vec16i(
+            lookup8(index.get_low(), table1),
+            lookup8(index.get_high(), table1));
+    }
+    if constexpr (n <= 16) return lookup16(index, Vec16i().load(table));
     // n > 16. Limit index
     Vec16ui i1;
-    if ((n & (n-1)) == 0) {
+    if constexpr (n == INT_MAX) {
+        i1 = index;
+    }
+    else if constexpr ((n & (n - 1)) == 0) {
         // n is a power of 2, make index modulo n
-        i1 = Vec16ui(index) & (n-1);
+        i1 = Vec16ui(index) & (n - 1);
     }
     else {
         // n is not a power of 2, limit to n-1
-        i1 = min(Vec16ui(index), n-1);
+        i1 = min(Vec16ui(index), n - 1);
     }
     int32_t const * t = (int32_t const *)table;
-    return Vec16i(t[i1[0]],t[i1[1]],t[i1[2]],t[i1[3]],t[i1[4]],t[i1[5]],t[i1[6]],t[i1[7]],
-        t[i1[8]],t[i1[9]],t[i1[10]],t[i1[11]],t[i1[12]],t[i1[13]],t[i1[14]],t[i1[15]]);
+    return Vec16i(t[i1[0]], t[i1[1]], t[i1[2]], t[i1[3]], t[i1[4]], t[i1[5]], t[i1[6]], t[i1[7]],
+        t[i1[8]], t[i1[9]], t[i1[10]], t[i1[11]], t[i1[12]], t[i1[13]], t[i1[14]], t[i1[15]]);
+}
+
+static inline Vec16i lookup32(Vec16i const index, Vec16i const table1, Vec16i const table2) {
+    int32_t tab[32];
+    table1.store(tab);  table2.store(tab+16);
+    Vec8i t0 = lookup<32>(index.get_low(), tab);
+    Vec8i t1 = lookup<32>(index.get_high(), tab);
+    return Vec16i(t0, t1);
+}
+
+static inline Vec16i lookup64(Vec16i const index, Vec16i const table1, Vec16i const table2, Vec16i const table3, Vec16i const table4) {
+    int32_t tab[64];
+    table1.store(tab);  table2.store(tab + 16);  table3.store(tab + 32);  table4.store(tab + 48);
+    Vec8i t0 = lookup<64>(index.get_low(), tab);
+    Vec8i t1 = lookup<64>(index.get_high(), tab);
+    return Vec16i(t0, t1);
 }
 
-static inline Vec8q lookup8(Vec8q const & index, Vec8q const & table) {
+
+static inline Vec8q lookup8(Vec8q const index, Vec8q const table) {
     int64_t tab[8];
     table.store(tab);
     Vec4q t0 = lookup<8>(index.get_low(), tab);
@@ -2105,20 +1889,23 @@ static inline Vec8q lookup8(Vec8q const & index, Vec8q const & table) {
 }
 
 template <int n>
-static inline Vec8q lookup(Vec8q const & index, void const * table) {
-    if (n <= 0) return 0;
-    if (n <= 4) {
-        Vec4q table1 = Vec4q().load(table);        
-        return Vec8q(       
+static inline Vec8q lookup(Vec8q const index, void const * table) {
+    if constexpr (n <= 0) return 0;
+    if constexpr (n <= 4) {
+        Vec4q table1 = Vec4q().load(table);
+        return Vec8q(
             lookup4 (index.get_low(),  table1),
             lookup4 (index.get_high(), table1));
     }
-    if (n <= 8) {
+    if constexpr (n <= 8) {
         return lookup8(index, Vec8q().load(table));
     }
     // n > 8. Limit index
     Vec8uq i1;
-    if ((n & (n-1)) == 0) {
+    if constexpr (n == INT_MAX) {
+        i1 = index;
+    }
+    else if constexpr ((n & (n-1)) == 0) {
         // n is a power of 2, make index modulo n
         i1 = Vec8uq(index) & (n-1);
     }
@@ -2134,31 +1921,11 @@ static inline Vec8q lookup(Vec8q const & index, void const * table) {
 *
 *          Vector scatter functions
 *
-******************************************************************************
-*
-* These functions write the elements of a vector to arbitrary positions in an
-* array in memory. Each vector element is written to an array position
-* determined by an index. An element is not written if the corresponding
-* index is out of range.
-* The indexes can be specified as constant template parameters or as an
-* integer vector.
-*
-* The scatter functions are useful if the data are distributed in a sparce
-* manner into the array. If the array is dense then it is more efficient
-* to permute the data into the right positions and then write the whole
-* permuted vector into the array.
-*
-* Example:
-* Vec8q a(10,11,12,13,14,15,16,17);
-* int64_t b[16] = {0};
-* scatter<0,2,14,10,1,-1,5,9>(a,b);
-* // Now, b = {10,14,11,0,0,16,0,0,0,17,13,0,0,0,12,0}
-*
 *****************************************************************************/
 
 template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7,
     int i8, int i9, int i10, int i11, int i12, int i13, int i14, int i15>
-    static inline void scatter(Vec16i const & data, void * array) {
+    static inline void scatter(Vec16i const data, void * array) {
     int32_t* arr = (int32_t*)array;
     const int index[16] = {i0,i1,i2,i3,i4,i5,i6,i7,i8,i9,i10,i11,i12,i13,i14,i15};
     for (int i = 0; i < 16; i++) {
@@ -2167,7 +1934,7 @@ template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7,
 }
 
 template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7>
-static inline void scatter(Vec8q const & data, void * array) {
+static inline void scatter(Vec8q const data, void * array) {
     int64_t* arr = (int64_t*)array;
     const int index[8] = {i0,i1,i2,i3,i4,i5,i6,i7};
     for (int i = 0; i < 8; i++) {
@@ -2175,84 +1942,94 @@ static inline void scatter(Vec8q const & data, void * array) {
     }
 }
 
-static inline void scatter(Vec16i const & index, uint32_t limit, Vec16i const & data, void * array) {
+static inline void scatter(Vec16i const index, uint32_t limit, Vec16i const data, void * array) {
     int32_t* arr = (int32_t*)array;
     for (int i = 0; i < 16; i++) {
         if (uint32_t(index[i]) < limit) arr[index[i]] = data[i];
     }
 }
 
-static inline void scatter(Vec8q const & index, uint32_t limit, Vec8q const & data, void * array) {
+static inline void scatter(Vec8q const index, uint32_t limit, Vec8q const data, void * array) {
     int64_t* arr = (int64_t*)array;
     for (int i = 0; i < 8; i++) {
         if (uint64_t(index[i]) < uint64_t(limit)) arr[index[i]] = data[i];
     }
 }
 
-static inline void scatter(Vec8i const & index, uint32_t limit, Vec8q const & data, void * array) {
+static inline void scatter(Vec8i const index, uint32_t limit, Vec8q const data, void * array) {
     int64_t* arr = (int64_t*)array;
     for (int i = 0; i < 8; i++) {
         if (uint32_t(index[i]) < limit) arr[index[i]] = data[i];
     }
 }
 
+// Scatter functions with variable indexes:
+
+static inline void scatter16i(Vec16i index, uint32_t limit, Vec16i data, void * destination) {
+    uint32_t ix[16];  index.store(ix);
+    for (int i = 0; i < 16; i++) {
+        if (ix[i] < limit) ((int*)destination)[ix[i]] = data[i];
+    }
+}
+
+static inline void scatter8q(Vec8q index, uint32_t limit, Vec8q data, void * destination) {
+    uint64_t ix[8];  index.store(ix);
+    for (int i = 0; i < 8; i++) {
+        if (ix[i] < limit) ((int64_t*)destination)[ix[i]] = data[i];
+    }
+}
+
+static inline void scatter8i(Vec8i index, uint32_t limit, Vec8i data, void * destination) {
+    uint32_t ix[8];  index.store(ix);
+    for (int i = 0; i < 8; i++) {
+        if (ix[i] < limit) ((int*)destination)[ix[i]] = data[i];
+    }
+}
+
+static inline void scatter4q(Vec4q index, uint32_t limit, Vec4q data, void * destination) {
+    uint64_t ix[4];  index.store(ix);
+    for (int i = 0; i < 4; i++) {
+        if (ix[i] < limit) ((int64_t*)destination)[ix[i]] = data[i];
+    }
+}
+
+static inline void scatter4i(Vec4i index, uint32_t limit, Vec4i data, void * destination) {
+    uint32_t ix[4];  index.store(ix);
+    for (int i = 0; i < 4; i++) {
+        if (ix[i] < limit) ((int*)destination)[ix[i]] = data[i];
+    }
+}
+
 /*****************************************************************************
 *
 *          Gather functions with fixed indexes
 *
 *****************************************************************************/
-// Load elements from array a with indices i0,i1,i2,i3,i4,i5,i6,i7,i8,i9,i10,i11,i12,i13,i14,i15
-template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7, 
+
+template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7,
 int i8, int i9, int i10, int i11, int i12, int i13, int i14, int i15>
 static inline Vec16i gather16i(void const * a) {
-    Static_error_check<(i0|i1|i2|i3|i4|i5|i6|i7|i8|i9|i10|i11|i12|i13|i14|i15)>=0> Negative_array_index;  // Error message if index is negative
-    // find smallest and biggest index, using only compile-time constant expressions
-    const int i01min   = i0  < i1  ? i0  : i1;
-    const int i23min   = i2  < i3  ? i2  : i3;
-    const int i45min   = i4  < i5  ? i4  : i5;
-    const int i67min   = i6  < i7  ? i6  : i7;
-    const int i89min   = i8  < i9  ? i8  : i9;
-    const int i1011min = i10 < i11 ? i10 : i11;
-    const int i1213min = i12 < i13 ? i12 : i13;
-    const int i1415min = i14 < i15 ? i14 : i15;
-    const int i0_3min   = i01min   < i23min    ? i01min   : i23min;
-    const int i4_7min   = i45min   < i67min    ? i45min   : i67min;
-    const int i8_11min  = i89min   < i1011min  ? i89min   : i1011min;
-    const int i12_15min = i1213min < i1415min  ? i1213min : i1415min;
-    const int i0_7min   = i0_3min  < i4_7min   ? i0_3min  : i4_7min;
-    const int i8_15min  = i8_11min < i12_15min ? i8_11min : i12_15min;
-    const int imin      = i0_7min  < i8_15min  ? i0_7min  : i8_15min;
-    const int i01max   = i0  > i1  ? i0  : i1;
-    const int i23max   = i2  > i3  ? i2  : i3;
-    const int i45max   = i4  > i5  ? i4  : i5;
-    const int i67max   = i6  > i7  ? i6  : i7;
-    const int i89max   = i8  > i9  ? i8  : i9;
-    const int i1011max = i10 > i11 ? i10 : i11;
-    const int i1213max = i12 > i13 ? i12 : i13;
-    const int i1415max = i14 > i15 ? i14 : i15;
-    const int i0_3max   = i01max   > i23max    ? i01max   : i23max;
-    const int i4_7max   = i45max   > i67max    ? i45max   : i67max;
-    const int i8_11max  = i89max   > i1011max  ? i89max   : i1011max;
-    const int i12_15max = i1213max > i1415max  ? i1213max : i1415max;
-    const int i0_7max   = i0_3max  > i4_7max   ? i0_3max  : i4_7max;
-    const int i8_15max  = i8_11max > i12_15max ? i8_11max : i12_15max;
-    const int imax      = i0_7max  > i8_15max  ? i0_7max  : i8_15max;
-    if (imax - imin <= 15) {
+    int constexpr indexs[16] = { i0, i1, i2, i3, i4, i5, i6, i7, i8, i9, i10, i11, i12, i13, i14, i15 };
+    constexpr int imin = min_index(indexs);
+    constexpr int imax = max_index(indexs);
+    static_assert(imin >= 0, "Negative index in gather function");
+
+    if constexpr (imax - imin <= 15) {
         // load one contiguous block and permute
-        if (imax > 15) {
+        if constexpr (imax > 15) {
             // make sure we don't read past the end of the array
             Vec16i b = Vec16i().load((int32_t const *)a + imax-15);
-            return permute16i<i0-imax+15, i1-imax+15, i2-imax+15, i3-imax+15, i4-imax+15, i5-imax+15, i6-imax+15, i7-imax+15,
+            return permute16<i0-imax+15, i1-imax+15, i2-imax+15, i3-imax+15, i4-imax+15, i5-imax+15, i6-imax+15, i7-imax+15,
                 i8-imax+15, i9-imax+15, i10-imax+15, i11-imax+15, i12-imax+15, i13-imax+15, i14-imax+15, i15-imax+15> (b);
         }
         else {
             Vec16i b = Vec16i().load((int32_t const *)a + imin);
-            return permute16i<i0-imin, i1-imin, i2-imin, i3-imin, i4-imin, i5-imin, i6-imin, i7-imin,
+            return permute16<i0-imin, i1-imin, i2-imin, i3-imin, i4-imin, i5-imin, i6-imin, i7-imin,
                 i8-imin, i9-imin, i10-imin, i11-imin, i12-imin, i13-imin, i14-imin, i15-imin> (b);
         }
     }
-    if ((i0<imin+16  || i0>imax-16)  && (i1<imin+16  || i1>imax-16)  && (i2<imin+16  || i2>imax-16)  && (i3<imin+16  || i3>imax-16)
-    &&  (i4<imin+16  || i4>imax-16)  && (i5<imin+16  || i5>imax-16)  && (i6<imin+16  || i6>imax-16)  && (i7<imin+16  || i7>imax-16)    
+    if constexpr ((i0<imin+16  || i0>imax-16)  && (i1<imin+16  || i1>imax-16)  && (i2<imin+16  || i2>imax-16)  && (i3<imin+16  || i3>imax-16)
+    &&  (i4<imin+16  || i4>imax-16)  && (i5<imin+16  || i5>imax-16)  && (i6<imin+16  || i6>imax-16)  && (i7<imin+16  || i7>imax-16)
     &&  (i8<imin+16  || i8>imax-16)  && (i9<imin+16  || i9>imax-16)  && (i10<imin+16 || i10>imax-16) && (i11<imin+16 || i11>imax-16)
     &&  (i12<imin+16 || i12>imax-16) && (i13<imin+16 || i13>imax-16) && (i14<imin+16 || i14>imax-16) && (i15<imin+16 || i15>imax-16) ) {
         // load two contiguous blocks and blend
@@ -2274,7 +2051,7 @@ static inline Vec16i gather16i(void const * a) {
         const int j13 = i13<imin+16 ? i13-imin : 31-imax+i13;
         const int j14 = i14<imin+16 ? i14-imin : 31-imax+i14;
         const int j15 = i15<imin+16 ? i15-imin : 31-imax+i15;
-        return blend16i<j0,j1,j2,j3,j4,j5,j6,j7,j8,j9,j10,j11,j12,j13,j14,j15>(b, c);
+        return blend16<j0,j1,j2,j3,j4,j5,j6,j7,j8,j9,j10,j11,j12,j13,j14,j15>(b, c);
     }
     // use lookup function
     return lookup<imax+1>(Vec16i(i0,i1,i2,i3,i4,i5,i6,i7,i8,i9,i10,i11,i12,i13,i14,i15), a);
@@ -2283,35 +2060,24 @@ static inline Vec16i gather16i(void const * a) {
 
 template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7>
 static inline Vec8q gather8q(void const * a) {
-    Static_error_check<(i0|i1|i2|i3|i4|i5|i6|i7)>=0> Negative_array_index;  // Error message if index is negative
-
-    const int i01min = i0 < i1 ? i0 : i1;
-    const int i23min = i2 < i3 ? i2 : i3;
-    const int i45min = i4 < i5 ? i4 : i5;
-    const int i67min = i6 < i7 ? i6 : i7;
-    const int i0123min = i01min < i23min ? i01min : i23min;
-    const int i4567min = i45min < i67min ? i45min : i67min;
-    const int imin = i0123min < i4567min ? i0123min : i4567min;
-    const int i01max = i0 > i1 ? i0 : i1;
-    const int i23max = i2 > i3 ? i2 : i3;
-    const int i45max = i4 > i5 ? i4 : i5;
-    const int i67max = i6 > i7 ? i6 : i7;
-    const int i0123max = i01max > i23max ? i01max : i23max;
-    const int i4567max = i45max > i67max ? i45max : i67max;
-    const int imax = i0123max > i4567max ? i0123max : i4567max;
-    if (imax - imin <= 7) {
+    int constexpr indexs[8] = { i0, i1, i2, i3, i4, i5, i6, i7 }; // indexes as array
+    constexpr int imin = min_index(indexs);
+    constexpr int imax = max_index(indexs);
+    static_assert(imin >= 0, "Negative index in gather function");
+
+    if constexpr (imax - imin <= 7) {
         // load one contiguous block and permute
-        if (imax > 7) {
+        if constexpr (imax > 7) {
             // make sure we don't read past the end of the array
             Vec8q b = Vec8q().load((int64_t const *)a + imax-7);
-            return permute8q<i0-imax+7, i1-imax+7, i2-imax+7, i3-imax+7, i4-imax+7, i5-imax+7, i6-imax+7, i7-imax+7> (b);
+            return permute8<i0-imax+7, i1-imax+7, i2-imax+7, i3-imax+7, i4-imax+7, i5-imax+7, i6-imax+7, i7-imax+7> (b);
         }
         else {
             Vec8q b = Vec8q().load((int64_t const *)a + imin);
-            return permute8q<i0-imin, i1-imin, i2-imin, i3-imin, i4-imin, i5-imin, i6-imin, i7-imin> (b);
+            return permute8<i0-imin, i1-imin, i2-imin, i3-imin, i4-imin, i5-imin, i6-imin, i7-imin> (b);
         }
     }
-    if ((i0<imin+8 || i0>imax-8) && (i1<imin+8 || i1>imax-8) && (i2<imin+8 || i2>imax-8) && (i3<imin+8 || i3>imax-8)
+    if constexpr ((i0<imin+8 || i0>imax-8) && (i1<imin+8 || i1>imax-8) && (i2<imin+8 || i2>imax-8) && (i3<imin+8 || i3>imax-8)
     &&  (i4<imin+8 || i4>imax-8) && (i5<imin+8 || i5>imax-8) && (i6<imin+8 || i6>imax-8) && (i7<imin+8 || i7>imax-8)) {
         // load two contiguous blocks and blend
         Vec8q b = Vec8q().load((int64_t const *)a + imin);
@@ -2324,7 +2090,7 @@ static inline Vec8q gather8q(void const * a) {
         const int j5 = i5<imin+8 ? i5-imin : 15-imax+i5;
         const int j6 = i6<imin+8 ? i6-imin : 15-imax+i6;
         const int j7 = i7<imin+8 ? i7-imin : 15-imax+i7;
-        return blend8q<j0, j1, j2, j3, j4, j5, j6, j7>(b, c);
+        return blend8<j0, j1, j2, j3, j4, j5, j6, j7>(b, c);
     }
     // use lookup function
     return lookup<imax+1>(Vec8q(i0,i1,i2,i3,i4,i5,i6,i7), a);
@@ -2333,117 +2099,138 @@ static inline Vec8q gather8q(void const * a) {
 
 /*****************************************************************************
 *
-*          Functions for conversion between integer sizes
+*          Functions for conversion between integer sizes and vector types
 *
 *****************************************************************************/
 
 // Extend 16-bit integers to 32-bit integers, signed and unsigned
-
+/*
 // Function extend_to_int : extends Vec16s to Vec16i with sign extension
-static inline Vec16i extend_to_int (Vec16s const & a) {
+static inline Vec16i extend_to_int (Vec16s const a) {
     return Vec16i(extend_low(a), extend_high(a));
 }
 
 // Function extend_to_int : extends Vec16us to Vec16ui with zero extension
-static inline Vec16ui extend_to_int (Vec16us const & a) {
+static inline Vec16ui extend_to_int (Vec16us const a) {
     return Vec16i(extend_low(a), extend_high(a));
 }
 
 // Function extend_to_int : extends Vec16c to Vec16i with sign extension
-static inline Vec16i extend_to_int (Vec16c const & a) {
+static inline Vec16i extend_to_int (Vec16c const a) {
     return extend_to_int(Vec16s(extend_low(a), extend_high(a)));
 }
 
 // Function extend_to_int : extends Vec16uc to Vec16ui with zero extension
-static inline Vec16ui extend_to_int (Vec16uc const & a) {
+static inline Vec16ui extend_to_int (Vec16uc const a) {
     return extend_to_int(Vec16s(extend_low(a), extend_high(a)));
-}
+}*/
 
 
 // Extend 32-bit integers to 64-bit integers, signed and unsigned
 
 // Function extend_low : extends the low 8 elements to 64 bits with sign extension
-static inline Vec8q extend_low (Vec16i const & a) {
+static inline Vec8q extend_low (Vec16i const a) {
     return Vec8q(extend_low(a.get_low()), extend_high(a.get_low()));
 }
 
 // Function extend_high : extends the high 8 elements to 64 bits with sign extension
-static inline Vec8q extend_high (Vec16i const & a) {
+static inline Vec8q extend_high (Vec16i const a) {
     return Vec8q(extend_low(a.get_high()), extend_high(a.get_high()));
 }
 
 // Function extend_low : extends the low 8 elements to 64 bits with zero extension
-static inline Vec8uq extend_low (Vec16ui const & a) {
+static inline Vec8uq extend_low (Vec16ui const a) {
     return Vec8q(extend_low(a.get_low()), extend_high(a.get_low()));
 }
 
 // Function extend_high : extends the high 8 elements to 64 bits with zero extension
-static inline Vec8uq extend_high (Vec16ui const & a) {
+static inline Vec8uq extend_high (Vec16ui const a) {
     return Vec8q(extend_low(a.get_high()), extend_high(a.get_high()));
 }
 
-
 // Compress 32-bit integers to 8-bit integers, signed and unsigned, with and without saturation
-
+/*
 // Function compress : packs two vectors of 16-bit integers into one vector of 8-bit integers
 // Overflow wraps around
-static inline Vec16c compress_to_int8 (Vec16i const & a) {
+static inline Vec16c compress_to_int8 (Vec16i const a) {
     Vec16s b = compress(a.get_low(), a.get_high());
     Vec16c c = compress(b.get_low(), b.get_high());
     return c;
 }
 
-static inline Vec16s compress_to_int16 (Vec16i const & a) {
+static inline Vec16s compress_to_int16 (Vec16i const a) {
     return compress(a.get_low(), a.get_high());
 }
 
 // with signed saturation
-static inline Vec16c compress_to_int8_saturated (Vec16i const & a) {
+static inline Vec16c compress_to_int8_saturated (Vec16i const a) {
     Vec16s b = compress_saturated(a.get_low(), a.get_high());
     Vec16c c = compress_saturated(b.get_low(), b.get_high());
     return c;
 }
 
-static inline Vec16s compress_to_int16_saturated (Vec16i const & a) {
+static inline Vec16s compress_to_int16_saturated (Vec16i const a) {
     return compress_saturated(a.get_low(), a.get_high());
 }
 
 // with unsigned saturation
-static inline Vec16uc compress_to_int8_saturated (Vec16ui const & a) {
+static inline Vec16uc compress_to_int8_saturated (Vec16ui const a) {
     Vec16us b = compress_saturated(a.get_low(), a.get_high());
     Vec16uc c = compress_saturated(b.get_low(), b.get_high());
     return c;
 }
 
-static inline Vec16us compress_to_int16_saturated (Vec16ui const & a) {
+static inline Vec16us compress_to_int16_saturated (Vec16ui const a) {
     return compress_saturated(a.get_low(), a.get_high());
-}
+}*/
 
 // Compress 64-bit integers to 32-bit integers, signed and unsigned, with and without saturation
 
 // Function compress : packs two vectors of 64-bit integers into one vector of 32-bit integers
 // Overflow wraps around
-static inline Vec16i compress (Vec8q const & low, Vec8q const & high) {
+static inline Vec16i compress (Vec8q const low, Vec8q const high) {
     return Vec16i(compress(low.get_low(),low.get_high()), compress(high.get_low(),high.get_high()));
 }
 
 // Function compress_saturated : packs two vectors of 64-bit integers into one vector of 32-bit integers
 // Signed, with saturation
-static inline Vec16i compress_saturated (Vec8q const & low, Vec8q const & high) {
+static inline Vec16i compress_saturated (Vec8q const low, Vec8q const high) {
     return Vec16i(compress_saturated(low.get_low(),low.get_high()), compress_saturated(high.get_low(),high.get_high()));
 }
 
 // Function compress_saturated : packs two vectors of 64-bit integers into one vector of 32-bit integers
 // Unsigned, with saturation
-static inline Vec16ui compress_saturated (Vec8uq const & low, Vec8uq const & high) {
+static inline Vec16ui compress_saturated (Vec8uq const low, Vec8uq const high) {
     return Vec16ui(compress_saturated(low.get_low(),low.get_high()), compress_saturated(high.get_low(),high.get_high()));
 }
 
+// extend vectors to double size by adding zeroes
+static inline Vec16i extend_z(Vec8i a) {
+    return Vec16i(a, Vec8i(0));
+}
+static inline Vec16ui extend_z(Vec8ui a) {
+    return Vec16ui(a, Vec8ui(0));
+}
+static inline Vec8q extend_z(Vec4q a) {
+    return Vec8q(a, Vec4q(0));
+}
+static inline Vec8uq extend_z(Vec4uq a) {
+    return Vec8uq(a, Vec4uq(0));
+}
+
+// broad boolean vectors
+
+static inline Vec16ib extend_z(Vec8ib a) {
+    return Vec16ib(a, Vec8ib(false));
+}
+static inline Vec8qb extend_z(Vec4qb a) {
+    return Vec8qb(a, Vec4qb(false));
+} 
+
 
 /*****************************************************************************
 *
 *          Integer division operators
-*
 *          Please see the file vectori128.h for explanation.
 *
 *****************************************************************************/
@@ -2451,23 +2238,23 @@ static inline Vec16ui compress_saturated (Vec8uq const & low, Vec8uq const & hig
 // vector operator / : divide each element by divisor
 
 // vector operator / : divide all elements by same integer
-static inline Vec16i operator / (Vec16i const & a, Divisor_i const & d) {
+static inline Vec16i operator / (Vec16i const a, Divisor_i const d) {
     return Vec16i(a.get_low() / d, a.get_high() / d);
 }
 
 // vector operator /= : divide
-static inline Vec16i & operator /= (Vec16i & a, Divisor_i const & d) {
+static inline Vec16i & operator /= (Vec16i & a, Divisor_i const d) {
     a = a / d;
     return a;
 }
 
 // vector operator / : divide all elements by same integer
-static inline Vec16ui operator / (Vec16ui const & a, Divisor_ui const & d) {
+static inline Vec16ui operator / (Vec16ui const a, Divisor_ui const d) {
     return Vec16ui(a.get_low() / d, a.get_high() / d);
 }
 
 // vector operator /= : divide
-static inline Vec16ui & operator /= (Vec16ui & a, Divisor_ui const & d) {
+static inline Vec16ui & operator /= (Vec16ui & a, Divisor_ui const d) {
     a = a / d;
     return a;
 }
@@ -2481,20 +2268,20 @@ static inline Vec16ui & operator /= (Vec16ui & a, Divisor_ui const & d) {
 
 // Divide Vec16i by compile-time constant
 template <int32_t d>
-static inline Vec16i divide_by_i(Vec16i const & a) {
+static inline Vec16i divide_by_i(Vec16i const a) {
     return Vec16i(divide_by_i<d>(a.get_low()), divide_by_i<d>(a.get_high()));
 }
 
 // define Vec16i a / const_int(d)
 template <int32_t d>
-static inline Vec16i operator / (Vec16i const & a, Const_int_t<d>) {
+static inline Vec16i operator / (Vec16i const a, Const_int_t<d>) {
     return divide_by_i<d>(a);
 }
 
 // define Vec16i a / const_uint(d)
 template <uint32_t d>
-static inline Vec16i operator / (Vec16i const & a, Const_uint_t<d>) {
-    Static_error_check< (d<0x80000000u) > Error_overflow_dividing_signed_by_unsigned; // Error: dividing signed by overflowing unsigned
+static inline Vec16i operator / (Vec16i const a, Const_uint_t<d>) {
+    static_assert(d < 0x80000000u, "Dividing signed integer by overflowing unsigned");
     return divide_by_i<int32_t(d)>(a);                               // signed divide
 }
 
@@ -2514,20 +2301,20 @@ static inline Vec16i & operator /= (Vec16i & a, Const_uint_t<d> b) {
 
 // Divide Vec16ui by compile-time constant
 template <uint32_t d>
-static inline Vec16ui divide_by_ui(Vec16ui const & a) {
+static inline Vec16ui divide_by_ui(Vec16ui const a) {
     return Vec16ui( divide_by_ui<d>(a.get_low()), divide_by_ui<d>(a.get_high()));
 }
 
 // define Vec16ui a / const_uint(d)
 template <uint32_t d>
-static inline Vec16ui operator / (Vec16ui const & a, Const_uint_t<d>) {
+static inline Vec16ui operator / (Vec16ui const a, Const_uint_t<d>) {
     return divide_by_ui<d>(a);
 }
 
 // define Vec16ui a / const_int(d)
 template <int32_t d>
-static inline Vec16ui operator / (Vec16ui const & a, Const_int_t<d>) {
-    Static_error_check< (d>=0) > Error_dividing_unsigned_by_negative;// Error: dividing unsigned by negative is ambiguous
+static inline Vec16ui operator / (Vec16ui const a, Const_int_t<d>) {
+    static_assert(d >= 0, "Dividing unsigned integer by negative is ambiguous");
     return divide_by_ui<d>(a);                                       // unsigned divide
 }
 
@@ -2546,39 +2333,6 @@ static inline Vec16ui & operator /= (Vec16ui & a, Const_int_t<d> b) {
 }
 
 
-/*****************************************************************************
-*
-*          Horizontal scan functions
-*
-*****************************************************************************/
-
-// Get index to the first element that is true. Return -1 if all are false
-static inline int horizontal_find_first(Vec16ib const & x) {
-    int a1 = horizontal_find_first(x.get_low());
-    if (a1 >= 0) return a1;
-    int a2 = horizontal_find_first(x.get_high());
-    if (a2 < 0) return a2;
-    return a2 + 8;
-}
-
-static inline int horizontal_find_first(Vec8qb const & x) {
-    int a1 = horizontal_find_first(x.get_low());
-    if (a1 >= 0) return a1;
-    int a2 = horizontal_find_first(x.get_high());
-    if (a2 < 0) return a2;
-    return a2 + 4;
-}
-
-// count the number of true elements
-static inline uint32_t horizontal_count(Vec16ib const & x) {
-    return horizontal_count(x.get_low()) + horizontal_count(x.get_high());
-}
-
-static inline uint32_t horizontal_count(Vec8qb const & x) {
-    return horizontal_count(x.get_low()) + horizontal_count(x.get_high());
-}
-
-
 /*****************************************************************************
 *
 *          Boolean <-> bitfield conversion functions
@@ -2586,32 +2340,23 @@ static inline uint32_t horizontal_count(Vec8qb const & x) {
 *****************************************************************************/
 
 // to_bits: convert to integer bitfield
-static inline uint16_t to_bits(Vec16b const & a) {
-    return to_bits(a.get_low()) | ((uint16_t)to_bits(a.get_high()) << 8);
+static inline uint16_t to_bits(Vec16b const a) {
+    return uint16_t(to_bits(a.get_low()) | ((uint16_t)to_bits(a.get_high()) << 8));
 }
 
 // to_bits: convert to integer bitfield
-static inline uint16_t to_bits(Vec16ib const & a) {
-    return to_bits(a.get_low()) | ((uint16_t)to_bits(a.get_high()) << 8);
-}
-
-// to_Vec16ib: convert integer bitfield to boolean vector
-static inline Vec16ib to_Vec16ib(uint16_t const & x) {
-    return Vec16i(to_Vec8ib(uint8_t(x)), to_Vec8ib(uint8_t(x>>8)));
+static inline uint16_t to_bits(Vec16ib const a) {
+    return uint16_t(to_bits(a.get_low()) | ((uint16_t)to_bits(a.get_high()) << 8));
 }
 
 // to_bits: convert to integer bitfield
-static inline uint8_t to_bits(Vec8b const & a) {
-    return to_bits(a.get_low()) | (to_bits(a.get_high()) << 4);
+static inline uint8_t to_bits(Vec8b const a) {
+    return uint8_t(to_bits(a.get_low()) | (to_bits(a.get_high()) << 4));
 }
 
-// to_Vec8qb: convert integer bitfield to boolean vector
-static inline Vec8qb to_Vec8qb(uint8_t x) {
-    return Vec8q(to_Vec4qb(x), to_Vec4qb(x>>4));
-}
 
 #ifdef VCL_NAMESPACE
 }
 #endif
 
-#endif // VECTORI512_H
+#endif // VECTORI512E_H
diff --git a/EEDI3/vectorclass/vectori512s.h b/EEDI3/vectorclass/vectori512s.h
new file mode 100644
index 0000000..6d19ef4
--- /dev/null
+++ b/EEDI3/vectorclass/vectori512s.h
@@ -0,0 +1,2332 @@
+/****************************  vectori512s.h   ********************************
+* Author:        Agner Fog
+* Date created:  2019-04-20
+* Last modified: 2023-07-04
+* Version:       2.02.02
+* Project:       vector classes
+* Description:
+* Header file defining 512-bit integer vector classes for 8 and 16 bit integers.
+* For x86 microprocessors with AVX512BW and later instruction sets.
+*
+* Instructions: see vcl_manual.pdf
+*
+* The following vector classes are defined here:
+* Vec64c    Vector of  64  8-bit  signed   integers
+* Vec64uc   Vector of  64  8-bit  unsigned integers
+* Vec64cb   Vector of  64  booleans for use with Vec64c and Vec64uc
+* Vec32s    Vector of  32  16-bit signed   integers
+* Vec32us   Vector of  32  16-bit unsigned integers
+* Vec32sb   Vector of  32  booleans for use with Vec32s and Vec32us
+* Other 512-bit integer vectors are defined in Vectori512.h
+*
+* Each vector object is represented internally in the CPU as a 512-bit register.
+* This header file defines operators and functions for these vectors.
+*
+* (c) Copyright 2012-2023 Agner Fog.
+* Apache License version 2.0 or later.
+******************************************************************************/
+
+#ifndef VECTORI512S_H
+#define VECTORI512S_H
+
+#ifndef VECTORCLASS_H
+#include "vectorclass.h"
+#endif
+
+#if VECTORCLASS_H < 20200
+#error Incompatible versions of vector class library mixed
+#endif
+
+// check combination of header files
+#ifdef VECTORI512SE_H
+#error Two different versions of vectorf256.h included
+#endif
+
+
+#ifdef VCL_NAMESPACE
+namespace VCL_NAMESPACE {
+#endif
+
+
+/*****************************************************************************
+*
+*          Vector of 64 8-bit signed integers
+*
+*****************************************************************************/
+
+class Vec64c: public Vec512b {
+public:
+    // Default constructor:
+    Vec64c() = default;
+    // Constructor to broadcast the same value into all elements:
+    Vec64c(int8_t i) {
+        zmm = _mm512_set1_epi8(i);
+    }
+    // Constructor to build from all elements:
+    Vec64c(int8_t i0, int8_t i1, int8_t i2, int8_t i3, int8_t i4, int8_t i5, int8_t i6, int8_t i7,
+        int8_t i8,  int8_t i9,  int8_t i10, int8_t i11, int8_t i12, int8_t i13, int8_t i14, int8_t i15,
+        int8_t i16, int8_t i17, int8_t i18, int8_t i19, int8_t i20, int8_t i21, int8_t i22, int8_t i23,
+        int8_t i24, int8_t i25, int8_t i26, int8_t i27, int8_t i28, int8_t i29, int8_t i30, int8_t i31,
+        int8_t i32, int8_t i33, int8_t i34, int8_t i35, int8_t i36, int8_t i37, int8_t i38, int8_t i39,
+        int8_t i40, int8_t i41, int8_t i42, int8_t i43, int8_t i44, int8_t i45, int8_t i46, int8_t i47,
+        int8_t i48, int8_t i49, int8_t i50, int8_t i51, int8_t i52, int8_t i53, int8_t i54, int8_t i55,
+        int8_t i56, int8_t i57, int8_t i58, int8_t i59, int8_t i60, int8_t i61, int8_t i62, int8_t i63) {
+        // _mm512_set_epi8 and _mm512_set_epi16 missing in GCC 7.4.0
+        int8_t aa[64] = {
+            i0, i1, i2, i3, i4, i5, i6, i7,i8, i9, i10, i11, i12, i13, i14, i15,
+            i16, i17, i18, i19, i20, i21, i22, i23, i24, i25, i26, i27, i28, i29, i30, i31,
+            i32, i33, i34, i35, i36, i37, i38, i39, i40, i41, i42, i43, i44, i45, i46, i47,
+            i48, i49, i50, i51, i52, i53, i54, i55, i56, i57, i58, i59, i60, i61, i62, i63 };
+        load(aa);
+    }
+    // Constructor to build from two Vec32c:
+    Vec64c(Vec32c const a0, Vec32c const a1) {
+        zmm = _mm512_inserti64x4(_mm512_castsi256_si512(a0), a1, 1);
+    }
+    // Constructor to convert from type __m512i used in intrinsics:
+    Vec64c(__m512i const x) {
+        zmm = x;
+    }
+    // Assignment operator to convert from type __m512i used in intrinsics:
+    Vec64c & operator = (__m512i const x) {
+        zmm = x;
+        return *this;
+    }
+    // Type cast operator to convert to __m512i used in intrinsics
+    operator __m512i() const {
+        return zmm;
+    }
+    // Member function to load from array (unaligned)
+    Vec64c & load(void const * p) {
+        zmm = _mm512_loadu_si512(p);
+        return *this;
+    }
+    // Member function to load from array, aligned by 64
+    Vec64c & load_a(void const * p) {
+        zmm = _mm512_load_si512(p);
+        return *this;
+    }
+    // Partial load. Load n elements and set the rest to 0
+    Vec64c & load_partial(int n, void const * p) {
+        if (n >= 64) {
+            zmm = _mm512_loadu_si512(p);
+        }
+        else {
+            zmm = _mm512_maskz_loadu_epi8(__mmask64(((uint64_t)1 << n) - 1), p);
+        }
+        return *this;
+    }
+    // Partial store. Store n elements
+    void store_partial(int n, void * p) const {
+        if (n >= 64) {
+            // _mm512_storeu_epi8(p, zmm);
+            _mm512_storeu_si512(p, zmm);
+        }
+        else {
+            _mm512_mask_storeu_epi8(p, __mmask64(((uint64_t)1 << n) - 1), zmm);
+        }
+    }
+    // cut off vector to n elements. The last 64-n elements are set to zero
+    Vec64c & cutoff(int n) {
+        if (n < 64) {
+            zmm = _mm512_maskz_mov_epi8(__mmask64(((uint64_t)1 << n) - 1), zmm);
+        }
+        return *this;
+    }
+    // Member function to change a single element in vector
+    Vec64c const insert(int index, int8_t value) {
+        zmm = _mm512_mask_set1_epi8(zmm, __mmask64((uint64_t)1 << index), value);
+        return *this;
+    }
+    // Member function extract a single element from vector
+    int8_t extract(int index) const {
+#if INSTRSET >= 10 && defined (__AVX512VBMI2__)
+        __m512i x = _mm512_maskz_compress_epi8(__mmask64((uint64_t)1 << index), zmm);
+        return (int8_t)_mm_cvtsi128_si32(_mm512_castsi512_si128(x));
+#else
+        int8_t a[64];
+        store(a);
+        return a[index & 63];
+#endif
+    }
+    // Extract a single element. Use store function if extracting more than one element.
+    // Operator [] can only read an element, not write.
+    int8_t operator [] (int index) const {
+        return extract(index);
+    }
+    // Member functions to split into two Vec32c:
+    Vec32c get_low() const {
+        return _mm512_castsi512_si256(zmm);
+    }
+    Vec32c get_high() const {
+        return _mm512_extracti64x4_epi64(zmm,1);
+    }
+    static constexpr int size() {
+        return 64;
+    }
+    static constexpr int elementtype() {
+        return 4;
+    }
+};
+
+
+/*****************************************************************************
+*
+*          Vec64b: Vector of 64 Booleans for use with Vec64c and Vec64uc
+*
+*****************************************************************************/
+
+class Vec64b {
+protected:
+    __mmask64  mm; // Boolean vector
+public:
+    // Default constructor:
+    Vec64b () = default;
+    // Constructor to build from all elements:
+    /*
+    Vec64b(bool b0, bool b1, bool b2, bool b3, bool b4, bool b5, bool b6, bool b7,
+        bool b8,  bool b9,  bool b10, bool b11, bool b12, bool b13, bool b14, bool b15,
+        bool b16, bool b17, bool b18, bool b19, bool b20, bool b21, bool b22, bool b23,
+        bool b24, bool b25, bool b26, bool b27, bool b28, bool b29, bool b30, bool b31,
+        bool b32, bool b33, bool b34, bool b35, bool b36, bool b37, bool b38, bool b39,
+        bool b40, bool b41, bool b42, bool b43, bool b44, bool b45, bool b46, bool b47,
+        bool b48, bool b49, bool b50, bool b51, bool b52, bool b53, bool b54, bool b55,
+        bool b56, bool b57, bool b58, bool b59, bool b60, bool b61, bool b62, bool b63) {
+        mm = uint64_t(
+            (uint64_t)b0        | (uint64_t)b1  << 1  | (uint64_t)b2  << 2  | (uint64_t)b3  << 3  |
+            (uint64_t)b4  << 4  | (uint64_t)b5  << 5  | (uint64_t)b6  << 6  | (uint64_t)b7  << 7  |
+            (uint64_t)b8  << 8  | (uint64_t)b9  << 9  | (uint64_t)b10 << 10 | (uint64_t)b11 << 11 |
+            (uint64_t)b12 << 12 | (uint64_t)b13 << 13 | (uint64_t)b14 << 14 | (uint64_t)b15 << 15 |
+            (uint64_t)b16 << 16 | (uint64_t)b17 << 17 | (uint64_t)b18 << 18 | (uint64_t)b19 << 19 |
+            (uint64_t)b20 << 20 | (uint64_t)b21 << 21 | (uint64_t)b22 << 22 | (uint64_t)b23 << 23 |
+            (uint64_t)b24 << 24 | (uint64_t)b25 << 25 | (uint64_t)b26 << 26 | (uint64_t)b27 << 27 |
+            (uint64_t)b28 << 28 | (uint64_t)b29 << 29 | (uint64_t)b30 << 30 | (uint64_t)b31 << 31 |
+            (uint64_t)b32 << 32 | (uint64_t)b33 << 33 | (uint64_t)b34 << 34 | (uint64_t)b35 << 35 |
+            (uint64_t)b36 << 36 | (uint64_t)b37 << 37 | (uint64_t)b38 << 38 | (uint64_t)b39 << 39 |
+            (uint64_t)b40 << 40 | (uint64_t)b41 << 41 | (uint64_t)b42 << 42 | (uint64_t)b43 << 43 |
+            (uint64_t)b44 << 44 | (uint64_t)b45 << 45 | (uint64_t)b46 << 46 | (uint64_t)b47 << 47 |
+            (uint64_t)b48 << 48 | (uint64_t)b49 << 49 | (uint64_t)b50 << 50 | (uint64_t)b51 << 51 |
+            (uint64_t)b52 << 52 | (uint64_t)b53 << 53 | (uint64_t)b54 << 54 | (uint64_t)b55 << 55 |
+            (uint64_t)b56 << 56 | (uint64_t)b57 << 57 | (uint64_t)b58 << 58 | (uint64_t)b59 << 59 |
+            (uint64_t)b60 << 60 | (uint64_t)b61 << 61 | (uint64_t)b62 << 62 | (uint64_t)b63 << 63);
+    } */
+    // Constructor to convert from type __mmask64 used in intrinsics:
+    Vec64b (__mmask64 x) {
+        mm = x;
+    }
+    // Constructor to broadcast single value:
+    Vec64b(bool b) {
+        mm = __mmask64(-int64_t(b));
+    }
+    // Constructor to make from two halves
+    Vec64b(Vec32b const x0, Vec32b const x1) {
+        mm = uint32_t(__mmask32(x0)) | uint64_t(__mmask32(x1)) << 32;
+    }
+    // Assignment operator to convert from type __mmask64 used in intrinsics:
+    Vec64b & operator = (__mmask64 x) {
+        mm = x;
+        return *this;
+    }
+    // Assignment operator to broadcast scalar value:
+    Vec64b & operator = (bool b) {
+        mm = Vec64b(b);
+        return *this;
+    }
+    // split into two halves
+    Vec32b get_low() const {
+        return Vec32b(__mmask32(mm));
+    }
+    Vec32b get_high() const {
+        return Vec32b(__mmask32(mm >> 32));
+    }
+    // Member function to change a single element in vector
+    Vec64b & insert (uint32_t index, bool a) {
+        uint64_t mask = uint64_t(1) << index;
+        mm = (mm & ~mask) | uint64_t(a) << index;
+        return *this;
+    }
+    // Member function extract a single element from vector
+    bool extract(int index) const {
+        return ((mm >> index) & 1) != 0;
+    }
+    // Extract a single element. Use store function if extracting more than one element.
+    // Operator [] can only read an element, not write.
+    bool operator [] (int index) const {
+        return extract(index);
+    }
+    // Type cast operator to convert to __mmask64 used in intrinsics
+    operator __mmask64() const {
+        return mm;
+    }
+    // Member function to change a bitfield to a boolean vector
+    Vec64b & load_bits(uint64_t a) {
+        mm = __mmask64(a);
+        return *this;
+    }
+    static constexpr int size() {
+        return 64;
+    }
+    static constexpr int elementtype() {
+        return 2;
+    }
+};
+
+typedef Vec64b Vec64cb;   // compact boolean vector
+typedef Vec64b Vec64ucb;  // compact boolean vector
+
+
+/*****************************************************************************
+*
+*          Define operators and functions for Vec64cb
+*
+*****************************************************************************/
+
+// vector operator & : bitwise and
+static inline Vec64cb operator & (Vec64cb const a, Vec64cb const b) {
+    //return _kand_mask64(a, b);
+    return __mmask64(a) & __mmask64(b);
+}
+static inline Vec64cb operator && (Vec64cb const a, Vec64cb const b) {
+    return a & b;
+}
+// vector operator &= : bitwise and
+static inline Vec64cb & operator &= (Vec64cb & a, Vec64cb const b) {
+    a = a & b;
+    return a;
+}
+
+// vector operator | : bitwise or
+static inline Vec64cb operator | (Vec64cb const a, Vec64cb const b) {
+    //return _kor_mask64(a, b);
+    return __mmask64(a) | __mmask64(b);
+}
+static inline Vec64cb operator || (Vec64cb const a, Vec64cb const b) {
+    return a | b;
+}
+// vector operator |= : bitwise or
+static inline Vec64cb & operator |= (Vec64cb & a, Vec64cb const b) {
+    a = a | b;
+    return a;
+}
+
+// vector operator ^ : bitwise xor
+static inline Vec64cb operator ^ (Vec64cb const a, Vec64cb const b) {
+    //return _kxor_mask64(a, b);
+    return __mmask64(a) ^ __mmask64(b);
+}
+// vector operator ^= : bitwise xor
+static inline Vec64cb & operator ^= (Vec64cb & a, Vec64cb const b) {
+    a = a ^ b;
+    return a;
+}
+
+// vector operator == : xnor
+static inline Vec64cb operator == (Vec64cb const a, Vec64cb const b) {
+    return __mmask64(a) ^ ~ __mmask64(b);
+    //return _kxnor_mask64(a, b); // not all compilers have this intrinsic
+}
+
+// vector operator != : xor
+static inline Vec64cb operator != (Vec64cb const a, Vec64cb const b) {
+    //return _kxor_mask64(a, b);
+    return __mmask64(a) ^ __mmask64(b);
+}
+
+// vector operator ~ : bitwise not
+static inline Vec64cb operator ~ (Vec64cb const a) {
+    //return _knot_mask64(a);
+    return ~ __mmask64(a);
+}
+
+// vector operator ! : element not
+static inline Vec64cb operator ! (Vec64cb const a) {
+    return ~a;
+}
+
+// vector function andnot
+static inline Vec64cb andnot (Vec64cb const a, Vec64cb const b) {
+    //return  _kxnor_mask64(b, a);
+    return __mmask64(a) & ~ __mmask64(b);
+}
+
+// horizontal_and. Returns true if all bits are 1
+static inline bool horizontal_and (Vec64cb const a) {
+    return int64_t(__mmask64(a)) == -(int64_t)(1);
+}
+
+// horizontal_or. Returns true if at least one bit is 1
+static inline bool horizontal_or (Vec64cb const a) {
+    return int64_t(__mmask64(a)) != 0;
+}
+
+// to_bits: convert boolean vector to integer bitfield
+static inline uint64_t to_bits(Vec64cb x) {
+    return uint64_t(__mmask64(x));
+}
+
+
+/*****************************************************************************
+*
+*          Define operators for Vec64c
+*
+*****************************************************************************/
+
+// vector operator + : add element by element
+static inline Vec64c operator + (Vec64c const a, Vec64c const b) {
+    return _mm512_add_epi8(a, b);
+}
+// vector operator += : add
+static inline Vec64c & operator += (Vec64c & a, Vec64c const b) {
+    a = a + b;
+    return a;
+}
+
+// postfix operator ++
+static inline Vec64c operator ++ (Vec64c & a, int) {
+    Vec64c a0 = a;
+    a = a + 1;
+    return a0;
+}
+// prefix operator ++
+static inline Vec64c & operator ++ (Vec64c & a) {
+    a = a + 1;
+    return a;
+}
+
+// vector operator - : subtract element by element
+static inline Vec64c operator - (Vec64c const a, Vec64c const b) {
+    return _mm512_sub_epi8(a, b);
+}
+// vector operator - : unary minus
+static inline Vec64c operator - (Vec64c const a) {
+    return _mm512_sub_epi8(_mm512_setzero_epi32(), a);
+}
+// vector operator -= : subtract
+static inline Vec64c & operator -= (Vec64c & a, Vec64c const b) {
+    a = a - b;
+    return a;
+}
+
+// postfix operator --
+static inline Vec64c operator -- (Vec64c & a, int) {
+    Vec64c a0 = a;
+    a = a - 1;
+    return a0;
+}
+// prefix operator --
+static inline Vec64c & operator -- (Vec64c & a) {
+    a = a - 1;
+    return a;
+}
+
+// vector operator * : multiply element by element
+static inline Vec64c operator * (Vec64c const a, Vec64c const b) {
+    // There is no 8-bit multiply. Split into two 16-bit multiplies
+    __m512i aodd    = _mm512_srli_epi16(a,8);              // odd numbered elements of a
+    __m512i bodd    = _mm512_srli_epi16(b,8);              // odd numbered elements of b
+    __m512i muleven = _mm512_mullo_epi16(a,b);             // product of even numbered elements
+    __m512i mulodd  = _mm512_mullo_epi16(aodd,bodd);       // product of odd  numbered elements
+    mulodd          = _mm512_slli_epi16(mulodd,8);         // put odd numbered elements back in place
+    __m512i product = _mm512_mask_mov_epi8(muleven, 0xAAAAAAAAAAAAAAAA, mulodd); // interleave even and odd
+    return product;
+}
+
+// vector operator *= : multiply
+static inline Vec64c & operator *= (Vec64c & a, Vec64c const b) {
+    a = a * b;
+    return a;
+}
+
+// vector operator / : divide all elements by same integer. See bottom of file
+
+// vector operator << : shift left
+static inline Vec64c operator << (Vec64c const a, int32_t b) {
+    uint32_t mask = (uint32_t)0xFF >> (uint32_t)b;                   // mask to remove bits that are shifted out
+    __m512i am    = _mm512_and_si512(a,_mm512_set1_epi8((char)mask));// remove bits that will overflow
+    __m512i res   = _mm512_sll_epi16(am,_mm_cvtsi32_si128(b));       // 16-bit shifts
+    return res;
+}
+
+// vector operator <<= : shift left
+static inline Vec64c & operator <<= (Vec64c & a, int32_t b) {
+    a = a << b;
+    return a;
+}
+
+// vector operator >> : shift right arithmetic
+static inline Vec64c operator >> (Vec64c const a, int32_t b) {
+    __m512i aeven = _mm512_slli_epi16(a, 8);                            // even numbered elements of a. get sign bit in position
+    aeven         = _mm512_sra_epi16(aeven, _mm_cvtsi32_si128(b + 8));  // shift arithmetic, back to position
+    __m512i aodd  = _mm512_sra_epi16(a, _mm_cvtsi32_si128(b));          // shift odd numbered elements arithmetic
+    __m512i res = _mm512_mask_mov_epi8(aeven, 0xAAAAAAAAAAAAAAAA, aodd);// interleave even and odd
+    return  res;
+}
+// vector operator >>= : shift right arithmetic
+static inline Vec64c & operator >>= (Vec64c & a, int32_t b) {
+    a = a >> b;
+    return a;
+}
+
+// vector operator == : returns true for elements for which a == b
+static inline Vec64cb operator == (Vec64c const a, Vec64c const b) {
+    return _mm512_cmpeq_epi8_mask(a, b);
+}
+
+// vector operator != : returns true for elements for which a != b
+static inline Vec64cb operator != (Vec64c const a, Vec64c const b) {
+    return _mm512_cmpneq_epi8_mask(a, b);
+}
+
+// vector operator > : returns true for elements for which a > b
+static inline Vec64cb operator > (Vec64c const a, Vec64c const b) {
+    return _mm512_cmp_epi8_mask(a, b, 6);
+}
+
+// vector operator < : returns true for elements for which a < b
+static inline Vec64cb operator < (Vec64c const a, Vec64c const b) {
+    return _mm512_cmp_epi8_mask(a, b, 1);
+}
+
+// vector operator >= : returns true for elements for which a >= b (signed)
+static inline Vec64cb operator >= (Vec64c const a, Vec64c const b) {
+    return _mm512_cmp_epi8_mask(a, b, 5);
+}
+
+// vector operator <= : returns true for elements for which a <= b (signed)
+static inline Vec64cb operator <= (Vec64c const a, Vec64c const b) {
+    return _mm512_cmp_epi8_mask(a, b, 2);
+}
+
+// vector operator & : bitwise and
+static inline Vec64c operator & (Vec64c const a, Vec64c const b) {
+    return _mm512_and_epi32(a, b);
+}
+
+// vector operator &= : bitwise and
+static inline Vec64c & operator &= (Vec64c & a, Vec64c const b) {
+    a = a & b;
+    return a;
+}
+
+// vector operator | : bitwise or
+static inline Vec64c operator | (Vec64c const a, Vec64c const b) {
+    return _mm512_or_epi32(a, b);
+}
+
+// vector operator |= : bitwise or
+static inline Vec64c & operator |= (Vec64c & a, Vec64c const b) {
+    a = a | b;
+    return a;
+}
+
+// vector operator ^ : bitwise xor
+static inline Vec64c operator ^ (Vec64c const a, Vec64c const b) {
+    return _mm512_xor_epi32(a, b);
+}
+
+// vector operator ^= : bitwise xor
+static inline Vec64c & operator ^= (Vec64c & a, Vec64c const b) {
+    a = a ^ b;
+    return a;
+}
+
+// vector operator ~ : bitwise not
+static inline Vec64c operator ~ (Vec64c const a) {
+    return Vec64c(~ Vec16i(a));
+}
+
+// Functions for this class
+
+// Select between two operands. Corresponds to this pseudocode:
+// for (int i = 0; i < 16; i++) result[i] = s[i] ? a[i] : b[i];
+static inline Vec64c select (Vec64cb const s, Vec64c const a, Vec64c const b) {
+    return _mm512_mask_mov_epi8(b, s, a);  // conditional move may be optimized better by the compiler than blend
+}
+
+// Conditional add: For all vector elements i: result[i] = f[i] ? (a[i] + b[i]) : a[i]
+static inline Vec64c if_add (Vec64cb const f, Vec64c const a, Vec64c const b) {
+    return _mm512_mask_add_epi8(a, f, a, b);
+}
+
+// Conditional subtract
+static inline Vec64c if_sub (Vec64cb const f, Vec64c const a, Vec64c const b) {
+    return _mm512_mask_sub_epi8(a, f, a, b);
+}
+
+// Conditional multiply
+static inline Vec64c if_mul (Vec64cb const f, Vec64c const a, Vec64c const b) {
+    Vec64c m = a * b;
+    return select(f, m, a);
+}
+
+// Horizontal add: Calculates the sum of all vector elements. Overflow will wrap around
+static inline int8_t horizontal_add (Vec64c const a) {
+    __m512i sum1 = _mm512_sad_epu8(a,_mm512_setzero_si512());
+    return (int8_t)horizontal_add(Vec8q(sum1));
+}
+
+// Horizontal add extended: Calculates the sum of all vector elements.
+// Each element is sign-extended before addition to avoid overflow
+static inline int32_t horizontal_add_x (Vec64c const a) {
+    return horizontal_add_x(a.get_low()) + horizontal_add_x(a.get_high());
+}
+
+// function add_saturated: add element by element, signed with saturation
+static inline Vec64c add_saturated(Vec64c const a, Vec64c const b) {
+    return _mm512_adds_epi8(a, b);
+}
+
+// function sub_saturated: subtract element by element, signed with saturation
+static inline Vec64c sub_saturated(Vec64c const a, Vec64c const b) {
+    return _mm512_subs_epi8(a, b);
+}
+
+// function max: a > b ? a : b
+static inline Vec64c max(Vec64c const a, Vec64c const b) {
+    return _mm512_max_epi8(a,b);
+}
+
+// function min: a < b ? a : b
+static inline Vec64c min(Vec64c const a, Vec64c const b) {
+    return _mm512_min_epi8(a,b);
+
+}
+
+// function abs: a >= 0 ? a : -a
+static inline Vec64c abs(Vec64c const a) {
+    return _mm512_abs_epi8(a);
+}
+
+// function abs_saturated: same as abs, saturate if overflow
+static inline Vec64c abs_saturated(Vec64c const a) {
+    return _mm512_min_epu8(abs(a), Vec64c(0x7F));
+}
+
+// function rotate_left all elements
+// Use negative count to rotate right
+static inline Vec64c rotate_left(Vec64c const a, int b) {
+    int8_t  mask  = int8_t(0xFFu << b);            // mask off overflow bits
+    __m512i m     = _mm512_set1_epi8(mask);
+    __m128i bb    = _mm_cvtsi32_si128(b & 7);      // b modulo 8
+    __m128i mbb   = _mm_cvtsi32_si128((- b) & 7);  // 8-b modulo 8
+    __m512i left  = _mm512_sll_epi16(a, bb);       // a << b
+    __m512i right = _mm512_srl_epi16(a, mbb);      // a >> 8-b
+            left  = _mm512_and_si512(m, left);     // mask off overflow bits
+            right = _mm512_andnot_si512(m, right);
+    return  _mm512_or_si512(left, right);          // combine left and right shifted bits
+}
+
+
+/*****************************************************************************
+*
+*          Vector of 64 8-bit unsigned integers
+*
+*****************************************************************************/
+
+class Vec64uc : public Vec64c {
+public:
+    // Default constructor:
+    Vec64uc() = default;
+    // Constructor to broadcast the same value into all elements:
+    Vec64uc(uint8_t i) {
+        zmm = _mm512_set1_epi8((int8_t)i);
+    }
+    // Constructor to build from all elements:
+    Vec64uc(uint8_t i0, uint8_t i1, uint8_t i2, uint8_t i3, uint8_t i4, uint8_t i5, uint8_t i6, uint8_t i7,
+        uint8_t i8, uint8_t i9, uint8_t i10, uint8_t i11, uint8_t i12, uint8_t i13, uint8_t i14, uint8_t i15,
+        uint8_t i16, uint8_t i17, uint8_t i18, uint8_t i19, uint8_t i20, uint8_t i21, uint8_t i22, uint8_t i23,
+        uint8_t i24, uint8_t i25, uint8_t i26, uint8_t i27, uint8_t i28, uint8_t i29, uint8_t i30, uint8_t i31,
+        uint8_t i32, uint8_t i33, uint8_t i34, uint8_t i35, uint8_t i36, uint8_t i37, uint8_t i38, uint8_t i39,
+        uint8_t i40, uint8_t i41, uint8_t i42, uint8_t i43, uint8_t i44, uint8_t i45, uint8_t i46, uint8_t i47,
+        uint8_t i48, uint8_t i49, uint8_t i50, uint8_t i51, uint8_t i52, uint8_t i53, uint8_t i54, uint8_t i55,
+        uint8_t i56, uint8_t i57, uint8_t i58, uint8_t i59, uint8_t i60, uint8_t i61, uint8_t i62, uint8_t i63)
+        : Vec64c(int8_t(i0), int8_t(i1), int8_t(i2), int8_t(i3), int8_t(i4), int8_t(i5), int8_t(i6), int8_t(i7), int8_t(i8), int8_t(i9), int8_t(i10), int8_t(i11), int8_t(i12), int8_t(i13), int8_t(i14), int8_t(i15),
+            int8_t(i16), int8_t(i17), int8_t(i18), int8_t(i19), int8_t(i20), int8_t(i21), int8_t(i22), int8_t(i23), int8_t(i24), int8_t(i25), int8_t(i26), int8_t(i27), int8_t(i28), int8_t(i29), int8_t(i30), int8_t(i31),
+            int8_t(i32), int8_t(i33), int8_t(i34), int8_t(i35), int8_t(i36), int8_t(i37), int8_t(i38), int8_t(i39), int8_t(i40), int8_t(i41), int8_t(i42), int8_t(i43), int8_t(i44), int8_t(i45), int8_t(i46), int8_t(i47),
+            int8_t(i48), int8_t(i49), int8_t(i50), int8_t(i51), int8_t(i52), int8_t(i53), int8_t(i54), int8_t(i55), int8_t(i56), int8_t(i57), int8_t(i58), int8_t(i59), int8_t(i60), int8_t(i61), int8_t(i62), int8_t(i63)) {}
+
+    // Constructor to build from two Vec32uc:
+    Vec64uc(Vec32uc const a0, Vec32uc const a1) {
+        zmm = _mm512_inserti64x4(_mm512_castsi256_si512(a0), a1, 1);
+    }
+    // Constructor to convert from type __m512i used in intrinsics:
+    Vec64uc(__m512i const x) {
+        zmm = x;
+    }
+    // Assignment operator to convert from type __m512i used in intrinsics:
+    Vec64uc & operator = (__m512i const x) {
+        zmm = x;
+        return *this;
+    }
+    // Member function to load from array (unaligned)
+    Vec64uc & load(void const * p) {
+        Vec64c::load(p);
+        return *this;
+    }
+    // Member function to load from array, aligned by 64
+    Vec64uc & load_a(void const * p) {
+        Vec64c::load_a(p);
+        return *this;
+    }
+    // Member function to change a single element in vector
+    Vec64uc const insert(int index, uint8_t value) {
+        Vec64c::insert(index, (int8_t)value);
+        return *this;
+    }
+    // Member function extract a single element from vector
+    uint8_t extract(int index) const {
+        return (uint8_t)Vec64c::extract(index);
+    }
+    // Extract a single element. Use store function if extracting more than one element.
+    // Operator [] can only read an element, not write.
+    uint8_t operator [] (int index) const {
+        return (uint8_t)Vec64c::extract(index);
+    }
+    // Member functions to split into two Vec32uc:
+    Vec32uc get_low() const {
+        return Vec32uc(Vec64c::get_low());
+    }
+    Vec32uc get_high() const {
+        return Vec32uc(Vec64c::get_high());
+    }
+    static constexpr int elementtype() {
+        return 5;
+    }
+};
+
+// Define operators for this class
+
+// vector operator + : add element by element
+static inline Vec64uc operator + (Vec64uc const a, Vec64uc const b) {
+    return _mm512_add_epi8(a, b);
+}
+
+// vector operator - : subtract element by element
+static inline Vec64uc operator - (Vec64uc const a, Vec64uc const b) {
+    return _mm512_sub_epi8(a, b);
+}
+
+// vector operator ' : multiply element by element
+static inline Vec64uc operator * (Vec64uc const a, Vec64uc const b) {
+    return Vec64uc(Vec64c(a) * Vec64c(b));
+}
+
+// vector operator / : divide. See bottom of file
+
+// vector operator >> : shift right logical all elements
+static inline Vec64uc operator >> (Vec64uc const a, uint32_t b) {
+    uint32_t mask = (uint32_t)0xFF << (uint32_t)b;                     // mask to remove bits that are shifted out
+    __m512i am    = _mm512_and_si512(a,_mm512_set1_epi8((char)mask));  // remove bits that will overflow
+    __m512i res   = _mm512_srl_epi16(am,_mm_cvtsi32_si128((int32_t)b));// 16-bit shifts
+    return res;
+}
+static inline Vec64uc operator >> (Vec64uc const a, int b) {
+    return a >> (uint32_t)b;
+}
+
+// vector operator >>= : shift right logical
+static inline Vec64uc & operator >>= (Vec64uc & a, uint32_t b) {
+    a = a >> b;
+    return a;
+}
+
+// vector operator >>= : shift right logical (signed b)
+static inline Vec64uc & operator >>= (Vec64uc & a, int32_t b) {
+    a = a >> uint32_t(b);
+    return a;
+}
+
+// vector operator << : shift left all elements
+static inline Vec64uc operator << (Vec64uc const a, uint32_t b) {
+    return Vec64uc(Vec64c(a) << int32_t(b));
+}
+static inline Vec64uc operator << (Vec64uc const a, int b) {
+    return a << (uint32_t)b;
+}
+
+// vector operator < : returns true for elements for which a < b (unsigned)
+static inline Vec64cb operator < (Vec64uc const a, Vec64uc const b) {
+    return _mm512_cmp_epu8_mask(a, b, 1);
+}
+
+// vector operator > : returns true for elements for which a > b (unsigned)
+static inline Vec64cb operator > (Vec64uc const a, Vec64uc const b) {
+    return _mm512_cmp_epu8_mask(a, b, 6);
+}
+
+// vector operator >= : returns true for elements for which a >= b (unsigned)
+static inline Vec64cb operator >= (Vec64uc const a, Vec64uc const b) {
+    return _mm512_cmp_epu8_mask(a, b, 5);
+}
+
+// vector operator <= : returns true for elements for which a <= b (unsigned)
+static inline Vec64cb operator <= (Vec64uc const a, Vec64uc const b) {
+    return _mm512_cmp_epu8_mask(a, b, 2);
+}
+
+// vector operator & : bitwise and
+static inline Vec64uc operator & (Vec64uc const a, Vec64uc const b) {
+    return Vec64uc(Vec64c(a) & Vec64c(b));
+}
+
+// vector operator | : bitwise or
+static inline Vec64uc operator | (Vec64uc const a, Vec64uc const b) {
+    return Vec64uc(Vec64c(a) | Vec64c(b));
+}
+
+// vector operator ^ : bitwise xor
+static inline Vec64uc operator ^ (Vec64uc const a, Vec64uc const b) {
+    return Vec64uc(Vec64c(a) ^ Vec64c(b));
+}
+
+// vector operator ~ : bitwise not
+static inline Vec64uc operator ~ (Vec64uc const a) {
+    return Vec64uc( ~ Vec64c(a));
+}
+
+
+// Functions for this class
+
+// Select between two operands. Corresponds to this pseudocode:
+// for (int i = 0; i < 16; i++) result[i] = s[i] ? a[i] : b[i];
+static inline Vec64uc select (Vec64cb const s, Vec64uc const a, Vec64uc const b) {
+    return Vec64uc(select(s, Vec64c(a), Vec64c(b)));
+}
+
+// Conditional add: For all vector elements i: result[i] = f[i] ? (a[i] + b[i]) : a[i]
+static inline Vec64uc if_add (Vec64cb const f, Vec64uc const a, Vec64uc const b) {
+    return _mm512_mask_add_epi8(a, f, a, b);
+}
+
+// Conditional subtract
+static inline Vec64uc if_sub (Vec64cb const f, Vec64uc const a, Vec64uc const b) {
+    return _mm512_mask_sub_epi8(a, f, a, b);
+}
+
+// Conditional multiply
+static inline Vec64uc if_mul (Vec64cb const f, Vec64uc const a, Vec64uc const b) {
+    Vec64uc m = a * b;
+    return select(f, m, a);
+}
+
+// function add_saturated: add element by element, unsigned with saturation
+static inline Vec64uc add_saturated(Vec64uc const a, Vec64uc const b) {
+    return _mm512_adds_epu8(a, b);
+}
+
+// function sub_saturated: subtract element by element, unsigned with saturation
+static inline Vec64uc sub_saturated(Vec64uc const a, Vec64uc const b) {
+    return _mm512_subs_epu8(a, b);
+}
+
+// function max: a > b ? a : b
+static inline Vec64uc max(Vec64uc const a, Vec64uc const b) {
+    return _mm512_max_epu8(a,b);
+}
+
+// function min: a < b ? a : b
+static inline Vec64uc min(Vec64uc const a, Vec64uc const b) {
+    return _mm512_min_epu8(a,b);
+}
+
+
+/*****************************************************************************
+*
+*          Vector of 32 16-bit signed integers
+*
+*****************************************************************************/
+
+class Vec32s: public Vec512b {
+public:
+    // Default constructor:
+    Vec32s() = default;
+    // Constructor to broadcast the same value into all elements:
+    Vec32s(int16_t i) {
+        zmm = _mm512_set1_epi16(i);
+    }
+    // Constructor to build from all elements:
+    Vec32s(int16_t i0, int16_t i1, int16_t i2, int16_t i3, int16_t i4, int16_t i5, int16_t i6, int16_t i7,
+        int16_t i8, int16_t i9, int16_t i10, int16_t i11, int16_t i12, int16_t i13, int16_t i14, int16_t i15,
+        int16_t i16, int16_t i17, int16_t i18, int16_t i19, int16_t i20, int16_t i21, int16_t i22, int16_t i23,
+        int16_t i24, int16_t i25, int16_t i26, int16_t i27, int16_t i28, int16_t i29, int16_t i30, int16_t i31) {
+#if true
+        // _mm512_set_epi16 missing in GCC 7.4.0. This may be more efficient after all:
+        int16_t aa[32] = {
+            i0, i1, i2, i3, i4, i5, i6, i7, i8, i9, i10, i11, i12, i13, i14, i15,
+            i16, i17, i18, i19, i20, i21, i22, i23, i24, i25, i26, i27, i28, i29, i30, i31 };
+        load(aa);
+#else
+        zmm = _mm512_set_epi16(
+            i31, i30, i29, i28, i27, i26, i25, i24, i23, i22, i21, i20, i19, i18, i17, i16,
+            i15, i14, i13, i12, i11, i10, i9,  i8,  i7,  i6,  i5,  i4,  i3,  i2,  i1,  i0);
+#endif
+    }
+    // Constructor to build from two Vec16s:
+    Vec32s(Vec16s const a0, Vec16s const a1) {
+        zmm = _mm512_inserti64x4(_mm512_castsi256_si512(a0), a1, 1);
+    }
+    // Constructor to convert from type __m512i used in intrinsics:
+    Vec32s(__m512i const x) {
+        zmm = x;
+    }
+    // Assignment operator to convert from type __m512i used in intrinsics:
+    Vec32s & operator = (__m512i const x) {
+        zmm = x;
+        return *this;
+    }
+    // Type cast operator to convert to __m512i used in intrinsics
+    operator __m512i() const {
+        return zmm;
+    }
+    // Member function to load from array (unaligned)
+    Vec32s & load(void const * p) {
+        zmm = _mm512_loadu_si512(p);
+        return *this;
+    }
+    // Member function to load from array, aligned by 64
+    Vec32s & load_a(void const * p) {
+        zmm = _mm512_load_si512(p);
+        return *this;
+    }
+    // Partial load. Load n elements and set the rest to 0
+    Vec32s & load_partial(int n, void const * p) {
+        zmm = _mm512_maskz_loadu_epi16(__mmask32(((uint64_t)1 << n) - 1), p);
+        return *this;
+    }
+    // Partial store. Store n elements
+    void store_partial(int n, void * p) const {
+        _mm512_mask_storeu_epi16(p, __mmask32(((uint64_t)1 << n) - 1), zmm);
+    }
+    // cut off vector to n elements. The last 32-n elements are set to zero
+    Vec32s & cutoff(int n) {
+        zmm = _mm512_maskz_mov_epi16(__mmask32(((uint64_t)1 << n) - 1), zmm);
+        return *this;
+    }
+    // Member function to change a single element in vector
+    Vec32s const insert(int index, int16_t value) {
+        zmm = _mm512_mask_set1_epi16(zmm, __mmask64((uint64_t)1 << index), value);
+        return *this;
+    }
+    // Member function extract a single element from vector
+    int16_t extract(int index) const {
+#if INSTRSET >= 10 && defined (__AVX512VBMI2__)
+        __m512i x = _mm512_maskz_compress_epi16(__mmask32(1u << index), zmm);
+        return (int16_t)_mm_cvtsi128_si32(_mm512_castsi512_si128(x));
+#else
+        int16_t a[32];
+        store(a);
+        return a[index & 31];
+#endif
+    }
+    // Extract a single element. Use store function if extracting more than one element.
+    // Operator [] can only read an element, not write.
+    int16_t operator [] (int index) const {
+        return extract(index);
+    }
+    // Member functions to split into two Vec16s:
+    Vec16s get_low() const {
+        return _mm512_castsi512_si256(zmm);
+    }
+    Vec16s get_high() const {
+        return _mm512_extracti64x4_epi64(zmm,1);
+    }
+    static constexpr int size() {
+        return 32;
+    }
+    static constexpr int elementtype() {
+        return 6;
+    }
+};
+
+
+/*****************************************************************************
+*
+*          Vec32sb: Vector of 64 Booleans for use with Vec32s and Vec32us
+*
+*****************************************************************************/
+
+typedef Vec32b Vec32sb;  // compact boolean vector
+
+
+/*****************************************************************************
+*
+*          Define operators for Vec32s
+*
+*****************************************************************************/
+
+// vector operator + : add element by element
+static inline Vec32s operator + (Vec32s const a, Vec32s const b) {
+    return _mm512_add_epi16(a, b);
+}
+// vector operator += : add
+static inline Vec32s & operator += (Vec32s & a, Vec32s const b) {
+    a = a + b;
+    return a;
+}
+
+// postfix operator ++
+static inline Vec32s operator ++ (Vec32s & a, int) {
+    Vec32s a0 = a;
+    a = a + 1;
+    return a0;
+}
+// prefix operator ++
+static inline Vec32s & operator ++ (Vec32s & a) {
+    a = a + 1;
+    return a;
+}
+
+// vector operator - : subtract element by element
+static inline Vec32s operator - (Vec32s const a, Vec32s const b) {
+    return _mm512_sub_epi16(a, b);
+}
+// vector operator - : unary minus
+static inline Vec32s operator - (Vec32s const a) {
+    return _mm512_sub_epi16(_mm512_setzero_epi32(), a);
+}
+// vector operator -= : subtract
+static inline Vec32s & operator -= (Vec32s & a, Vec32s const b) {
+    a = a - b;
+    return a;
+}
+
+// postfix operator --
+static inline Vec32s operator -- (Vec32s & a, int) {
+    Vec32s a0 = a;
+    a = a - 1;
+    return a0;
+}
+// prefix operator --
+static inline Vec32s & operator -- (Vec32s & a) {
+    a = a - 1;
+    return a;
+}
+
+// vector operator * : multiply element by element
+static inline Vec32s operator * (Vec32s const a, Vec32s const b) {
+    return _mm512_mullo_epi16(a, b);
+}
+
+// vector operator *= : multiply
+static inline Vec32s & operator *= (Vec32s & a, Vec32s const b) {
+    a = a * b;
+    return a;
+}
+
+// vector operator / : divide all elements by same integer. See bottom of file
+
+// vector operator << : shift left
+static inline Vec32s operator << (Vec32s const a, int32_t b) {
+    return _mm512_sll_epi16(a, _mm_cvtsi32_si128(b));
+}
+// vector operator <<= : shift left
+static inline Vec32s & operator <<= (Vec32s & a, int32_t b) {
+    a = a << b;
+    return a;
+}
+
+// vector operator >> : shift right arithmetic
+static inline Vec32s operator >> (Vec32s const a, int32_t b) {
+    return _mm512_sra_epi16(a, _mm_cvtsi32_si128(b));
+}
+// vector operator >>= : shift right arithmetic
+static inline Vec32s & operator >>= (Vec32s & a, int32_t b) {
+    a = a >> b;
+    return a;
+}
+
+// vector operator == : returns true for elements for which a == b
+static inline Vec32sb operator == (Vec32s const a, Vec32s const b) {
+    return _mm512_cmpeq_epi16_mask(a, b);
+}
+
+// vector operator != : returns true for elements for which a != b
+static inline Vec32sb operator != (Vec32s const a, Vec32s const b) {
+    return _mm512_cmpneq_epi16_mask(a, b);
+}
+
+// vector operator > : returns true for elements for which a > b
+static inline Vec32sb operator > (Vec32s const a, Vec32s const b) {
+    return _mm512_cmp_epi16_mask(a, b, 6);
+}
+
+// vector operator < : returns true for elements for which a < b
+static inline Vec32sb operator < (Vec32s const a, Vec32s const b) {
+    return _mm512_cmp_epi16_mask(a, b, 1);
+}
+
+// vector operator >= : returns true for elements for which a >= b (signed)
+static inline Vec32sb operator >= (Vec32s const a, Vec32s const b) {
+    return _mm512_cmp_epi16_mask(a, b, 5);
+}
+
+// vector operator <= : returns true for elements for which a <= b (signed)
+static inline Vec32sb operator <= (Vec32s const a, Vec32s const b) {
+    return _mm512_cmp_epi16_mask(a, b, 2);
+}
+
+// vector operator & : bitwise and
+static inline Vec32s operator & (Vec32s const a, Vec32s const b) {
+    return _mm512_and_epi32(a, b);
+}
+
+// vector operator &= : bitwise and
+static inline Vec32s & operator &= (Vec32s & a, Vec32s const b) {
+    a = a & b;
+    return a;
+}
+
+// vector operator | : bitwise or
+static inline Vec32s operator | (Vec32s const a, Vec32s const b) {
+    return _mm512_or_epi32(a, b);
+}
+
+// vector operator |= : bitwise or
+static inline Vec32s & operator |= (Vec32s & a, Vec32s const b) {
+    a = a | b;
+    return a;
+}
+
+// vector operator ^ : bitwise xor
+static inline Vec32s operator ^ (Vec32s const a, Vec32s const b) {
+    return _mm512_xor_epi32(a, b);
+}
+
+// vector operator ^= : bitwise xor
+static inline Vec32s & operator ^= (Vec32s & a, Vec32s const b) {
+    a = a ^ b;
+    return a;
+}
+
+// vector operator ~ : bitwise not
+static inline Vec32s operator ~ (Vec32s const a) {
+    return Vec32s(~ Vec16i(a));
+}
+
+// Functions for this class
+
+// Select between two operands. Corresponds to this pseudocode:
+// for (int i = 0; i < 16; i++) result[i] = s[i] ? a[i] : b[i];
+static inline Vec32s select (Vec32sb const s, Vec32s const a, Vec32s const b) {
+    return _mm512_mask_mov_epi16(b, s, a);  // conditional move may be optimized better by the compiler than blend
+}
+
+// Conditional add: For all vector elements i: result[i] = f[i] ? (a[i] + b[i]) : a[i]
+static inline Vec32s if_add (Vec32sb const f, Vec32s const a, Vec32s const b) {
+    return _mm512_mask_add_epi16(a, f, a, b);
+}
+
+// Conditional subtract
+static inline Vec32s if_sub (Vec32sb const f, Vec32s const a, Vec32s const b) {
+    return _mm512_mask_sub_epi16(a, f, a, b);
+}
+
+// Conditional multiply
+static inline Vec32s if_mul (Vec32sb const f, Vec32s const a, Vec32s const b) {
+    return _mm512_mask_mullo_epi16(a, f, a, b);
+}
+
+// Horizontal add: Calculates the sum of all vector elements.
+// Overflow will wrap around
+static inline int16_t horizontal_add (Vec32s const a) {
+    Vec16s s = a.get_low() + a.get_high();
+    return horizontal_add(s);
+}
+
+// Horizontal add extended: Calculates the sum of all vector elements.
+// Each element is sign-extended before addition to avoid overflow
+static inline int32_t horizontal_add_x (Vec32s const a) {
+    return horizontal_add_x(a.get_low()) + horizontal_add_x(a.get_high());
+}
+
+// function add_saturated: add element by element, signed with saturation
+static inline Vec32s add_saturated(Vec32s const a, Vec32s const b) {
+    return _mm512_adds_epi16(a, b);
+}
+
+// function sub_saturated: subtract element by element, signed with saturation
+static inline Vec32s sub_saturated(Vec32s const a, Vec32s const b) {
+    return _mm512_subs_epi16(a, b);
+}
+
+// function max: a > b ? a : b
+static inline Vec32s max(Vec32s const a, Vec32s const b) {
+    return _mm512_max_epi16(a,b);
+}
+
+// function min: a < b ? a : b
+static inline Vec32s min(Vec32s const a, Vec32s const b) {
+    return _mm512_min_epi16(a,b);
+}
+
+// function abs: a >= 0 ? a : -a
+static inline Vec32s abs(Vec32s const a) {
+    return _mm512_abs_epi16(a);
+}
+
+// function abs_saturated: same as abs, saturate if overflow
+static inline Vec32s abs_saturated(Vec32s const a) {
+    return _mm512_min_epu16(abs(a), Vec32s(0x7FFF));
+}
+
+// function rotate_left all elements
+// Use negative count to rotate right
+static inline Vec32s rotate_left(Vec32s const a, int b) {
+    __m512i left  = _mm512_sll_epi16(a,_mm_cvtsi32_si128(b & 0xF));      // a << b
+    __m512i right = _mm512_srl_epi16(a,_mm_cvtsi32_si128((16-b) & 0xF)); // a >> (32 - b)
+    __m512i rot   = _mm512_or_si512(left,right);                         // or
+    return  rot;
+}
+
+
+/*****************************************************************************
+*
+*          Vector of 32 16-bit unsigned integers
+*
+*****************************************************************************/
+
+class Vec32us : public Vec32s {
+public:
+    // Default constructor:
+    Vec32us() = default;
+    // Constructor to broadcast the same value into all elements:
+    Vec32us(uint16_t i) {
+        zmm = _mm512_set1_epi16((int16_t)i);
+    }
+    // Constructor to build from all elements. Inherit from Vec32s
+    Vec32us(uint16_t i0, uint16_t i1, uint16_t i2, uint16_t i3, uint16_t i4, uint16_t i5, uint16_t i6, uint16_t i7,
+        uint16_t i8,  uint16_t i9,  uint16_t i10, uint16_t i11, uint16_t i12, uint16_t i13, uint16_t i14, uint16_t i15,
+        uint16_t i16, uint16_t i17, uint16_t i18, uint16_t i19, uint16_t i20, uint16_t i21, uint16_t i22, uint16_t i23,
+        uint16_t i24, uint16_t i25, uint16_t i26, uint16_t i27, uint16_t i28, uint16_t i29, uint16_t i30, uint16_t i31)
+    : Vec32s(int16_t(i0), int16_t(i1), int16_t(i2), int16_t(i3), int16_t(i4), int16_t(i5), int16_t(i6), int16_t(i7), int16_t(i8), int16_t(i9), int16_t(i10), int16_t(i11), int16_t(i12), int16_t(i13), int16_t(i14), int16_t(i15),
+         int16_t(i16), int16_t(i17), int16_t(i18), int16_t(i19), int16_t(i20), int16_t(i21), int16_t(i22), int16_t(i23), int16_t(i24), int16_t(i25), int16_t(i26), int16_t(i27), int16_t(i28), int16_t(i29), int16_t(i30), int16_t(i31)) {}
+
+    // Constructor to build from two Vec16us:
+    Vec32us(Vec16us const a0, Vec16us const a1) {
+        zmm = _mm512_inserti64x4(_mm512_castsi256_si512(a0), a1, 1);
+    }
+    // Constructor to convert from type __m512i used in intrinsics:
+    Vec32us(__m512i const x) {
+        zmm = x;
+    }
+    // Assignment operator to convert from type __m512i used in intrinsics:
+    Vec32us & operator = (__m512i const x) {
+        zmm = x;
+        return *this;
+    }
+    // Member function to load from array (unaligned)
+    Vec32us & load(void const * p) {
+        Vec32s::load(p);
+        return *this;
+    }
+    // Member function to load from array, aligned by 64
+    Vec32us & load_a(void const * p) {
+        Vec32s::load_a(p);
+        return *this;
+    }
+    // Member function to change a single element in vector
+    Vec32us const insert(int index, uint16_t value) {
+        Vec32s::insert(index, (int16_t)value);
+        return *this;
+    }
+    // Member function extract a single element from vector
+    uint16_t extract(int index) const {
+        return (uint16_t)Vec32s::extract(index);
+    }
+    // Extract a single element. Use store function if extracting more than one element.
+    // Operator [] can only read an element, not write.
+    uint16_t operator [] (int index) const {
+        return (uint16_t)Vec32s::extract(index);
+    }
+    // Member functions to split into two Vec16us:
+    Vec16us get_low() const {
+        return Vec16us(Vec32s::get_low());
+    }
+    Vec16us get_high() const {
+        return Vec16us(Vec32s::get_high());
+    }
+    static constexpr int elementtype() {
+        return 7;
+    }
+};
+
+// Define operators for this class
+
+// vector operator + : add element by element
+static inline Vec32us operator + (Vec32us const a, Vec32us const b) {
+    return _mm512_add_epi16(a, b);
+}
+
+// vector operator - : subtract element by element
+static inline Vec32us operator - (Vec32us const a, Vec32us const b) {
+    return _mm512_sub_epi16(a, b);
+}
+
+// vector operator * : multiply element by element
+static inline Vec32us operator * (Vec32us const a, Vec32us const b) {
+    return _mm512_mullo_epi16(a, b);
+}
+
+// vector operator / : divide
+// See bottom of file
+
+// vector operator >> : shift right logical all elements
+static inline Vec32us operator >> (Vec32us const a, uint32_t b) {
+    return _mm512_srl_epi16(a, _mm_cvtsi32_si128((int)b));
+}
+static inline Vec32us operator >> (Vec32us const a, int b) {
+    return a >> uint32_t(b);
+}
+
+// vector operator >>= : shift right logical
+static inline Vec32us & operator >>= (Vec32us & a, uint32_t b) {
+    a = a >> b;
+    return a;
+}
+
+// vector operator >>= : shift right logical (signed b)
+static inline Vec32us & operator >>= (Vec32us & a, int32_t b) {
+    a = a >> uint32_t(b);
+    return a;
+}
+
+// vector operator << : shift left all elements
+static inline Vec32us operator << (Vec32us const a, uint32_t b) {
+    return _mm512_sll_epi16(a, _mm_cvtsi32_si128((int)b));
+}
+static inline Vec32us operator << (Vec32us const a, int b) {
+    return a << uint32_t(b);
+}
+
+// vector operator < : returns true for elements for which a < b (unsigned)
+static inline Vec32sb operator < (Vec32us const a, Vec32us const b) {
+    return _mm512_cmp_epu16_mask(a, b, 1);
+}
+
+// vector operator > : returns true for elements for which a > b (unsigned)
+static inline Vec32sb operator > (Vec32us const a, Vec32us const b) {
+    return _mm512_cmp_epu16_mask(a, b, 6);
+}
+
+// vector operator >= : returns true for elements for which a >= b (unsigned)
+static inline Vec32sb operator >= (Vec32us const a, Vec32us const b) {
+    return _mm512_cmp_epu16_mask(a, b, 5);
+}
+
+// vector operator <= : returns true for elements for which a <= b (unsigned)
+static inline Vec32sb operator <= (Vec32us const a, Vec32us const b) {
+    return _mm512_cmp_epu16_mask(a, b, 2);
+}
+
+// vector operator & : bitwise and
+static inline Vec32us operator & (Vec32us const a, Vec32us const b) {
+    return Vec32us(Vec32s(a) & Vec32s(b));
+}
+
+// vector operator | : bitwise or
+static inline Vec32us operator | (Vec32us const a, Vec32us const b) {
+    return Vec32us(Vec32s(a) | Vec32s(b));
+}
+
+// vector operator ^ : bitwise xor
+static inline Vec32us operator ^ (Vec32us const a, Vec32us const b) {
+    return Vec32us(Vec32s(a) ^ Vec32s(b));
+}
+
+// vector operator ~ : bitwise not
+static inline Vec32us operator ~ (Vec32us const a) {
+    return Vec32us( ~ Vec32s(a));
+}
+
+// Functions for this class
+
+// Select between two operands. Corresponds to this pseudocode:
+// for (int i = 0; i < 16; i++) result[i] = s[i] ? a[i] : b[i];
+static inline Vec32us select (Vec32sb const s, Vec32us const a, Vec32us const b) {
+    return Vec32us(select(s, Vec32s(a), Vec32s(b)));
+}
+
+// Conditional add: For all vector elements i: result[i] = f[i] ? (a[i] + b[i]) : a[i]
+static inline Vec32us if_add (Vec32sb const f, Vec32us const a, Vec32us const b) {
+    return _mm512_mask_add_epi16(a, f, a, b);
+}
+
+// Conditional subtract
+static inline Vec32us if_sub (Vec32sb const f, Vec32us const a, Vec32us const b) {
+    return _mm512_mask_sub_epi16(a, f, a, b);
+}
+
+// Conditional multiply
+static inline Vec32us if_mul (Vec32sb const f, Vec32us const a, Vec32us const b) {
+    return _mm512_mask_mullo_epi16(a, f, a, b);
+}
+
+// function add_saturated: add element by element, unsigned with saturation
+static inline Vec32us add_saturated(Vec32us const a, Vec32us const b) {
+    return _mm512_adds_epu16(a, b);
+}
+
+// function sub_saturated: subtract element by element, unsigned with saturation
+static inline Vec32us sub_saturated(Vec32us const a, Vec32us const b) {
+    return _mm512_subs_epu16(a, b);
+}
+
+// function max: a > b ? a : b
+static inline Vec32us max(Vec32us const a, Vec32us const b) {
+    return _mm512_max_epu16(a,b);
+}
+
+// function min: a < b ? a : b
+static inline Vec32us min(Vec32us const a, Vec32us const b) {
+    return _mm512_min_epu16(a,b);
+}
+
+
+/*****************************************************************************
+*
+*          Vector permute functions
+*
+******************************************************************************
+*
+* These permute functions can reorder the elements of a vector and optionally
+* set some elements to zero. See Vectori128.h for description
+*
+*****************************************************************************/
+
+// Permute vector of 32 16-bit integers.
+// Index -1 gives 0, index V_DC means don't care.
+template <int... i0 >
+    static inline Vec32s permute32(Vec32s const a) {
+    int constexpr indexs[32] = { i0... };
+    __m512i y = a;  // result
+    // get flags for possibilities that fit the permutation pattern
+    constexpr uint64_t flags = perm_flags<Vec32s>(indexs);
+
+    static_assert(sizeof... (i0) == 32, "permute32 must have 32 indexes");
+    static_assert((flags & perm_outofrange) == 0, "Index out of range in permute function");
+
+    if constexpr ((flags & perm_allzero) != 0) return _mm512_setzero_si512();  // just return zero
+
+    if constexpr ((flags & perm_perm) != 0) {                   // permutation needed
+
+        if constexpr ((flags & perm_largeblock) != 0) {         // use larger permutation
+            constexpr EList<int, 16> L = largeblock_perm<32>(indexs); // permutation pattern
+            y = permute16 <L.a[0], L.a[1], L.a[2], L.a[3], L.a[4], L.a[5], L.a[6], L.a[7],
+                L.a[8], L.a[9], L.a[10], L.a[11], L.a[12], L.a[13], L.a[14], L.a[15]> (Vec16i(a));
+            if (!(flags & perm_addz)) return y;                 // no remaining zeroing
+        }
+        else if constexpr ((flags & perm_same_pattern) != 0) {  // same pattern in all lanes
+            if constexpr ((flags & perm_rotate) != 0) {         // fits palignr. rotate within lanes
+                y = _mm512_alignr_epi8(a, a, (flags >> perm_rot_count) & 0xF);
+            }
+            else if constexpr ((flags & perm_swap) != 0) {      // swap adjacent elements. rotate 32 bits
+                y = _mm512_rol_epi32(a, 16);
+
+            }
+            else { // use pshufb
+                constexpr EList <int8_t, 64> bm = pshufb_mask<Vec32s>(indexs);
+                return _mm512_shuffle_epi8(a, Vec32s().load(bm.a));
+            }
+        }
+        else {  // different patterns in all lanes
+            if constexpr ((flags & perm_cross_lane) == 0) {     // no lane crossing. Use pshufb
+                constexpr EList <int8_t, 64> bm = pshufb_mask<Vec32s>(indexs);
+                return _mm512_shuffle_epi8(a, Vec32s().load(bm.a));
+            }
+            else if constexpr ((flags & perm_rotate_big) != 0) {// fits full rotate
+                constexpr uint8_t rot = uint8_t(flags >> perm_rot_count) * 2; // rotate count
+                constexpr uint8_t r1 = (rot >> 4 << 1) & 7;
+                constexpr uint8_t r2 = (r1 + 2) & 7;
+                __m512i y1 = a, y2 = a;
+                if constexpr (r1 != 0) y1 = _mm512_alignr_epi64 (a, a, r1); // rotate 128-bit blocks
+                if constexpr (r2 != 0) y2 = _mm512_alignr_epi64 (a, a, r2); // rotate 128-bit blocks
+                y = _mm512_alignr_epi8(y2, y1, rot & 15);
+            }
+            else if constexpr ((flags & perm_broadcast) != 0 && (flags >> perm_rot_count) == 0) {
+                y = _mm512_broadcastw_epi16(_mm512_castsi512_si128(y));     // broadcast first element
+            }
+            else if constexpr ((flags & perm_zext) != 0) {     // fits zero extension
+                y = _mm512_cvtepu16_epi32(_mm512_castsi512_si256(y));  // zero extension
+                if constexpr ((flags & perm_addz2) == 0) return y;
+            }
+#if defined (__AVX512VBMI2__)
+            else if constexpr ((flags & perm_compress) != 0) {
+                y = _mm512_maskz_compress_epi16(__mmask32(compress_mask(indexs)), y); // compress
+                if constexpr ((flags & perm_addz2) == 0) return y;
+            }
+            else if constexpr ((flags & perm_expand) != 0) {
+                y = _mm512_maskz_expand_epi16(__mmask32(expand_mask(indexs)), y); // expand
+                if constexpr ((flags & perm_addz2) == 0) return y;
+            }
+#endif  // AVX512VBMI2
+            else {  // full permute needed
+                constexpr EList <int16_t, 32> bm = perm_mask_broad<Vec32s>(indexs);
+                y = _mm512_permutexvar_epi16 (Vec32s().load(bm.a), y);
+            }
+        }
+    }
+    if constexpr ((flags & perm_zeroing) != 0) {           // additional zeroing needed
+        y = _mm512_maskz_mov_epi16(zero_mask<32>(indexs), y);
+    }
+    return y;
+}
+
+template <int... i0 >
+    static inline Vec32us permute32(Vec32us const a) {
+    return Vec32us (permute32<i0...> (Vec32s(a)));
+}
+
+
+// Permute vector of 64 8-bit integers.
+// Index -1 gives 0, index V_DC means don't care.
+template <int... i0 >
+static inline Vec64c permute64(Vec64c const a) {
+    int constexpr indexs[64] = { i0... };
+    __m512i y = a;  // result
+    // get flags for possibilities that fit the permutation pattern
+    constexpr uint64_t flags = perm_flags<Vec64c>(indexs);
+
+    static_assert(sizeof... (i0) == 64, "permute64 must have 64 indexes");
+    static_assert((flags & perm_outofrange) == 0, "Index out of range in permute function");
+
+    if constexpr ((flags & perm_allzero) != 0) {
+        return _mm512_setzero_si512();                                    // just return zero
+    }
+    if constexpr ((flags & perm_perm) != 0) {                             // permutation needed
+
+        if constexpr ((flags & perm_largeblock) != 0) {                   // use larger permutation
+            constexpr EList<int, 32> L = largeblock_perm<64>(indexs);      // permutation pattern
+            y = permute32 <
+                L.a[0],  L.a[1],  L.a[2],  L.a[3],  L.a[4],  L.a[5],  L.a[6],  L.a[7],
+                L.a[8],  L.a[9],  L.a[10], L.a[11], L.a[12], L.a[13], L.a[14], L.a[15],
+                L.a[16], L.a[17], L.a[18], L.a[19], L.a[20], L.a[21], L.a[22], L.a[23],
+                L.a[24], L.a[25], L.a[26], L.a[27], L.a[28], L.a[29], L.a[30], L.a[31]>
+                (Vec32s(a));
+            if (!(flags & perm_addz)) return y;                           // no remaining zeroing
+        }
+        else {
+            if constexpr ((flags & perm_cross_lane) == 0) {               // no lane crossing. Use pshufb
+                constexpr EList <int8_t, 64> bm = pshufb_mask<Vec64c>(indexs);
+                return _mm512_shuffle_epi8(a, Vec64c().load(bm.a));
+            }
+            else if constexpr ((flags & perm_rotate_big) != 0) {          // fits full rotate
+                constexpr uint8_t rot = uint8_t(flags >> perm_rot_count); // rotate count
+                constexpr uint8_t r1 = (rot >> 4 << 1) & 7;
+                constexpr uint8_t r2 = (r1 + 2) & 7;
+                __m512i y1 = a, y2 = a;
+                if constexpr (r1 != 0) y1 = _mm512_alignr_epi64(y, y, r1);// rotate 128-bit blocks
+                if constexpr (r2 != 0) y2 = _mm512_alignr_epi64(a, a, r2);// rotate 128-bit blocks
+                y = _mm512_alignr_epi8(y2, y1, rot & 15);
+            }
+            else if constexpr ((flags & perm_broadcast) != 0 && (flags >> perm_rot_count) == 0) {
+                y = _mm512_broadcastb_epi8(_mm512_castsi512_si128(y));    // broadcast first element
+            }
+            else if constexpr ((flags & perm_zext) != 0) {                // fits zero extension
+                y = _mm512_cvtepu8_epi16(_mm512_castsi512_si256(y));      // zero extension
+                if constexpr ((flags & perm_addz2) == 0) return y;
+            }
+#if defined (__AVX512VBMI2__)
+            else if constexpr ((flags & perm_compress) != 0) {
+                y = _mm512_maskz_compress_epi8(__mmask64(compress_mask(indexs)), y); // compress
+                if constexpr ((flags & perm_addz2) == 0) return y;
+            }
+            else if constexpr ((flags & perm_expand) != 0) {
+                y = _mm512_maskz_expand_epi8(__mmask64(expand_mask(indexs)), y); // expand
+                if constexpr ((flags & perm_addz2) == 0) return y;
+            }
+#endif  // AVX512VBMI2
+            else {      // full permute needed
+#ifdef __AVX512VBMI__   // full permute instruction available
+                constexpr EList <int8_t, 64> bm = perm_mask_broad<Vec64c>(indexs);
+                y = _mm512_permutexvar_epi8(Vec64c().load(bm.a), y);
+#else
+                // There is no 8-bit full permute. Use 16-bit permute
+                // getevenmask: get permutation mask for destination bytes with even position
+                auto getevenmask = [](int const (&indexs)[64]) constexpr {
+                    EList<uint16_t, 32> u = {{0}};       // list to return
+                    for (int i = 0; i < 64; i += 2) {    // loop through even indexes
+                        uint16_t ix = indexs[i] & 63;
+                        // source bytes with odd position are in opposite 16-bit word becase of 32-bit rotation
+                        u.a[i>>1] = ((ix >> 1) ^ (ix & 1)) | (((ix & 1) ^ 1) << 5);
+                    }
+                    return u;
+                };
+                // getoddmask: get permutation mask for destination bytes with odd position
+                auto getoddmask = [](int const (&indexs)[64]) constexpr {
+                    EList<uint16_t, 32> u = {{0}};       // list to return
+                    for (int i = 1; i < 64; i += 2) {  // loop through odd indexes
+                        uint16_t ix = indexs[i] & 63;
+                        u.a[i>>1] = (ix >> 1) | ((ix & 1) << 5);
+                    }
+                    return u;
+                };
+                EList<uint16_t, 32> evenmask = getevenmask(indexs);
+                EList<uint16_t, 32> oddmask  = getoddmask (indexs);
+                // Rotate to get odd bytes into even position, and vice versa.
+                // There is no 16-bit rotate, use 32-bit rotate.
+                // The wrong position of the odd bytes is compensated for in getevenmask
+                __m512i ro    = _mm512_rol_epi32 (a, 8);                     // rotate
+                __m512i yeven = _mm512_permutex2var_epi16(ro, Vec32s().load(evenmask.a), a);  // destination bytes with even position
+                __m512i yodd  = _mm512_permutex2var_epi16(ro, Vec32s().load(oddmask.a),  a);  // destination bytes with odd  position
+                __mmask64 maske = 0x5555555555555555;                        // mask for even position
+                y = _mm512_mask_mov_epi8(yodd, maske, yeven);                // interleave even and odd position bytes
+#endif
+            }
+        }
+    }
+    if constexpr ((flags & perm_zeroing) != 0) {      // additional zeroing needed
+        y = _mm512_maskz_mov_epi8(zero_mask<64>(indexs), y);
+    }
+    return y;
+}
+
+template <int... i0 >
+static inline Vec64uc permute64(Vec64uc const a) {
+    return Vec64uc(permute64<i0...>(Vec64c(a)));
+}
+
+
+/*****************************************************************************
+*
+*          Vector blend functions
+*
+*****************************************************************************/
+
+// permute and blend Vec32s
+template <int ... i0 >
+static inline Vec32s blend32(Vec32s const a, Vec32s const b) {
+    int constexpr indexs[32] = { i0 ... }; // indexes as array
+    static_assert(sizeof... (i0) == 32, "blend32 must have 32 indexes");
+    __m512i y = a;                                         // result
+    constexpr uint64_t flags = blend_flags<Vec32s>(indexs);// get flags for possibilities that fit the index pattern
+
+    static_assert((flags & blend_outofrange) == 0, "Index out of range in blend function");
+
+    if constexpr ((flags & blend_allzero) != 0) return _mm512_setzero_si512();  // just return zero
+
+    if constexpr ((flags & blend_b) == 0) {                // nothing from b. just permute a
+        return permute32 <i0 ... >(a);
+    }
+    if constexpr ((flags & blend_a) == 0) {                // nothing from a. just permute b
+        constexpr EList<int, 64> L = blend_perm_indexes<32, 2>(indexs); // get permutation indexes
+        return permute32 <
+            L.a[32], L.a[33], L.a[34], L.a[35], L.a[36], L.a[37], L.a[38], L.a[39],
+            L.a[40], L.a[41], L.a[42], L.a[43], L.a[44], L.a[45], L.a[46], L.a[47],
+            L.a[48], L.a[49], L.a[50], L.a[51], L.a[52], L.a[53], L.a[54], L.a[55],
+            L.a[56], L.a[57], L.a[58], L.a[59], L.a[60], L.a[61], L.a[62], L.a[63] > (b);
+    }
+    if constexpr ((flags & (blend_perma | blend_permb)) == 0) { // no permutation, only blending
+        constexpr uint32_t mb = (uint32_t)make_bit_mask<32, 0x305>(indexs);  // blend mask
+        y = _mm512_mask_mov_epi16(a, mb, b);
+    }
+    else if constexpr ((flags & blend_largeblock) != 0) {  // blend and permute 32-bit blocks
+        constexpr EList<int, 16> L = largeblock_perm<32>(indexs); // get 32-bit blend pattern
+        y = blend16 <L.a[0], L.a[1], L.a[2], L.a[3], L.a[4], L.a[5], L.a[6], L.a[7],
+            L.a[8], L.a[9], L.a[10], L.a[11], L.a[12], L.a[13], L.a[14], L.a[15] >
+            (Vec16i(a), Vec16i(b));
+        if (!(flags & blend_addz)) return y;               // no remaining zeroing
+    }
+    else { // No special cases
+        constexpr EList <int16_t, 32> bm = perm_mask_broad<Vec32s>(indexs);      // full permute
+        y = _mm512_permutex2var_epi16(a, Vec32s().load(bm.a), b);
+    }
+    if constexpr ((flags & blend_zeroing) != 0) {          // additional zeroing needed
+        y = _mm512_maskz_mov_epi16(zero_mask<32>(indexs), y);
+    }
+    return y;
+}
+
+template <int ... i0 >
+    static inline Vec32us blend32(Vec32us const a, Vec32us const b) {
+    return Vec32us(blend32<i0 ...> (Vec32s(a),Vec32s(b)));
+}
+
+    // permute and blend Vec64c
+template <int ... i0 >
+static inline Vec64c blend64(Vec64c const a, Vec64c const b) {
+    int constexpr indexs[64] = { i0 ... }; // indexes as array
+    static_assert(sizeof... (i0) == 64, "blend64 must have 64 indexes");
+    __m512i y = a;                                         // result
+    constexpr uint64_t flags = blend_flags<Vec64c>(indexs);// get flags for possibilities that fit the index pattern
+
+    static_assert((flags & blend_outofrange) == 0, "Index out of range in blend function");
+
+    if constexpr ((flags & blend_allzero) != 0) return _mm512_setzero_si512();  // just return zero
+
+    if constexpr ((flags & blend_b) == 0) {                // nothing from b. just permute a
+        return permute64 <i0 ... >(a);
+    }
+    if constexpr ((flags & blend_a) == 0) {                // nothing from a. just permute b
+        constexpr EList<int, 128> L = blend_perm_indexes<64, 2>(indexs); // get permutation indexes
+        return permute64 <
+            L.a[64],  L.a[65],  L.a[66],  L.a[67],  L.a[68],  L.a[69],  L.a[70],  L.a[71],
+            L.a[72],  L.a[73],  L.a[74],  L.a[75],  L.a[76],  L.a[77],  L.a[78],  L.a[79],
+            L.a[80],  L.a[81],  L.a[82],  L.a[83],  L.a[84],  L.a[85],  L.a[86],  L.a[87],
+            L.a[88],  L.a[89],  L.a[90],  L.a[91],  L.a[92],  L.a[93],  L.a[94],  L.a[95],
+            L.a[96],  L.a[97],  L.a[98],  L.a[99],  L.a[100], L.a[101], L.a[102], L.a[103],
+            L.a[104], L.a[105], L.a[106], L.a[107], L.a[108], L.a[109], L.a[110], L.a[111],
+            L.a[112], L.a[113], L.a[114], L.a[115], L.a[116], L.a[117], L.a[118], L.a[119],
+            L.a[120], L.a[121], L.a[122], L.a[123], L.a[124], L.a[125], L.a[126], L.a[127]
+        > (b);
+    }
+    if constexpr ((flags & (blend_perma | blend_permb)) == 0) { // no permutation, only blending
+        constexpr uint64_t mb = make_bit_mask<64, 0x306>(indexs);  // blend mask
+        y = _mm512_mask_mov_epi8(a, mb, b);
+    }
+    else if constexpr ((flags & blend_largeblock) != 0) {  // blend and permute 16-bit blocks
+        constexpr EList<int, 32> L = largeblock_perm<64>(indexs); // get 16-bit blend pattern
+        y = blend32 <
+            L.a[0],  L.a[1],  L.a[2],  L.a[3],  L.a[4],  L.a[5],  L.a[6],  L.a[7],
+            L.a[8],  L.a[9],  L.a[10], L.a[11], L.a[12], L.a[13], L.a[14], L.a[15],
+            L.a[16], L.a[17], L.a[18], L.a[19], L.a[20], L.a[21], L.a[22], L.a[23],
+            L.a[24], L.a[25], L.a[26], L.a[27], L.a[28], L.a[29], L.a[30], L.a[31]
+        > (Vec32s(a), Vec32s(b));
+        if (!(flags & blend_addz)) return y;               // no remaining zeroing
+    }
+    else { // No special cases
+#ifdef  __AVX512VBMI__   // AVX512VBMI
+        constexpr EList <int8_t, 64> bm = perm_mask_broad<Vec64c>(indexs);      // full permute
+        y = _mm512_permutex2var_epi8(a, Vec64c().load(bm.a), b);
+#else   // split into two permutes
+        constexpr EList<int, 128> L = blend_perm_indexes<64, 0> (indexs);
+        __m512i ya = permute64 <
+            L.a[0],  L.a[1],  L.a[2],  L.a[3],  L.a[4],  L.a[5],  L.a[6],  L.a[7],
+            L.a[8],  L.a[9],  L.a[10], L.a[11], L.a[12], L.a[13], L.a[14], L.a[15],
+            L.a[16], L.a[17], L.a[18], L.a[19], L.a[20], L.a[21], L.a[22], L.a[23],
+            L.a[24], L.a[25], L.a[26], L.a[27], L.a[28], L.a[29], L.a[30], L.a[31],
+            L.a[32], L.a[33], L.a[34], L.a[35], L.a[36], L.a[37], L.a[38], L.a[39],
+            L.a[40], L.a[41], L.a[42], L.a[43], L.a[44], L.a[45], L.a[46], L.a[47],
+            L.a[48], L.a[49], L.a[50], L.a[51], L.a[52], L.a[53], L.a[54], L.a[55],
+            L.a[56], L.a[57], L.a[58], L.a[59], L.a[60], L.a[61], L.a[62], L.a[63]
+        > (a);
+        __m512i yb = permute64 <
+            L.a[64],  L.a[65],  L.a[66],  L.a[67],  L.a[68],  L.a[69],  L.a[70],  L.a[71],
+            L.a[72],  L.a[73],  L.a[74],  L.a[75],  L.a[76],  L.a[77],  L.a[78],  L.a[79],
+            L.a[80],  L.a[81],  L.a[82],  L.a[83],  L.a[84],  L.a[85],  L.a[86],  L.a[87],
+            L.a[88],  L.a[89],  L.a[90],  L.a[91],  L.a[92],  L.a[93],  L.a[94],  L.a[95],
+            L.a[96],  L.a[97],  L.a[98],  L.a[99],  L.a[100], L.a[101], L.a[102], L.a[103],
+            L.a[104], L.a[105], L.a[106], L.a[107], L.a[108], L.a[109], L.a[110], L.a[111],
+            L.a[112], L.a[113], L.a[114], L.a[115], L.a[116], L.a[117], L.a[118], L.a[119],
+            L.a[120], L.a[121], L.a[122], L.a[123], L.a[124], L.a[125], L.a[126], L.a[127]
+        > (b);
+        uint64_t bm = make_bit_mask<64, 0x306> (indexs);
+        y = _mm512_mask_mov_epi8(ya, bm, yb);
+#endif
+    }
+    if constexpr ((flags & blend_zeroing) != 0) {          // additional zeroing needed
+        y = _mm512_maskz_mov_epi8(zero_mask<64>(indexs), y);
+    }
+    return y;
+}
+
+template <int ... i0 >
+static inline Vec64uc blend64(Vec64uc const a, Vec64uc const b) {
+    return Vec64uc(blend64 <i0 ...>(Vec64c(a), Vec64c(b)));
+}
+
+
+/*****************************************************************************
+*
+*          Vector lookup functions
+*
+******************************************************************************
+*
+* These functions use vector elements as indexes into a table.
+* The table is given as one or more vectors
+*
+*****************************************************************************/
+
+// lookup in table of 64 int8_t values
+static inline Vec64c lookup64(Vec64c const index, Vec64c const table) {
+#ifdef  __AVX512VBMI__   // AVX512VBMI instruction set not supported yet (April 2019)
+    return _mm512_permutexvar_epi8(index, table);
+#else
+    // broadcast each 128-bit lane, because int8_t shuffle is only within 128-bit lanes
+    __m512i lane0 = _mm512_broadcast_i32x4(_mm512_castsi512_si128(table));
+    __m512i lane1 = _mm512_shuffle_i64x2(table, table, 0x55);
+    __m512i lane2 = _mm512_shuffle_i64x2(table, table, 0xAA);
+    __m512i lane3 = _mm512_shuffle_i64x2(table, table, 0xFF);
+    Vec64c  laneselect = index >> 4;  // upper part of index selects lane
+    // select and permute from each lane
+    Vec64c  dat0  = _mm512_maskz_shuffle_epi8(      laneselect==0, lane0, index);
+    Vec64c  dat1  = _mm512_mask_shuffle_epi8 (dat0, laneselect==1, lane1, index);
+    Vec64c  dat2  = _mm512_maskz_shuffle_epi8(      laneselect==2, lane2, index);
+    Vec64c  dat3  = _mm512_mask_shuffle_epi8 (dat2, laneselect==3, lane3, index);
+    return dat1 | dat3;
+#endif
+}
+
+// lookup in table of 128 int8_t values
+static inline Vec64c lookup128(Vec64c const index, Vec64c const table1, Vec64c const table2) {
+#ifdef  __AVX512VBMI__   // AVX512VBMI instruction set not supported yet (April 2019)
+    return _mm512_permutex2var_epi8(table1, index, table2);
+
+#else
+    // use 16-bits permute, which is included in AVX512BW
+    __m512i ieven2 = _mm512_srli_epi16 (index, 1);              // even pos bytes of index / 2 (extra bits will be ignored)
+    __m512i e1 = _mm512_permutex2var_epi16(table1, ieven2, table2); // 16-bits results for even pos index
+    __mmask32 me1 = (Vec32s(index) & 1) != 0;                   // even pos indexes are odd value
+    __m512i e2 = _mm512_mask_srli_epi16(e1, me1, e1, 8);        // combined results for even pos index. get upper 8 bits down if index was odd
+    __m512i iodd2  = _mm512_srli_epi16 (index, 9);              // odd  pos bytes of index / 2
+    __m512i o1 = _mm512_permutex2var_epi16(table1, iodd2, table2); // 16-bits results for odd pos index
+    __mmask32 mo1 = (Vec32s(index) & 0x100) == 0;               // odd pos indexes have even value
+    __m512i o2 = _mm512_mask_slli_epi16(o1, mo1, o1, 8);        // combined results for odd pos index. get lower 8 bits up if index was even
+    __mmask64 maske = 0x5555555555555555;                       // mask for even position
+    return  _mm512_mask_mov_epi8(o2, maske, e2);                // interleave even and odd position result
+#endif
+}
+
+// lookup in table of 256 int8_t values.
+// The complete table of all possible 256 byte values is contained in four vectors
+// The index is treated as unsigned
+static inline Vec64c lookup256(Vec64c const index, Vec64c const table1, Vec64c const table2, Vec64c const table3, Vec64c const table4) {
+#ifdef  __AVX512VBMI__   // AVX512VBMI instruction set not supported yet (April 2019)
+    Vec64c d12 = _mm512_permutex2var_epi8(table1, index, table2);
+    Vec64c d34 = _mm512_permutex2var_epi8(table3, index, table4);
+    return select(index < 0, d34, d12);  // use sign bit to select
+#else
+    // the AVX512BW version of lookup128 ignores upper bytes of index
+    // (the compiler will optimize away common subexpressions of the two lookup128)
+    Vec64c d12 = lookup128(index, table1, table2);
+    Vec64c d34 = lookup128(index, table3, table4);
+    return select(index < 0, d34, d12);
+#endif
+}
+
+
+// lookup in table of 32 values
+static inline Vec32s lookup32(Vec32s const index, Vec32s const table) {
+    return _mm512_permutexvar_epi16(index, table);
+}
+
+// lookup in table of 64 values
+static inline Vec32s lookup64(Vec32s const index, Vec32s const table1, Vec32s const table2) {
+    return _mm512_permutex2var_epi16(table1, index, table2);
+}
+
+// lookup in table of 128 values
+static inline Vec32s lookup128(Vec32s const index, Vec32s const table1, Vec32s const table2, Vec32s const table3, Vec32s const table4) {
+    Vec32s d12 = _mm512_permutex2var_epi16(table1, index, table2);
+    Vec32s d34 = _mm512_permutex2var_epi16(table3, index, table4);
+    return select((index >> 6) != 0, d34, d12);
+}
+
+
+/*****************************************************************************
+*
+*          Byte shifts
+*
+*****************************************************************************/
+
+// Function shift_bytes_up: shift whole vector left by b bytes.
+template <unsigned int b>
+static inline Vec64c shift_bytes_up(Vec64c const a) {
+    __m512i ahi, alo;
+    if constexpr (b == 0) return a;
+    else if constexpr ((b & 3) == 0) {  // b is divisible by 4
+        return _mm512_alignr_epi32(a, _mm512_setzero_si512(), (16 - (b >> 2)) & 15);
+    }
+    else if constexpr (b < 16) {
+        alo = a;
+        ahi = _mm512_maskz_shuffle_i64x2(0xFC, a, a, 0x90);  // shift a 16 bytes up, zero lower part
+    }
+    else if constexpr (b < 32) {
+        alo = _mm512_maskz_shuffle_i64x2(0xFC, a, a, 0x90);  // shift a 16 bytes up, zero lower part
+        ahi = _mm512_maskz_shuffle_i64x2(0xF0, a, a, 0x40);  // shift a 32 bytes up, zero lower part
+    }
+    else if constexpr (b < 48) {
+        alo = _mm512_maskz_shuffle_i64x2(0xF0, a, a, 0x40);  // shift a 32 bytes up, zero lower part
+        ahi = _mm512_maskz_shuffle_i64x2(0xC0, a, a, 0x00);  // shift a 48 bytes up, zero lower part
+    }
+    else if constexpr (b < 64) {
+        alo = _mm512_maskz_shuffle_i64x2(0xC0, a, a, 0x00);  // shift a 48 bytes up, zero lower part
+        ahi = _mm512_setzero_si512();                        // zero
+    }
+    else {
+        return _mm512_setzero_si512();                       // zero
+    }
+    return _mm512_alignr_epi8(alo, ahi, 16-(b & 0xF));       // shift within 16-bytes lane
+}
+
+// Function shift_bytes_down: shift whole vector right by b bytes
+template <unsigned int b>
+static inline Vec64c shift_bytes_down(Vec64c const a) {
+    if constexpr ((b & 3) == 0) {  // b is divisible by 4
+        return _mm512_alignr_epi32(_mm512_setzero_si512(), a, ((b >> 2) & 15));
+    }
+    __m512i ahi, alo;
+    if constexpr (b < 16) {
+        alo =  _mm512_maskz_shuffle_i64x2(0x3F, a, a, 0x39);  // shift a 16 bytes down, zero upper part
+        ahi = a;
+    }
+    else if constexpr (b < 32) {
+        alo = _mm512_maskz_shuffle_i64x2(0x0F, a, a, 0x0E);  // shift a 32 bytes down, zero upper part
+        ahi = _mm512_maskz_shuffle_i64x2(0x3F, a, a, 0x39);  // shift a 16 bytes down, zero upper part
+    }
+    else if constexpr (b < 48) {
+        alo = _mm512_maskz_shuffle_i64x2(0x03, a, a, 0x03);  // shift a 48 bytes down, zero upper part
+        ahi = _mm512_maskz_shuffle_i64x2(0x0F, a, a, 0x0E);  // shift a 32 bytes down, zero upper part
+    }
+    else if constexpr (b < 64) {
+        alo = _mm512_setzero_si512();
+        ahi = _mm512_maskz_shuffle_i64x2(0x03, a, a, 0x03);  // shift a 48 bytes down, zero upper part
+    }
+    else {
+        return _mm512_setzero_si512();                       // zero
+    }
+    return _mm512_alignr_epi8(alo, ahi, b & 0xF);            // shift within 16-bytes lane
+}
+
+
+/*****************************************************************************
+*
+*          Functions for conversion between integer sizes and vector types
+*
+*****************************************************************************/
+
+// Extend 8-bit integers to 16-bit integers, signed and unsigned
+
+// Function extend_low : extends the low 32 elements to 16 bits with sign extension
+static inline Vec32s extend_low (Vec64c const a) {
+    __m512i a2   = permute8<0,V_DC,1,V_DC,2,V_DC,3,V_DC>(Vec8q(a));  // get low 64-bit blocks
+    Vec64cb sign = _mm512_cmpgt_epi8_mask(_mm512_setzero_si512(),a2);// 0 > a2
+    __m512i ss   = _mm512_maskz_set1_epi8(sign, -1);
+    return         _mm512_unpacklo_epi8(a2, ss);                     // interleave with sign extensions
+}
+
+// Function extend_high : extends the high 16 elements to 16 bits with sign extension
+static inline Vec32s extend_high (Vec64c const a) {
+    __m512i a2   = permute8<4,V_DC,5,V_DC,6,V_DC,7,V_DC>(Vec8q(a));  // get low 64-bit blocks
+    Vec64cb sign = _mm512_cmpgt_epi8_mask(_mm512_setzero_si512(),a2);// 0 > a2
+    __m512i ss   = _mm512_maskz_set1_epi8(sign, -1);
+    return         _mm512_unpacklo_epi8(a2, ss);                     // interleave with sign extensions
+}
+
+// Function extend_low : extends the low 16 elements to 16 bits with zero extension
+static inline Vec32us extend_low (Vec64uc const a) {
+    __m512i a2   = permute8<0,V_DC,1,V_DC,2,V_DC,3,V_DC>(Vec8q(a));  // get low 64-bit blocks
+    return    _mm512_unpacklo_epi8(a2, _mm512_setzero_si512());      // interleave with zero extensions
+}
+
+// Function extend_high : extends the high 19 elements to 16 bits with zero extension
+static inline Vec32us extend_high (Vec64uc const a) {
+    __m512i a2   = permute8<4,V_DC,5,V_DC,6,V_DC,7,V_DC>(Vec8q(a));  // get low 64-bit blocks
+    return    _mm512_unpacklo_epi8(a2, _mm512_setzero_si512());      // interleave with zero extensions
+}
+
+// Extend 16-bit integers to 32-bit integers, signed and unsigned
+
+// Function extend_low : extends the low 8 elements to 32 bits with sign extension
+static inline Vec16i extend_low (Vec32s const a) {
+    __m512i a2   = permute8<0,V_DC,1,V_DC,2,V_DC,3,V_DC>(Vec8q(a));  // get low 64-bit blocks
+    Vec32sb sign = _mm512_cmpgt_epi16_mask(_mm512_setzero_si512(),a2);// 0 > a2
+    __m512i ss   = _mm512_maskz_set1_epi16(sign, -1);
+    return         _mm512_unpacklo_epi16(a2, ss);                    // interleave with sign extensions
+}
+
+// Function extend_high : extends the high 8 elements to 32 bits with sign extension
+static inline Vec16i extend_high (Vec32s const a) {
+    __m512i a2   = permute8<4,V_DC,5,V_DC,6,V_DC,7,V_DC>(Vec8q(a));  // get low 64-bit blocks
+    Vec32sb sign = _mm512_cmpgt_epi16_mask(_mm512_setzero_si512(),a2);// 0 > a2
+    __m512i ss   = _mm512_maskz_set1_epi16(sign, -1);
+    return         _mm512_unpacklo_epi16(a2, ss);                    // interleave with sign extensions
+}
+
+// Function extend_low : extends the low 8 elements to 32 bits with zero extension
+static inline Vec16ui extend_low (Vec32us const a) {
+    __m512i a2   = permute8<0,V_DC,1,V_DC,2,V_DC,3,V_DC>(Vec8q(a));  // get low 64-bit blocks
+    return    _mm512_unpacklo_epi16(a2, _mm512_setzero_si512());     // interleave with zero extensions
+}
+
+// Function extend_high : extends the high 8 elements to 32 bits with zero extension
+static inline Vec16ui extend_high (Vec32us const a) {
+    __m512i a2   = permute8<4,V_DC,5,V_DC,6,V_DC,7,V_DC>(Vec8q(a));  // get low 64-bit blocks
+    return    _mm512_unpacklo_epi16(a2, _mm512_setzero_si512());     // interleave with zero extensions
+}
+
+
+// Compress 16-bit integers to 8-bit integers, signed and unsigned, with and without saturation
+
+// Function compress : packs two vectors of 16-bit integers into one vector of 8-bit integers
+// Overflow wraps around
+static inline Vec64c compress (Vec32s const low, Vec32s const high) {
+    __mmask64 mask = 0x5555555555555555;
+    __m512i lowm  = _mm512_maskz_mov_epi8 (mask, low);     // bytes of low
+    __m512i highm = _mm512_maskz_mov_epi8 (mask, high);    // bytes of high
+    __m512i pk    = _mm512_packus_epi16(lowm, highm);      // unsigned pack
+    __m512i in    = constant16ui<0,0,2,0,4,0,6,0,1,0,3,0,5,0,7,0>();
+    return  _mm512_permutexvar_epi64(in, pk);              // put in right place
+}
+
+// Function compress : packs two vectors of 16-bit integers into one vector of 8-bit integers
+// Signed, with saturation
+static inline Vec64c compress_saturated (Vec32s const low, Vec32s const high) {
+    __m512i pk    = _mm512_packs_epi16(low,high);          // packed with signed saturation
+    __m512i in    = constant16ui<0,0,2,0,4,0,6,0,1,0,3,0,5,0,7,0>();
+    return  _mm512_permutexvar_epi64(in, pk);              // put in right place
+}
+
+// Function compress : packs two vectors of 16-bit integers to one vector of 8-bit integers
+// Unsigned, overflow wraps around
+static inline Vec64uc compress (Vec32us const low, Vec32us const high) {
+    return  Vec64uc (compress((Vec32s)low, (Vec32s)high));
+}
+
+// Function compress : packs two vectors of 16-bit integers into one vector of 8-bit integers
+// Unsigned, with saturation
+static inline Vec64uc compress_saturated (Vec32us const low, Vec32us const high) {
+    __m512i maxval  = _mm512_set1_epi32(0x00FF00FF);       // maximum value
+    __m512i low1    = _mm512_min_epu16(low,maxval);        // upper limit
+    __m512i high1   = _mm512_min_epu16(high,maxval);       // upper limit
+    __m512i pk      = _mm512_packus_epi16(low1,high1);     // this instruction saturates from signed 32 bit to unsigned 16 bit
+    __m512i in    = constant16ui<0,0,2,0,4,0,6,0,1,0,3,0,5,0,7,0>();
+    return  _mm512_permutexvar_epi64(in, pk);              // put in right place
+}
+
+// Compress 32-bit integers to 16-bit integers, signed and unsigned, with and without saturation
+
+// Function compress : packs two vectors of 32-bit integers into one vector of 16-bit integers
+// Overflow wraps around
+static inline Vec32s compress (Vec16i const low, Vec16i const high) {
+    __mmask32 mask = 0x55555555;
+    __m512i lowm  = _mm512_maskz_mov_epi16 (mask, low);    // words of low
+    __m512i highm = _mm512_maskz_mov_epi16 (mask, high);   // words of high
+    __m512i pk    = _mm512_packus_epi32(lowm, highm);      // unsigned pack
+    __m512i in    = constant16ui<0,0,2,0,4,0,6,0,1,0,3,0,5,0,7,0>();
+    return  _mm512_permutexvar_epi64(in, pk);              // put in right place
+}
+
+// Function compress : packs two vectors of 32-bit integers into one vector of 16-bit integers
+// Signed with saturation
+static inline Vec32s compress_saturated (Vec16i const low, Vec16i const high) {
+    __m512i pk    =  _mm512_packs_epi32(low,high);         // pack with signed saturation
+    __m512i in    = constant16ui<0,0,2,0,4,0,6,0,1,0,3,0,5,0,7,0>();
+    return  _mm512_permutexvar_epi64(in, pk);              // put in right place
+}
+
+// Function compress : packs two vectors of 32-bit integers into one vector of 16-bit integers
+// Overflow wraps around
+static inline Vec32us compress (Vec16ui const low, Vec16ui const high) {
+    return Vec32us (compress((Vec16i)low, (Vec16i)high));
+}
+
+// Function compress : packs two vectors of 32-bit integers into one vector of 16-bit integers
+// Unsigned, with saturation
+static inline Vec32us compress_saturated (Vec16ui const low, Vec16ui const high) {
+    __m512i maxval  = _mm512_set1_epi32(0x0000FFFF);       // maximum value
+    __m512i low1    = _mm512_min_epu32(low,maxval);        // upper limit
+    __m512i high1   = _mm512_min_epu32(high,maxval);       // upper limit
+    __m512i pk      = _mm512_packus_epi32(low1,high1);     // this instruction saturates from signed 32 bit to unsigned 16 bit
+    __m512i in    = constant16ui<0,0,2,0,4,0,6,0,1,0,3,0,5,0,7,0>();
+    return  _mm512_permutexvar_epi64(in, pk);              // put in right place
+}
+
+#ifdef ZEXT_MISSING
+// GCC v. 9 and earlier are missing the _mm512_zextsi256_si512 intrinsic
+
+// extend vectors to double size by adding zeroes
+static inline Vec64c extend_z(Vec32c a) {
+    return Vec64c(a, Vec32c(0));
+}
+static inline Vec64uc extend_z(Vec32uc a) {
+    return Vec64uc(a, Vec32uc(0));
+}
+static inline Vec32s extend_z(Vec16s a) {
+    return Vec32s(a, Vec16s(0));
+}
+static inline Vec32us extend_z(Vec16us a) {
+    return Vec32us(a, Vec16us(0));
+}
+#else
+// extend vectors to double size by adding zeroes
+static inline Vec64c extend_z(Vec32c a) {
+    return _mm512_zextsi256_si512(a);
+}
+static inline Vec64uc extend_z(Vec32uc a) {
+    return _mm512_zextsi256_si512(a);
+}
+static inline Vec32s extend_z(Vec16s a) {
+    return _mm512_zextsi256_si512(a);
+}
+static inline Vec32us extend_z(Vec16us a) {
+    return _mm512_zextsi256_si512(a);
+}
+#endif
+
+// compact boolean vectors
+
+static inline Vec64b extend_z(Vec32b a) {
+    return __mmask64(__mmask32(a));
+}
+//static inline Vec32sb extend_z(Vec16sb a); same as Vec32cb extend_z(Vec16cb a) {
+
+
+/*****************************************************************************
+*
+*          Integer division operators
+*
+*          Please see the file vectori128.h for explanation.
+*
+*****************************************************************************/
+
+// vector operator / : divide each element by divisor
+
+// vector of 32 16-bit signed integers
+static inline Vec32s operator / (Vec32s const a, Divisor_s const d) {
+    __m512i m   = _mm512_broadcastq_epi64(d.getm());       // broadcast multiplier
+    __m512i sgn = _mm512_broadcastq_epi64(d.getsign());    // broadcast sign of d
+    __m512i t1  = _mm512_mulhi_epi16(a, m);                // multiply high signed words
+    __m512i t2  = _mm512_add_epi16(t1,a);                  // + a
+    __m512i t3  = _mm512_sra_epi16(t2,d.gets1());          // shift right artihmetic
+    __m512i t4  = _mm512_srai_epi16(a,15);                 // sign of a
+    __m512i t5  = _mm512_sub_epi16(t4,sgn);                // sign of a - sign of d
+    __m512i t6  = _mm512_sub_epi16(t3,t5);                 // + 1 if a < 0, -1 if d < 0
+    return        _mm512_xor_si512(t6,sgn);                // change sign if divisor negative
+}
+
+// vector of 16 16-bit unsigned integers
+static inline Vec32us operator / (Vec32us const a, Divisor_us const d) {
+    __m512i m   = _mm512_broadcastq_epi64(d.getm());       // broadcast multiplier
+    __m512i t1  = _mm512_mulhi_epu16(a, m);                // multiply high signed words
+    __m512i t2  = _mm512_sub_epi16(a,t1);                  // subtract
+    __m512i t3  = _mm512_srl_epi16(t2,d.gets1());          // shift right logical
+    __m512i t4  = _mm512_add_epi16(t1,t3);                 // add
+    return        _mm512_srl_epi16(t4,d.gets2());          // shift right logical
+}
+
+// vector of 32 8-bit signed integers
+static inline Vec64c operator / (Vec64c const a, Divisor_s const d) {
+    // sign-extend even-numbered and odd-numbered elements to 16 bits
+    Vec32s  even = _mm512_srai_epi16(_mm512_slli_epi16(a, 8),8);
+    Vec32s  odd  = _mm512_srai_epi16(a, 8);
+    Vec32s  evend = even / d;         // divide even-numbered elements
+    Vec32s  oddd  = odd  / d;         // divide odd-numbered  elements
+            oddd  = _mm512_slli_epi16(oddd, 8); // shift left to put back in place
+    __m512i res  = _mm512_mask_mov_epi8(evend, 0xAAAAAAAAAAAAAAAA, oddd); // interleave even and odd
+    return res;
+}
+
+// vector of 32 8-bit unsigned integers
+static inline Vec64uc operator / (Vec64uc const a, Divisor_us const d) {
+    // zero-extend even-numbered and odd-numbered elements to 16 bits
+    Vec32us  even = _mm512_maskz_mov_epi8(__mmask64(0x5555555555555555), a);
+    Vec32us  odd  = _mm512_srli_epi16(a, 8);
+    Vec32us  evend = even / d;         // divide even-numbered elements
+    Vec32us  oddd  = odd  / d;         // divide odd-numbered  elements
+    oddd  = _mm512_slli_epi16(oddd, 8); // shift left to put back in place
+    __m512i res  = _mm512_mask_mov_epi8(evend, 0xAAAAAAAAAAAAAAAA, oddd); // interleave even and odd
+    return res;
+}
+
+// vector operator /= : divide
+static inline Vec32s & operator /= (Vec32s & a, Divisor_s const d) {
+    a = a / d;
+    return a;
+}
+
+// vector operator /= : divide
+static inline Vec32us & operator /= (Vec32us & a, Divisor_us const d) {
+    a = a / d;
+    return a;
+
+}
+
+// vector operator /= : divide
+static inline Vec64c & operator /= (Vec64c & a, Divisor_s const d) {
+    a = a / d;
+    return a;
+}
+
+// vector operator /= : divide
+static inline Vec64uc & operator /= (Vec64uc & a, Divisor_us const d) {
+    a = a / d;
+    return a;
+}
+
+/*****************************************************************************
+*
+*          Integer division 2: divisor is a compile-time constant
+*
+*****************************************************************************/
+
+// Divide Vec32s by compile-time constant
+template <int d>
+static inline Vec32s divide_by_i(Vec32s const x) {
+    constexpr int16_t d0 = int16_t(d);                               // truncate d to 16 bits
+    static_assert(d0 != 0, "Integer division by zero");
+    if constexpr (d0 ==  1) return  x;                               // divide by  1
+    if constexpr (d0 == -1) return -x;                               // divide by -1
+    if constexpr (uint16_t(d0) == 0x8000u) {
+        return _mm512_maskz_set1_epi16(x == Vec32s((int16_t)0x8000u), 1); // avoid overflow of abs(d). return (x == 0x80000000) ? 1 : 0;
+    }
+    constexpr uint16_t d1 = d0 > 0 ? d0 : -d0;                       // compile-time abs(d0)
+    if constexpr ((d1 & (d1-1)) == 0) {
+        // d is a power of 2. use shift
+        constexpr int k = bit_scan_reverse_const(uint32_t(d1));
+        __m512i sign;
+        if constexpr (k > 1) sign = _mm512_srai_epi16(x, k-1); else sign = x;  // k copies of sign bit
+        __m512i bias    = _mm512_srli_epi16(sign, 16-k);             // bias = x >= 0 ? 0 : k-1
+        __m512i xpbias  = _mm512_add_epi16 (x, bias);                // x + bias
+        __m512i q       = _mm512_srai_epi16(xpbias, k);              // (x + bias) >> k
+        if (d0 > 0)  return q;                                       // d0 > 0: return  q
+        return _mm512_sub_epi16(_mm512_setzero_si512(), q);          // d0 < 0: return -q
+    }
+    // general case
+    constexpr int L = bit_scan_reverse_const(uint16_t(d1-1)) + 1;        // ceil(log2(d)). (d < 2 handled above)
+    constexpr int16_t mult = int16_t(1 + (1u << (15+L)) / uint32_t(d1) - 0x10000);// multiplier
+    constexpr int shift1 = L - 1;
+    const Divisor_s div(mult, shift1, d0 > 0 ? 0 : -1);
+    return x / div;
+}
+
+// define Vec32s a / const_int(d)
+template <int d>
+static inline Vec32s operator / (Vec32s const a, Const_int_t<d>) {
+    return divide_by_i<d>(a);
+}
+
+// define Vec32s a / const_uint(d)
+template <uint32_t d>
+static inline Vec32s operator / (Vec32s const a, Const_uint_t<d>) {
+    static_assert(d < 0x8000u, "Dividing signed integer by overflowing unsigned");
+    return divide_by_i<int(d)>(a);                                   // signed divide
+}
+
+// vector operator /= : divide
+template <int32_t d>
+static inline Vec32s & operator /= (Vec32s & a, Const_int_t<d> b) {
+    a = a / b;
+    return a;
+}
+
+// vector operator /= : divide
+template <uint32_t d>
+static inline Vec32s & operator /= (Vec32s & a, Const_uint_t<d> b) {
+    a = a / b;
+    return a;
+}
+
+// Divide Vec32us by compile-time constant
+template <uint32_t d>
+static inline Vec32us divide_by_ui(Vec32us const x) {
+    constexpr uint16_t d0 = uint16_t(d);                             // truncate d to 16 bits
+    static_assert(d0 != 0, "Integer division by zero");
+    if constexpr (d0 == 1) return x;                                 // divide by 1
+    constexpr int b = bit_scan_reverse_const(d0);                    // floor(log2(d))
+    if constexpr ((d0 & (d0-1)) == 0) {
+        // d is a power of 2. use shift
+        return  _mm512_srli_epi16(x, b);                             // x >> b
+    }
+    // general case (d > 2)
+    constexpr uint16_t mult = uint16_t((uint32_t(1) << (b+16)) / d0);// multiplier = 2^(32+b) / d
+    constexpr uint32_t rem = (uint32_t(1) << (b+16)) - uint32_t(d0)*mult;// remainder 2^(32+b) % d
+    constexpr bool round_down = (2*rem < d0);                        // check if fraction is less than 0.5
+    Vec32us x1 = x;
+    if constexpr (round_down) {
+        x1 = x1 + 1;                                                 // round down mult and compensate by adding 1 to x
+    }
+    constexpr uint16_t mult1 = round_down ? mult : mult + 1;
+    const __m512i multv = _mm512_set1_epi16(mult1);                  // broadcast mult
+    __m512i xm = _mm512_mulhi_epu16(x1, multv);                      // high part of 16x16->32 bit unsigned multiplication
+    Vec32us q    = _mm512_srli_epi16(xm, b);                         // shift right by b
+    if constexpr (round_down) {
+        Vec32sb overfl = (x1 == Vec32us(_mm512_setzero_si512()));    // check for overflow of x+1
+        return select(overfl, Vec32us(uint32_t(mult1 >> b)), q);     // deal with overflow (rarely needed)
+    }
+    else {
+        return q;                                                    // no overflow possible
+    }
+}
+
+// define Vec32us a / const_uint(d)
+template <uint32_t d>
+static inline Vec32us operator / (Vec32us const a, Const_uint_t<d>) {
+    return divide_by_ui<d>(a);
+}
+
+// define Vec32us a / const_int(d)
+template <int d>
+static inline Vec32us operator / (Vec32us const a, Const_int_t<d>) {
+    static_assert(d >= 0, "Dividing unsigned integer by negative is ambiguous");
+    return divide_by_ui<d>(a);                             // unsigned divide
+}
+
+// vector operator /= : divide
+template <uint32_t d>
+static inline Vec32us & operator /= (Vec32us & a, Const_uint_t<d> b) {
+    a = a / b;
+    return a;
+}
+
+// vector operator /= : divide
+template <int32_t d>
+static inline Vec32us & operator /= (Vec32us & a, Const_int_t<d> b) {
+    a = a / b;
+    return a;
+}
+
+
+// define Vec64c a / const_int(d)
+template <int d>
+static inline Vec64c operator / (Vec64c const a, Const_int_t<d>) {
+    // expand into two Vec32s
+    Vec32s low  = extend_low(a)  / Const_int_t<d>();
+    Vec32s high = extend_high(a) / Const_int_t<d>();
+    return compress(low,high);
+}
+
+// define Vec64c a / const_uint(d)
+template <uint32_t d>
+static inline Vec64c operator / (Vec64c const a, Const_uint_t<d>) {
+    static_assert(uint8_t(d) < 0x80u, "Dividing signed integer by overflowing unsigned");
+    return a / Const_int_t<d>();                           // signed divide
+}
+
+// vector operator /= : divide
+template <int32_t d>
+static inline Vec64c & operator /= (Vec64c & a, Const_int_t<d> b) {
+    a = a / b;
+    return a;
+}
+// vector operator /= : divide
+template <uint32_t d>
+static inline Vec64c & operator /= (Vec64c & a, Const_uint_t<d> b) {
+    a = a / b;
+    return a;
+}
+
+// define Vec64uc a / const_uint(d)
+template <uint32_t d>
+static inline Vec64uc operator / (Vec64uc const a, Const_uint_t<d>) {
+    // expand into two Vec32us
+    Vec32us low  = extend_low(a)  / Const_uint_t<d>();
+    Vec32us high = extend_high(a) / Const_uint_t<d>();
+    return compress(low,high);
+}
+
+// define Vec64uc a / const_int(d)
+template <int d>
+static inline Vec64uc operator / (Vec64uc const a, Const_int_t<d>) {
+    static_assert(int8_t(d) >= 0, "Dividing unsigned integer by negative is ambiguous");
+    return a / Const_uint_t<d>();                          // unsigned divide
+}
+
+// vector operator /= : divide
+template <uint32_t d>
+static inline Vec64uc & operator /= (Vec64uc & a, Const_uint_t<d> b) {
+    a = a / b;
+    return a;
+}
+
+// vector operator /= : divide
+template <int32_t d>
+static inline Vec64uc & operator /= (Vec64uc & a, Const_int_t<d> b) {
+    a = a / b;
+    return a;
+}
+
+#ifdef VCL_NAMESPACE
+}
+#endif
+
+#endif // VECTORI512S_H
diff --git a/EEDI3/vectorclass/vectori512se.h b/EEDI3/vectorclass/vectori512se.h
new file mode 100644
index 0000000..178662f
--- /dev/null
+++ b/EEDI3/vectorclass/vectori512se.h
@@ -0,0 +1,2095 @@
+/****************************  vectori512se.h   *******************************
+* Author:        Agner Fog
+* Date created:  2019-04-20
+* Last modified: 2022-07-20
+* Version:       2.02.00
+* Project:       vector class library
+* Description:
+* Header file defining 512-bit integer vector classes for 8 and 16 bit integers.
+* Emulated for processors without AVX512BW instruction set.
+*
+* Instructions: see vcl_manual.pdf
+*
+* The following vector classes are defined here:
+* Vec64c    Vector of  64  8-bit  signed   integers
+* Vec64uc   Vector of  64  8-bit  unsigned integers
+* Vec64cb   Vector of  64  booleans for use with Vec64c and Vec64uc
+* Vec32s    Vector of  32  16-bit signed   integers
+* Vec32us   Vector of  32  16-bit unsigned integers
+* Vec32sb   Vector of  32  booleans for use with Vec32s and Vec32us
+* Other 512-bit integer vectors are defined in Vectori512.h
+*
+* Each vector object is represented internally in the CPU as two 256-bit registers.
+* This header file defines operators and functions for these vectors.
+*
+* (c) Copyright 2012-2022 Agner Fog.
+* Apache License version 2.0 or later.
+******************************************************************************/
+
+#ifndef VECTORI512SE_H
+#define VECTORI512SE_H
+
+#ifndef VECTORCLASS_H
+#include "vectorclass.h"
+#endif
+
+#if VECTORCLASS_H < 20200
+#error Incompatible versions of vector class library mixed
+#endif
+
+// check combination of header files
+#ifdef VECTORI512S_H
+#error Two different versions of vectorf256.h included
+#endif
+
+
+#ifdef VCL_NAMESPACE
+namespace VCL_NAMESPACE {
+#endif
+
+
+/*****************************************************************************
+*
+*          Vector of 64 8-bit signed integers
+*
+*****************************************************************************/
+
+class Vec64c  {
+protected:
+    Vec256b z0;          // lower 256 bits
+    Vec256b z1;          // higher 256 bits
+public:
+    // Default constructor:
+    Vec64c() = default;
+    // Constructor to build from two Vec32c:
+    Vec64c(Vec32c const a0, Vec32c const a1) {
+        z0 = a0;
+        z1 = a1;
+    }
+    // Constructor to broadcast the same value into all elements:
+    Vec64c(int8_t i) {
+        z0 = z1 = Vec32c(i);
+    }
+    // Constructor to build from all elements:
+    Vec64c(int8_t i0, int8_t i1, int8_t i2, int8_t i3, int8_t i4, int8_t i5, int8_t i6, int8_t i7,
+        int8_t i8, int8_t i9, int8_t i10, int8_t i11, int8_t i12, int8_t i13, int8_t i14, int8_t i15,
+        int8_t i16, int8_t i17, int8_t i18, int8_t i19, int8_t i20, int8_t i21, int8_t i22, int8_t i23,
+        int8_t i24, int8_t i25, int8_t i26, int8_t i27, int8_t i28, int8_t i29, int8_t i30, int8_t i31,
+        int8_t i32, int8_t i33, int8_t i34, int8_t i35, int8_t i36, int8_t i37, int8_t i38, int8_t i39,
+        int8_t i40, int8_t i41, int8_t i42, int8_t i43, int8_t i44, int8_t i45, int8_t i46, int8_t i47,
+        int8_t i48, int8_t i49, int8_t i50, int8_t i51, int8_t i52, int8_t i53, int8_t i54, int8_t i55,
+        int8_t i56, int8_t i57, int8_t i58, int8_t i59, int8_t i60, int8_t i61, int8_t i62, int8_t i63) {
+        // _mm512_set_epi8 and _mm512_set_epi16 missing in GCC 7.4.0
+        int8_t aa[64] = {
+            i0, i1, i2, i3, i4, i5, i6, i7,i8, i9, i10, i11, i12, i13, i14, i15,
+            i16, i17, i18, i19, i20, i21, i22, i23, i24, i25, i26, i27, i28, i29, i30, i31,
+            i32, i33, i34, i35, i36, i37, i38, i39, i40, i41, i42, i43, i44, i45, i46, i47,
+            i48, i49, i50, i51, i52, i53, i54, i55, i56, i57, i58, i59, i60, i61, i62, i63 };
+        load(aa);
+    }
+#ifdef VECTORI512_H
+    // Constructor to convert from type __m512i used in intrinsics:
+    Vec64c(__m512i const x) {
+        z0 = Vec16i(x).get_low();
+        z1 = Vec16i(x).get_high();
+    }
+    // Assignment operator to convert from type __m512i used in intrinsics:
+    Vec64c & operator = (__m512i const x) {
+        return *this = Vec64c(x);
+    }
+    // Type cast operator to convert to __m512i used in intrinsics
+    operator __m512i() const {
+        return Vec16i(Vec8i(z0),Vec8i(z1));
+    }
+#else
+    // Assignment operator to convert from type __m512i used in intrinsics:
+    Vec64c & operator = (Vec512b const x) {
+        z0 = x.get_low();
+        z1 = x.get_high();
+        return *this;
+    }
+#endif
+    // Constructor to convert from type Vec512b
+    Vec64c(Vec512b const x) {
+        z0 = x.get_low();
+        z1 = x.get_high();
+     }
+    // Type cast operator to convert to Vec512b used in emulation
+    operator Vec512b() const {
+        return Vec512b(z0,z1);
+    }
+    // Member function to load from array (unaligned)
+    Vec64c & load(void const * p) {
+        Vec16i x = Vec16i().load(p);
+        z0 = x.get_low();
+        z1 = x.get_high();
+        return *this;
+    }
+    // Member function to load from array, aligned by 64
+    Vec64c & load_a(void const * p) {
+        Vec16i x = Vec16i().load_a(p);
+        z0 = x.get_low();
+        z1 = x.get_high();
+        return *this;
+    }
+    // Partial load. Load n elements and set the rest to 0
+    Vec64c & load_partial(int n, void const * p) {
+        Vec32c lo, hi;
+        if ((uint32_t)n < 32) {
+            lo = Vec32c().load_partial(n,p);
+            hi = Vec32c(0);
+        }
+        else {
+            lo = Vec32c().load(p);
+            hi = Vec32c().load_partial(n-32, ((int8_t*)p)+32);
+        }
+        *this = Vec64c(lo, hi);
+        return *this;
+    }
+    // store
+    void store(void * p) const {
+        Vec16i x = Vec16i(Vec8i(z0),Vec8i(z1));
+        x.store(p);
+    }
+    // store aligned
+    void store_a(void * p) const {
+        Vec16i x = Vec16i(Vec8i(z0),Vec8i(z1));
+        x.store_a(p);
+    } 
+    // Member function storing to aligned uncached memory (non-temporal store).
+    // Note: Will generate runtime error if p is not aligned by 64
+    void store_nt(void * p) const {
+        Vec16i x = Vec16i(Vec8i(z0),Vec8i(z1));
+        x.store_nt(p);
+    }
+    // Partial store. Store n elements
+    void store_partial(int n, void * p) const {
+        if ((uint32_t)n < 32) {
+            get_low().store_partial(n, p);
+        }
+        else {
+            get_low().store(p);
+            get_high().store_partial(n-32, ((int8_t*)p)+32);
+        }
+    }
+    // cut off vector to n elements. The last 64-n elements are set to zero
+    Vec64c & cutoff(int n) {
+        Vec32c lo, hi;
+        if ((uint32_t)n < 32) {
+            lo = Vec32c(get_low()).cutoff(n);
+            hi = Vec32c(0);
+        }
+        else {
+            lo = get_low();
+            hi = Vec32c(get_high()).cutoff(n-32);
+        }
+        *this = Vec64c(lo, hi);
+        return *this;
+    }
+    // Member function to change a single element in vector
+    Vec64c const insert(int index, int8_t value) {
+        Vec32c lo, hi;
+        if ((uint32_t)index < 32) {
+            lo = Vec32c(get_low()).insert(index, value);
+            hi = get_high();
+        }
+        else {
+            lo = get_low();
+            hi = Vec32c(get_high()).insert(index-32, value);
+        }
+        *this = Vec64c(lo, hi);
+        return *this;
+    }
+    // Member function extract a single element from vector
+    int8_t extract(int index) const {
+        if ((uint32_t)index < 32) {
+            return Vec32c(get_low()).extract(index);
+        }
+        else {
+            return Vec32c(get_high()).extract(index-32);
+        }
+    }
+    // Extract a single element. Use store function if extracting more than one element.
+    // Operator [] can only read an element, not write.
+    int8_t operator [] (int index) const {
+        return extract(index);
+    }
+    // Member functions to split into two Vec32c:
+    Vec32c get_low() const {
+        return z0;
+    }
+    Vec32c get_high() const {
+        return z1;
+    }
+    static constexpr int size() {
+        return 64;
+    }
+    static constexpr int elementtype() {
+        return 4;
+    }
+};
+
+
+/*****************************************************************************
+*
+*          Vec64cb: Vector of 64 Booleans for use with Vec64c and Vec64uc
+*
+*****************************************************************************/
+
+class Vec64cb : public Vec64c {
+public:
+    // Default constructor:
+    Vec64cb() = default;
+
+    Vec64cb (Vec64c const a) : Vec64c(a) {}
+
+    // Constructor to build from all elements: Not implemented
+
+    // Constructor to convert from type __mmask64 used in intrinsics: not possible
+    // Vec64cb (__mmask64 x);
+
+    // Constructor to broadcast single value:
+    Vec64cb(bool b) {
+        z0 = z1 = Vec32c(-int8_t(b));
+    }
+    // Constructor to make from two halves (big booleans)
+    Vec64cb (Vec32cb const x0, Vec32cb const x1) : Vec64c(x0,x1) {}
+
+    // Assignment operator to convert from type __mmask64 used in intrinsics: not possible
+    //Vec64cb & operator = (__mmask64 x);
+
+    // Member functions to split into two Vec32cb:
+    Vec32cb get_low() const {
+        return Vec32c(z0);
+    }
+    Vec32cb get_high() const {
+        return Vec32c(z1);
+    }
+    // Assignment operator to broadcast scalar value:
+    Vec64cb & operator = (bool b) {
+        *this = Vec64cb(b);
+        return *this;
+    }
+    // Member function to change a single element in vector
+    Vec64cb & insert (int index, bool a) {
+        if ((uint32_t)index < 32) {
+            z0 = get_low().insert(index, a);
+        }
+        else {
+            z1 = get_high().insert(index-32, a);
+        }
+        return *this;
+    }
+    // Member function extract a single element from vector
+    bool extract(int index) const {
+        if (index < 32) {
+            return get_low().extract(index);
+        }
+        else {
+            return get_high().extract(index-32);
+        }
+    }
+    // Extract a single element. Use store function if extracting more than one element.
+    // Operator [] can only read an element, not write.
+    bool operator [] (int index) const {
+        return extract(index);
+    }
+    // Type cast operator to convert to __mmask64 used in intrinsics. not possible
+    //operator __mmask64() const;
+
+    // Member function to change a bitfield to a boolean vector
+    Vec64cb & load_bits(uint64_t a) {
+        Vec32cb x0 = Vec32cb().load_bits(uint32_t(a));
+        Vec32cb x1 = Vec32cb().load_bits(uint32_t(a>>32));
+        *this = Vec64cb(x0,x1);
+        return *this;
+    }
+    static constexpr int size() {
+        return 64;
+    }
+    static constexpr int elementtype() {
+        return 3;
+    }
+    Vec64cb(int b) = delete; // Prevent constructing from int, etc.
+    Vec64cb & operator = (int x) = delete; // Prevent assigning int because of ambiguity
+};
+
+
+/*****************************************************************************
+*
+*          Define operators and functions for Vec64cb
+*
+*****************************************************************************/
+
+// vector operator & : bitwise and
+static inline Vec64cb operator & (Vec64cb const a, Vec64cb const b) {
+    return Vec64cb(a.get_low() & b.get_low(), a.get_high() & b.get_high());
+}
+static inline Vec64cb operator && (Vec64cb const a, Vec64cb const b) {
+    return a & b;
+}
+// vector operator &= : bitwise and
+static inline Vec64cb & operator &= (Vec64cb & a, Vec64cb const b) {
+    a = a & b;
+    return a;
+}
+
+// vector operator | : bitwise or
+static inline Vec64cb operator | (Vec64cb const a, Vec64cb const b) {
+    return Vec64cb(a.get_low() | b.get_low(), a.get_high() | b.get_high());
+}
+static inline Vec64cb operator || (Vec64cb const a, Vec64cb const b) {
+    return a | b;
+}
+// vector operator |= : bitwise or
+static inline Vec64cb & operator |= (Vec64cb & a, Vec64cb const b) {
+    a = a | b;
+    return a;
+}
+
+// vector operator ^ : bitwise xor
+static inline Vec64cb operator ^ (Vec64cb const a, Vec64cb const b) {
+    return Vec64cb(a.get_low() ^ b.get_low(), a.get_high() ^ b.get_high());
+}
+// vector operator ^= : bitwise xor
+static inline Vec64cb & operator ^= (Vec64cb & a, Vec64cb const b) {
+    a = a ^ b;
+    return a;
+}
+
+// vector operator == : xnor
+static inline Vec64cb operator == (Vec64cb const a, Vec64cb const b) {
+    return Vec64cb(a.get_low() == b.get_low(), a.get_high() == b.get_high());
+}
+
+// vector operator != : xor
+static inline Vec64cb operator != (Vec64cb const a, Vec64cb const b) {
+    return a ^ b;
+}
+
+// vector operator ~ : bitwise not
+static inline Vec64cb operator ~ (Vec64cb const a) {
+    return Vec64cb(~a.get_low(), ~a.get_high());}
+
+// vector operator ! : element not
+static inline Vec64cb operator ! (Vec64cb const a) {
+    return ~a;
+}
+
+// vector function andnot
+static inline Vec64cb andnot (Vec64cb const a, Vec64cb const b) {
+    return Vec64cb(andnot(a.get_low(), b.get_low()), andnot(a.get_high(), b.get_high()));}
+
+// horizontal_and. Returns true if all bits are 1
+static inline bool horizontal_and (Vec64cb const a) {
+    return horizontal_and(a.get_low()) && horizontal_and(a.get_high());
+}
+
+// horizontal_or. Returns true if at least one bit is 1
+static inline bool horizontal_or (Vec64cb const a) {
+    return horizontal_or(a.get_low()) || horizontal_or(a.get_high());
+}
+
+// to_bits: convert boolean vector to integer bitfield
+static inline uint64_t to_bits(Vec64cb x) {
+    return (uint64_t(to_bits(x.get_high())) << 32) | to_bits(x.get_low());
+}
+
+
+/*****************************************************************************
+*
+*          Define operators for Vec64c
+*
+*****************************************************************************/
+
+// vector operator + : add element by element
+static inline Vec64c operator + (Vec64c const a, Vec64c const b) {
+    return Vec64c(a.get_low() + b.get_low(), a.get_high() + b.get_high());
+}
+
+// vector operator += : add
+static inline Vec64c & operator += (Vec64c & a, Vec64c const b) {
+    a = a + b;
+    return a;
+}
+
+// postfix operator ++
+static inline Vec64c operator ++ (Vec64c & a, int) {
+    Vec64c a0 = a;
+    a = a + 1;
+    return a0;
+}
+
+// prefix operator ++
+static inline Vec64c & operator ++ (Vec64c & a) {
+    a = a + 1;
+    return a;
+}
+
+// vector operator - : subtract element by element
+static inline Vec64c operator - (Vec64c const a, Vec64c const b) {
+    return Vec64c(a.get_low() - b.get_low(), a.get_high() - b.get_high());
+}
+
+// vector operator - : unary minus
+static inline Vec64c operator - (Vec64c const a) {
+    return Vec64c(-a.get_low(), -a.get_high());
+}
+
+// vector operator -= : subtract
+static inline Vec64c & operator -= (Vec64c & a, Vec64c const b) {
+    a = a - b;
+    return a;
+}
+
+// postfix operator --
+static inline Vec64c operator -- (Vec64c & a, int) {
+    Vec64c a0 = a;
+    a = a - 1;
+    return a0;
+}
+
+// prefix operator --
+static inline Vec64c & operator -- (Vec64c & a) {
+    a = a - 1;
+    return a;
+}
+
+// vector operator * : multiply element by element
+static inline Vec64c operator * (Vec64c const a, Vec64c const b) {
+    return Vec64c(a.get_low() * b.get_low(), a.get_high() * b.get_high());
+}
+
+// vector operator *= : multiply
+static inline Vec64c & operator *= (Vec64c & a, Vec64c const b) {
+    a = a * b;
+    return a;
+}
+
+// vector operator / : divide all elements by same integer
+// See bottom of file
+
+// vector operator << : shift left
+static inline Vec64c operator << (Vec64c const a, int32_t b) {
+    return Vec64c(a.get_low() << b, a.get_high() << b);
+}
+
+// vector operator <<= : shift left
+static inline Vec64c & operator <<= (Vec64c & a, int32_t b) {
+    a = a << b;
+    return a;
+}
+
+// vector operator >> : shift right arithmetic
+static inline Vec64c operator >> (Vec64c const a, int32_t b) {
+    return Vec64c(a.get_low() >> b, a.get_high() >> b);
+}
+
+// vector operator >>= : shift right arithmetic
+static inline Vec64c & operator >>= (Vec64c & a, int32_t b) {
+    a = a >> b;
+    return a;
+}
+
+// vector operator == : returns true for elements for which a == b
+static inline Vec64cb operator == (Vec64c const a, Vec64c const b) {
+    return Vec64cb(a.get_low() == b.get_low(), a.get_high() == b.get_high());
+}
+
+// vector operator != : returns true for elements for which a != b
+static inline Vec64cb operator != (Vec64c const a, Vec64c const b) {
+    return Vec64cb(a.get_low() != b.get_low(), a.get_high() != b.get_high());
+}
+
+// vector operator > : returns true for elements for which a > b
+static inline Vec64cb operator > (Vec64c const a, Vec64c const b) {
+    return Vec64cb(a.get_low() > b.get_low(), a.get_high() > b.get_high());
+}
+
+// vector operator < : returns true for elements for which a < b
+static inline Vec64cb operator < (Vec64c const a, Vec64c const b) {
+    return b > a;
+}
+
+// vector operator >= : returns true for elements for which a >= b (signed)
+static inline Vec64cb operator >= (Vec64c const a, Vec64c const b) {
+    return Vec64cb(a.get_low() >= b.get_low(), a.get_high() >= b.get_high());
+}
+
+// vector operator <= : returns true for elements for which a <= b (signed)
+static inline Vec64cb operator <= (Vec64c const a, Vec64c const b) {
+    return b >= a;
+}
+
+// vector operator & : bitwise and
+static inline Vec64c operator & (Vec64c const a, Vec64c const b) {
+    return Vec64c(a.get_low() & b.get_low(), a.get_high() & b.get_high());
+}
+
+// vector operator &= : bitwise and
+static inline Vec64c & operator &= (Vec64c & a, Vec64c const b) {
+    a = a & b;
+    return a;
+}
+
+// vector operator | : bitwise or
+static inline Vec64c operator | (Vec64c const a, Vec64c const b) {
+    return Vec64c(a.get_low() | b.get_low(), a.get_high() | b.get_high());
+}
+
+// vector operator |= : bitwise or
+static inline Vec64c & operator |= (Vec64c & a, Vec64c const b) {
+    a = a | b;
+    return a;
+}
+
+// vector operator ^ : bitwise xor
+static inline Vec64c operator ^ (Vec64c const a, Vec64c const b) {
+    return Vec64c(a.get_low() ^ b.get_low(), a.get_high() ^ b.get_high());
+}
+
+// vector operator ^= : bitwise xor
+static inline Vec64c & operator ^= (Vec64c & a, Vec64c const b) {
+    a = a ^ b;
+    return a;
+}
+
+// vector operator ~ : bitwise not
+static inline Vec64c operator ~ (Vec64c const a) {
+    return Vec64c(~a.get_low(), ~a.get_high());
+}
+
+// Functions for this class
+
+// Select between two operands. Corresponds to this pseudocode:
+// for (int i = 0; i < 16; i++) result[i] = s[i] ? a[i] : b[i];
+static inline Vec64c select (Vec64cb const s, Vec64c const a, Vec64c const b) {
+    return Vec64c(select(s.get_low(), a.get_low(), b.get_low()), select(s.get_high(), a.get_high(), b.get_high()));
+}
+
+// Conditional add: For all vector elements i: result[i] = f[i] ? (a[i] + b[i]) : a[i]
+static inline Vec64c if_add (Vec64cb const f, Vec64c const a, Vec64c const b) {
+    return Vec64c(if_add(f.get_low(), a.get_low(), b.get_low()), if_add(f.get_high(), a.get_high(), b.get_high()));
+}
+
+// Conditional subtract
+static inline Vec64c if_sub (Vec64cb const f, Vec64c const a, Vec64c const b) {
+    return Vec64c(if_sub(f.get_low(), a.get_low(), b.get_low()), if_sub(f.get_high(), a.get_high(), b.get_high()));
+}
+
+// Conditional multiply
+static inline Vec64c if_mul (Vec64cb const f, Vec64c const a, Vec64c const b) {
+    return Vec64c(if_mul(f.get_low(), a.get_low(), b.get_low()), if_mul(f.get_high(), a.get_high(), b.get_high()));
+}
+
+// Horizontal add: Calculates the sum of all vector elements. Overflow will wrap around
+static inline int8_t horizontal_add (Vec64c const a) {
+    return (int8_t)horizontal_add(a.get_low() + a.get_high());
+}
+
+// Horizontal add extended: Calculates the sum of all vector elements.
+// Each element is sign-extended before addition to avoid overflow
+static inline int32_t horizontal_add_x (Vec64c const a) {
+    return horizontal_add_x(a.get_low()) + horizontal_add_x(a.get_high());
+}
+
+// function add_saturated: add element by element, signed with saturation
+static inline Vec64c add_saturated(Vec64c const a, Vec64c const b) {
+    return Vec64c(add_saturated(a.get_low(), b.get_low()), add_saturated(a.get_high(), b.get_high()));
+}
+
+// function sub_saturated: subtract element by element, signed with saturation
+static inline Vec64c sub_saturated(Vec64c const a, Vec64c const b) {
+    return Vec64c(sub_saturated(a.get_low(), b.get_low()), sub_saturated(a.get_high(), b.get_high()));
+}
+
+// function max: a > b ? a : b
+static inline Vec64c max(Vec64c const a, Vec64c const b) {
+    return Vec64c(max(a.get_low(), b.get_low()), max(a.get_high(), b.get_high()));
+}
+
+// function min: a < b ? a : b
+static inline Vec64c min(Vec64c const a, Vec64c const b) {
+    return Vec64c(min(a.get_low(), b.get_low()), min(a.get_high(), b.get_high()));
+}
+
+// function abs: a >= 0 ? a : -a
+static inline Vec64c abs(Vec64c const a) {
+    return Vec64c(abs(a.get_low()), abs(a.get_high()));
+}
+
+// function abs_saturated: same as abs, saturate if overflow
+static inline Vec64c abs_saturated(Vec64c const a) {
+    return Vec64c(abs_saturated(a.get_low()), abs_saturated(a.get_high()));
+}
+
+// function rotate_left all elements
+// Use negative count to rotate right
+static inline Vec64c rotate_left(Vec64c const a, int b) {
+    return Vec64c(rotate_left(a.get_low(), b), rotate_left(a.get_high(), b));
+}
+
+
+/*****************************************************************************
+*
+*          Vector of 64 8-bit unsigned integers
+*
+*****************************************************************************/
+
+class Vec64uc : public Vec64c {
+public:
+    // Default constructor:
+    Vec64uc() = default;
+    // Construct from Vec64c
+    Vec64uc(Vec64c const a) : Vec64c(a) {
+    }
+    // Constructor to broadcast the same value into all elements:
+    Vec64uc(uint8_t i) : Vec64c(int8_t(i)) {
+    }
+    // Constructor to build from two Vec32uc:
+    Vec64uc(Vec32uc const a0, Vec32uc const a1) : Vec64c(a0,a1) {
+    }
+    // Constructor to build from all elements:
+    Vec64uc(uint8_t i0, uint8_t i1, uint8_t i2, uint8_t i3, uint8_t i4, uint8_t i5, uint8_t i6, uint8_t i7,
+        uint8_t i8, uint8_t i9, uint8_t i10, uint8_t i11, uint8_t i12, uint8_t i13, uint8_t i14, uint8_t i15,
+        uint8_t i16, uint8_t i17, uint8_t i18, uint8_t i19, uint8_t i20, uint8_t i21, uint8_t i22, uint8_t i23,
+        uint8_t i24, uint8_t i25, uint8_t i26, uint8_t i27, uint8_t i28, uint8_t i29, uint8_t i30, uint8_t i31,
+        uint8_t i32, uint8_t i33, uint8_t i34, uint8_t i35, uint8_t i36, uint8_t i37, uint8_t i38, uint8_t i39,
+        uint8_t i40, uint8_t i41, uint8_t i42, uint8_t i43, uint8_t i44, uint8_t i45, uint8_t i46, uint8_t i47,
+        uint8_t i48, uint8_t i49, uint8_t i50, uint8_t i51, uint8_t i52, uint8_t i53, uint8_t i54, uint8_t i55,
+        uint8_t i56, uint8_t i57, uint8_t i58, uint8_t i59, uint8_t i60, uint8_t i61, uint8_t i62, uint8_t i63) {
+        // _mm512_set_epi8 and _mm512_set_epi16 missing in GCC 7.4.0
+        uint8_t aa[64] = {
+            i0, i1, i2, i3, i4, i5, i6, i7,i8, i9, i10, i11, i12, i13, i14, i15,
+            i16, i17, i18, i19, i20, i21, i22, i23, i24, i25, i26, i27, i28, i29, i30, i31,
+            i32, i33, i34, i35, i36, i37, i38, i39, i40, i41, i42, i43, i44, i45, i46, i47,
+            i48, i49, i50, i51, i52, i53, i54, i55, i56, i57, i58, i59, i60, i61, i62, i63 };
+        load(aa);
+    }
+
+#ifdef VECTORI512_H
+   // Constructor to convert from type __m512i used in intrinsics:
+   Vec64uc(__m512i const x) : Vec64c(x) {};
+
+   // Assignment operator to convert from type __m512i used in intrinsics:
+   Vec64uc & operator = (__m512i const x) {
+       return *this = Vec64uc(x);
+   }
+#else
+    // Constructor to convert from type Vec512b
+    Vec64uc(Vec512b const x) : Vec64c(x) {}
+
+    // Assignment operator to convert from type __m512i used in intrinsics:
+    Vec64uc & operator = (Vec512b const x) {
+        return *this = Vec64uc(x);
+    }
+#endif
+    // Member function to load from array (unaligned)
+    Vec64uc & load(void const * p) {
+        Vec64c::load(p);
+        return *this;
+    }
+    // Member function to load from array, aligned by 64
+    Vec64uc & load_a(void const * p) {
+        Vec64c::load_a(p);
+        return *this;
+    }
+    // Member function to change a single element in vector
+    // Note: This function is inefficient. Use load function if changing more than one element
+    Vec64uc const insert(int index, uint8_t value) {
+        Vec64c::insert(index, (int8_t)value);
+        return *this;
+    }
+    // Member function extract a single element from vector
+    uint8_t extract(int index) const {
+        return (uint8_t)Vec64c::extract(index);
+    }
+    // Extract a single element. Use store function if extracting more than one element.
+    // Operator [] can only read an element, not write.
+    uint8_t operator [] (int index) const {
+        return (uint8_t)Vec64c::extract(index);
+    }
+    // Member functions to split into two Vec32uc:
+    Vec32uc get_low() const {
+        return Vec32uc(Vec64c::get_low());
+    }
+    Vec32uc get_high() const {
+        return Vec32uc(Vec64c::get_high());
+    }
+    static constexpr int elementtype() {
+        return 5;
+    }
+};
+
+// Define operators for this class
+
+// vector operator + : add element by element
+static inline Vec64uc operator + (Vec64uc const a, Vec64uc const b) {
+    return Vec64uc(a.get_low() + b.get_low(), a.get_high() + b.get_high());
+}
+
+// vector operator - : subtract element by element
+static inline Vec64uc operator - (Vec64uc const a, Vec64uc const b) {
+    return Vec64uc(a.get_low() - b.get_low(), a.get_high() - b.get_high());
+}
+
+// vector operator ' : multiply element by element
+static inline Vec64uc operator * (Vec64uc const a, Vec64uc const b) {
+    return Vec64uc(a.get_low() * b.get_low(), a.get_high() * b.get_high());
+}
+
+// vector operator / : divide
+// See bottom of file
+
+// vector operator >> : shift right logical all elements
+static inline Vec64uc operator >> (Vec64uc const a, uint32_t b) {
+    return Vec64uc(a.get_low() >> b, a.get_high() >> b);
+}
+static inline Vec64uc operator >> (Vec64uc const a, int b) {
+    return a >> uint32_t(b);
+}
+
+// vector operator >>= : shift right logical
+static inline Vec64uc & operator >>= (Vec64uc & a, uint32_t b) {
+    a = a >> b;
+    return a;
+}
+
+// vector operator >>= : shift right logical (signed b)
+static inline Vec64uc & operator >>= (Vec64uc & a, int32_t b) {
+    a = a >> uint32_t(b);
+    return a;
+}
+
+// vector operator << : shift left all elements
+static inline Vec64uc operator << (Vec64uc const a, uint32_t b) {
+    return Vec64uc(a.get_low() << b, a.get_high() << b);
+}
+static inline Vec64uc operator << (Vec64uc const a, int b) {
+    return a << uint32_t(b);
+}
+
+// vector operator < : returns true for elements for which a < b (unsigned)
+static inline Vec64cb operator < (Vec64uc const a, Vec64uc const b) {
+    return Vec64cb(a.get_low() < b.get_low(), a.get_high() < b.get_high());
+}
+
+// vector operator > : returns true for elements for which a > b (unsigned)
+static inline Vec64cb operator > (Vec64uc const a, Vec64uc const b) {
+    return b < a;
+}
+
+// vector operator >= : returns true for elements for which a >= b (unsigned)
+static inline Vec64cb operator >= (Vec64uc const a, Vec64uc const b) {
+    return Vec64cb(a.get_low() >= b.get_low(), a.get_high() >= b.get_high());
+}
+
+// vector operator <= : returns true for elements for which a <= b (unsigned)
+static inline Vec64cb operator <= (Vec64uc const a, Vec64uc const b) {
+    return b >= a;
+}
+
+// vector operator & : bitwise and
+static inline Vec64uc operator & (Vec64uc const a, Vec64uc const b) {
+    return Vec64uc(Vec64c(a) & Vec64c(b));
+}
+
+// vector operator | : bitwise or
+static inline Vec64uc operator | (Vec64uc const a, Vec64uc const b) {
+    return Vec64uc(Vec64c(a) | Vec64c(b));
+}
+
+// vector operator ^ : bitwise xor
+static inline Vec64uc operator ^ (Vec64uc const a, Vec64uc const b) {
+    return Vec64uc(Vec64c(a) ^ Vec64c(b));
+}
+
+// vector operator ~ : bitwise not
+static inline Vec64uc operator ~ (Vec64uc const a) {
+    return Vec64uc( ~ Vec64c(a));
+}
+
+// Functions for this class
+
+// Select between two operands. Corresponds to this pseudocode:
+// for (int i = 0; i < 16; i++) result[i] = s[i] ? a[i] : b[i];
+static inline Vec64uc select (Vec64cb const s, Vec64uc const a, Vec64uc const b) {
+    return Vec64uc(select(s.get_low(), a.get_low(), b.get_low()), select(s.get_high(), a.get_high(), b.get_high()));
+}
+
+// Conditional add: For all vector elements i: result[i] = f[i] ? (a[i] + b[i]) : a[i]
+static inline Vec64uc if_add (Vec64cb const f, Vec64uc const a, Vec64uc const b) {
+    return Vec64uc(if_add(f.get_low(), a.get_low(), b.get_low()), if_add(f.get_high(), a.get_high(), b.get_high()));
+}
+
+// Conditional subtract
+static inline Vec64uc if_sub (Vec64cb const f, Vec64uc const a, Vec64uc const b) {
+    return Vec64uc(if_sub(f.get_low(), a.get_low(), b.get_low()), if_sub(f.get_high(), a.get_high(), b.get_high()));
+}
+
+// Conditional multiply
+static inline Vec64uc if_mul (Vec64cb const f, Vec64uc const a, Vec64uc const b) {
+    return Vec64uc(if_mul(f.get_low(), a.get_low(), b.get_low()), if_mul(f.get_high(), a.get_high(), b.get_high()));
+}
+
+// function add_saturated: add element by element, unsigned with saturation
+static inline Vec64uc add_saturated(Vec64uc const a, Vec64uc const b) {
+    return Vec64uc(add_saturated(a.get_low(), b.get_low()), add_saturated(a.get_high(), b.get_high()));
+}
+
+// function sub_saturated: subtract element by element, unsigned with saturation
+static inline Vec64uc sub_saturated(Vec64uc const a, Vec64uc const b) {
+    return Vec64uc(sub_saturated(a.get_low(), b.get_low()), sub_saturated(a.get_high(), b.get_high()));
+}
+
+// function max: a > b ? a : b
+static inline Vec64uc max(Vec64uc const a, Vec64uc const b) {
+    return Vec64uc(max(a.get_low(), b.get_low()), max(a.get_high(), b.get_high()));
+}
+
+// function min: a < b ? a : b
+static inline Vec64uc min(Vec64uc const a, Vec64uc const b) {
+    return Vec64uc(min(a.get_low(), b.get_low()), min(a.get_high(), b.get_high()));
+}
+
+
+/*****************************************************************************
+*
+*          Vector of 32 16-bit signed integers
+*
+*****************************************************************************/
+
+class Vec32s : public Vec64c {
+public:
+    // Default constructor:
+    Vec32s() = default;
+    // Constructor to broadcast the same value into all elements:
+    Vec32s(int16_t i) {
+        z0 = z1 = Vec16s(i);
+    }
+    // Constructor to build from all elements:
+    Vec32s(int16_t i0, int16_t i1, int16_t i2, int16_t i3, int16_t i4, int16_t i5, int16_t i6, int16_t i7,
+        int16_t i8, int16_t i9, int16_t i10, int16_t i11, int16_t i12, int16_t i13, int16_t i14, int16_t i15,
+        int16_t i16, int16_t i17, int16_t i18, int16_t i19, int16_t i20, int16_t i21, int16_t i22, int16_t i23,
+        int16_t i24, int16_t i25, int16_t i26, int16_t i27, int16_t i28, int16_t i29, int16_t i30, int16_t i31) {
+        Vec16s x0 = Vec16s(i0, i1, i2, i3, i4, i5, i6, i7, i8, i9, i10, i11, i12, i13, i14, i15);
+        Vec16s x1 = Vec16s(i16, i17, i18, i19, i20, i21, i22, i23, i24, i25, i26, i27, i28, i29, i30, i31);
+        *this = Vec32s(x0,x1);
+    }
+    // Constructor to build from two Vec16s:
+    Vec32s(Vec16s const a0, Vec16s const a1) {
+        z0 = a0;  z1 = a1;
+    }
+#ifdef VECTORI512_H
+    // Constructor to convert from type __m512i used in intrinsics:
+    Vec32s(__m512i const x) {
+        Vec16i zz(x);
+        z0 = zz.get_low();
+        z1 = zz.get_high();
+    }
+    // Assignment operator to convert from type __m512i used in intrinsics:
+    Vec32s & operator = (__m512i const x) {
+        Vec16i zz(x);
+        z0 = zz.get_low();
+        z1 = zz.get_high();
+        return *this;
+    }
+#else
+    // Constructor to convert from type Vec512b
+    Vec32s(Vec512b const x) {
+        z0 = x.get_low();
+        z1 = x.get_high();
+    }
+    // Assignment operator to convert from type Vec512b
+    Vec32s & operator = (Vec512b const x) {
+        z0 = x.get_low();
+        z1 = x.get_high();
+        return *this;
+    }
+#endif
+    // Member function to load from array (unaligned)
+    Vec32s & load(void const * p) {
+        z0 = Vec16s().load(p);
+        z1 = Vec16s().load((int16_t*)p + 16);
+        return *this;
+    }
+    // Member function to load from array, aligned by 64
+    Vec32s & load_a(void const * p) {
+        z0 = Vec16s().load_a(p);
+        z1 = Vec16s().load_a((int16_t*)p + 16);
+        return *this;
+    }
+    // Partial load. Load n elements and set the rest to 0
+    Vec32s & load_partial(int n, void const * p) {
+        if (uint32_t(n) < 16) {
+            z0 = Vec16s().load_partial(n, p);
+            z1 = Vec16s(0);
+        }
+        else {
+            z0 = Vec16s().load(p);
+            z1 = Vec16s().load_partial(n-16, (int16_t*)p + 16);
+        }
+        return *this;
+    }
+    // store
+    void store(void * p) const {
+        Vec16s(z0).store(p);
+        Vec16s(z1).store((int16_t*)p + 16);
+    }
+    // store aligned
+    void store_a(void * p) const {
+        Vec16s(z0).store_a(p);
+        Vec16s(z1).store_a((int16_t*)p + 16);
+    }
+    // Partial store. Store n elements
+    void store_partial(int n, void * p) const {
+        if (uint32_t(n) < 16) {
+            Vec16s(z0).store_partial(n, p);
+        }
+        else {
+            Vec16s(z0).store(p);
+            Vec16s(z1).store_partial(n-16, (int16_t*)p + 16);
+        }
+    }
+    // cut off vector to n elements. The last 32-n elements are set to zero
+    Vec32s & cutoff(int n) {
+        if (uint32_t(n) < 16) {
+            z0 = Vec16s(z0).cutoff(n);
+            z1 = Vec16s(0);
+        }
+        else {
+            z1 = Vec16s(z1).cutoff(n-16);
+        }
+        return *this;
+    }
+    // Member function to change a single element in vector
+    Vec32s const insert(int index, int16_t value) {
+        if ((uint32_t)index < 16) {
+            z0 = Vec16s(z0).insert(index, value);
+        }
+        else {
+            z1 = Vec16s(z1).insert(index-16, value);
+        }
+        return *this;
+    }
+    // Member function extract a single element from vector
+    int16_t extract(int index) const {
+        if (index < 16) {
+            return Vec16s(z0).extract(index);
+        }
+        else {
+            return Vec16s(z1).extract(index-16);
+        }
+    }
+    // Extract a single element. Use store function if extracting more than one element.
+    // Operator [] can only read an element, not write.
+    int16_t operator [] (int index) const {
+        return extract(index);
+    }
+    // Member functions to split into two Vec16s:
+    Vec16s get_low() const {
+        return z0;
+    }
+    Vec16s get_high() const {
+        return z1;
+    }
+    static constexpr int size() {
+        return 32;
+    }
+    static constexpr int elementtype() {
+        return 6;
+    }
+};
+
+
+/*****************************************************************************
+*
+*          Vec32sb: Vector of 64 Booleans for use with Vec32s and Vec32us
+*
+*****************************************************************************/
+
+class Vec32sb : public Vec32s {
+public:
+    // Default constructor:
+    Vec32sb() = default;
+    // Constructor to build from all elements: Not implemented
+
+    // Constructor to convert from type __mmask32 used in intrinsics: not possible
+
+    // Constructor to broadcast single value:
+    Vec32sb(bool b) {
+        z0 = z1 = Vec16s(-int16_t(b));
+    }
+    // Constructor to make from two halves
+    Vec32sb (Vec16sb const x0, Vec16sb const x1) {
+        z0 = x0;  z1 = x1;
+    }
+    // Assignment operator to convert from type __mmask32 used in intrinsics: not possible
+
+    // Assignment operator to broadcast scalar value:
+    Vec32sb & operator = (bool b) {
+        *this = Vec32sb(b);
+        return *this;
+    }
+    // Member functions to split into two Vec16sb:
+    Vec16sb get_low() const {
+        return z0;
+    }
+    Vec16sb get_high() const {
+        return z1;
+    }
+    // Member function to change a single element in vector
+    Vec32sb & insert(int index, bool a) {
+        Vec32s::insert(index, -(int16_t)a);
+        return *this;
+    }
+    // Member function extract a single element from vector
+    bool extract(int index) const {
+        return Vec32s::extract(index) != 0;
+    }
+    // Extract a single element. Use store function if extracting more than one element.
+    // Operator [] can only read an element, not write.
+    bool operator [] (int index) const {
+        return extract(index);
+    }
+    // Type cast operator to convert to __mmask64 used in intrinsics. Not possible
+
+    // Member function to change a bitfield to a boolean vector
+    Vec32sb & load_bits(uint32_t a) {
+        z0 = Vec16sb().load_bits(uint16_t(a));
+        z1 = Vec16sb().load_bits(uint16_t(a>>16));
+        return *this;
+    }
+    static constexpr int elementtype() {
+        return 3;
+    }
+    Vec32sb(int b) = delete; // Prevent constructing from int, etc.
+    Vec32sb & operator = (int x) = delete; // Prevent assigning int because of ambiguity
+};
+
+
+/*****************************************************************************
+*
+*          Define operators and functions for Vec32sb
+*
+*****************************************************************************/
+
+// vector operator & : bitwise and
+static inline Vec32sb operator & (Vec32sb const a, Vec32sb const b) {
+    return Vec32sb(a.get_low() & b.get_low(), a.get_high() & b.get_high());
+}
+static inline Vec32sb operator && (Vec32sb const a, Vec32sb const b) {
+    return a & b;
+}
+// vector operator &= : bitwise and
+static inline Vec32sb & operator &= (Vec32sb & a, Vec32sb const b) {
+    a = a & b;
+    return a;
+}
+
+// vector operator | : bitwise or
+static inline Vec32sb operator | (Vec32sb const a, Vec32sb const b) {
+    return Vec32sb(a.get_low() | b.get_low(), a.get_high() | b.get_high());
+}
+static inline Vec32sb operator || (Vec32sb const a, Vec32sb const b) {
+    return a | b;
+}
+// vector operator |= : bitwise or
+static inline Vec32sb & operator |= (Vec32sb & a, Vec32sb const b) {
+    a = a | b;
+    return a;
+}
+
+// vector operator ^ : bitwise xor
+static inline Vec32sb operator ^ (Vec32sb const a, Vec32sb const b) {
+    return Vec32sb(a.get_low() ^ b.get_low(), a.get_high() ^ b.get_high());
+}
+// vector operator ^= : bitwise xor
+static inline Vec32sb & operator ^= (Vec32sb & a, Vec32sb const b) {
+    a = a ^ b;
+    return a;
+}
+
+// vector operator == : xnor
+static inline Vec32sb operator == (Vec32sb const a, Vec32sb const b) {
+    return Vec32sb(a.get_low() == b.get_low(), a.get_high() == b.get_high());}
+
+// vector operator != : xor
+static inline Vec32sb operator != (Vec32sb const a, Vec32sb const b) {
+    return Vec32sb(a.get_low() ^ b.get_low(), a.get_high() ^ b.get_high());}
+
+// vector operator ~ : bitwise not
+static inline Vec32sb operator ~ (Vec32sb const a) {
+    return Vec32sb(~a.get_low(), ~a.get_high());}
+
+// vector operator ! : element not
+static inline Vec32sb operator ! (Vec32sb const a) {
+    return ~a;
+}
+
+// vector function andnot
+static inline Vec32sb andnot (Vec32sb const a, Vec32sb const b) {
+    return Vec32sb(andnot(a.get_low(), b.get_low()), andnot(a.get_high(), b.get_high()));}
+
+// horizontal_and. Returns true if all bits are 1
+static inline bool horizontal_and (Vec32sb const a) {
+    return horizontal_and(a.get_low()) && horizontal_and(a.get_high());
+}
+
+// horizontal_or. Returns true if at least one bit is 1
+static inline bool horizontal_or (Vec32sb const a) {
+    return horizontal_or(a.get_low()) || horizontal_or(a.get_high());
+}
+
+// to_bits: convert boolean vector to integer bitfield
+static inline uint32_t to_bits(Vec32sb a) {
+    return uint32_t(to_bits(a.get_high())) << 16 | to_bits(a.get_low());
+}
+
+
+/*****************************************************************************
+*
+*          Define operators for Vec32s
+*
+*****************************************************************************/
+
+// vector operator + : add element by element
+static inline Vec32s operator + (Vec32s const a, Vec32s const b) {
+    return Vec32s(a.get_low() + b.get_low(), a.get_high() + b.get_high());
+}
+
+// vector operator += : add
+static inline Vec32s & operator += (Vec32s & a, Vec32s const b) {
+    a = a + b;
+    return a;
+}
+
+// postfix operator ++
+static inline Vec32s operator ++ (Vec32s & a, int) {
+    Vec32s a0 = a;
+    a = a + 1;
+    return a0;
+}
+
+// prefix operator ++
+static inline Vec32s & operator ++ (Vec32s & a) {
+    a = a + 1;
+    return a;
+}
+
+// vector operator - : subtract element by element
+static inline Vec32s operator - (Vec32s const a, Vec32s const b) {
+    return Vec32s(a.get_low() - b.get_low(), a.get_high() - b.get_high());
+}
+
+// vector operator - : unary minus
+static inline Vec32s operator - (Vec32s const a) {
+    return Vec32s(-a.get_low(), -a.get_high());
+}
+
+// vector operator -= : subtract
+static inline Vec32s & operator -= (Vec32s & a, Vec32s const b) {
+    a = a - b;
+    return a;
+}
+
+// postfix operator --
+static inline Vec32s operator -- (Vec32s & a, int) {
+    Vec32s a0 = a;
+    a = a - 1;
+    return a0;
+}
+
+// prefix operator --
+static inline Vec32s & operator -- (Vec32s & a) {
+    a = a - 1;
+    return a;
+}
+
+// vector operator * : multiply element by element
+static inline Vec32s operator * (Vec32s const a, Vec32s const b) {
+    return Vec32s(a.get_low() * b.get_low(), a.get_high() * b.get_high());
+}
+
+// vector operator *= : multiply
+static inline Vec32s & operator *= (Vec32s & a, Vec32s const b) {
+    a = a * b;
+    return a;
+}
+
+// vector operator / : divide all elements by same integer
+// See bottom of file
+
+// vector operator << : shift left
+static inline Vec32s operator << (Vec32s const a, int32_t b) {
+    return Vec32s(a.get_low() << b, a.get_high() << b);
+}
+
+// vector operator <<= : shift left
+static inline Vec32s & operator <<= (Vec32s & a, int32_t b) {
+    a = a << b;
+    return a;
+}
+
+// vector operator >> : shift right arithmetic
+static inline Vec32s operator >> (Vec32s const a, int32_t b) {
+    return Vec32s(a.get_low() >> b, a.get_high() >> b);
+}
+
+// vector operator >>= : shift right arithmetic
+static inline Vec32s & operator >>= (Vec32s & a, int32_t b) {
+    a = a >> b;
+    return a;
+}
+
+// vector operator == : returns true for elements for which a == b
+static inline Vec32sb operator == (Vec32s const a, Vec32s const b) {
+    return Vec32sb(a.get_low() == b.get_low(), a.get_high() == b.get_high());
+}
+
+// vector operator != : returns true for elements for which a != b
+static inline Vec32sb operator != (Vec32s const a, Vec32s const b) {
+    return Vec32sb(a.get_low() != b.get_low(), a.get_high() != b.get_high());
+}
+
+// vector operator > : returns true for elements for which a > b
+static inline Vec32sb operator > (Vec32s const a, Vec32s const b) {
+    return Vec32sb(a.get_low() > b.get_low(), a.get_high() > b.get_high());
+}
+
+// vector operator < : returns true for elements for which a < b
+static inline Vec32sb operator < (Vec32s const a, Vec32s const b) {
+    return b > a;
+}
+
+// vector operator >= : returns true for elements for which a >= b (signed)
+static inline Vec32sb operator >= (Vec32s const a, Vec32s const b) {
+    return Vec32sb(a.get_low() >= b.get_low(), a.get_high() >= b.get_high());
+}
+
+// vector operator <= : returns true for elements for which a <= b (signed)
+static inline Vec32sb operator <= (Vec32s const a, Vec32s const b) {
+    return b >= a;
+}
+
+// vector operator & : bitwise and
+static inline Vec32s operator & (Vec32s const a, Vec32s const b) {
+    return Vec32s(a.get_low() & b.get_low(), a.get_high() & b.get_high());
+}
+
+// vector operator &= : bitwise and
+static inline Vec32s & operator &= (Vec32s & a, Vec32s const b) {
+    a = a & b;
+    return a;
+}
+
+// vector operator | : bitwise or
+static inline Vec32s operator | (Vec32s const a, Vec32s const b) {
+    return Vec32s(a.get_low() | b.get_low(), a.get_high() | b.get_high());
+}
+
+// vector operator |= : bitwise or
+static inline Vec32s & operator |= (Vec32s & a, Vec32s const b) {
+    a = a | b;
+    return a;
+}
+
+// vector operator ^ : bitwise xor
+static inline Vec32s operator ^ (Vec32s const a, Vec32s const b) {
+    return Vec32s(a.get_low() ^ b.get_low(), a.get_high() ^ b.get_high());
+}
+
+// vector operator ^= : bitwise xor
+static inline Vec32s & operator ^= (Vec32s & a, Vec32s const b) {
+    a = a ^ b;
+    return a;
+}
+
+// vector operator ~ : bitwise not
+static inline Vec32s operator ~ (Vec32s const a) {
+    return Vec32s(~a.get_low(), ~a.get_high());
+}
+
+// Functions for this class
+
+// Select between two operands. Corresponds to this pseudocode:
+// for (int i = 0; i < 16; i++) result[i] = s[i] ? a[i] : b[i];
+static inline Vec32s select (Vec32sb const s, Vec32s const a, Vec32s const b) {
+    return Vec32s(select(s.get_low(), a.get_low(), b.get_low()), select(s.get_high(), a.get_high(), b.get_high()));
+}
+
+// Conditional add: For all vector elements i: result[i] = f[i] ? (a[i] + b[i]) : a[i]
+static inline Vec32s if_add (Vec32sb const f, Vec32s const a, Vec32s const b) {
+    return Vec32s(if_add(f.get_low(), a.get_low(), b.get_low()), if_add(f.get_high(), a.get_high(), b.get_high()));
+}
+
+// Conditional subtract
+static inline Vec32s if_sub (Vec32sb const f, Vec32s const a, Vec32s const b) {
+    return Vec32s(if_sub(f.get_low(), a.get_low(), b.get_low()), if_sub(f.get_high(), a.get_high(), b.get_high()));
+}
+
+// Conditional multiply
+static inline Vec32s if_mul (Vec32sb const f, Vec32s const a, Vec32s const b) {
+    return Vec32s(if_mul(f.get_low(), a.get_low(), b.get_low()), if_mul(f.get_high(), a.get_high(), b.get_high()));
+}
+
+// Horizontal add: Calculates the sum of all vector elements. Overflow will wrap around
+static inline int16_t horizontal_add (Vec32s const a) {
+    Vec16s s = a.get_low() + a.get_high();
+    return (int16_t)horizontal_add(s);
+}
+
+// Horizontal add extended: Calculates the sum of all vector elements.
+// Each element is sign-extended before addition to avoid overflow
+static inline int32_t horizontal_add_x (Vec32s const a) {
+    return horizontal_add_x(a.get_low()) + horizontal_add_x(a.get_high());
+}
+
+// function add_saturated: add element by element, signed with saturation
+static inline Vec32s add_saturated(Vec32s const a, Vec32s const b) {
+    return Vec32s(add_saturated(a.get_low(), b.get_low()), add_saturated(a.get_high(), b.get_high()));
+}
+
+// function sub_saturated: subtract element by element, signed with saturation
+static inline Vec32s sub_saturated(Vec32s const a, Vec32s const b) {
+    return Vec32s(sub_saturated(a.get_low(), b.get_low()), sub_saturated(a.get_high(), b.get_high()));
+}
+
+// function max: a > b ? a : b
+static inline Vec32s max(Vec32s const a, Vec32s const b) {
+    return Vec32s(max(a.get_low(), b.get_low()), max(a.get_high(), b.get_high()));
+}
+
+// function min: a < b ? a : b
+static inline Vec32s min(Vec32s const a, Vec32s const b) {
+    return Vec32s(min(a.get_low(), b.get_low()), min(a.get_high(), b.get_high()));
+}
+
+// function abs: a >= 0 ? a : -a
+static inline Vec32s abs(Vec32s const a) {
+    return Vec32s(abs(a.get_low()), abs(a.get_high()));
+}
+
+// function abs_saturated: same as abs, saturate if overflow
+static inline Vec32s abs_saturated(Vec32s const a) {
+    return Vec32s(abs_saturated(a.get_low()), abs_saturated(a.get_high()));
+}
+
+// function rotate_left all elements
+// Use negative count to rotate right
+static inline Vec32s rotate_left(Vec32s const a, int b) {
+    return Vec32s(rotate_left(a.get_low(), b), rotate_left(a.get_high(), b));
+}
+
+
+/*****************************************************************************
+*
+*          Vector of 32 16-bit unsigned integers
+*
+*****************************************************************************/
+
+class Vec32us : public Vec32s {
+public:
+    // Default constructor:
+    Vec32us() = default;
+    // Construct from Vec32s
+    Vec32us(Vec32s const a) {
+        z0 = a.get_low();  z1 = a.get_high();
+    }
+    // Constructor to broadcast the same value into all elements:
+    Vec32us(uint16_t i) {
+        z0 = z1 = Vec16us(i);
+    }
+    // Constructor to build from all elements:
+    Vec32us(uint16_t i0, uint16_t i1, uint16_t i2, uint16_t i3, uint16_t i4, uint16_t i5, uint16_t i6, uint16_t i7,
+        uint16_t i8, uint16_t i9, uint16_t i10, uint16_t i11, uint16_t i12, uint16_t i13, uint16_t i14, uint16_t i15,
+        uint16_t i16, uint16_t i17, uint16_t i18, uint16_t i19, uint16_t i20, uint16_t i21, uint16_t i22, uint16_t i23,
+        uint16_t i24, uint16_t i25, uint16_t i26, uint16_t i27, uint16_t i28, uint16_t i29, uint16_t i30, uint16_t i31) {
+        Vec16us x0 = Vec16us(i0, i1, i2, i3, i4, i5, i6, i7, i8, i9, i10, i11, i12, i13, i14, i15);
+        Vec16us x1 = Vec16us(i16, i17, i18, i19, i20, i21, i22, i23, i24, i25, i26, i27, i28, i29, i30, i31);
+        *this = Vec32us(x0,x1);
+    }
+    // Constructor to build from two Vec16us:
+    Vec32us(Vec16us const a0, Vec16us const a1) {
+        z0 = a0;  z1 = a1;
+    }
+#ifdef VECTORI512_H
+    // Constructor to convert from type __m512i used in intrinsics:
+    Vec32us(__m512i const x) : Vec32s(x) {
+    }
+    // Assignment operator to convert from type __m512i used in intrinsics:
+    Vec32us & operator = (__m512i const x) {
+        return *this = Vec32us(x);
+    }
+#else
+    // Constructor to convert from type Vec512b
+    Vec32us(Vec512b const x) : Vec32s(x) {}
+    // Assignment operator to convert from type Vec512b
+    Vec32us & operator = (Vec512b const x) {
+        z0 = x.get_low();
+        z1 = x.get_high();
+        return *this;
+    }
+#endif
+    // Member function to load from array (unaligned)
+    Vec32us & load(void const * p) {
+        Vec32s::load(p);
+        return *this;
+    }
+    // Member function to load from array, aligned by 64
+    Vec32us & load_a(void const * p) {
+        Vec32s::load_a(p);
+        return *this;
+    }
+    // Member function to change a single element in vector
+    Vec32us const insert(int index, uint16_t value) {
+        Vec32s::insert(index, (int16_t)value);
+        return *this;
+    }
+    // Member function extract a single element from vector
+    uint16_t extract(int index) const {
+        return (uint16_t)Vec32s::extract(index);
+    }
+    // Extract a single element. Use store function if extracting more than one element.
+    // Operator [] can only read an element, not write.
+    uint16_t operator [] (int index) const {
+        return (uint16_t)Vec32s::extract(index);
+    }
+    // Member functions to split into two Vec16us:
+    Vec16us get_low() const {
+        return Vec16us(Vec32s::get_low());
+    }
+    Vec16us get_high() const {
+        return Vec16us(Vec32s::get_high());
+    }
+    static constexpr int elementtype() {
+        return 7;
+    }
+};
+
+// Define operators for this class
+
+// vector operator + : add element by element
+static inline Vec32us operator + (Vec32us const a, Vec32us const b) {
+    return Vec32us(a.get_low() + b.get_low(), a.get_high() + b.get_high());
+}
+
+// vector operator - : subtract element by element
+static inline Vec32us operator - (Vec32us const a, Vec32us const b) {
+    return Vec32us(a.get_low() - b.get_low(), a.get_high() - b.get_high());
+}
+
+// vector operator * : multiply element by element
+static inline Vec32us operator * (Vec32us const a, Vec32us const b) {
+    return Vec32us(a.get_low() * b.get_low(), a.get_high() * b.get_high());
+}
+
+// vector operator / : divide. See bottom of file
+
+// vector operator >> : shift right logical all elements
+static inline Vec32us operator >> (Vec32us const a, uint32_t b) {
+    return Vec32us(a.get_low() >> b, a.get_high() >> b);
+}
+static inline Vec32us operator >> (Vec32us const a, int b) {
+    return a >> uint32_t(b);
+}
+
+// vector operator >>= : shift right logical
+static inline Vec32us & operator >>= (Vec32us & a, uint32_t b) {
+    a = a >> b;
+    return a;
+}
+
+// vector operator >>= : shift right logical (signed b)
+static inline Vec32us & operator >>= (Vec32us & a, int32_t b) {
+    a = a >> uint32_t(b);
+    return a;
+}
+
+// vector operator << : shift left all elements
+static inline Vec32us operator << (Vec32us const a, uint32_t b) {
+    return Vec32us(a.get_low() << b, a.get_high() << b);
+}
+static inline Vec32us operator << (Vec32us const a, int b) {
+    return a << uint32_t(b);
+}
+
+// vector operator < : returns true for elements for which a < b (unsigned)
+static inline Vec32sb operator < (Vec32us const a, Vec32us const b) {
+    return Vec32sb(a.get_low() < b.get_low(), a.get_high() < b.get_high());
+}
+
+// vector operator > : returns true for elements for which a > b (unsigned)
+static inline Vec32sb operator > (Vec32us const a, Vec32us const b) {
+    return b < a;
+}
+
+// vector operator >= : returns true for elements for which a >= b (unsigned)
+static inline Vec32sb operator >= (Vec32us const a, Vec32us const b) {
+    return Vec32sb(a.get_low() >= b.get_low(), a.get_high() >= b.get_high());
+}
+
+// vector operator <= : returns true for elements for which a <= b (unsigned)
+static inline Vec32sb operator <= (Vec32us const a, Vec32us const b) {
+    return b >= a;
+}
+
+// vector operator & : bitwise and
+static inline Vec32us operator & (Vec32us const a, Vec32us const b) {
+    return Vec32us(Vec32s(a) & Vec32s(b));
+}
+
+// vector operator | : bitwise or
+static inline Vec32us operator | (Vec32us const a, Vec32us const b) {
+    return Vec32us(Vec32s(a) | Vec32s(b));
+}
+
+// vector operator ^ : bitwise xor
+static inline Vec32us operator ^ (Vec32us const a, Vec32us const b) {
+    return Vec32us(Vec32s(a) ^ Vec32s(b));
+}
+
+// vector operator ~ : bitwise not
+static inline Vec32us operator ~ (Vec32us const a) {
+    return Vec32us( ~ Vec32s(a));
+}
+
+// Functions for this class
+
+// Select between two operands. Corresponds to this pseudocode:
+// for (int i = 0; i < 16; i++) result[i] = s[i] ? a[i] : b[i];
+static inline Vec32us select (Vec32sb const s, Vec32us const a, Vec32us const b) {
+    return Vec32us(select(s.get_low(), a.get_low(), b.get_low()), select(s.get_high(), a.get_high(), b.get_high()));
+}
+
+// Conditional add: For all vector elements i: result[i] = f[i] ? (a[i] + b[i]) : a[i]
+static inline Vec32us if_add (Vec32sb const f, Vec32us const a, Vec32us const b) {
+    return Vec32us(if_add(f.get_low(), a.get_low(), b.get_low()), if_add(f.get_high(), a.get_high(), b.get_high()));
+}
+
+// Conditional subtract
+static inline Vec32us if_sub (Vec32sb const f, Vec32us const a, Vec32us const b) {
+    return Vec32us(if_sub(f.get_low(), a.get_low(), b.get_low()), if_sub(f.get_high(), a.get_high(), b.get_high()));
+}
+
+// Conditional multiply
+static inline Vec32us if_mul (Vec32sb const f, Vec32us const a, Vec32us const b) {
+    return Vec32us(if_mul(f.get_low(), a.get_low(), b.get_low()), if_mul(f.get_high(), a.get_high(), b.get_high()));
+}
+
+// function add_saturated: add element by element, unsigned with saturation
+static inline Vec32us add_saturated(Vec32us const a, Vec32us const b) {
+    return Vec32us(add_saturated(a.get_low(), b.get_low()), add_saturated(a.get_high(), b.get_high()));
+}
+
+// function sub_saturated: subtract element by element, unsigned with saturation
+static inline Vec32us sub_saturated(Vec32us const a, Vec32us const b) {
+    return Vec32us(sub_saturated(a.get_low(), b.get_low()), sub_saturated(a.get_high(), b.get_high()));
+}
+
+// function max: a > b ? a : b
+static inline Vec32us max(Vec32us const a, Vec32us const b) {
+    return Vec32us(max(a.get_low(), b.get_low()), max(a.get_high(), b.get_high()));
+}
+
+// function min: a < b ? a : b
+static inline Vec32us min(Vec32us const a, Vec32us const b) {
+    return Vec32us(min(a.get_low(), b.get_low()), min(a.get_high(), b.get_high()));
+}
+
+
+/*****************************************************************************
+*
+*          Vector permute and blend functions
+*
+*****************************************************************************/
+
+// Permute vector of 32 16-bit integers.
+template <int i0,  int i1,  int i2,  int i3,  int i4,  int i5,  int i6,  int i7,
+    int i8,  int i9,  int i10, int i11, int i12, int i13, int i14, int i15,
+    int i16, int i17, int i18, int i19, int i20, int i21, int i22, int i23,
+    int i24, int i25, int i26, int i27, int i28, int i29, int i30, int i31 >
+    static inline Vec32s permute32(Vec32s const a) {
+    return Vec32s(
+        blend16<i0, i1, i2 ,i3 ,i4 ,i5 ,i6 ,i7, i8, i9, i10,i11,i12,i13,i14,i15> (a.get_low(), a.get_high()),
+        blend16<i16,i17,i18,i19,i20,i21,i22,i23,i24,i25,i26,i27,i28,i29,i30,i31> (a.get_low(), a.get_high()));
+}
+
+template <int... i0 >
+    static inline Vec32us permute32(Vec32us const a) {
+    return Vec32us (permute32<i0...> (Vec32s(a)));
+}
+
+// Permute vector of 64 8-bit integers.
+template <
+    int i0,  int i1,  int i2,  int i3,  int i4,  int i5,  int i6,  int i7,
+    int i8,  int i9,  int i10, int i11, int i12, int i13, int i14, int i15,
+    int i16, int i17, int i18, int i19, int i20, int i21, int i22, int i23,
+    int i24, int i25, int i26, int i27, int i28, int i29, int i30, int i31,
+    int i32, int i33, int i34, int i35, int i36, int i37, int i38, int i39,
+    int i40, int i41, int i42, int i43, int i44, int i45, int i46, int i47,
+    int i48, int i49, int i50, int i51, int i52, int i53, int i54, int i55,
+    int i56, int i57, int i58, int i59, int i60, int i61, int i62, int i63 >
+    static inline Vec64c permute64(Vec64c const a) {
+    return Vec64c(
+        blend32 <
+        i0, i1, i2, i3, i4, i5, i6, i7, i8, i9, i10, i11, i12, i13, i14, i15,
+        i16, i17, i18, i19, i20, i21, i22, i23, i24, i25, i26, i27, i28, i29, i30, i31
+        > (a.get_low(), a.get_high()),
+        blend32 <
+        i32, i33, i34, i35, i36, i37, i38, i39, i40, i41, i42, i43, i44, i45, i46, i47,
+        i48, i49, i50, i51, i52, i53, i54, i55, i56, i57, i58, i59, i60, i61, i62, i63
+        > (a.get_low(), a.get_high()));
+}
+
+template <int... i0 >
+static inline Vec64uc permute64(Vec64uc const a) {
+    return Vec64uc (permute64<i0...> (Vec64c(a)));
+}
+
+// Blend vector of 32 16-bit integers
+template <int i0, int i1, int i2, int i3, int i4, int i5, int i6, int i7,
+    int i8,  int i9,  int i10, int i11, int i12, int i13, int i14, int i15,
+    int i16, int i17, int i18, int i19, int i20, int i21, int i22, int i23,
+    int i24, int i25, int i26, int i27, int i28, int i29, int i30, int i31 >
+    static inline Vec32s blend32(Vec32s const& a, Vec32s const& b) {
+    Vec16s x0 = blend_half<Vec32s, i0, i1, i2, i3, i4, i5, i6, i7, i8, i9, i10, i11, i12, i13, i14, i15>(a, b);
+    Vec16s x1 = blend_half<Vec32s, i16, i17, i18, i19, i20, i21, i22, i23, i24, i25, i26, i27, i28, i29, i30, i31>(a, b);
+    return Vec32s(x0, x1);
+}
+
+template <int ... i0 >
+static inline Vec32us blend32(Vec32us const a, Vec32us const b) {
+    return Vec32us(blend32<i0 ...> (Vec32s(a),Vec32s(b)));
+}
+
+// Blend vector of 64 8-bit integers
+template <
+    int i0,  int i1,  int i2,  int i3,  int i4,  int i5,  int i6,  int i7,
+    int i8,  int i9,  int i10, int i11, int i12, int i13, int i14, int i15,
+    int i16, int i17, int i18, int i19, int i20, int i21, int i22, int i23,
+    int i24, int i25, int i26, int i27, int i28, int i29, int i30, int i31,
+    int i32, int i33, int i34, int i35, int i36, int i37, int i38, int i39,
+    int i40, int i41, int i42, int i43, int i44, int i45, int i46, int i47,
+    int i48, int i49, int i50, int i51, int i52, int i53, int i54, int i55,
+    int i56, int i57, int i58, int i59, int i60, int i61, int i62, int i63 >
+    static inline Vec64c blend64(Vec64c const a, Vec64c const b) {
+    Vec32c x0 = blend_half < Vec64c,
+        i0, i1, i2, i3, i4, i5, i6, i7, i8, i9, i10, i11, i12, i13, i14, i15,
+        i16, i17, i18, i19, i20, i21, i22, i23, i24, i25, i26, i27, i28, i29, i30, i31 > (a, b);
+    Vec32c x1 = blend_half < Vec64c,
+        i32, i33, i34, i35, i36, i37, i38, i39, i40, i41, i42, i43, i44, i45, i46, i47,
+        i48, i49, i50, i51, i52, i53, i54, i55, i56, i57, i58, i59, i60, i61, i62, i63 > (a, b);
+    return Vec64c(x0, x1);
+}
+
+template <int ... i0 >
+static inline Vec64uc blend64(Vec64uc const a, Vec64uc const b) {
+    return Vec64uc(blend64 <i0 ...>(Vec64c(a), Vec64c(b)));
+}
+
+
+/*****************************************************************************
+*
+*          Vector lookup functions
+*
+******************************************************************************
+*
+* These functions use vector elements as indexes into a table.
+* The table is given as one or more vectors
+*
+*****************************************************************************/
+
+// lookup in table of 64 int8_t values
+static inline Vec64c lookup64(Vec64c const index, Vec64c const table1) {
+    int8_t table[64], result[64];
+    table1.store(table);
+    for (int i=0; i<64; i++) result[i] = table[index[i] & 63];
+    return Vec64c().load(result);
+}
+
+// lookup in table of 128 int8_t values
+static inline Vec64c lookup128(Vec64c const index, Vec64c const table1, Vec64c const table2) {
+    int8_t table[128], result[64];
+    table1.store(table);  table2.store(table+64);
+    for (int i=0; i<64; i++) result[i] = table[index[i] & 127];
+    return Vec64c().load(result);
+}
+
+// lookup in table of 256 int8_t values.
+// The complete table of all possible 256 byte values is contained in four vectors
+// The index is treated as unsigned
+static inline Vec64c lookup256(Vec64c const index, Vec64c const table1, Vec64c const table2, Vec64c const table3, Vec64c const table4) {
+    int8_t table[256], result[64];
+    table1.store(table);  table2.store(table+64);  table3.store(table+128);  table4.store(table+192);
+    for (int i=0; i<64; i++) result[i] = table[index[i] & 255];
+    return Vec64c().load(result);
+}
+
+// lookup in table of 32 values
+static inline Vec32s lookup32(Vec32s const index, Vec32s const table1) {
+    int16_t table[32], result[32];
+    table1.store(table);
+    for (int i=0; i<32; i++) result[i] = table[index[i] & 31];
+    return Vec32s().load(result);
+}
+
+// lookup in table of 64 values
+static inline Vec32s lookup64(Vec32s const index, Vec32s const table1, Vec32s const table2) {
+    int16_t table[64], result[32];
+    table1.store(table);  table2.store(table+32);
+    for (int i=0; i<32; i++) result[i] = table[index[i] & 63];
+    return Vec32s().load(result);
+}
+
+// lookup in table of 128 values
+static inline Vec32s lookup128(Vec32s const index, Vec32s const table1, Vec32s const table2, Vec32s const table3, Vec32s const table4) {
+    int16_t table[128], result[32];
+    table1.store(table);  table2.store(table+32);  table3.store(table+64);  table4.store(table+96);
+    for (int i=0; i<32; i++) result[i] = table[index[i] & 127];
+    return Vec32s().load(result);
+}
+
+
+/*****************************************************************************
+*
+*          Byte shifts
+*
+*****************************************************************************/
+
+// Function shift_bytes_up: shift whole vector left by b bytes.
+template <unsigned int b>
+static inline Vec64c shift_bytes_up(Vec64c const a) {
+    int8_t dat[128];
+    if (b < 64) {
+        Vec64c(0).store(dat);
+        a.store(dat+b);
+        return Vec64c().load(dat);
+    }
+    else return 0;
+}
+
+// Function shift_bytes_down: shift whole vector right by b bytes
+template <unsigned int b>
+static inline Vec64c shift_bytes_down(Vec64c const a) {
+    int8_t dat[128];
+    if (b < 64) {
+        a.store(dat);
+        Vec64c(0).store(dat+64);
+        return Vec64c().load(dat+b);
+    }
+    else return 0;
+}
+
+
+/*****************************************************************************
+*
+*          Functions for conversion between integer sizes and vector types
+*
+*****************************************************************************/
+
+// Extend 8-bit integers to 16-bit integers, signed and unsigned
+
+// Function extend_low : extends the low 32 elements to 16 bits with sign extension
+static inline Vec32s extend_low (Vec64c const a) {
+    return Vec32s(extend_low(a.get_low()), extend_high(a.get_low()));
+}
+
+// Function extend_high : extends the high 16 elements to 16 bits with sign extension
+static inline Vec32s extend_high (Vec64c const a) {
+    return Vec32s(extend_low(a.get_high()), extend_high(a.get_high()));
+}
+
+// Function extend_low : extends the low 16 elements to 16 bits with zero extension
+static inline Vec32us extend_low (Vec64uc const a) {
+    return Vec32us(extend_low(a.get_low()), extend_high(a.get_low()));
+}
+
+// Function extend_high : extends the high 19 elements to 16 bits with zero extension
+static inline Vec32us extend_high (Vec64uc const a) {
+    return Vec32us(extend_low(a.get_high()), extend_high(a.get_high()));
+}
+
+// Extend 16-bit integers to 32-bit integers, signed and unsigned
+
+// Function extend_low : extends the low 8 elements to 32 bits with sign extension
+static inline Vec16i extend_low (Vec32s const a) {
+    return Vec16i(extend_low(a.get_low()), extend_high(a.get_low()));
+}
+
+// Function extend_high : extends the high 8 elements to 32 bits with sign extension
+static inline Vec16i extend_high (Vec32s const a) {
+    return Vec16i(extend_low(a.get_high()), extend_high(a.get_high()));
+}
+
+// Function extend_low : extends the low 8 elements to 32 bits with zero extension
+static inline Vec16ui extend_low (Vec32us const a) {
+    return Vec16ui(extend_low(a.get_low()), extend_high(a.get_low()));
+}
+
+// Function extend_high : extends the high 8 elements to 32 bits with zero extension
+static inline Vec16ui extend_high (Vec32us const a) {
+    return Vec16ui(extend_low(a.get_high()), extend_high(a.get_high()));
+}
+
+
+// Compress 16-bit integers to 8-bit integers, signed and unsigned, with and without saturation
+
+// Function compress : packs two vectors of 16-bit integers into one vector of 8-bit integers
+// Overflow wraps around
+static inline Vec64c compress (Vec32s const low, Vec32s const high) {
+    return Vec64c(compress(low.get_low(),low.get_high()), compress(high.get_low(),high.get_high()));
+}
+
+// Function compress : packs two vectors of 16-bit integers into one vector of 8-bit integers
+// Signed, with saturation
+static inline Vec64c compress_saturated (Vec32s const low, Vec32s const high) {
+    return Vec64c(compress_saturated(low.get_low(),low.get_high()), compress_saturated(high.get_low(),high.get_high()));
+}
+
+// Function compress : packs two vectors of 16-bit integers to one vector of 8-bit integers
+// Unsigned, overflow wraps around
+static inline Vec64uc compress (Vec32us const low, Vec32us const high) {
+    return  Vec64uc(compress((Vec32s)low, (Vec32s)high));
+}
+
+// Function compress : packs two vectors of 16-bit integers into one vector of 8-bit integers
+// Unsigned, with saturation
+static inline Vec64uc compress_saturated (Vec32us const low, Vec32us const high) {
+    return Vec64uc(compress_saturated(low.get_low(),low.get_high()), compress_saturated(high.get_low(),high.get_high()));
+}
+
+// Compress 32-bit integers to 16-bit integers, signed and unsigned, with and without saturation
+
+// Function compress : packs two vectors of 32-bit integers into one vector of 16-bit integers
+// Overflow wraps around
+static inline Vec32s compress (Vec16i const low, Vec16i const high) {
+    return Vec32s(compress(low.get_low(),low.get_high()), compress(high.get_low(),high.get_high()));
+}
+
+// Function compress : packs two vectors of 32-bit integers into one vector of 16-bit integers
+// Signed with saturation
+static inline Vec32s compress_saturated (Vec16i const low, Vec16i const high) {
+    return Vec32s(compress_saturated(low.get_low(),low.get_high()), compress_saturated(high.get_low(),high.get_high()));
+}
+
+// Function compress : packs two vectors of 32-bit integers into one vector of 16-bit integers
+// Overflow wraps around
+static inline Vec32us compress (Vec16ui const low, Vec16ui const high) {
+    return Vec32us (compress((Vec16i)low, (Vec16i)high));
+}
+
+// Function compress : packs two vectors of 32-bit integers into one vector of 16-bit integers
+// Unsigned, with saturation
+static inline Vec32us compress_saturated (Vec16ui const low, Vec16ui const high) {
+    return Vec32us(compress_saturated(low.get_low(),low.get_high()), compress_saturated(high.get_low(),high.get_high()));
+}
+
+// extend vectors to double size by adding zeroes
+static inline Vec64c extend_z(Vec32c a) {
+    return Vec64c(a, Vec32c(0));
+}
+static inline Vec64uc extend_z(Vec32uc a) {
+    return Vec64uc(a, Vec32uc(0));
+}
+static inline Vec32s extend_z(Vec16s a) {
+    return Vec32s(a, Vec16s(0));
+}
+static inline Vec32us extend_z(Vec16us a) {
+    return Vec32us(a, Vec16us(0));
+}
+
+// broad boolean vectors
+
+static inline Vec64cb extend_z(Vec32cb a) {
+    return Vec64cb(a, Vec32cb(false));
+}
+static inline Vec32sb extend_z(Vec16sb a) {
+    return Vec32sb(a, Vec16sb(false));
+}
+
+
+
+/*****************************************************************************
+*
+*          Integer division operators
+*
+*          Please see the file vectori128.h for explanation.
+*
+*****************************************************************************/
+
+// vector operator / : divide each element by divisor
+
+// vector of 32 16-bit signed integers
+static inline Vec32s operator / (Vec32s const a, Divisor_s const d) {
+    return Vec32s(a.get_low() / d, a.get_high() / d);
+}
+
+// vector of 16 16-bit unsigned integers
+static inline Vec32us operator / (Vec32us const a, Divisor_us const d) {
+    return Vec32us(a.get_low() / d, a.get_high() / d);
+}
+
+// vector of 32 8-bit signed integers
+static inline Vec64c operator / (Vec64c const a, Divisor_s const d) {
+    return Vec64c(a.get_low() / d, a.get_high() / d);
+}
+
+// vector of 32 8-bit unsigned integers
+static inline Vec64uc operator / (Vec64uc const a, Divisor_us const d) {
+    return Vec64uc(a.get_low() / d, a.get_high() / d);
+}
+
+// vector operator /= : divide
+static inline Vec32s & operator /= (Vec32s & a, Divisor_s const d) {
+    a = a / d;
+    return a;
+}
+
+// vector operator /= : divide
+static inline Vec32us & operator /= (Vec32us & a, Divisor_us const d) {
+    a = a / d;
+    return a;
+}
+
+// vector operator /= : divide
+static inline Vec64c & operator /= (Vec64c & a, Divisor_s const d) {
+    a = a / d;
+    return a;
+}
+
+// vector operator /= : divide
+static inline Vec64uc & operator /= (Vec64uc & a, Divisor_us const d) {
+    a = a / d;
+    return a;
+}
+
+
+/*****************************************************************************
+*
+*          Integer division 2: divisor is a compile-time constant
+*
+*****************************************************************************/
+
+
+// Divide Vec32s by compile-time constant
+template <int d>
+static inline Vec32s divide_by_i(Vec32s const a) {
+    return Vec32s(divide_by_i<d>(a.get_low()), divide_by_i<d>(a.get_high()));
+}
+
+// define Vec32s a / const_int(d)
+template <int d>
+static inline Vec32s operator / (Vec32s const a, Const_int_t<d>) {
+    return Vec32s(divide_by_i<d>(a.get_low()), divide_by_i<d>(a.get_high()));
+}
+
+// define Vec32s a / const_uint(d)
+template <uint32_t d>
+static inline Vec32s operator / (Vec32s const a, Const_uint_t<d>) {
+    return Vec32s(divide_by_i<d>(a.get_low()), divide_by_i<d>(a.get_high()));
+}
+
+// vector operator /= : divide
+template <int32_t d>
+static inline Vec32s & operator /= (Vec32s & a, Const_int_t<d> b) {
+    a = a / b;
+    return a;
+}
+
+// vector operator /= : divide
+template <uint32_t d>
+static inline Vec32s & operator /= (Vec32s & a, Const_uint_t<d> b) {
+    a = a / b;
+    return a;
+}
+
+// Divide Vec32us by compile-time constant
+template <uint32_t d>
+static inline Vec32us divide_by_ui(Vec32us const a) {
+    return Vec32us( divide_by_ui<d>(a.get_low()), divide_by_ui<d>(a.get_high()));
+}
+
+// define Vec32us a / const_uint(d)
+template <uint32_t d>
+static inline Vec32us operator / (Vec32us const a, Const_uint_t<d>) {
+    return divide_by_ui<d>(a);
+}
+
+// define Vec32us a / const_int(d)
+template <int d>
+static inline Vec32us operator / (Vec32us const a, Const_int_t<d>) {
+    static_assert(d >= 0, "Dividing unsigned integer by negative is ambiguous");
+    return divide_by_ui<d>(a);                                       // unsigned divide
+}
+
+// vector operator /= : divide
+template <uint32_t d>
+static inline Vec32us & operator /= (Vec32us & a, Const_uint_t<d> b) {
+    a = a / b;
+    return a;
+}
+
+// vector operator /= : divide
+template <int32_t d>
+static inline Vec32us & operator /= (Vec32us & a, Const_int_t<d> b) {
+    a = a / b;
+    return a;
+}
+
+// define Vec64c a / const_int(d)
+template <int d>
+static inline Vec64c operator / (Vec64c const a, Const_int_t<d> b) {
+    return Vec64c( a.get_low() / b, a.get_high() / b);
+}
+
+// define Vec64c a / const_uint(d)
+template <uint32_t d>
+static inline Vec64c operator / (Vec64c const a, Const_uint_t<d> b) {
+    return Vec64c( a.get_low() / b, a.get_high() / b);
+}
+
+// vector operator /= : divide
+template <int32_t d>
+static inline Vec64c & operator /= (Vec64c & a, Const_int_t<d> b) {
+    a = a / b;
+    return a;
+}
+// vector operator /= : divide
+template <uint32_t d>
+static inline Vec64c & operator /= (Vec64c & a, Const_uint_t<d> b) {
+    a = a / b;
+    return a;
+}
+
+// define Vec64uc a / const_uint(d)
+template <uint32_t d>
+static inline Vec64uc operator / (Vec64uc const a, Const_uint_t<d> b) {
+    return Vec64uc( a.get_low() / b, a.get_high() / b);
+}
+
+// define Vec64uc a / const_int(d)
+template <int d>
+static inline Vec64uc operator / (Vec64uc const a, Const_int_t<d> b) {
+    return Vec64uc( a.get_low() / b, a.get_high() / b);
+}
+
+// vector operator /= : divide
+template <uint32_t d>
+static inline Vec64uc & operator /= (Vec64uc & a, Const_uint_t<d> b) {
+    a = a / b;
+    return a;
+}
+
+// vector operator /= : divide
+template <int32_t d>
+static inline Vec64uc & operator /= (Vec64uc & a, Const_int_t<d> b) {
+    a = a / b;
+    return a;
+}
+
+#ifdef VCL_NAMESPACE
+}
+#endif
+
+#endif // VECTORI512S_H
diff --git a/EEDI3/vectorclass/vectormath_common.h b/EEDI3/vectorclass/vectormath_common.h
new file mode 100644
index 0000000..b006f03
--- /dev/null
+++ b/EEDI3/vectorclass/vectormath_common.h
@@ -0,0 +1,327 @@
+/***************************  vectormath_common.h   ****************************
+* Author:        Agner Fog
+* Date created:  2014-04-18
+* Last modified: 2022-07-20
+* Version:       2.02.00
+* Project:       vector classes
+* Description:
+* Header file containing common code for inline version of mathematical functions.
+*
+* For detailed instructions, see VectorClass.pdf
+*
+* (c) Copyright 2014-2022 Agner Fog.
+* Apache License version 2.0 or later.
+******************************************************************************/
+
+#ifndef VECTORMATH_COMMON_H
+#define VECTORMATH_COMMON_H  2
+
+#ifdef VECTORMATH_LIB_H
+#error conflicting header files. More than one implementation of mathematical functions included
+#endif
+
+#include <cmath>
+
+#ifndef VECTORCLASS_H
+#include "vectorclass.h"
+#endif
+
+#if VECTORCLASS_H < 20200
+#error Incompatible versions of vector class library mixed
+#endif
+
+
+/******************************************************************************
+                    Define NAN payload values
+******************************************************************************/
+#define NAN_LOG 0x101  // logarithm for x<0
+#define NAN_POW 0x102  // negative number raised to non-integer power
+#define NAN_HYP 0x104  // acosh for x<1 and atanh for abs(x)>1
+
+
+/******************************************************************************
+                    Define mathematical constants
+******************************************************************************/
+#define VM_PI       3.14159265358979323846           // pi
+#define VM_PI_2     1.57079632679489661923           // pi / 2
+#define VM_PI_4     0.785398163397448309616          // pi / 4
+#define VM_SQRT2    1.41421356237309504880           // sqrt(2)
+#define VM_LOG2E    1.44269504088896340736           // 1/log(2)
+#define VM_LOG10E   0.434294481903251827651          // 1/log(10)
+#define VM_LOG210   3.321928094887362347808          // log2(10)
+#define VM_LN2      0.693147180559945309417          // log(2)
+#define VM_LN10     2.30258509299404568402           // log(10)
+#define VM_SMALLEST_NORMAL  2.2250738585072014E-308  // smallest normal number, double
+#define VM_SMALLEST_NORMALF 1.17549435E-38f          // smallest normal number, float
+
+
+#ifdef VCL_NAMESPACE
+namespace VCL_NAMESPACE {
+#endif
+
+/******************************************************************************
+      templates for producing infinite and nan in desired vector type
+******************************************************************************/
+template <typename VTYPE>
+static inline VTYPE infinite_vec();
+
+template <>
+inline Vec2d infinite_vec<Vec2d>() {
+    return infinite2d();
+}
+
+template <>
+inline Vec4f infinite_vec<Vec4f>() {
+    return infinite4f();
+}
+
+#if MAX_VECTOR_SIZE >= 256
+
+template <>
+inline Vec4d infinite_vec<Vec4d>() {
+    return infinite4d();
+}
+
+template <>
+inline Vec8f infinite_vec<Vec8f>() {
+    return infinite8f();
+}
+
+#endif // MAX_VECTOR_SIZE >= 256
+
+#if MAX_VECTOR_SIZE >= 512
+
+template <>
+inline Vec8d infinite_vec<Vec8d>() {
+    return infinite8d();
+}
+
+template <>
+inline Vec16f infinite_vec<Vec16f>() {
+    return infinite16f();
+}
+
+#endif // MAX_VECTOR_SIZE >= 512
+
+
+
+/******************************************************************************
+*                 Detect NAN codes
+*
+* These functions return the code hidden in a NAN. The sign bit is ignored
+******************************************************************************/
+
+static inline Vec4ui nan_code(Vec4f const x) {
+    Vec4ui a = Vec4ui(reinterpret_i(x));
+    Vec4ui const n = 0x007FFFFF;
+    return select(Vec4ib(is_nan(x)), a & n, 0);
+}
+
+// This function returns the code hidden in a NAN. The sign bit is ignored
+static inline Vec2uq nan_code(Vec2d const x) {
+    Vec2uq a = Vec2uq(reinterpret_i(x));
+    return select(Vec2qb(is_nan(x)), a << 12 >> (12+29), 0);
+}
+
+#if MAX_VECTOR_SIZE >= 256
+
+// This function returns the code hidden in a NAN. The sign bit is ignored
+static inline Vec8ui nan_code(Vec8f const x) {
+    Vec8ui a = Vec8ui(reinterpret_i(x));
+    Vec8ui const n = 0x007FFFFF;
+    return select(Vec8ib(is_nan(x)), a & n, 0);
+}
+
+// This function returns the code hidden in a NAN. The sign bit is ignored
+static inline Vec4uq nan_code(Vec4d const x) {
+    Vec4uq a = Vec4uq(reinterpret_i(x));
+    return select(Vec4qb(is_nan(x)), a << 12 >> (12+29), 0);
+}
+
+#endif // MAX_VECTOR_SIZE >= 256
+#if MAX_VECTOR_SIZE >= 512
+
+// This function returns the code hidden in a NAN. The sign bit is ignored
+static inline Vec16ui nan_code(Vec16f const x) {
+    Vec16ui a = Vec16ui(reinterpret_i(x));
+    Vec16ui const n = 0x007FFFFF;
+    return select(Vec16ib(is_nan(x)), a & n, 0);
+}
+
+// This function returns the code hidden in a NAN. The sign bit is ignored
+static inline Vec8uq nan_code(Vec8d const x) {
+    Vec8uq a = Vec8uq(reinterpret_i(x));
+    return select(Vec8qb(is_nan(x)), a << 12 >> (12+29), 0);
+}
+
+#endif // MAX_VECTOR_SIZE >= 512
+
+
+/******************************************************************************
+                  templates for polynomials
+Using Estrin's scheme to make shorter dependency chains and use FMA, starting
+longest dependency chains first.
+******************************************************************************/
+
+// template <typedef VECTYPE, typedef CTYPE>
+template <class VTYPE, class CTYPE>
+static inline VTYPE polynomial_2(VTYPE const x, CTYPE c0, CTYPE c1, CTYPE c2) {
+    // calculates polynomial c2*x^2 + c1*x + c0
+    // VTYPE may be a vector type, CTYPE is a scalar type
+    VTYPE x2 = x * x;
+    //return = x2 * c2 + (x * c1 + c0);
+    return mul_add(x2, c2, mul_add(x, c1, c0));
+}
+
+template<class VTYPE, class CTYPE>
+static inline VTYPE polynomial_3(VTYPE const x, CTYPE c0, CTYPE c1, CTYPE c2, CTYPE c3) {
+    // calculates polynomial c3*x^3 + c2*x^2 + c1*x + c0
+    // VTYPE may be a vector type, CTYPE is a scalar type
+    VTYPE x2 = x * x;
+    //return (c2 + c3*x)*x2 + (c1*x + c0);
+    return mul_add(mul_add(c3, x, c2), x2, mul_add(c1, x, c0));
+}
+
+template<class VTYPE, class CTYPE>
+static inline VTYPE polynomial_4(VTYPE const x, CTYPE c0, CTYPE c1, CTYPE c2, CTYPE c3, CTYPE c4) {
+    // calculates polynomial c4*x^4 + c3*x^3 + c2*x^2 + c1*x + c0
+    // VTYPE may be a vector type, CTYPE is a scalar type
+    VTYPE x2 = x * x;
+    VTYPE x4 = x2 * x2;
+    //return (c2+c3*x)*x2 + ((c0+c1*x) + c4*x4);
+    return mul_add(mul_add(c3, x, c2), x2, mul_add(c1, x, c0) + c4*x4);
+}
+
+template<class VTYPE, class CTYPE>
+static inline VTYPE polynomial_4n(VTYPE const x, CTYPE c0, CTYPE c1, CTYPE c2, CTYPE c3) {
+    // calculates polynomial 1*x^4 + c3*x^3 + c2*x^2 + c1*x + c0
+    // VTYPE may be a vector type, CTYPE is a scalar type
+    VTYPE x2 = x * x;
+    VTYPE x4 = x2 * x2;
+    //return (c2+c3*x)*x2 + ((c0+c1*x) + x4);
+    return mul_add(mul_add(c3, x, c2), x2, mul_add(c1, x, c0) + x4);
+}
+
+template<class VTYPE, class CTYPE>
+static inline VTYPE polynomial_5(VTYPE const x, CTYPE c0, CTYPE c1, CTYPE c2, CTYPE c3, CTYPE c4, CTYPE c5) {
+    // calculates polynomial c5*x^5 + c4*x^4 + c3*x^3 + c2*x^2 + c1*x + c0
+    // VTYPE may be a vector type, CTYPE is a scalar type
+    VTYPE x2 = x * x;
+    VTYPE x4 = x2 * x2;
+    //return (c2+c3*x)*x2 + ((c4+c5*x)*x4 + (c0+c1*x));
+    return mul_add(mul_add(c3, x, c2), x2, mul_add(mul_add(c5, x, c4), x4, mul_add(c1, x, c0)));
+}
+
+template<class VTYPE, class CTYPE>
+static inline VTYPE polynomial_5n(VTYPE const x, CTYPE c0, CTYPE c1, CTYPE c2, CTYPE c3, CTYPE c4) {
+    // calculates polynomial 1*x^5 + c4*x^4 + c3*x^3 + c2*x^2 + c1*x + c0
+    // VTYPE may be a vector type, CTYPE is a scalar type
+    VTYPE x2 = x * x;
+    VTYPE x4 = x2 * x2;
+    //return (c2+c3*x)*x2 + ((c4+x)*x4 + (c0+c1*x));
+    return mul_add(mul_add(c3, x, c2), x2, mul_add(c4 + x, x4, mul_add(c1, x, c0)));
+}
+
+template<class VTYPE, class CTYPE>
+static inline VTYPE polynomial_6(VTYPE const x, CTYPE c0, CTYPE c1, CTYPE c2, CTYPE c3, CTYPE c4, CTYPE c5, CTYPE c6) {
+    // calculates polynomial c6*x^6 + c5*x^5 + c4*x^4 + c3*x^3 + c2*x^2 + c1*x + c0
+    // VTYPE may be a vector type, CTYPE is a scalar type
+    VTYPE x2 = x * x;
+    VTYPE x4 = x2 * x2;
+    //return  (c4+c5*x+c6*x2)*x4 + ((c2+c3*x)*x2 + (c0+c1*x));
+    return mul_add(mul_add(c6, x2, mul_add(c5, x, c4)), x4, mul_add(mul_add(c3, x, c2), x2, mul_add(c1, x, c0)));
+}
+
+template<class VTYPE, class CTYPE>
+static inline VTYPE polynomial_6n(VTYPE const x, CTYPE c0, CTYPE c1, CTYPE c2, CTYPE c3, CTYPE c4, CTYPE c5) {
+    // calculates polynomial 1*x^6 + c5*x^5 + c4*x^4 + c3*x^3 + c2*x^2 + c1*x + c0
+    // VTYPE may be a vector type, CTYPE is a scalar type
+    VTYPE x2 = x * x;
+    VTYPE x4 = x2 * x2;
+    //return  (c4+c5*x+x2)*x4 + ((c2+c3*x)*x2 + (c0+c1*x));
+    return mul_add(mul_add(c5, x, c4 + x2), x4, mul_add(mul_add(c3, x, c2), x2, mul_add(c1, x, c0)));
+}
+
+template<class VTYPE, class CTYPE>
+static inline VTYPE polynomial_7(VTYPE const x, CTYPE c0, CTYPE c1, CTYPE c2, CTYPE c3, CTYPE c4, CTYPE c5, CTYPE c6, CTYPE c7) {
+    // calculates polynomial c7*x^7 + c6*x^6 + c5*x^5 + c4*x^4 + c3*x^3 + c2*x^2 + c1*x + c0
+    // VTYPE may be a vector type, CTYPE is a scalar type
+    VTYPE x2 = x * x;
+    VTYPE x4 = x2 * x2;
+    //return  ((c6+c7*x)*x2 + (c4+c5*x))*x4 + ((c2+c3*x)*x2 + (c0+c1*x));
+    return mul_add(mul_add(mul_add(c7, x, c6), x2, mul_add(c5, x, c4)), x4, mul_add(mul_add(c3, x, c2), x2, mul_add(c1, x, c0)));
+}
+
+template<class VTYPE, class CTYPE>
+static inline VTYPE polynomial_8(VTYPE const x, CTYPE c0, CTYPE c1, CTYPE c2, CTYPE c3, CTYPE c4, CTYPE c5, CTYPE c6, CTYPE c7, CTYPE c8) {
+    // calculates polynomial c8*x^8 + c7*x^7 + c6*x^6 + c5*x^5 + c4*x^4 + c3*x^3 + c2*x^2 + c1*x + c0
+    // VTYPE may be a vector type, CTYPE is a scalar type
+    VTYPE x2 = x  * x;
+    VTYPE x4 = x2 * x2;
+    VTYPE x8 = x4 * x4;
+    //return  ((c6+c7*x)*x2 + (c4+c5*x))*x4 + (c8*x8 + (c2+c3*x)*x2 + (c0+c1*x));
+    return mul_add(mul_add(mul_add(c7, x, c6), x2, mul_add(c5, x, c4)), x4,
+        mul_add(mul_add(c3, x, c2), x2, mul_add(c1, x, c0) + c8*x8));
+}
+
+template<class VTYPE, class CTYPE>
+static inline VTYPE polynomial_9(VTYPE const x, CTYPE c0, CTYPE c1, CTYPE c2, CTYPE c3, CTYPE c4, CTYPE c5, CTYPE c6, CTYPE c7, CTYPE c8, CTYPE c9) {
+    // calculates polynomial c9*x^9 + c8*x^8 + c7*x^7 + c6*x^6 + c5*x^5 + c4*x^4 + c3*x^3 + c2*x^2 + c1*x + c0
+    // VTYPE may be a vector type, CTYPE is a scalar type
+    VTYPE x2 = x  * x;
+    VTYPE x4 = x2 * x2;
+    VTYPE x8 = x4 * x4;
+    //return  (((c6+c7*x)*x2 + (c4+c5*x))*x4 + (c8+c9*x)*x8) + ((c2+c3*x)*x2 + (c0+c1*x));
+    return mul_add(mul_add(c9, x, c8), x8, mul_add(
+        mul_add(mul_add(c7, x, c6), x2, mul_add(c5, x, c4)), x4,
+        mul_add(mul_add(c3, x, c2), x2, mul_add(c1, x, c0))));
+}
+
+template<class VTYPE, class CTYPE>
+static inline VTYPE polynomial_10(VTYPE const x, CTYPE c0, CTYPE c1, CTYPE c2, CTYPE c3, CTYPE c4, CTYPE c5, CTYPE c6, CTYPE c7, CTYPE c8, CTYPE c9, CTYPE c10) {
+    // calculates polynomial c10*x^10 + c9*x^9 + c8*x^8 + c7*x^7 + c6*x^6 + c5*x^5 + c4*x^4 + c3*x^3 + c2*x^2 + c1*x + c0
+    // VTYPE may be a vector type, CTYPE is a scalar type
+    VTYPE x2 = x  * x;
+    VTYPE x4 = x2 * x2;
+    VTYPE x8 = x4 * x4;
+    //return  (((c6+c7*x)*x2 + (c4+c5*x))*x4 + (c8+c9*x+c10*x2)*x8) + ((c2+c3*x)*x2 + (c0+c1*x));
+    return mul_add(mul_add(x2, c10, mul_add(c9, x, c8)), x8,
+        mul_add(mul_add(mul_add(c7, x, c6), x2, mul_add(c5, x, c4)), x4,
+            mul_add(mul_add(c3, x, c2), x2, mul_add(c1, x, c0))));
+}
+
+template<class VTYPE, class CTYPE>
+static inline VTYPE polynomial_13(VTYPE const x, CTYPE c0, CTYPE c1, CTYPE c2, CTYPE c3, CTYPE c4, CTYPE c5, CTYPE c6, CTYPE c7, CTYPE c8, CTYPE c9, CTYPE c10, CTYPE c11, CTYPE c12, CTYPE c13) {
+    // calculates polynomial c13*x^13 + c12*x^12 + ... + c1*x + c0
+    // VTYPE may be a vector type, CTYPE is a scalar type
+    VTYPE x2 = x  * x;
+    VTYPE x4 = x2 * x2;
+    VTYPE x8 = x4 * x4;
+    return mul_add(
+        mul_add(
+            mul_add(c13, x, c12), x4,
+            mul_add(mul_add(c11, x, c10), x2, mul_add(c9, x, c8))), x8,
+        mul_add(
+            mul_add(mul_add(c7, x, c6), x2, mul_add(c5, x, c4)), x4,
+            mul_add(mul_add(c3, x, c2), x2, mul_add(c1, x, c0))));
+}
+
+
+template<class VTYPE, class CTYPE>
+static inline VTYPE polynomial_13m(VTYPE const x, CTYPE c2, CTYPE c3, CTYPE c4, CTYPE c5, CTYPE c6, CTYPE c7, CTYPE c8, CTYPE c9, CTYPE c10, CTYPE c11, CTYPE c12, CTYPE c13) {
+    // calculates polynomial c13*x^13 + c12*x^12 + ... + x + 0
+    // VTYPE may be a vector type, CTYPE is a scalar type
+    VTYPE x2 = x  * x;
+    VTYPE x4 = x2 * x2;
+    VTYPE x8 = x4 * x4;
+    // return  ((c8+c9*x) + (c10+c11*x)*x2 + (c12+c13*x)*x4)*x8 + (((c6+c7*x)*x2 + (c4+c5*x))*x4 + ((c2+c3*x)*x2 + x));
+    return mul_add(
+        mul_add(mul_add(c13, x, c12), x4, mul_add(mul_add(c11, x, c10), x2, mul_add(c9, x, c8))), x8,
+        mul_add(mul_add(mul_add(c7, x, c6), x2, mul_add(c5, x, c4)), x4, mul_add(mul_add(c3, x, c2), x2, x)));
+}
+
+#ifdef VCL_NAMESPACE
+}
+#endif
+
+#endif
diff --git a/EEDI3/vectorclass/vectormath_exp.h b/EEDI3/vectorclass/vectormath_exp.h
new file mode 100644
index 0000000..6713d8b
--- /dev/null
+++ b/EEDI3/vectorclass/vectormath_exp.h
@@ -0,0 +1,2173 @@
+/****************************  vectormath_exp.h   ******************************
+* Author:        Agner Fog
+* Date created:  2014-04-18
+* Last modified: 2022-07-20
+* Version:       2.02.00
+* Project:       vector class library
+* Description:
+* Header file containing inline vector functions of logarithms, exponential
+* and power functions:
+* exp         exponential function
+* exp2        exponential function base 2
+* exp10       exponential function base 10
+* exmp1       exponential function minus 1
+* log         natural logarithm
+* log2        logarithm base 2
+* log10       logarithm base 10
+* log1p       natural logarithm of 1+x
+* cbrt        cube root
+* pow         raise vector elements to power
+* pow_ratio   raise vector elements to rational power
+*
+* Theory, methods and inspiration based partially on these sources:
+* > Moshier, Stephen Lloyd Baluk: Methods and programs for mathematical functions.
+*   Ellis Horwood, 1989.
+* > VDT library developed on CERN by Danilo Piparo, Thomas Hauth and Vincenzo Innocente,
+*   2012, https://root.cern.ch/doc/v606_/md_math_vdt_ReadMe.html
+* > Cephes math library by Stephen L. Moshier 1992,
+*   http://www.netlib.org/cephes/
+*
+* For detailed instructions see vcl_manual.pdf
+*
+* (c) Copyright 2014-2022 Agner Fog.
+* Apache License version 2.0 or later.
+******************************************************************************/
+
+#ifndef VECTORMATH_EXP_H
+#define VECTORMATH_EXP_H  202
+
+#include "vectormath_common.h"
+
+#ifdef VCL_NAMESPACE
+namespace VCL_NAMESPACE {
+#endif
+
+/******************************************************************************
+*                 Exponential functions
+******************************************************************************/
+
+// Helper functions, used internally:
+
+// This function calculates pow(2,n) where n must be an integer. Does not check for overflow or underflow
+static inline Vec2d vm_pow2n (Vec2d const n) {
+    const double pow2_52 = 4503599627370496.0;   // 2^52
+    const double bias = 1023.0;                  // bias in exponent
+    Vec2d a = n + (bias + pow2_52);              // put n + bias in least significant bits
+    Vec2q b = reinterpret_i(a);                  // bit-cast to integer
+    Vec2q c = b << 52;                           // shift left 52 places to get into exponent field
+    Vec2d d = reinterpret_d(c);                  // bit-cast back to double
+    return d;
+}
+
+static inline Vec4f vm_pow2n (Vec4f const n) {
+    const float pow2_23 =  8388608.0;            // 2^23
+    const float bias = 127.0;                    // bias in exponent
+    Vec4f a = n + (bias + pow2_23);              // put n + bias in least significant bits
+    Vec4i b = reinterpret_i(a);                  // bit-cast to integer
+    Vec4i c = b << 23;                           // shift left 23 places to get into exponent field
+    Vec4f d = reinterpret_f(c);                  // bit-cast back to float
+    return d;
+}
+
+#if MAX_VECTOR_SIZE >= 256
+
+static inline Vec4d vm_pow2n (Vec4d const n) {
+    const double pow2_52 = 4503599627370496.0;   // 2^52
+    const double bias = 1023.0;                  // bias in exponent
+    Vec4d a = n + (bias + pow2_52);              // put n + bias in least significant bits
+    Vec4q b = reinterpret_i(a);                  // bit-cast to integer
+    Vec4q c = b << 52;                           // shift left 52 places to get value into exponent field
+    Vec4d d = reinterpret_d(c);                  // bit-cast back to double
+    return d;
+}
+
+static inline Vec8f vm_pow2n (Vec8f const n) {
+    const float pow2_23 =  8388608.0;            // 2^23
+    const float bias = 127.0;                    // bias in exponent
+    Vec8f a = n + (bias + pow2_23);              // put n + bias in least significant bits
+    Vec8i b = reinterpret_i(a);                  // bit-cast to integer
+    Vec8i c = b << 23;                           // shift left 23 places to get into exponent field
+    Vec8f d = reinterpret_f(c);                  // bit-cast back to float
+    return d;
+}
+
+#endif // MAX_VECTOR_SIZE >= 256
+
+#if MAX_VECTOR_SIZE >= 512
+
+static inline Vec8d vm_pow2n (Vec8d const n) {
+#ifdef __AVX512ER__
+    return _mm512_exp2a23_round_pd(n, _MM_FROUND_NO_EXC); // this is exact only for integral n
+#else
+    const double pow2_52 = 4503599627370496.0;   // 2^52
+    const double bias = 1023.0;                  // bias in exponent
+    Vec8d a = n + (bias + pow2_52);              // put n + bias in least significant bits
+    Vec8q b = Vec8q(reinterpret_i(a));           // bit-cast to integer
+    Vec8q c = b << 52;                           // shift left 52 places to get value into exponent field
+    Vec8d d = Vec8d(reinterpret_d(c));           // bit-cast back to double
+    return d;
+#endif
+}
+
+static inline Vec16f vm_pow2n (Vec16f const n) {
+#ifdef __AVX512ER__
+    return _mm512_exp2a23_round_ps(n, _MM_FROUND_NO_EXC);
+#else
+    const float pow2_23 =  8388608.0;            // 2^23
+    const float bias = 127.0;                    // bias in exponent
+    Vec16f a = n + (bias + pow2_23);             // put n + bias in least significant bits
+    Vec16i b = Vec16i(reinterpret_i(a));         // bit-cast to integer
+    Vec16i c = b << 23;                          // shift left 23 places to get into exponent field
+    Vec16f d = Vec16f(reinterpret_f(c));         // bit-cast back to float
+    return d;
+#endif
+}
+
+#endif // MAX_VECTOR_SIZE >= 512
+
+
+// Template for exp function, double precision
+// The limit of abs(x) is defined by max_x below
+// This function does not produce denormals
+// Template parameters:
+// VTYPE:  double vector type
+// M1: 0 for exp, 1 for expm1
+// BA: 0 for exp, 1 for 0.5*exp, 2 for pow(2,x), 10 for pow(10,x)
+
+#if true  // choose method
+
+// Taylor expansion
+template<typename VTYPE, int M1, int BA>
+static inline VTYPE exp_d(VTYPE const initial_x) {
+
+    // Taylor coefficients, 1/n!
+    // Not using minimax approximation because we prioritize precision close to x = 0
+    const double p2  = 1./2.;
+    const double p3  = 1./6.;
+    const double p4  = 1./24.;
+    const double p5  = 1./120.;
+    const double p6  = 1./720.;
+    const double p7  = 1./5040.;
+    const double p8  = 1./40320.;
+    const double p9  = 1./362880.;
+    const double p10 = 1./3628800.;
+    const double p11 = 1./39916800.;
+    const double p12 = 1./479001600.;
+    const double p13 = 1./6227020800.;
+
+    // maximum abs(x), value depends on BA, defined below
+    // The lower limit of x is slightly more restrictive than the upper limit.
+    // We are specifying the lower limit, except for BA = 1 because it is not used for negative x
+    double max_x;
+
+    // data vectors
+    VTYPE  x, r, z, n2;
+
+    if constexpr (BA <= 1) { // exp(x)
+        max_x = BA == 0 ? 708.39 : 709.7;        // lower limit for 0.5*exp(x) is -707.6, but we are using 0.5*exp(x) only for positive x in hyperbolic functions
+        const double ln2d_hi = 0.693145751953125;
+        const double ln2d_lo = 1.42860682030941723212E-6;
+        x  = initial_x;
+        r  = round(initial_x*VM_LOG2E);
+        // subtraction in two steps for higher precision
+        x = nmul_add(r, ln2d_hi, x);             //  x -= r * ln2d_hi;
+        x = nmul_add(r, ln2d_lo, x);             //  x -= r * ln2d_lo;
+    }
+    else if constexpr (BA == 2) { // pow(2,x)
+        max_x = 1022.0;
+        r  = round(initial_x);
+        x  = initial_x - r;
+        x *= VM_LN2;
+    }
+    else if constexpr (BA == 10) { // pow(10,x)
+        max_x = 307.65;
+        const double log10_2_hi = 0.30102999554947019; // log10(2) in two parts
+        const double log10_2_lo = 1.1451100899212592E-10;
+        x  = initial_x;
+        r  = round(initial_x*(VM_LOG2E*VM_LN10));
+        // subtraction in two steps for higher precision
+        x  = nmul_add(r, log10_2_hi, x);         //  x -= r * log10_2_hi;
+        x  = nmul_add(r, log10_2_lo, x);         //  x -= r * log10_2_lo;
+        x *= VM_LN10;
+    }
+    else  {  // undefined value of BA
+        return 0.;
+    }
+
+    z = polynomial_13m(x, p2, p3, p4, p5, p6, p7, p8, p9, p10, p11, p12, p13);
+
+    if constexpr (BA == 1) r--;  // 0.5 * exp(x)
+
+    // multiply by power of 2
+    n2 = vm_pow2n(r);
+
+    if constexpr (M1 == 0) {
+        // exp
+        z = (z + 1.0) * n2;
+    }
+    else {
+        // expm1
+        z = mul_add(z, n2, n2 - 1.0);            // z = z * n2 + (n2 - 1.0);
+#ifdef SIGNED_ZERO                               // pedantic preservation of signed zero
+        z = select(initial_x == 0., initial_x, z);
+#endif
+    }
+
+    // check for overflow
+    auto inrange  = abs(initial_x) < max_x;      // boolean vector
+    // check for INF and NAN
+    inrange &= is_finite(initial_x);
+
+    if (horizontal_and(inrange)) {
+        // fast normal path
+        return z;
+    }
+    else {
+        // overflow, underflow and NAN
+        r = select(sign_bit(initial_x), 0.-(M1&1), infinite_vec<VTYPE>()); // value in case of +/- overflow or INF
+        z = select(inrange, z, r);                         // +/- underflow
+        z = select(is_nan(initial_x), initial_x, z);       // NAN goes through
+        return z;
+    }
+}
+
+#else
+
+// Pade expansion uses less code and fewer registers, but is slower
+template<typename VTYPE, int M1, int BA>
+static inline VTYPE exp_d(VTYPE const initial_x) {
+
+    // define constants
+    const double ln2p1   = 0.693145751953125;
+    const double ln2p2   = 1.42860682030941723212E-6;
+    const double log2e   = VM_LOG2E;
+    const double max_exp = 708.39;
+    // coefficients of pade polynomials
+    const double P0exp = 9.99999999999999999910E-1;
+    const double P1exp = 3.02994407707441961300E-2;
+    const double P2exp = 1.26177193074810590878E-4;
+    const double Q0exp = 2.00000000000000000009E0;
+    const double Q1exp = 2.27265548208155028766E-1;
+    const double Q2exp = 2.52448340349684104192E-3;
+    const double Q3exp = 3.00198505138664455042E-6;
+
+    VTYPE  x, r, xx, px, qx, y, n2;              // data vectors
+
+    x = initial_x;
+    r = round(initial_x*log2e);
+
+    // subtraction in one step would gives loss of precision
+    x -= r * ln2p1;
+    x -= r * ln2p2;
+
+    xx = x * x;
+
+    // px = x * P(x^2).
+    px = polynomial_2(xx, P0exp, P1exp, P2exp) * x;
+
+    // Evaluate Q(x^2).
+    qx = polynomial_3(xx, Q0exp, Q1exp, Q2exp, Q3exp);
+
+    // e^x = 1 + 2*P(x^2)/( Q(x^2) - P(x^2) )
+    y = (2.0 * px) / (qx - px);
+
+    // Get 2^n in double.
+    // n  = round_to_int64_limited(r);
+    // n2 = exp2(n);
+    n2 = vm_pow2n(r);  // this is faster
+
+    if constexpr (M1 == 0) {
+        // exp
+        y = (y + 1.0) * n2;
+    }
+    else {
+        // expm1
+        y = y * n2 + (n2 - 1.0);
+    }
+
+    // overflow
+    auto inrange  = abs(initial_x) < max_exp;
+    // check for INF and NAN
+    inrange &= is_finite(initial_x);
+
+    if (horizontal_and(inrange)) {
+        // fast normal path
+        return y;
+    }
+    else {
+        // overflow, underflow and NAN
+        r = select(sign_bit(initial_x), 0.-M1, infinite_vec<VTYPE>()); // value in case of overflow or INF
+        y = select(inrange, y, r);                                     // +/- overflow
+        y = select(is_nan(initial_x), initial_x, y);                   // NAN goes through
+        return y;
+    }
+}
+#endif
+
+// instances of exp_d template
+static inline Vec2d exp(Vec2d const x) {
+    return exp_d<Vec2d, 0, 0>(x);
+}
+
+static inline Vec2d expm1(Vec2d const x) {
+    return exp_d<Vec2d, 3, 0>(x);
+}
+
+static inline Vec2d exp2(Vec2d const x) {
+    return exp_d<Vec2d, 0, 2>(x);
+}
+
+static inline Vec2d exp10(Vec2d const x) {
+    return exp_d<Vec2d, 0, 10>(x);
+}
+
+#if MAX_VECTOR_SIZE >= 256
+
+static inline Vec4d exp(Vec4d const x) {
+    return exp_d<Vec4d, 0, 0>(x);
+}
+
+static inline Vec4d expm1(Vec4d const x) {
+    return exp_d<Vec4d, 3, 0>(x);
+}
+
+static inline Vec4d exp2(Vec4d const x) {
+    return exp_d<Vec4d, 0, 2>(x);
+}
+
+static inline Vec4d exp10(Vec4d const x) {
+    return exp_d<Vec4d, 0, 10>(x);
+}
+
+#endif // MAX_VECTOR_SIZE >= 256
+
+#if MAX_VECTOR_SIZE >= 512
+
+static inline Vec8d exp(Vec8d const x) {
+    return exp_d<Vec8d, 0, 0>(x);
+}
+
+static inline Vec8d expm1(Vec8d const x) {
+    return exp_d<Vec8d, 3, 0>(x);
+}
+
+static inline Vec8d exp2(Vec8d const x) {
+    return exp_d<Vec8d, 0, 2>(x);
+}
+
+static inline Vec8d exp10(Vec8d const x) {
+    return exp_d<Vec8d, 0, 10>(x);
+}
+
+#endif // MAX_VECTOR_SIZE >= 512
+
+
+// Template for exp function, single precision
+// The limit of abs(x) is defined by max_x below
+// This function does not produce denormals
+// Template parameters:
+// VTYPE:  float vector type
+// M1: 0 for exp, 1 for expm1
+// BA: 0 for exp, 1 for 0.5*exp, 2 for pow(2,x), 10 for pow(10,x)
+
+template<typename VTYPE, int M1, int BA>
+static inline VTYPE exp_f(VTYPE const initial_x) {
+
+    // Taylor coefficients
+    const float P0expf   =  1.f/2.f;
+    const float P1expf   =  1.f/6.f;
+    const float P2expf   =  1.f/24.f;
+    const float P3expf   =  1.f/120.f;
+    const float P4expf   =  1.f/720.f;
+    const float P5expf   =  1.f/5040.f;
+
+    VTYPE  x, r, x2, z, n2;                      // data vectors
+
+    // maximum abs(x), value depends on BA, defined below
+    // The lower limit of x is slightly more restrictive than the upper limit.
+    // We are specifying the lower limit, except for BA = 1 because it is not used for negative x
+    float max_x;
+
+    if constexpr (BA <= 1) { // exp(x)
+        const float ln2f_hi  =  0.693359375f;
+        const float ln2f_lo  = -2.12194440e-4f;
+        max_x = (BA == 0) ? 87.3f : 89.0f;
+
+        x = initial_x;
+        r = round(initial_x*float(VM_LOG2E));
+        x = nmul_add(r, VTYPE(ln2f_hi), x);      //  x -= r * ln2f_hi;
+        x = nmul_add(r, VTYPE(ln2f_lo), x);      //  x -= r * ln2f_lo;
+    }
+    else if constexpr (BA == 2) {                // pow(2,x)
+        max_x = 126.f;
+        r = round(initial_x);
+        x = initial_x - r;
+        x = x * (float)VM_LN2;
+    }
+    else if constexpr (BA == 10) {               // pow(10,x)
+        max_x = 37.9f;
+        const float log10_2_hi = 0.301025391f;   // log10(2) in two parts
+        const float log10_2_lo = 4.60503907E-6f;
+        x = initial_x;
+        r = round(initial_x*float(VM_LOG2E*VM_LN10));
+        x = nmul_add(r, VTYPE(log10_2_hi), x);   //  x -= r * log10_2_hi;
+        x = nmul_add(r, VTYPE(log10_2_lo), x);   //  x -= r * log10_2_lo;
+        x = x * (float)VM_LN10;
+    }
+    else  {  // undefined value of BA
+        return 0.;
+    }
+
+    x2 = x * x;
+    z = polynomial_5(x,P0expf,P1expf,P2expf,P3expf,P4expf,P5expf);
+    z = mul_add(z, x2, x);                       // z *= x2;  z += x;
+
+    if constexpr (BA == 1) r--;                  // 0.5 * exp(x)
+
+    // multiply by power of 2
+    n2 = vm_pow2n(r);
+
+    if constexpr (M1 == 0) {
+        // exp
+        z = (z + 1.0f) * n2;
+    }
+    else {
+        // expm1
+        z = mul_add(z, n2, n2 - 1.0f);           //  z = z * n2 + (n2 - 1.0f);
+#ifdef SIGNED_ZERO                               // pedantic preservation of signed zero
+        z = select(initial_x == 0.f, initial_x, z);
+#endif
+    }
+
+    // check for overflow
+    auto inrange  = abs(initial_x) < max_x;      // boolean vector
+    // check for INF and NAN
+    inrange &= is_finite(initial_x);
+
+    if (horizontal_and(inrange)) {
+        // fast normal path
+        return z;
+    }
+    else {
+        // overflow, underflow and NAN
+        r = select(sign_bit(initial_x), 0.f-(M1&1), infinite_vec<VTYPE>()); // value in case of +/- overflow or INF
+        z = select(inrange, z, r);                         // +/- underflow
+        z = select(is_nan(initial_x), initial_x, z);       // NAN goes through
+        return z;
+    }
+}
+#if defined(__AVX512ER__) && MAX_VECTOR_SIZE >= 512
+// forward declarations of fast 512 bit versions
+static Vec16f exp(Vec16f const x);
+static Vec16f exp2(Vec16f const x);
+static Vec16f exp10(Vec16f const x);
+#endif
+
+// instances of exp_f template
+static inline Vec4f exp(Vec4f const x) {
+#if defined(__AVX512ER__) && MAX_VECTOR_SIZE >= 512        // use faster 512 bit version
+    return _mm512_castps512_ps128(exp(Vec16f(_mm512_castps128_ps512(x))));
+#else
+    return exp_f<Vec4f, 0, 0>(x);
+#endif
+}
+
+static inline Vec4f expm1(Vec4f const x) {
+    return exp_f<Vec4f, 3, 0>(x);
+}
+
+static inline Vec4f exp2(Vec4f const x) {
+#if defined(__AVX512ER__) && MAX_VECTOR_SIZE >= 512        // use faster 512 bit version
+    return _mm512_castps512_ps128(exp2(Vec16f(_mm512_castps128_ps512(x))));
+#else
+    return exp_f<Vec4f, 0, 2>(x);
+#endif
+}
+
+static inline Vec4f exp10(Vec4f const x) {
+#if defined(__AVX512ER__) && MAX_VECTOR_SIZE >= 512        // use faster 512 bit version
+    return _mm512_castps512_ps128(exp10(Vec16f(_mm512_castps128_ps512(x))));
+#else
+    return exp_f<Vec4f, 0, 10>(x);
+#endif
+}
+
+#if MAX_VECTOR_SIZE >= 256
+
+static inline Vec8f exp(Vec8f const x) {
+#if defined(__AVX512ER__) && MAX_VECTOR_SIZE >= 512        // use faster 512 bit version
+    return _mm512_castps512_ps256(exp(Vec16f(_mm512_castps256_ps512(x))));
+#else
+    return exp_f<Vec8f, 0, 0>(x);
+#endif
+}
+
+static inline Vec8f expm1(Vec8f const x) {
+    return exp_f<Vec8f, 3, 0>(x);
+}
+
+static inline Vec8f exp2(Vec8f const x) {
+#if defined(__AVX512ER__) && MAX_VECTOR_SIZE >= 512        // use faster 512 bit version
+    return _mm512_castps512_ps256(exp2(Vec16f(_mm512_castps256_ps512(x))));
+#else
+    return exp_f<Vec8f, 0, 2>(x);
+#endif
+}
+
+static inline Vec8f exp10(Vec8f const x) {
+#if defined(__AVX512ER__) && MAX_VECTOR_SIZE >= 512        // use faster 512 bit version
+    return _mm512_castps512_ps256(exp10(Vec16f(_mm512_castps256_ps512(x))));
+#else
+    return exp_f<Vec8f, 0, 10>(x);
+#endif
+}
+
+#endif // MAX_VECTOR_SIZE >= 256
+
+#if MAX_VECTOR_SIZE >= 512
+
+static inline Vec16f exp(Vec16f const x) {
+#ifdef __AVX512ER__  // AVX512ER instruction set includes fast exponential function
+#ifdef VCL_FASTEXP
+    // very fast, but less precise for large x:
+    return _mm512_exp2a23_round_ps(x*float(VM_LOG2E), _MM_FROUND_NO_EXC);
+#else
+    // best precision, also for large x:
+    const Vec16f log2e = float(VM_LOG2E);
+    const float ln2f_hi = 0.693359375f;
+    const float ln2f_lo = -2.12194440e-4f;
+    Vec16f x1 = x, r, y;
+    r = round(x1*log2e);
+    x1 = nmul_add(r, Vec16f(ln2f_hi), x1);       //  x -= r * ln2f_hi;
+    x1 = nmul_add(r, Vec16f(ln2f_lo), x1);       //  x -= r * ln2f_lo;
+    x1 = x1 * log2e;
+    y = _mm512_exp2a23_round_ps(r, _MM_FROUND_NO_EXC);
+    // y = vm_pow2n(r);
+    return y * _mm512_exp2a23_round_ps(x1, _MM_FROUND_NO_EXC);
+#endif // VCL_FASTEXP
+#else  // no AVX512ER, use above template
+    return exp_f<Vec16f, 0, 0>(x);
+#endif
+}
+
+static inline Vec16f expm1(Vec16f const x) {
+    return exp_f<Vec16f, 3, 0>(x);
+}
+
+static inline Vec16f exp2(Vec16f const x) {
+#ifdef __AVX512ER__
+    return Vec16f(_mm512_exp2a23_round_ps(x, _MM_FROUND_NO_EXC));
+#else
+    return exp_f<Vec16f, 0, 2>(x);
+#endif
+}
+
+static inline Vec16f exp10(Vec16f const x) {
+#ifdef __AVX512ER__  // AVX512ER instruction set includes fast exponential function
+#ifdef VCL_FASTEXP
+    // very fast, but less precise for large x:
+    return _mm512_exp2a23_round_ps(x*float(VM_LOG210), _MM_FROUND_NO_EXC);
+#else
+    // best precision, also for large x:
+    const float log10_2_hi = 0.301025391f;       // log10(2) in two parts
+    const float log10_2_lo = 4.60503907E-6f;
+    Vec16f x1 = x, r, y;
+    Vec16f log210 = float(VM_LOG210);
+    r = round(x1*log210);
+    x1 = nmul_add(r, Vec16f(log10_2_hi), x1);    //  x -= r * log10_2_hi
+    x1 = nmul_add(r, Vec16f(log10_2_lo), x1);    //  x -= r * log10_2_lo
+    x1 = x1 * log210;
+    // y = vm_pow2n(r);
+    y = _mm512_exp2a23_round_ps(r, _MM_FROUND_NO_EXC);
+    return y * _mm512_exp2a23_round_ps(x1, _MM_FROUND_NO_EXC);
+#endif // VCL_FASTEXP
+#else  // no AVX512ER, use above template
+    return exp_f<Vec16f, 0, 10>(x);
+#endif
+}
+
+#endif // MAX_VECTOR_SIZE >= 512
+
+
+
+/******************************************************************************
+*                 Logarithm functions
+******************************************************************************/
+
+// Helper function: fraction_2(x) = fraction(x)*0.5
+
+// Modified fraction function:
+// Extract the fraction part of a floating point number, and divide by 2
+// The fraction function is defined in vectorf128.h etc.
+// fraction_2(x) = fraction(x)*0.5
+// This version gives half the fraction without extra delay
+// Does not work for x = 0
+static inline Vec4f fraction_2(Vec4f const a) {
+    Vec4ui t1 = _mm_castps_si128(a);                       // reinterpret as 32-bit integer
+    Vec4ui t2 = Vec4ui((t1 & 0x007FFFFF) | 0x3F000000);    // set exponent to 0 + bias
+    return _mm_castsi128_ps(t2);
+}
+
+static inline Vec2d fraction_2(Vec2d const a) {
+    Vec2uq t1 = _mm_castpd_si128(a);                       // reinterpret as 64-bit integer
+    Vec2uq t2 = Vec2uq((t1 & 0x000FFFFFFFFFFFFFll) | 0x3FE0000000000000ll); // set exponent to 0 + bias
+    return _mm_castsi128_pd(t2);
+}
+
+#if MAX_VECTOR_SIZE >= 256
+
+static inline Vec8f fraction_2(Vec8f const a) {
+#if defined (VECTORI256_H) && VECTORI256_H > 2             // 256 bit integer vectors are available, AVX2
+    Vec8ui t1 = _mm256_castps_si256(a);                    // reinterpret as 32-bit integer
+    Vec8ui t2 = (t1 & 0x007FFFFF) | 0x3F000000;            // set exponent to 0 + bias
+    return _mm256_castsi256_ps(t2);
+#else
+    return Vec8f(fraction_2(a.get_low()), fraction_2(a.get_high()));
+#endif
+}
+
+static inline Vec4d fraction_2(Vec4d const a) {
+#if VECTORI256_H > 1  // AVX2
+    Vec4uq t1 = _mm256_castpd_si256(a);                    // reinterpret as 64-bit integer
+    Vec4uq t2 = Vec4uq((t1 & 0x000FFFFFFFFFFFFFll) | 0x3FE0000000000000ll); // set exponent to 0 + bias
+    return _mm256_castsi256_pd(t2);
+#else
+    return Vec4d(fraction_2(a.get_low()), fraction_2(a.get_high()));
+#endif
+}
+
+#endif // MAX_VECTOR_SIZE >= 256
+
+#if MAX_VECTOR_SIZE >= 512
+
+static inline Vec16f fraction_2(Vec16f const a) {
+#if INSTRSET >= 9                                          // 512 bit integer vectors are available, AVX512
+    return _mm512_getmant_ps(a, _MM_MANT_NORM_p5_1, _MM_MANT_SIGN_zero);
+    //return Vec16f(_mm512_getmant_ps(a, _MM_MANT_NORM_1_2, _MM_MANT_SIGN_zero)) * 0.5f;
+#else
+    return Vec16f(fraction_2(a.get_low()), fraction_2(a.get_high()));
+#endif
+}
+
+static inline Vec8d fraction_2(Vec8d const a) {
+#if INSTRSET >= 9                                          // 512 bit integer vectors are available, AVX512
+    return _mm512_getmant_pd(a, _MM_MANT_NORM_p5_1, _MM_MANT_SIGN_zero);
+    //return Vec8d(_mm512_getmant_pd(a, _MM_MANT_NORM_1_2, _MM_MANT_SIGN_zero)) * 0.5;
+#else
+    return Vec8d(fraction_2(a.get_low()), fraction_2(a.get_high()));
+#endif
+}
+
+#endif // MAX_VECTOR_SIZE >= 512
+
+
+// Helper function: exponent_f(x) = exponent(x) as floating point number
+
+union vm_ufi {
+    float f;
+    uint32_t i;
+};
+
+union vm_udi {
+    double d;
+    uint64_t i;
+};
+
+// extract exponent of a positive number x as a floating point number
+static inline Vec4f exponent_f(Vec4f const x) {
+#if INSTRSET >= 10                               // AVX512VL
+    // prevent returning -inf for x=0
+    return _mm_maskz_getexp_ps(_mm_cmp_ps_mask(x,Vec4f(0.f),4), x);
+#else
+    const float pow2_23 =  8388608.0f;           // 2^23
+    const float bias = 127.f;                    // bias in exponent
+    const vm_ufi upow2_23 = {pow2_23};
+    Vec4ui a = reinterpret_i(x);                 // bit-cast x to integer
+    Vec4ui b = a >> 23;                          // shift down exponent to low bits
+    Vec4ui c = b | Vec4ui(upow2_23.i);           // insert new exponent
+    Vec4f  d = reinterpret_f(c);                 // bit-cast back to double
+    Vec4f  e = d - (pow2_23 + bias);             // subtract magic number and bias
+    return e;
+#endif
+}
+
+static inline Vec2d exponent_f(Vec2d const x) {
+#if INSTRSET >= 10                               // AVX512VL
+    // prevent returning -inf for x=0
+    //return _mm_maskz_getexp_pd(x != 0., x);
+    return _mm_maskz_getexp_pd(_mm_cmp_pd_mask(x,Vec2d(0.),4), x);
+
+#else
+    const double pow2_52 = 4503599627370496.0;   // 2^52
+    const double bias = 1023.0;                  // bias in exponent
+    const vm_udi upow2_52 = {pow2_52};
+
+    Vec2uq a = reinterpret_i(x);                 // bit-cast x to integer
+    Vec2uq b = a >> 52;                          // shift down exponent to low bits
+    Vec2uq c = b | Vec2uq(upow2_52.i);           // insert new exponent
+    Vec2d  d = reinterpret_d(c);                 // bit-cast back to double
+    Vec2d  e = d - (pow2_52 + bias);             // subtract magic number and bias
+    return e;
+#endif
+}
+
+#if MAX_VECTOR_SIZE >= 256
+
+static inline Vec8f exponent_f(Vec8f const x) {
+#if INSTRSET >= 10
+    // prevent returning -inf for x=0
+    //return _mm256_maskz_getexp_ps(x != 0.f, x);
+    return _mm256_maskz_getexp_ps(_mm256_cmp_ps_mask(x,Vec8f(0.f),4), x);
+#else
+    const float pow2_23 =  8388608.0f;           // 2^23
+    const float bias = 127.f;                    // bias in exponent
+    const vm_ufi upow2_23 = {pow2_23};
+    Vec8ui a = reinterpret_i(x);                 // bit-cast x to integer
+    Vec8ui b = a >> 23;                          // shift down exponent to low bits
+    Vec8ui c = b | Vec8ui(upow2_23.i);           // insert new exponent
+    Vec8f  d = reinterpret_f(c);                 // bit-cast back to double
+    Vec8f  e = d - (pow2_23 + bias);             // subtract magic number and bias
+    return e;
+#endif
+}
+
+// extract exponent of a positive number x as a floating point number
+static inline Vec4d exponent_f(Vec4d const x) {
+#if INSTRSET >= 10
+    // prevent returning -inf for x=0
+    //return _mm256_maskz_getexp_pd(x != 0., x);
+    return _mm256_maskz_getexp_pd(_mm256_cmp_pd_mask(x,Vec4d(0.),4), x);
+#else
+    const double pow2_52 = 4503599627370496.0;   // 2^52
+    const double bias = 1023.0;                  // bias in exponent
+    const vm_udi upow2_52 = {pow2_52};
+    Vec4uq a = reinterpret_i(x);                 // bit-cast x to integer
+    Vec4uq b = a >> 52;                          // shift down exponent to low bits
+    Vec4uq c = b | Vec4uq(upow2_52.i);           // insert new exponent
+    Vec4d  d = reinterpret_d(c);                 // bit-cast back to double
+    Vec4d  e = d - (pow2_52 + bias);             // subtract magic number and bias
+    return e;
+#endif
+}
+
+#endif // MAX_VECTOR_SIZE >= 256
+
+#if MAX_VECTOR_SIZE >= 512
+
+static inline Vec16f exponent_f(Vec16f const x) {
+#if INSTRSET >= 9                                // AVX512
+    // prevent returning -inf for x=0
+    return _mm512_maskz_getexp_ps(x != 0.f, x);
+#else
+    return Vec16f(exponent_f(x.get_low()), exponent_f(x.get_high()));
+#endif
+}
+
+// extract exponent of a positive number x as a floating point number
+static inline Vec8d exponent_f(Vec8d const x) {
+#if INSTRSET >= 9                                // AVX512
+    // prevent returning -inf for x=0
+    return _mm512_maskz_getexp_pd(uint8_t(x != 0.), x);
+#else
+    return Vec8d(exponent_f(x.get_low()), exponent_f(x.get_high()));
+#endif
+}
+
+#endif // MAX_VECTOR_SIZE >= 512
+
+// Helper function: log_special_cases(x,r). Handle special cases for log function
+#if MAX_VECTOR_SIZE >= 512
+static inline Vec8d log_special_cases(Vec8d const x1, Vec8d const r) {
+    Vec8d res = r;
+#if INSTRSET >= 10  // AVX512DQ
+    Vec8db specialcases = _mm512_fpclass_pd_mask(x1, 0x7E);// zero, subnormal, negative, +-inf
+    if (!horizontal_or(specialcases)) {
+        return res;            // normal path
+    }
+    res = _mm512_fixupimm_pd(res, x1, Vec8q(0x03530411),0);// handle most cases
+    res = select(Vec8db(_mm512_fpclass_pd_mask(x1, 0x26)),-infinite_vec<Vec8d>(),res);  // subnormal -> -INF
+    res = select(Vec8db(_mm512_fpclass_pd_mask(x1, 0x50)),nan_vec<Vec8d>(NAN_LOG),res); // negative -> specific NAN
+    return res;
+#else
+    Vec8db overflow = !is_finite(x1);
+    Vec8db underflow = x1 < VM_SMALLEST_NORMAL;  // denormals not supported by this functions
+    if (!horizontal_or(overflow | underflow)) {
+        return res;                              // normal path
+    }
+    // overflow and underflow
+    res = select(underflow, nan_vec<Vec8d>(NAN_LOG), res);                // x1  < 0 gives NAN
+    res = select(is_zero_or_subnormal(x1), -infinite_vec<Vec8d>(), res);  // x1 == 0 gives -INF
+    res = select(overflow, x1, res);                                      // INF or NAN goes through
+    res = select(is_inf(x1) & sign_bit(x1), nan_vec<Vec8d>(NAN_LOG), res);// -INF gives NAN
+    return res;
+#endif // INSTRSET
+}
+
+static inline Vec16f log_special_cases(Vec16f const x1, Vec16f const r) {
+    Vec16f res = r;
+#if INSTRSET >= 10  // AVX512DQ
+    Vec16fb specialcases = _mm512_fpclass_ps_mask(x1, 0x7E);  // zero, subnormal, negative, +-inf
+    if (!horizontal_or(specialcases)) {
+        return res;          // normal path
+    }
+    res = _mm512_fixupimm_ps(res, x1, Vec16i(0x03530411), 0); // handle most cases
+    res = select(Vec16fb(_mm512_fpclass_ps_mask(x1, 0x26)),-infinite_vec<Vec16f>(),res);  // subnormal -> -INF
+    res = select(Vec16fb(_mm512_fpclass_ps_mask(x1, 0x50)),nan_vec<Vec16f>(NAN_LOG),res); // negative -> specific NAN
+    return res;
+#else
+    Vec16fb overflow = !is_finite(x1);
+    Vec16fb underflow = x1 < VM_SMALLEST_NORMALF;// denormals not supported by this functions
+    if (!horizontal_or(overflow | underflow)) {
+        return res;                              // normal path
+    }
+    // overflow and underflow
+    res = select(underflow, nan_vec<Vec16f>(NAN_LOG), res);                // x1  < 0 gives NAN
+    res = select(is_zero_or_subnormal(x1), -infinite_vec<Vec16f>(), res);  // x1 == 0 gives -INF
+    res = select(overflow, x1, res);                                       // INF or NAN goes through
+    res = select(is_inf(x1) & sign_bit(x1), nan_vec<Vec16f>(NAN_LOG), res);// -INF gives NAN
+    return res;
+#endif // INSTRSET
+}
+
+#endif // MAX_VECTOR_SIZE >= 512
+
+#if MAX_VECTOR_SIZE >= 256
+static inline Vec4d log_special_cases(Vec4d const x1, Vec4d const r) {
+    Vec4d res = r;
+#if INSTRSET >= 10  // AVX512DQ AVX512VL
+    __mmask8 specialcases = _mm256_fpclass_pd_mask(x1, 0x7E);  // zero, subnormal, negative, +-inf
+    if (specialcases == 0) {
+        return res;          // normal path
+    }
+    res = _mm256_fixupimm_pd(res, x1, Vec4q(0x03530411), 0);   // handle most cases
+    res = _mm256_mask_mov_pd(res, _mm256_fpclass_pd_mask(x1, 0x26), -infinite_vec<Vec4d>());  // subnormal -> -INF
+    res = _mm256_mask_mov_pd(res, _mm256_fpclass_pd_mask(x1, 0x50), nan_vec<Vec4d>(NAN_LOG)); // negative -> specific NAN
+    return res;
+#else
+    Vec4db overflow = !is_finite(x1);
+    Vec4db underflow = x1 < VM_SMALLEST_NORMAL;  // denormals not supported by this functions
+    if (!horizontal_or(overflow | underflow)) {
+        return res;                              // normal path
+    }
+    // overflow and underflow
+    res = select(underflow, nan_vec<Vec4d>(NAN_LOG), res);                // x1  < 0 gives NAN
+    res = select(is_zero_or_subnormal(x1), -infinite_vec<Vec4d>(), res);  // x1 == 0 gives -INF
+    res = select(overflow, x1, res);                                      // INF or NAN goes through
+    res = select(is_inf(x1) & sign_bit(x1), nan_vec<Vec4d>(NAN_LOG), res);// -INF gives NAN
+    return res;
+#endif // INSTRSET
+}
+
+static inline Vec8f log_special_cases(Vec8f const x1, Vec8f const r) {
+    Vec8f res = r;
+#if INSTRSET >= 10  // AVX512DQ AVX512VL
+    __mmask8 specialcases = _mm256_fpclass_ps_mask(x1, 0x7E); // zero, subnormal, negative, +-inf
+    if (specialcases == 0) {
+        return res;          // normal path
+    }
+    res = _mm256_fixupimm_ps(res, x1, Vec8i(0x03530411), 0);  // handle most cases
+    res = _mm256_mask_mov_ps(res, _mm256_fpclass_ps_mask(x1, 0x26), -infinite_vec<Vec8f>());  // subnormal -> -INF
+    res = _mm256_mask_mov_ps(res, _mm256_fpclass_ps_mask(x1, 0x50), nan_vec<Vec8f>(NAN_LOG)); // negative -> specific NAN
+    return res;
+#else
+    Vec8fb overflow = !is_finite(x1);
+    Vec8fb underflow = x1 < VM_SMALLEST_NORMALF; // denormals not supported by this functions
+    if (!horizontal_or(overflow | underflow)) {
+        return res;                              // normal path
+    }
+    // overflow and underflow
+    res = select(underflow, nan_vec<Vec8f>(NAN_LOG), res);                // x1  < 0 gives NAN
+    res = select(is_zero_or_subnormal(x1), -infinite_vec<Vec8f>(), res);  // x1 == 0 gives -INF
+    res = select(overflow, x1, res);                                      // INF or NAN goes through
+    res = select(is_inf(x1) & sign_bit(x1), nan_vec<Vec8f>(NAN_LOG), res);// -INF gives NAN
+    return res;
+#endif // INSTRSET
+}
+
+#endif // MAX_VECTOR_SIZE >= 256
+
+static inline Vec2d log_special_cases(Vec2d const x1, Vec2d const r) {
+    Vec2d res = r;
+#if INSTRSET >= 10  // AVX512DQ AVX512VL
+    __mmask8 specialcases = _mm_fpclass_pd_mask(x1, 0x7E); // zero, subnormal, negative, +-inf
+    if (specialcases == 0) {
+        return res;            // normal path
+    }
+    res = _mm_fixupimm_pd(res, x1, Vec2q(0x03530411), 0);  // handle most cases
+    res = _mm_mask_mov_pd(res, _mm_fpclass_pd_mask(x1, 0x26), -infinite_vec<Vec2d>());  // subnormal -> -INF
+    res = _mm_mask_mov_pd(res, _mm_fpclass_pd_mask(x1, 0x50), nan_vec<Vec2d>(NAN_LOG)); // negative -> specific NAN
+    return res;
+#else
+    Vec2db overflow = !is_finite(x1);
+    Vec2db underflow = x1 < VM_SMALLEST_NORMAL;  // denormals not supported by this functions
+    if (!horizontal_or(overflow | underflow)) {
+        return res;                              // normal path
+    }
+    // overflow and underflow
+    res = select(underflow, nan_vec<Vec2d>(NAN_LOG), res);                // x1  < 0 gives NAN
+    res = select(is_zero_or_subnormal(x1), -infinite_vec<Vec2d>(), res);  // x1 == 0 gives -INF
+    res = select(overflow, x1, res);                                      // INF or NAN goes through
+    res = select(is_inf(x1) & sign_bit(x1), nan_vec<Vec2d>(NAN_LOG), res);// -INF gives NAN
+    return res;
+#endif // INSTRSET
+}
+
+static inline Vec4f log_special_cases(Vec4f const x1, Vec4f const r) {
+    Vec4f res = r;
+#if INSTRSET >= 10  // AVX512DQ AVX512VL
+    __mmask8 specialcases = _mm_fpclass_ps_mask(x1, 0x7E); // zero, subnormal, negative, +-inf
+    if (specialcases == 0) {
+        return res;          // normal path
+    }
+    res = _mm_fixupimm_ps(res, x1, Vec4i(0x03530411), 0);  // handle most cases
+    res = _mm_mask_mov_ps(res, _mm_fpclass_ps_mask(x1, 0x26), -infinite_vec<Vec4f>());  // subnormal -> -INF
+    res = _mm_mask_mov_ps(res, _mm_fpclass_ps_mask(x1, 0x50), nan_vec<Vec4f>(NAN_LOG)); // negative -> specific NAN
+    return res;
+#else
+    Vec4fb overflow = !is_finite(x1);
+    Vec4fb underflow = x1 < VM_SMALLEST_NORMALF; // denormals not supported by this functions
+    if (!horizontal_or(overflow | underflow)) {
+        return res;                              // normal path
+    }
+    // overflow and underflow
+    res = select(underflow, nan_vec<Vec4f>(NAN_LOG), res);                // x1  < 0 gives NAN
+    res = select(is_zero_or_subnormal(x1), -infinite_vec<Vec4f>(), res);  // x1 == 0 gives -INF
+    res = select(overflow, x1, res);                                      // INF or NAN goes through
+    res = select(is_inf(x1) & sign_bit(x1), nan_vec<Vec4f>(NAN_LOG), res);// -INF gives NAN
+    return res;
+#endif // INSTRSET
+}
+
+
+// log function, double precision
+// template parameters:
+// VTYPE:  f.p. vector type
+// M1: 0 for log, 1 for log1p
+template<typename VTYPE, int M1>
+static inline VTYPE log_d(VTYPE const initial_x) {
+
+    // define constants
+    const double ln2_hi =  0.693359375;
+    const double ln2_lo = -2.121944400546905827679E-4;
+    const double P0log  =  7.70838733755885391666E0;
+    const double P1log  =  1.79368678507819816313E1;
+    const double P2log  =  1.44989225341610930846E1;
+    const double P3log  =  4.70579119878881725854E0;
+    const double P4log  =  4.97494994976747001425E-1;
+    const double P5log  =  1.01875663804580931796E-4;
+    const double Q0log  =  2.31251620126765340583E1;
+    const double Q1log  =  7.11544750618563894466E1;
+    const double Q2log  =  8.29875266912776603211E1;
+    const double Q3log  =  4.52279145837532221105E1;
+    const double Q4log  =  1.12873587189167450590E1;
+
+    VTYPE  x1, x, x2, px, qx, res, fe;           // data vectors
+
+    if constexpr (M1 == 0) {
+        x1 = initial_x;                          // log(x)
+    }
+    else {
+        x1 = initial_x + 1.0;                    // log(x+1)
+    }
+    // separate mantissa from exponent
+    // VTYPE x  = fraction(x1) * 0.5;
+    x  = fraction_2(x1);
+    fe = exponent_f(x1);
+
+    auto blend = x > VM_SQRT2*0.5;               // boolean vector
+    x  = if_add(!blend, x, x);                   // conditional add
+    fe = if_add(blend, fe, 1.);                  // conditional add
+
+    if constexpr (M1 == 0) {
+        // log(x). Expand around 1.0
+        x -= 1.0;
+    }
+    else {
+        // log(x+1). Avoid loss of precision when adding 1 and later subtracting 1 if exponent = 0
+        x = select(fe==0., initial_x, x - 1.0);
+    }
+
+    // rational form
+    px  = polynomial_5 (x, P0log, P1log, P2log, P3log, P4log, P5log);
+    x2  = x * x;
+    px *= x * x2;
+    qx  = polynomial_5n(x, Q0log, Q1log, Q2log, Q3log, Q4log);
+    res = px / qx ;
+
+    // add exponent
+    res  = mul_add(fe, ln2_lo, res);             // res += fe * ln2_lo;
+    res += nmul_add(x2, 0.5, x);                 // res += x  - 0.5 * x2;
+    res  = mul_add(fe, ln2_hi, res);             // res += fe * ln2_hi;
+#ifdef SIGNED_ZERO                               // pedantic preservation of signed zero
+    res = select(initial_x == 0., initial_x, res);
+#endif
+    // handle special cases, or return res
+    return log_special_cases(x1, res);
+}
+
+
+static inline Vec2d log(Vec2d const x) {
+    return log_d<Vec2d, 0>(x);
+}
+
+static inline Vec2d log1p(Vec2d const x) {
+    return log_d<Vec2d, 3>(x);
+}
+
+static inline Vec2d log2(Vec2d const x) {
+    return VM_LOG2E * log_d<Vec2d, 0>(x);
+}
+
+static inline Vec2d log10(Vec2d const x) {
+    return VM_LOG10E * log_d<Vec2d, 0>(x);
+}
+
+#if MAX_VECTOR_SIZE >= 256
+
+static inline Vec4d log(Vec4d const x) {
+    return log_d<Vec4d, 0>(x);
+}
+
+static inline Vec4d log1p(Vec4d const x) {
+    return log_d<Vec4d, 3>(x);
+}
+
+static inline Vec4d log2(Vec4d const x) {
+    return VM_LOG2E * log_d<Vec4d, 0>(x);
+}
+
+static inline Vec4d log10(Vec4d const x) {
+    return VM_LOG10E * log_d<Vec4d, 0>(x);
+}
+
+#endif // MAX_VECTOR_SIZE >= 256
+
+#if MAX_VECTOR_SIZE >= 512
+
+static inline Vec8d log(Vec8d const x) {
+    return log_d<Vec8d, 0>(x);
+}
+
+static inline Vec8d log1p(Vec8d const x) {
+    return log_d<Vec8d, 3>(x);
+}
+
+static inline Vec8d log2(Vec8d const x) {
+    return VM_LOG2E * log_d<Vec8d, 0>(x);
+}
+
+static inline Vec8d log10(Vec8d const x) {
+    return VM_LOG10E * log_d<Vec8d, 0>(x);
+}
+
+#endif // MAX_VECTOR_SIZE >= 512
+
+
+// log function, single precision
+// template parameters:
+// VTYPE:  f.p. vector type
+// M1: 0 for log, 1 for log1p
+template<typename VTYPE, int M1>
+static inline VTYPE log_f(VTYPE const initial_x) {
+
+    // define constants
+    const float ln2f_hi =  0.693359375f;
+    const float ln2f_lo = -2.12194440E-4f;
+    const float P0logf  =  3.3333331174E-1f;
+    const float P1logf  = -2.4999993993E-1f;
+    const float P2logf  =  2.0000714765E-1f;
+    const float P3logf  = -1.6668057665E-1f;
+    const float P4logf  =  1.4249322787E-1f;
+    const float P5logf  = -1.2420140846E-1f;
+    const float P6logf  =  1.1676998740E-1f;
+    const float P7logf  = -1.1514610310E-1f;
+    const float P8logf  =  7.0376836292E-2f;
+
+    VTYPE  x1, x, res, x2, fe;                   // data vectors
+
+    if constexpr (M1 == 0) {
+        x1 = initial_x;                          // log(x)
+    }
+    else {
+        x1 = initial_x + 1.0f;                   // log(x+1)
+    }
+
+    // separate mantissa from exponent
+    x = fraction_2(x1);
+    auto e = exponent(x1);                       // integer vector
+
+    auto blend = x > float(VM_SQRT2*0.5);        // boolean vector
+    x  = if_add(!blend, x, x);                   // conditional add
+    e  = if_add(decltype(e>e)(blend),  e, decltype(e)(1));  // conditional add
+    fe = to_float(e);
+
+    if constexpr (M1 == 0) {
+        // log(x). Expand around 1.0
+        x -= 1.0f;
+    }
+    else {
+        // log(x+1). Avoid loss of precision when adding 1 and later subtracting 1 if exponent = 0
+        x = select(decltype(x>x)(e==0), initial_x, x - 1.0f);
+    }
+
+    // Taylor expansion
+    res = polynomial_8(x, P0logf, P1logf, P2logf, P3logf, P4logf, P5logf, P6logf, P7logf, P8logf);
+    x2  = x*x;
+    res *= x2*x;
+
+    // add exponent
+    res  = mul_add(fe, ln2f_lo, res);            // res += ln2f_lo  * fe;
+    res += nmul_add(x2, 0.5f, x);                // res += x - 0.5f * x2;
+    res  = mul_add(fe, ln2f_hi, res);            // res += ln2f_hi  * fe;
+#ifdef SIGNED_ZERO                               // pedantic preservation of signed zero
+    res = select(initial_x == 0.f, initial_x, res);
+#endif
+    // handle special cases, or return res
+    return log_special_cases(x1, res);
+}
+
+static inline Vec4f log(Vec4f const x) {
+    return log_f<Vec4f, 0>(x);
+}
+
+static inline Vec4f log1p(Vec4f const x) {
+    return log_f<Vec4f, 3>(x);
+}
+
+static inline Vec4f log2(Vec4f const x) {
+    return float(VM_LOG2E) * log_f<Vec4f, 0>(x);
+}
+
+static inline Vec4f log10(Vec4f const x) {
+    return float(VM_LOG10E) * log_f<Vec4f, 0>(x);
+}
+
+#if MAX_VECTOR_SIZE >= 256
+
+static inline Vec8f log(Vec8f const x) {
+    return log_f<Vec8f, 0>(x);
+}
+
+static inline Vec8f log1p(Vec8f const x) {
+    return log_f<Vec8f, 3>(x);
+}
+
+static inline Vec8f log2(Vec8f const x) {
+    return float(VM_LOG2E) * log_f<Vec8f, 0>(x);
+}
+
+static inline Vec8f log10(Vec8f const x) {
+    return float(VM_LOG10E) * log_f<Vec8f, 0>(x);
+}
+
+#endif // MAX_VECTOR_SIZE >= 256
+
+#if MAX_VECTOR_SIZE >= 512
+
+static inline Vec16f log(Vec16f const x) {
+    return log_f<Vec16f, 0>(x);
+}
+
+static inline Vec16f log1p(Vec16f const x) {
+    return log_f<Vec16f, 3>(x);
+}
+
+static inline Vec16f log2(Vec16f const x) {
+    return float(VM_LOG2E) * log_f<Vec16f, 0>(x);
+}
+
+static inline Vec16f log10(Vec16f const x) {
+    return float(VM_LOG10E) * log_f<Vec16f, 0>(x);
+}
+
+#endif // MAX_VECTOR_SIZE >= 512
+
+
+/******************************************************************************
+*           Cube root and reciprocal cube root
+******************************************************************************/
+
+// cube root template, double precision
+// template parameters:
+// VTYPE:  f.p. vector type
+// CR:     -1 for reciprocal cube root, 1 for cube root, 2 for cube root squared
+template<typename VTYPE, int CR>
+static inline VTYPE cbrt_d(VTYPE const x) {
+    const int iter = 7;     // iteration count of x^(-1/3) loop
+    int i;
+    typedef decltype(x < x) BVTYPE;              // boolean vector type
+    typedef decltype(roundi(x)) ITYPE64;         // 64 bit integer vector type
+    typedef decltype(roundi(compress(x,x))) ITYPE32; // 32 bit integer vector type
+
+    ITYPE32 m1, m2;
+    BVTYPE underflow;
+    ITYPE64 q1(0x5540000000000000ULL);           // exponent bias
+    ITYPE64 q2(0x0005555500000000ULL);           // exponent multiplier for 1/3
+    ITYPE64 q3(0x0010000000000000ULL);           // denormal limit
+
+    VTYPE  xa, xa3, a, a2;
+    const double one_third  = 1./3.;
+    const double four_third = 4./3.;
+
+    xa  = abs(x);
+    xa3 = one_third*xa;
+
+    // multiply exponent by -1/3
+    m1 = reinterpret_i(xa);
+    m2 = ITYPE32(q1) - (m1 >> 20) * ITYPE32(q2);
+    a  = reinterpret_d(m2);
+    underflow = BVTYPE(ITYPE64(m1) <= q3);       // true if denormal or zero
+
+    // Newton Raphson iteration. Warning: may overflow!
+    for (i = 0; i < iter-1; i++) {
+        a2 = a * a;
+        a = nmul_add(xa3, a2*a2, four_third*a);  // a = four_third*a - xa3*a2*a2;
+    }
+    // last iteration with better precision
+    a2 = a * a;
+    a = mul_add(one_third, nmul_add(xa, a2*a2, a), a); // a = a + one_third*(a - xa*a2*a2);
+
+    if constexpr (CR == -1) {                    // reciprocal cube root
+        a = select(underflow, infinite_vec<VTYPE>(), a); // generate INF if underflow
+        a = select(is_inf(x), VTYPE(0), a);      // special case for INF                                                 // get sign
+        a = sign_combine(a, x);                  // get sign
+    }
+    else if constexpr (CR == 1) {                // cube root
+        a = a * a * x;
+        a = select(underflow, 0., a);            // generate 0 if underflow
+        a = select(is_inf(x), x, a);             // special case for INF
+#ifdef SIGNED_ZERO
+        a = a | (x & VTYPE(-0.0));                      // get sign of x
+#endif
+    }
+    else if constexpr (CR == 2) {                // cube root squared
+        a = a * xa;
+        a = select(underflow, 0., a);            // generate 0 if underflow
+        a = select(is_inf(x), xa, a);            // special case for INF
+    }
+    return a;
+}
+
+// template instances for cbrt and reciprocal_cbrt
+
+// cube root
+static inline Vec2d cbrt(Vec2d const x) {
+    return cbrt_d<Vec2d, 1> (x);
+}
+
+// reciprocal cube root
+static inline Vec2d reciprocal_cbrt(Vec2d const x) {
+    return cbrt_d<Vec2d, -1> (x);
+}
+
+// square cube root
+static inline Vec2d square_cbrt(Vec2d const x) {
+    return cbrt_d<Vec2d, 2> (x);
+}
+
+#if MAX_VECTOR_SIZE >= 256
+
+static inline Vec4d cbrt(Vec4d const x) {
+    return cbrt_d<Vec4d, 1> (x);
+}
+
+static inline Vec4d reciprocal_cbrt(Vec4d const x) {
+    return cbrt_d<Vec4d, -1> (x);
+}
+
+static inline Vec4d square_cbrt(Vec4d const x) {
+    return cbrt_d<Vec4d, 2> (x);
+}
+
+#endif // MAX_VECTOR_SIZE >= 256
+
+#if MAX_VECTOR_SIZE >= 512
+
+static inline Vec8d cbrt(Vec8d const x) {
+    return cbrt_d<Vec8d, 1> (x);
+}
+
+static inline Vec8d reciprocal_cbrt(Vec8d const x) {
+    return cbrt_d<Vec8d, -1> (x);
+}
+
+static inline Vec8d square_cbrt(Vec8d const x) {
+    return cbrt_d<Vec8d, 2> (x);
+}
+
+#endif // MAX_VECTOR_SIZE >= 512
+
+
+// cube root template, single precision
+// template parameters:
+// VTYPE:  f.p. vector type
+// CR:     -1 for reciprocal cube root, 1 for cube root, 2 for cube root squared
+template<typename VTYPE, int CR>
+static inline VTYPE cbrt_f(VTYPE const x) {
+
+    const int iter = 4;                          // iteration count of x^(-1/3) loop
+    int i;
+
+    typedef decltype(roundi(x)) ITYPE;           // integer vector type
+    typedef decltype(x < x) BVTYPE;              // boolean vector type
+
+    VTYPE  xa, xa3, a, a2;
+    ITYPE  m1, m2;
+    BVTYPE underflow;
+    ITYPE  q1(0x54800000U);                      // exponent bias
+    ITYPE  q2(0x002AAAAAU);                      // exponent multiplier for 1/3
+    ITYPE  q3(0x00800000U);                      // denormal limit
+    const  float one_third  = float(1./3.);
+    const  float four_third = float(4./3.);
+
+    xa  = abs(x);
+    xa3 = one_third*xa;
+
+    // multiply exponent by -1/3
+    m1 = reinterpret_i(xa);
+    m2 = q1 - (m1 >> 23) * q2;
+    a  = reinterpret_f(m2);
+
+    underflow = BVTYPE(m1 <= q3);                // true if denormal or zero
+
+    // Newton Raphson iteration
+    for (i = 0; i < iter-1; i++) {
+        a2 = a*a;
+        a = nmul_add(xa3, a2*a2, four_third*a);  // a = four_third*a - xa3*a2*a2;
+    }
+    // last iteration with better precision
+    a2 = a*a;
+    a = mul_add(one_third, nmul_add(xa, a2*a2, a), a); //a = a + one_third*(a - xa*a2*a2);
+
+    if constexpr (CR == -1) {                    // reciprocal cube root
+        // generate INF if underflow
+        a = select(underflow, infinite_vec<VTYPE>(), a);
+        a = select(is_inf(x), VTYPE(0), a);      // special case for INF                                                 // get sign
+        a = sign_combine(a, x);
+    }
+    else if constexpr (CR == 1) {                // cube root
+        a = a * a * x;
+        a = select(underflow, 0.f, a);           // generate 0 if underflow
+        a = select(is_inf(x), x, a);             // special case for INF
+#ifdef SIGNED_ZERO
+        a = a | (x & VTYPE(-0.0f));                     // get sign of x
+#endif
+    }
+    else if constexpr (CR == 2) {                // cube root squared
+        a = a * xa;                              // abs only to fix -INF
+        a = select(underflow, 0., a);            // generate 0 if underflow
+        a = select(is_inf(x), xa, a);            // special case for INF
+    }
+    return a;
+}
+
+// template instances for cbrt and reciprocal_cbrt
+
+// cube root
+static inline Vec4f cbrt(Vec4f const x) {
+    return cbrt_f<Vec4f, 1> (x);
+}
+
+// reciprocal cube root
+static inline Vec4f reciprocal_cbrt(Vec4f const x) {
+    return cbrt_f<Vec4f, -1> (x);
+}
+
+// square cube root
+static inline Vec4f square_cbrt(Vec4f const x) {
+    return cbrt_f<Vec4f, 2> (x);
+}
+
+#if MAX_VECTOR_SIZE >= 256
+
+static inline Vec8f cbrt(Vec8f const x) {
+    return cbrt_f<Vec8f, 1> (x);
+}
+
+static inline Vec8f reciprocal_cbrt(Vec8f const x) {
+    return cbrt_f<Vec8f, -1> (x);
+}
+
+static inline Vec8f square_cbrt(Vec8f const x) {
+    return cbrt_f<Vec8f, 2> (x);
+}
+
+#endif // MAX_VECTOR_SIZE >= 256
+
+#if MAX_VECTOR_SIZE >= 512
+
+static inline Vec16f cbrt(Vec16f const x) {
+    return cbrt_f<Vec16f, 1> (x);
+}
+
+static inline Vec16f reciprocal_cbrt(Vec16f const x) {
+    return cbrt_f<Vec16f, -1> (x);
+}
+
+static inline Vec16f square_cbrt(Vec16f const x) {
+    return cbrt_f<Vec16f, 2> (x);
+}
+
+#endif // MAX_VECTOR_SIZE >= 512
+
+
+
+/* ****************************************************************************
+                    pow functions
+*******************************************************************************
+Note about standard conformance:
+This implementation of a pow function differs from the IEEE 754-2008 floating
+point standard regarding nan propagation.
+The standard has pow(nan,0) = 1, and pow(1,nan) = 1, probably for historic reasons.
+The present implementation is guaranteed to always propagate nan's for reasons
+explained in this report:
+Agner Fog: "NAN propagation versus fault trapping in floating point code", 2019,
+https://www.agner.org/optimize/nan_propagation.pdf
+
+The standard defines another function, powr, which propagates NAN's, but powr
+will be less useful to programmers because it does not allow integer powers of
+negative x.
+
+******************************************************************************/
+
+// Helper functions:
+
+#if MAX_VECTOR_SIZE >= 512
+
+// Helper function for power function: insert special values of pow(x,y) when x=0:
+// y<0 -> inf, y=0 -> 1, y>0 -> 0, y=nan -> nan
+static inline Vec8d wm_pow_case_x0(Vec8db const xiszero, Vec8d const y, Vec8d const z) {
+#if INSTRSET >= 9
+    const __m512i table = Vec8q(0x85858A00);
+    return _mm512_mask_fixupimm_pd(z, uint8_t(xiszero), y, table, 0);
+#else
+    return select(xiszero, select(y < 0., infinite_vec<Vec8d>(), select(y == 0., Vec8d(1.), Vec8d(0.))), z);
+#endif
+}
+
+// Helper function for power function: insert special values of pow(x,y) when x=0:
+// y<0 -> inf, y=0 -> 1, y>0 -> 0, y=nan -> nan
+static inline Vec16f wm_pow_case_x0(Vec16fb const xiszero, Vec16f const y, Vec16f const z) {
+#if INSTRSET >= 9
+    const __m512i table = Vec16ui(0x85858A00);
+    return _mm512_mask_fixupimm_ps(z, xiszero, y, table, 0);
+#else
+    return select(xiszero, select(y < 0.f, infinite_vec<Vec16f>(), select(y == 0.f, Vec16f(1.f), Vec16f(0.f))), z);
+#endif
+}
+
+#endif
+
+#if MAX_VECTOR_SIZE >= 256
+
+static inline Vec4d wm_pow_case_x0(Vec4db const xiszero, Vec4d const y, Vec4d const z) {
+//#if INSTRSET >= 10
+    //const __m256i table = Vec4q(0x85858A00);
+    //return _mm256_mask_fixupimm_pd(z, xiszero, y, table, 0);
+//#else
+    return select(xiszero, select(y < 0., infinite_vec<Vec4d>(), select(y == 0., Vec4d(1.), Vec4d(0.))), z);
+//#endif
+}
+
+static inline Vec8f wm_pow_case_x0(Vec8fb const xiszero, Vec8f const y, Vec8f const z) {
+    return select(xiszero, select(y < 0.f, infinite_vec<Vec8f>(), select(y == 0.f, Vec8f(1.f), Vec8f(0.f))), z);
+}
+
+#endif
+
+static inline Vec2d wm_pow_case_x0(Vec2db const xiszero, Vec2d const y, Vec2d const z) {
+//#if INSTRSET >= 10
+//    const __m128i table = Vec2q(0x85858A00);
+//    return _mm_mask_fixupimm_pd(z, xiszero, y, table, 0);
+//#else
+    return select(xiszero, select(y < 0., infinite_vec<Vec2d>(), select(y == 0., Vec2d(1.), Vec2d(0.))), z);
+//#endif
+}
+
+static inline Vec4f wm_pow_case_x0(Vec4fb const xiszero, Vec4f const y, Vec4f const z) {
+    return select(xiszero, select(y < 0.f, infinite_vec<Vec4f>(), select(y == 0.f, Vec4f(1.f), Vec4f(0.f))), z);
+}
+
+
+// ****************************************************************************
+//                pow template, double precision
+// ****************************************************************************
+// Calculate x to the power of y.
+
+// Precision is important here because rounding errors get multiplied by y.
+// The logarithm is calculated with extra precision, and the exponent is
+// calculated separately.
+// The logarithm is calculated by Pade approximation with 6'th degree
+// polynomials. A 7'th degree would be preferred for best precision by high y.
+// The alternative method: log(x) = z + z^3*R(z)/S(z), where z = 2(x-1)/(x+1)
+// did not give better precision.
+
+// Template parameters:
+// VTYPE:  data vector type
+template <typename VTYPE>
+static inline VTYPE pow_template_d(VTYPE const x0, VTYPE const y) {
+
+    // define constants
+    const double ln2d_hi = 0.693145751953125;           // log(2) in extra precision, high bits
+    const double ln2d_lo = 1.42860682030941723212E-6;   // low bits of log(2)
+    const double log2e   = VM_LOG2E;                    // 1/log(2)
+
+    // coefficients for Pade polynomials
+    const double P0logl =  2.0039553499201281259648E1;
+    const double P1logl =  5.7112963590585538103336E1;
+    const double P2logl =  6.0949667980987787057556E1;
+    const double P3logl =  2.9911919328553073277375E1;
+    const double P4logl =  6.5787325942061044846969E0;
+    const double P5logl =  4.9854102823193375972212E-1;
+    const double P6logl =  4.5270000862445199635215E-5;
+    const double Q0logl =  6.0118660497603843919306E1;
+    const double Q1logl =  2.1642788614495947685003E2;
+    const double Q2logl =  3.0909872225312059774938E2;
+    const double Q3logl =  2.2176239823732856465394E2;
+    const double Q4logl =  8.3047565967967209469434E1;
+    const double Q5logl =  1.5062909083469192043167E1;
+
+    // Taylor coefficients for exp function, 1/n!
+    const double p2  = 1./2.;
+    const double p3  = 1./6.;
+    const double p4  = 1./24.;
+    const double p5  = 1./120.;
+    const double p6  = 1./720.;
+    const double p7  = 1./5040.;
+    const double p8  = 1./40320.;
+    const double p9  = 1./362880.;
+    const double p10 = 1./3628800.;
+    const double p11 = 1./39916800.;
+    const double p12 = 1./479001600.;
+    const double p13 = 1./6227020800.;
+
+    typedef decltype(roundi(x0)) ITYPE;          // integer vector type
+    typedef decltype(x0 < x0) BVTYPE;            // boolean vector type
+
+    // data vectors
+    VTYPE x, x1, x2;                             // x variable
+    VTYPE px, qx, ef, yr, v;                     // calculation of logarithm
+    VTYPE lg, lg1;
+    VTYPE lgerr, x2err;
+    VTYPE e1, e2, ee;
+    VTYPE e3, z, z1;                             // calculation of exp and pow
+    VTYPE yodd(0);                               // has sign bit set if y is an odd integer
+    // integer vectors
+    ITYPE ei, ej;
+    // boolean vectors
+    BVTYPE blend, xzero, xsign;                  // x conditions
+    BVTYPE overflow, underflow, xfinite, yfinite, efinite; // error conditions
+
+    // remove sign
+    x1 = abs(x0);
+
+    // Separate mantissa from exponent
+    // This gives the mantissa * 0.5
+    x  = fraction_2(x1);
+
+    // reduce range of x = +/- sqrt(2)/2
+    blend = x > VM_SQRT2*0.5;
+    x  = if_add(!blend, x, x);                   // conditional add
+
+    // Pade approximation
+    // Higher precision than in log function. Still higher precision wanted
+    x -= 1.0;
+    x2 = x*x;
+    px = polynomial_6  (x, P0logl, P1logl, P2logl, P3logl, P4logl, P5logl, P6logl);
+    px *= x * x2;
+    qx = polynomial_6n (x, Q0logl, Q1logl, Q2logl, Q3logl, Q4logl, Q5logl);
+    lg1 = px / qx;
+
+    // extract exponent
+    ef = exponent_f(x1);
+    ef = if_add(blend, ef, 1.);                  // conditional add
+
+    // multiply exponent by y
+    // nearest integer e1 goes into exponent of result, remainder yr is added to log
+    e1 = round(ef * y);
+    yr = mul_sub_x(ef, y, e1);                   // calculate remainder yr. precision very important here
+
+    // add initial terms to Pade expansion
+    lg = nmul_add(0.5, x2, x) + lg1;             // lg = (x - 0.5 * x2) + lg1;
+    // calculate rounding errors in lg
+    // rounding error in multiplication 0.5*x*x
+    x2err = mul_sub_x(0.5*x, x, 0.5*x2);
+    // rounding error in additions and subtractions
+    lgerr = mul_add(0.5, x2, lg - x) - lg1;      // lgerr = ((lg - x) + 0.5 * x2) - lg1;
+
+    // extract something for the exponent
+    e2 = round(lg * y * VM_LOG2E);
+    // subtract this from lg, with extra precision
+    v = mul_sub_x(lg, y, e2 * ln2d_hi);
+    v = nmul_add(e2, ln2d_lo, v);                // v -= e2 * ln2d_lo;
+
+    // add remainder from ef * y
+    v = mul_add(yr, VM_LN2, v);                  // v += yr * VM_LN2;
+
+    // correct for previous rounding errors
+    v = nmul_add(lgerr + x2err, y, v);           // v -= (lgerr + x2err) * y;
+
+    // exp function
+
+    // extract something for the exponent if possible
+    x = v;
+    e3 = round(x*log2e);
+    // high precision multiplication not needed here because abs(e3) <= 1
+    x = nmul_add(e3, VM_LN2, x);                 // x -= e3 * VM_LN2;
+
+    z = polynomial_13m(x, p2, p3, p4, p5, p6, p7, p8, p9, p10, p11, p12, p13);
+    z = z + 1.0;
+
+    // contributions to exponent
+    ee = e1 + e2 + e3;
+    //ei = round_to_int64_limited(ee);
+    ei = roundi(ee);
+    // biased exponent of result:
+    ej = ei + (ITYPE(reinterpret_i(z)) >> 52);
+    // check exponent for overflow and underflow
+    overflow  = BVTYPE(ej >= 0x07FF) | (ee >  3000.);
+    underflow = BVTYPE(ej <= 0x0000) | (ee < -3000.);
+
+    // add exponent by integer addition
+    z = reinterpret_d(ITYPE(reinterpret_i(z)) + (ei << 52));
+
+    // check for special cases
+    xfinite   = is_finite(x0);
+    yfinite   = is_finite(y);
+    efinite   = is_finite(ee);
+    xzero     = is_zero_or_subnormal(x0);
+    xsign     = sign_bit(x0);  // sign of x0. include -0.
+
+    // check for overflow and underflow
+    if (horizontal_or(overflow | underflow)) {
+        // handle errors
+        z = select(underflow, VTYPE(0.), z);
+        z = select(overflow, infinite_vec<VTYPE>(), z);
+    }
+
+    // check for x == 0
+    z = wm_pow_case_x0(xzero, y, z);
+    //z = select(xzero, select(y < 0., infinite_vec<VTYPE>(), select(y == 0., VTYPE(1.), VTYPE(0.))), z);
+
+    // check for sign of x (include -0.). y must be integer
+    if (horizontal_or(xsign)) {
+        // test if y is an integer
+        BVTYPE yinteger = y == round(y);
+        // test if y is odd: convert to int and shift bit 0 into position of sign bit.
+        // this will be 0 if overflow
+        yodd = reinterpret_d(roundi(y) << 63);
+        z1 = select(yinteger, z | yodd,                    // y is integer. get sign if y is odd
+            select(x0 == 0., z, nan_vec<VTYPE>(NAN_POW))); // NAN unless x0 == -0.
+        yodd = select(yinteger, yodd, 0.);                 // yodd used below. only if y is integer
+        z = select(xsign, z1, z);
+    }
+
+    // check for range errors
+    if (horizontal_and(xfinite & yfinite & (efinite | xzero))) {
+        // fast return if no special cases
+        return z;
+    }
+
+    // handle special error cases: y infinite
+    z1 = select(yfinite & efinite, z,
+        select(x1 == 1., VTYPE(1.),
+            select((x1 > 1.) ^ sign_bit(y), infinite_vec<VTYPE>(), 0.)));
+
+    // handle x infinite
+    z1 = select(xfinite, z1,
+        select(y == 0., VTYPE(1.),
+            select(y < 0., yodd & z,      // 0.0 with the sign of z from above
+                abs(x0) | (x0 & yodd)))); // get sign of x0 only if y is odd integer
+
+    // Always propagate nan:
+    // Deliberately differing from the IEEE-754 standard which has pow(0,nan)=1, and pow(1,nan)=1
+    z1 = select(is_nan(x0)|is_nan(y), x0+y, z1);
+
+    return z1;
+}
+
+
+//This template is in vectorf128.h to prevent implicit conversion of float y to int when float version is not defined:
+//template <typename TT> static Vec2d pow(Vec2d const a, TT n);
+
+// instantiations of pow_template_d:
+template <>
+inline Vec2d pow<Vec2d>(Vec2d const x, Vec2d const y) {
+    return pow_template_d(x, y);
+}
+
+template <>
+inline Vec2d pow<double>(Vec2d const x, double const y) {
+    return pow_template_d<Vec2d>(x, y);
+}
+template <>
+inline Vec2d pow<float>(Vec2d const x, float const y) {
+    return pow_template_d<Vec2d>(x, (double)y);
+}
+
+#if MAX_VECTOR_SIZE >= 256
+
+template <>
+inline Vec4d pow<Vec4d>(Vec4d const x, Vec4d const y) {
+    return pow_template_d(x, y);
+}
+
+template <>
+inline Vec4d pow<double>(Vec4d const x, double const y) {
+    return pow_template_d<Vec4d>(x, y);
+}
+
+template <>
+inline Vec4d pow<float>(Vec4d const x, float const y) {
+    return pow_template_d<Vec4d>(x, (double)y);
+}
+
+#endif // MAX_VECTOR_SIZE >= 256
+
+#if MAX_VECTOR_SIZE >= 512
+
+template <>
+inline Vec8d pow<Vec8d>(Vec8d const x, Vec8d const y) {
+    return pow_template_d(x, y);
+}
+
+template <>
+inline Vec8d pow<double>(Vec8d const x, double const y) {
+    return pow_template_d<Vec8d>(x, y);
+}
+
+template <>
+inline Vec8d pow<float>(Vec8d const x, float const y) {
+    return pow_template_d<Vec8d>(x, (double)y);
+}
+
+#endif // MAX_VECTOR_SIZE >= 512
+
+
+// ****************************************************************************
+//                pow template, single precision
+// ****************************************************************************
+
+// Template parameters:
+// VTYPE:  data vector type
+// Calculate x to the power of y
+template <typename VTYPE>
+static inline VTYPE pow_template_f(VTYPE const x0, VTYPE const y) {
+
+    // define constants
+    const float ln2f_hi  =  0.693359375f;        // log(2), split in two for extended precision
+    const float ln2f_lo  = -2.12194440e-4f;
+    const float log2e    =  float(VM_LOG2E);     // 1/log(2)
+
+    const float P0logf  =  3.3333331174E-1f;     // coefficients for logarithm expansion
+    const float P1logf  = -2.4999993993E-1f;
+    const float P2logf  =  2.0000714765E-1f;
+    const float P3logf  = -1.6668057665E-1f;
+    const float P4logf  =  1.4249322787E-1f;
+    const float P5logf  = -1.2420140846E-1f;
+    const float P6logf  =  1.1676998740E-1f;
+    const float P7logf  = -1.1514610310E-1f;
+    const float P8logf  =  7.0376836292E-2f;
+
+    const float p2expf   =  1.f/2.f;             // coefficients for Taylor expansion of exp
+    const float p3expf   =  1.f/6.f;
+    const float p4expf   =  1.f/24.f;
+    const float p5expf   =  1.f/120.f;
+    const float p6expf   =  1.f/720.f;
+    const float p7expf   =  1.f/5040.f;
+
+    typedef decltype(roundi(x0)) ITYPE;          // integer vector type
+    typedef decltype(x0 < x0) BVTYPE;            // boolean vector type
+
+    // data vectors
+    VTYPE x, x1, x2;                             // x variable
+    VTYPE ef, e1, e2, e3, ee;                    // exponent
+    VTYPE yr;                                    // remainder
+    VTYPE lg, lg1, lgerr, x2err, v;              // logarithm
+    VTYPE z, z1;                                 // pow(x,y)
+    VTYPE yodd(0);                               // has sign bit set if y is an odd integer
+    // integer vectors
+    ITYPE ei, ej;                                // exponent
+    // boolean vectors
+    BVTYPE blend, xzero, xsign;                  // x conditions
+    BVTYPE overflow, underflow, xfinite, yfinite, efinite; // error conditions
+
+    // remove sign
+    x1 = abs(x0);
+
+    // Separate mantissa from exponent
+    // This gives the mantissa * 0.5
+    x  = fraction_2(x1);
+
+    // reduce range of x = +/- sqrt(2)/2
+    blend = x > float(VM_SQRT2 * 0.5);
+    x  = if_add(!blend, x, x);                   // conditional add
+
+    // Taylor expansion, high precision
+    x   -= 1.0f;
+    x2   = x * x;
+    lg1  = polynomial_8(x, P0logf, P1logf, P2logf, P3logf, P4logf, P5logf, P6logf, P7logf, P8logf);
+    lg1 *= x2 * x;
+
+    // extract exponent
+    ef = exponent_f(x1);
+    ef = if_add(blend, ef, 1.0f);                // conditional add
+
+    // multiply exponent by y
+    // nearest integer e1 goes into exponent of result, remainder yr is added to log
+    e1 = round(ef * y);
+    yr = mul_sub_x(ef, y, e1);                   // calculate remainder yr. precision very important here
+
+    // add initial terms to expansion
+    lg = nmul_add(0.5f, x2, x) + lg1;            // lg = (x - 0.5f * x2) + lg1;
+
+    // calculate rounding errors in lg
+    // rounding error in multiplication 0.5*x*x
+    x2err = mul_sub_x(0.5f*x, x, 0.5f * x2);
+    // rounding error in additions and subtractions
+    lgerr = mul_add(0.5f, x2, lg - x) - lg1;     // lgerr = ((lg - x) + 0.5f * x2) - lg1;
+
+    // extract something for the exponent
+    e2 = round(lg * y * float(VM_LOG2E));
+    // subtract this from lg, with extra precision
+    v = mul_sub_x(lg, y, e2 * ln2f_hi);
+    v = nmul_add(e2, ln2f_lo, v);                // v -= e2 * ln2f_lo;
+
+    // correct for previous rounding errors
+    v -= mul_sub(lgerr + x2err, y, yr * float(VM_LN2)); // v -= (lgerr + x2err) * y - yr * float(VM_LN2) ;
+
+    // exp function
+
+    // extract something for the exponent if possible
+    x = v;
+    e3 = round(x*log2e);
+    // high precision multiplication not needed here because abs(e3) <= 1
+    x = nmul_add(e3, float(VM_LN2), x);          // x -= e3 * float(VM_LN2);
+
+    // Taylor polynomial
+    x2  = x  * x;
+    z = polynomial_5(x, p2expf, p3expf, p4expf, p5expf, p6expf, p7expf)*x2 + x + 1.0f;
+
+    // contributions to exponent
+    ee = e1 + e2 + e3;
+    ei = roundi(ee);
+    // biased exponent of result:
+    ej = ei + (ITYPE(reinterpret_i(abs(z))) >> 23);
+    // check exponent for overflow and underflow
+    overflow  = BVTYPE(ej >= 0x0FF) | (ee >  300.f);
+    underflow = BVTYPE(ej <= 0x000) | (ee < -300.f);
+
+    // add exponent by integer addition
+    z = reinterpret_f(ITYPE(reinterpret_i(z)) + (ei << 23)); // the extra 0x10000 is shifted out here
+
+    // check for special cases
+    xfinite   = is_finite(x0);
+    yfinite   = is_finite(y);
+    efinite   = is_finite(ee);
+
+    xzero     = is_zero_or_subnormal(x0);
+    xsign     = sign_bit(x0);  // x is negative or -0.
+
+    // check for overflow and underflow
+    if (horizontal_or(overflow | underflow)) {
+        // handle errors
+        z = select(underflow, VTYPE(0.f), z);
+        z = select(overflow, infinite_vec<VTYPE>(), z);                
+    }
+
+    // check for x == 0
+    z = wm_pow_case_x0(xzero, y, z);
+    //z = select(xzero, select(y < 0.f, infinite_vec<VTYPE>(), select(y == 0.f, VTYPE(1.f), VTYPE(0.f))), z);
+
+    // check for sign of x (include -0.). y must be integer
+    if (horizontal_or(xsign)) {
+        // test if y is an integer
+        BVTYPE yinteger = y == round(y);
+        // test if y is odd: convert to int and shift bit 0 into position of sign bit.
+        // this will be 0 if overflow
+        yodd = reinterpret_f(roundi(y) << 31);
+        z1 = select(yinteger, z | yodd,                    // y is integer. get sign if y is odd
+            select(x0 == 0.f, z, nan_vec<VTYPE>(NAN_POW)));// NAN unless x0 == -0.
+        yodd = select(yinteger, yodd, 0);                  // yodd used below. only if y is integer
+        z = select(xsign, z1, z);
+    }
+
+    // check for range errors
+    if (horizontal_and(xfinite & yfinite & (efinite | xzero))) {
+        return z;            // fast return if no special cases
+    }
+
+    // handle special error cases: y infinite
+    z1 = select(yfinite & efinite, z,
+        select(x1 == 1.f, VTYPE(1.f),
+            select((x1 > 1.f) ^ sign_bit(y), infinite_vec<VTYPE>(), 0.f)));
+
+    // handle x infinite
+    z1 = select(xfinite, z1,
+        select(y == 0.f, VTYPE(1.f),
+            select(y < 0.f, yodd & z,     // 0.0 with the sign of z from above
+                abs(x0) | (x0 & yodd)))); // get sign of x0 only if y is odd integer
+
+    // Always propagate nan:
+    // Deliberately differing from the IEEE-754 standard which has pow(0,nan)=1, and pow(1,nan)=1
+    z1 = select(is_nan(x0)|is_nan(y), x0+y, z1);
+    return z1;
+}
+
+//This template is in vectorf128.h to prevent implicit conversion of float y to int when float version is not defined:
+//template <typename TT> static Vec4f pow(Vec4f const a, TT n);
+
+template <>
+inline Vec4f pow<Vec4f>(Vec4f const x, Vec4f const y) {
+    return pow_template_f(x, y);
+}
+
+template <>
+inline Vec4f pow<float>(Vec4f const x, float const y) {
+    return pow_template_f<Vec4f>(x, y);
+}
+
+template <>
+inline Vec4f pow<double>(Vec4f const x, double const y) {
+    return pow_template_f<Vec4f>(x, (float)y);
+}
+
+#if MAX_VECTOR_SIZE >= 256
+
+template <>
+inline Vec8f pow<Vec8f>(Vec8f const x, Vec8f const y) {
+    return pow_template_f(x, y);
+}
+
+template <>
+inline Vec8f pow<float>(Vec8f const x, float const y) {
+    return pow_template_f<Vec8f>(x, y);
+}
+template <>
+inline Vec8f pow<double>(Vec8f const x, double const y) {
+    return pow_template_f<Vec8f>(x, (float)y);
+}
+
+#endif // MAX_VECTOR_SIZE >= 256
+
+#if MAX_VECTOR_SIZE >= 512
+
+template <>
+inline Vec16f pow<Vec16f>(Vec16f const x, Vec16f const y) {
+    return pow_template_f(x, y);
+}
+
+template <>
+inline Vec16f pow<float>(Vec16f const x, float const y) {
+    return pow_template_f<Vec16f>(x, y);
+}
+
+template <>
+inline Vec16f pow<double>(Vec16f const x, double const y) {
+    return pow_template_f<Vec16f>(x, (float)y);
+}
+
+#endif // MAX_VECTOR_SIZE >= 512
+
+
+// *************************************************************
+//             power function with rational exponent
+// *************************************************************
+
+// macro to call template power_rational
+#define pow_ratio(x, a, b) (power_rational<decltype(x+x), a, b> (x))
+
+// Power function with rational exponent: pow(x,a/b)
+template <typename V, int a0, int b0>
+V power_rational (V const x) {
+
+    // constexpr lambda to reduce rational number a/b
+    auto reduce_rational = [](int const aa, int const bb) constexpr {
+        int a = aa, b = bb;
+        if (b < 0) {
+            a = -a; b = -b;                           // make b positive
+        }
+        while ((((a | b) & 1) == 0) && b > 0) {       // prime factor 2
+            a /= 2;  b /= 2;
+        }
+        while (a % 3 == 0 && b % 3 == 0 && b > 0) {   // prime factor 3
+            a /= 3;  b /= 3;
+        }
+        while (a % 5 == 0 && b % 5 == 0 && b > 0) {   // prime factor 5
+            a /= 5;  b /= 5;
+        }
+        return bb / b;                                // return common denominator
+    };
+    constexpr int d = reduce_rational(a0, b0);
+    constexpr int a = a0 / d;
+    constexpr int b = b0 / d;
+
+    // special cases
+    if constexpr (a == 0) return V(1.f);
+
+    else if constexpr (b == 1) return pow_n<V,a>(x);
+
+    else if constexpr (b == 2) {
+        V y, t = sqrt(x);
+        if constexpr (a == 1) y = t;
+        else if constexpr (a == -1) y = V(1.f) / t;
+        else {
+            constexpr int a2 = a > 0 ? a / 2 : (a - 1) / 2;
+            y = pow_n<V, a2>(x) * t;
+        }
+#ifdef SIGNED_ZERO
+        y = abs(y);    // pow(-0., a/2.) must be +0.
+#endif
+        return y;
+    }
+
+    else if constexpr (b == 3) {
+        V y;
+        constexpr int a3 = a % 3;
+        if constexpr (a3 == -2) {
+            V t = reciprocal_cbrt(x);
+            t *= t;
+            if constexpr (a == -2) y = t;
+            else y = t / pow_n<V, (-a-2)/3>(x);
+        }
+        else if constexpr (a3 == -1) {
+            V t = reciprocal_cbrt(x);
+            if constexpr (a == -1) y = t;
+            else y = t / pow_n<V, (-a-1)/3>(x);       // fail if INF
+        }
+        else if constexpr (a3 == 1) {
+            V t = cbrt(x);
+            if constexpr (a == 1) y = t;
+            else y = t * pow_n<V, a/3>(x);
+        }
+        else if constexpr (a3 == 2) {
+            V t = square_cbrt(x);
+            if constexpr (a == 2) y = t;
+            else y = t * pow_n<V, a/3>(x);
+        }
+        return y;
+    }
+
+    else if constexpr (b == 4) {
+        constexpr int a4 = a % 4;
+        V s1, s2, y;
+        s1 = sqrt(x);
+        if ((a & 1) == 1) s2 = sqrt(s1);
+
+        if constexpr (a4 == -3) {
+            y = s2 / pow_n<V, 1+(-a)/4>(x);
+        }
+        else if constexpr (a4 == -1) {
+            if constexpr (a != -1) s2 *= pow_n<V, (-a)/4>(x);
+            y = V(1.f) / s2;
+        }
+        else if constexpr (a4 == 1) {
+            if constexpr (a == 1) y = s2;
+            else y = s2 * pow_n<V, a/4>(x);
+        }
+        else if constexpr (a4 == 3) {
+            V t = s1 * s2;
+            if constexpr (a != 3) t *= pow_n<V, a/4>(x);
+            y = t;
+        }
+#ifdef SIGNED_ZERO
+        y = abs(y);
+#endif
+        return y;
+    }
+
+    else if constexpr (b == 6) {
+        constexpr int a6 = a % 6;
+        V y;
+        if constexpr (a6 == -5) {
+            V t = cbrt(sqrt(x)) / x;
+            if constexpr (a != -5) t /= pow_n<V, (-a)/6>(x);
+            y = t;
+        }
+        else if constexpr (a6 == -1) {
+            V t = reciprocal_cbrt(sqrt(x));
+            if constexpr (a != -1) t /= pow_n<V, (-a)/6>(x);
+            y = t;
+        }
+        else if constexpr (a6 == 1) {
+            V t = cbrt(sqrt(x));
+            if constexpr (a != 1) t *= pow_n<V, a/6>(x);
+            y = t;
+        }
+        else if constexpr (a6 == 5) {
+            V s1 = sqrt(x);
+            V t = cbrt(s1);
+            t = t*t*s1;
+            if constexpr (a != 5) t *= pow_n<V, a/6>(x);
+            y = t;
+        }
+#ifdef SIGNED_ZERO
+        y = abs(y);
+#endif
+        return y;
+    }
+
+    else if constexpr (b == 8) {
+        V s1 = sqrt(x);                // x^(1/2)
+        V s2 = sqrt(s1);               // x^(1/4)
+        V s3 = sqrt(s2);               // x^(1/8)
+        V y;
+        constexpr int a8 = a % 8;
+        if constexpr (a8 == -7) {
+            y = s3 / pow_n<V, 1+(-a)/8>(x);
+        }
+        else if constexpr (a8 == -5) {
+            y = s3 * (s2 / pow_n<V, 1+(-a)/8>(x));
+        }
+        else if constexpr (a8 == -3) {
+            y = s3 * (s1 / pow_n<V, 1+(-a)/8>(x));
+        }
+        else if constexpr (a8 == -1) {
+            if constexpr (a != -1) s3 *= pow_n<V, (-a)/8>(x);
+            y = V(1.f) / s3;
+        }
+        else if constexpr (a8 == 1) {
+            if constexpr (a == 1) y = s3;
+            else y = s3 * pow_n<V, a/8>(x);
+        }
+        else if constexpr (a8 == 3) {
+            V t = s2 * s3;
+            if constexpr (a != 3) t *= pow_n<V, a/8>(x);
+            y = t;
+        }
+        else if constexpr (a8 == 5) {
+            V t = s1 * s3;
+            if constexpr (a != 5) t *= pow_n<V, a/8>(x);
+            y = t;
+        }
+        else if constexpr (a8 == 7) {
+            V t = s2 * s3;
+            if constexpr (a != 7) s1 *= pow_n<V, a/8>(x);
+            t *= s1;
+            y = t;
+        }
+#ifdef SIGNED_ZERO
+        y = abs(y);
+#endif
+        return y;
+    }
+
+    else {
+        // general case
+        V y = x;
+        // negative x allowed when b odd or a even
+        // (if a is even then either b is odd or a/b can be reduced,
+        // but we can check a even anyway at no cost to be sure)
+        if constexpr (((b | ~a) & 1) == 1) y = abs(y);
+        y = pow(y, (double(a) / double(b)));
+        if constexpr ((a & b & 1) == 1) y = sign_combine(y, x); // apply sign if a and b both odd
+        return y;
+    }
+}
+
+
+#ifdef VCL_NAMESPACE
+}
+#endif
+
+#endif  // VECTORMATH_EXP_H
diff --git a/EEDI3/vectorclass/vectormath_hyp.h b/EEDI3/vectorclass/vectormath_hyp.h
new file mode 100644
index 0000000..bebac00
--- /dev/null
+++ b/EEDI3/vectorclass/vectormath_hyp.h
@@ -0,0 +1,717 @@
+/****************************  vectormath_hyp.h   ******************************
+* Author:        Agner Fog
+* Date created:  2014-07-09
+* Last modified: 2022-07-20
+* Version:       2.02.00
+* Project:       vector class library
+* Description:
+* Header file containing inline vector functions of hyperbolic and inverse
+* hyperbolic functions:
+* sinh        hyperbolic sine
+* cosh        hyperbolic cosine
+* tanh        hyperbolic tangent
+* asinh       inverse hyperbolic sine
+* acosh       inverse hyperbolic cosine
+* atanh       inverse hyperbolic tangent
+*
+* Theory, methods and inspiration based partially on these sources:
+* > Moshier, Stephen Lloyd Baluk: Methods and programs for mathematical functions.
+*   Ellis Horwood, 1989.
+* > VDT library developed on CERN by Danilo Piparo, Thomas Hauth and
+*   Vincenzo Innocente, 2012, https://svnweb.cern.ch/trac/vdt
+* > Cephes math library by Stephen L. Moshier 1992,
+*   http://www.netlib.org/cephes/
+*
+* For detailed instructions, see vectormath_common.h and vcl_manual.pdf
+*
+* (c) Copyright 2014-2022 Agner Fog.
+* Apache License version 2.0 or later.
+******************************************************************************/
+
+#ifndef VECTORMATH_HYP_H
+#define VECTORMATH_HYP_H  202
+
+#include "vectormath_exp.h"
+
+#ifdef VCL_NAMESPACE
+namespace VCL_NAMESPACE {
+#endif
+
+/******************************************************************************
+*                 Hyperbolic functions
+******************************************************************************/
+
+// Template for sinh function, double precision
+// This function does not produce denormals
+// Template parameters:
+// VTYPE:  double vector type
+template<typename VTYPE>
+static inline VTYPE sinh_d(VTYPE const x0) {
+// The limit of abs(x) is 709.7, as defined by max_x in vectormath_exp.h for 0.5*exp(x).
+
+    // Coefficients
+    const double p0 = -3.51754964808151394800E5;
+    const double p1 = -1.15614435765005216044E4;
+    const double p2 = -1.63725857525983828727E2;
+    const double p3 = -7.89474443963537015605E-1;
+
+    const double q0 = -2.11052978884890840399E6;
+    const double q1 =  3.61578279834431989373E4;
+    const double q2 = -2.77711081420602794433E2;
+    const double q3 =  1.0;
+
+    // data vectors
+    VTYPE  x, x2, y1, y2;
+
+    x = abs(x0);
+    auto x_small = x <= 1.0;                     // use Pade approximation if abs(x) <= 1
+
+    if (horizontal_or(x_small)) {
+        // At least one element needs small method
+        x2 = x*x;
+        y1 = polynomial_3(x2, p0, p1, p2, p3) / polynomial_3(x2, q0, q1, q2, q3);
+        y1 = mul_add(y1, x*x2, x);               // y1 = x + x2*(x*y1);
+    }
+    if (!horizontal_and(x_small)) {
+        // At least one element needs big method
+        y2 =  exp_d<VTYPE, 0, 1>(x);             //   0.5 * exp(x)
+        y2 -= 0.25 / y2;                         // - 0.5 * exp(-x)
+    }
+    y1 = select(x_small, y1, y2);                // choose method
+    y1 = sign_combine(y1, x0);                   // get original sign
+    // you can avoid the sign_combine by replacing x by x0 above, but at a loss of precision
+
+    return y1;
+}
+
+// instances of sinh_d template
+static inline Vec2d sinh(Vec2d const x) {
+    return sinh_d(x);
+}
+
+#if MAX_VECTOR_SIZE >= 256
+static inline Vec4d sinh(Vec4d const x) {
+    return sinh_d(x);
+}
+#endif // MAX_VECTOR_SIZE >= 256
+
+#if MAX_VECTOR_SIZE >= 512
+static inline Vec8d sinh(Vec8d const x) {
+    return sinh_d(x);
+}
+#endif // MAX_VECTOR_SIZE >= 512
+
+
+// Template for sinh function, single precision
+// This function does not produce denormals
+// Template parameters:
+// VTYPE:  double vector type
+template<typename VTYPE>
+static inline VTYPE sinh_f(VTYPE const x0) {
+// The limit of abs(x) is 89.0, as defined by max_x in vectormath_exp.h for 0.5*exp(x).
+
+    // Coefficients
+    const float r0 = 1.66667160211E-1f;
+    const float r1 = 8.33028376239E-3f;
+    const float r2 = 2.03721912945E-4f;
+
+    // data vectors
+    VTYPE x, x2, y1, y2;
+
+    x = abs(x0);
+    auto x_small = x <= 1.0f;                    // use polynomial approximation if abs(x) <= 1
+
+    if (horizontal_or(x_small)) {
+        // At least one element needs small method
+        x2 = x*x;
+        y1 = polynomial_2(x2, r0, r1, r2);
+        y1 = mul_add(y1, x2*x, x);               // y1 = x + x2*(x*y1);
+    }
+    if (!horizontal_and(x_small)) {
+        // At least one element needs big method
+        y2 =  exp_f<VTYPE, 0, 1>(x);             //   0.5 * exp(x)
+        y2 -= 0.25f / y2;                        // - 0.5 * exp(-x)
+    }
+    y1 = select(x_small, y1, y2);                // choose method
+    y1 = sign_combine(y1, x0);                   // get original sign
+    // you can avoid the sign_combine by replacing x by x0 above, but at a loss of precision
+
+    return y1;
+}
+
+// instances of sinh_f template
+static inline Vec4f sinh(Vec4f const x) {
+    return sinh_f(x);
+}
+
+#if MAX_VECTOR_SIZE >= 256
+static inline Vec8f sinh(Vec8f const x) {
+    return sinh_f(x);
+}
+#endif // MAX_VECTOR_SIZE >= 256
+
+#if MAX_VECTOR_SIZE >= 512
+static inline Vec16f sinh(Vec16f const x) {
+    return sinh_f(x);
+}
+#endif // MAX_VECTOR_SIZE >= 512
+
+
+// Template for cosh function, double precision
+// This function does not produce denormals
+// Template parameters:
+// VTYPE:  double vector type
+template<typename VTYPE>
+static inline VTYPE cosh_d(VTYPE const x0) {
+// The limit of abs(x) is 709.7, as defined by max_x in vectormath_exp.h for 0.5*exp(x).
+
+    // data vectors
+    VTYPE x, y;
+    x  = abs(x0);
+    y  = exp_d<VTYPE, 0, 1>(x);                  //   0.5 * exp(x)
+    y += 0.25 / y;                               // + 0.5 * exp(-x)
+    return y;
+}
+
+// instances of sinh_d template
+static inline Vec2d cosh(Vec2d const x) {
+    return cosh_d(x);
+}
+
+#if MAX_VECTOR_SIZE >= 256
+static inline Vec4d cosh(Vec4d const x) {
+    return cosh_d(x);
+}
+#endif // MAX_VECTOR_SIZE >= 256
+
+#if MAX_VECTOR_SIZE >= 512
+static inline Vec8d cosh(Vec8d const x) {
+    return cosh_d(x);
+}
+#endif // MAX_VECTOR_SIZE >= 512
+
+
+// Template for cosh function, single precision
+// This function does not produce denormals
+// Template parameters:
+// VTYPE:  double vector type
+template<typename VTYPE>
+static inline VTYPE cosh_f(VTYPE const x0) {
+// The limit of abs(x) is 89.0, as defined by max_x in vectormath_exp.h for 0.5*exp(x).
+
+    // data vectors
+    VTYPE x, y;
+    x  = abs(x0);
+    y  = exp_f<VTYPE, 0, 1>(x);                  //   0.5 * exp(x)
+    y += 0.25f / y;                              // + 0.5 * exp(-x)
+    return y;
+}
+
+// instances of sinh_d template
+static inline Vec4f cosh(Vec4f const x) {
+    return cosh_f(x);
+}
+
+#if MAX_VECTOR_SIZE >= 256
+static inline Vec8f cosh(Vec8f const x) {
+    return cosh_f(x);
+}
+#endif // MAX_VECTOR_SIZE >= 256
+
+#if MAX_VECTOR_SIZE >= 512
+static inline Vec16f cosh(Vec16f const x) {
+    return cosh_f(x);
+}
+#endif // MAX_VECTOR_SIZE >= 512
+
+
+// Template for tanh function, double precision
+// This function does not produce denormals
+// Template parameters:
+// VTYPE:  double vector type
+template<typename VTYPE>
+static inline VTYPE tanh_d(VTYPE const x0) {
+
+    // Coefficients
+    const double p0 = -1.61468768441708447952E3;
+    const double p1 = -9.92877231001918586564E1;
+    const double p2 = -9.64399179425052238628E-1;
+
+    const double q0 =  4.84406305325125486048E3;
+    const double q1 =  2.23548839060100448583E3;
+    const double q2 =  1.12811678491632931402E2;
+    const double q3 =  1.0;
+
+    // data vectors
+    VTYPE  x, x2, y1, y2;
+
+    x = abs(x0);
+    auto x_small = x <= 0.625;                   // use Pade approximation if abs(x) <= 5/8
+
+    if (horizontal_or(x_small)) {
+        // At least one element needs small method
+        x2 = x*x;
+        y1 = polynomial_2(x2, p0, p1, p2) / polynomial_3(x2, q0, q1, q2, q3);
+        y1 = mul_add(y1, x2*x, x);               // y1 = x + x2*(x*y1);
+    }
+    if (!horizontal_and(x_small)) {
+        // At least one element needs big method
+        y2 = exp(x+x);                           // exp(2*x)
+        y2 = 1.0 - 2.0 / (y2 + 1.0);             // tanh(x)
+    }
+    auto x_big = x > 350.;
+    y1 = select(x_small, y1, y2);                // choose method
+    y1 = select(x_big,  1.0, y1);                // avoid overflow
+    y1 = sign_combine(y1, x0);                   // get original sign
+    return y1;
+}
+
+// instances of tanh_d template
+static inline Vec2d tanh(Vec2d const x) {
+    return tanh_d(x);
+}
+
+#if MAX_VECTOR_SIZE >= 256
+static inline Vec4d tanh(Vec4d const x) {
+    return tanh_d(x);
+}
+#endif // MAX_VECTOR_SIZE >= 256
+
+#if MAX_VECTOR_SIZE >= 512
+static inline Vec8d tanh(Vec8d const x) {
+    return tanh_d(x);
+}
+#endif // MAX_VECTOR_SIZE >= 512
+
+
+// Template for tanh function, single precision
+// This function does not produce denormals
+// Template parameters:
+// VTYPE:  double vector type
+template<typename VTYPE>
+static inline VTYPE tanh_f(VTYPE const x0) {
+// The limit of abs(x) is 89.0, as defined by max_x in vectormath_exp.h for 0.5*exp(x).
+
+    // Coefficients
+    const float r0 = -3.33332819422E-1f;
+    const float r1 =  1.33314422036E-1f;
+    const float r2 = -5.37397155531E-2f;
+    const float r3 =  2.06390887954E-2f;
+    const float r4 = -5.70498872745E-3f;
+
+    // data vectors
+    VTYPE x, x2, y1, y2;
+
+    x = abs(x0);
+    auto x_small = x <= 0.625f;                  // use polynomial approximation if abs(x) <= 5/8
+
+    if (horizontal_or(x_small)) {
+        // At least one element needs small method
+        x2 = x*x;
+        y1 = polynomial_4(x2, r0, r1, r2, r3, r4);
+        y1 = mul_add(y1, x2*x, x);               // y1 = x + (x2*x)*y1;
+    }
+    if (!horizontal_and(x_small)) {
+        // At least one element needs big method
+        y2 = exp(x+x);                           // exp(2*x)
+        y2 = 1.0f - 2.0f / (y2 + 1.0f);          // tanh(x)
+    }
+    auto x_big = x > 44.4f;
+    y1 = select(x_small, y1, y2);                // choose method
+    y1 = select(x_big,  1.0f, y1);               // avoid overflow
+    y1 = sign_combine(y1, x0);                   // get original sign
+    return y1;
+}
+
+// instances of tanh_f template
+static inline Vec4f tanh(Vec4f const x) {
+    return tanh_f(x);
+}
+
+#if MAX_VECTOR_SIZE >= 256
+static inline Vec8f tanh(Vec8f const x) {
+    return tanh_f(x);
+}
+#endif // MAX_VECTOR_SIZE >= 256
+
+#if MAX_VECTOR_SIZE >= 512
+static inline Vec16f tanh(Vec16f const x) {
+    return tanh_f(x);
+}
+#endif // MAX_VECTOR_SIZE >= 512
+
+
+
+/******************************************************************************
+*                 Inverse hyperbolic functions
+******************************************************************************/
+
+// Template for asinh function, double precision
+// This function does not produce denormals
+// Template parameters:
+// VTYPE:  double vector type
+template<typename VTYPE>
+static inline VTYPE asinh_d(VTYPE const x0) {
+
+    // Coefficients
+    const double p0 = -5.56682227230859640450E0;
+    const double p1 = -9.09030533308377316566E0;
+    const double p2 = -4.37390226194356683570E0;
+    const double p3 = -5.91750212056387121207E-1;
+    const double p4 = -4.33231683752342103572E-3;
+
+    const double q0 =  3.34009336338516356383E1;
+    const double q1 =  6.95722521337257608734E1;
+    const double q2 =  4.86042483805291788324E1;
+    const double q3 =  1.28757002067426453537E1;
+    const double q4 =  1.0;
+
+    // data vectors
+    VTYPE  x, x2, y1, y2;
+
+    x2 = x0 * x0;
+    x  = abs(x0);
+    auto x_small = x <= 0.533;                   // use Pade approximation if abs(x) <= 0.5
+    // Both methods give the highest error close to 0.5.
+    // This limit is adjusted for minimum error
+    auto x_huge  = x > 1.E20;                    // simple approximation, avoid overflow
+
+    if (horizontal_or(x_small)) {
+        // At least one element needs small method
+        y1 = polynomial_4(x2, p0, p1, p2, p3, p4) / polynomial_4(x2, q0, q1, q2, q3, q4);
+        y1 = mul_add(y1, x2*x, x);               // y1 = x + (x2*x)*y1;
+    }
+    if (!horizontal_and(x_small)) {
+        // At least one element needs big method
+        y2 = log(x + sqrt(x2 + 1.0));
+        if (horizontal_or(x_huge)) {
+            // At least one element needs huge method to avoid overflow
+            y2 = select(x_huge, log(x) + VM_LN2, y2);
+        }
+    }
+    y1 = select(x_small, y1, y2);                // choose method
+    y1 = sign_combine(y1, x0);                   // get original sign
+    return y1;
+}
+
+// instances of asinh_d template
+static inline Vec2d asinh(Vec2d const x) {
+    return asinh_d(x);
+}
+
+#if MAX_VECTOR_SIZE >= 256
+static inline Vec4d asinh(Vec4d const x) {
+    return asinh_d(x);
+}
+#endif // MAX_VECTOR_SIZE >= 256
+
+#if MAX_VECTOR_SIZE >= 512
+static inline Vec8d asinh(Vec8d const x) {
+    return asinh_d(x);
+}
+#endif // MAX_VECTOR_SIZE >= 512
+
+
+// Template for asinh function, single precision
+// This function does not produce denormals
+// Template parameters:
+// VTYPE:  double vector type
+template<typename VTYPE>
+static inline VTYPE asinh_f(VTYPE const x0) {
+
+    // Coefficients
+    const float r0 = -1.6666288134E-1f;
+    const float r1 =  7.4847586088E-2f;
+    const float r2 = -4.2699340972E-2f;
+    const float r3 =  2.0122003309E-2f;
+
+    // data vectors
+    VTYPE  x, x2, y1, y2;
+
+    x2 = x0 * x0;
+    x  = abs(x0);
+    auto x_small = x <= 0.51f;                   // use polynomial approximation if abs(x) <= 0.5
+    auto x_huge  = x > 1.E10f;                   // simple approximation, avoid overflow
+
+    if (horizontal_or(x_small)) {
+        // At least one element needs small method
+        y1 = polynomial_3(x2, r0, r1, r2, r3);
+        y1 = mul_add(y1, x2*x, x);               // y1 = x + (x2*x)*y1;
+    }
+    if (!horizontal_and(x_small)) {
+        // At least one element needs big method
+        y2 = log(x + sqrt(x2 + 1.0f));
+        if (horizontal_or(x_huge)) {
+            // At least one element needs huge method to avoid overflow
+            y2 = select(x_huge, log(x) + (float)VM_LN2, y2);
+        }
+    }
+    y1 = select(x_small, y1, y2);                // choose method
+    y1 = sign_combine(y1, x0);                   // get original sign
+    return y1;
+}
+
+// instances of asinh_f template
+static inline Vec4f asinh(Vec4f const x) {
+    return asinh_f(x);
+}
+
+#if MAX_VECTOR_SIZE >= 256
+static inline Vec8f asinh(Vec8f const x) {
+    return asinh_f(x);
+}
+#endif // MAX_VECTOR_SIZE >= 256
+
+#if MAX_VECTOR_SIZE >= 512
+static inline Vec16f asinh(Vec16f const x) {
+    return asinh_f(x);
+}
+#endif // MAX_VECTOR_SIZE >= 512
+
+
+// Template for acosh function, double precision
+// This function does not produce denormals
+// Template parameters:
+// VTYPE:  double vector type
+template<typename VTYPE>
+static inline VTYPE acosh_d(VTYPE const x0) {
+
+    // Coefficients
+    const double p0 = 1.10855947270161294369E5;
+    const double p1 = 1.08102874834699867335E5;
+    const double p2 = 3.43989375926195455866E4;
+    const double p3 = 3.94726656571334401102E3;
+    const double p4 = 1.18801130533544501356E2;
+
+    const double q0 = 7.83869920495893927727E4;
+    const double q1 = 8.29725251988426222434E4;
+    const double q2 = 2.97683430363289370382E4;
+    const double q3 = 4.15352677227719831579E3;
+    const double q4 = 1.86145380837903397292E2;
+    const double q5 = 1.0;
+
+    // data vectors
+    VTYPE  x1, y1, y2;
+
+    x1      = x0 - 1.0;
+    auto undef   = x0 < 1.0;                     // result is NAN
+    auto x_small = x1 < 0.49;                    // use Pade approximation if abs(x-1) < 0.5
+    auto x_huge  = x1 > 1.E20;                   // simple approximation, avoid overflow
+
+    if (horizontal_or(x_small)) {
+        // At least one element needs small method
+        y1 = sqrt(x1) * (polynomial_4(x1, p0, p1, p2, p3, p4) / polynomial_5(x1, q0, q1, q2, q3, q4, q5));
+        // x < 1 generates NAN
+        y1 = select(undef, nan_vec<VTYPE>(NAN_HYP), y1);
+    }
+    if (!horizontal_and(x_small)) {
+        // At least one element needs big method
+        y2 = log(x0 + sqrt(mul_sub(x0,x0,1.0)));
+        if (horizontal_or(x_huge)) {
+            // At least one element needs huge method to avoid overflow
+            y2 = select(x_huge, log(x0) + VM_LN2, y2);
+        }
+    }
+    y1 = select(x_small, y1, y2);                // choose method
+    return y1;
+}
+
+// instances of acosh_d template
+static inline Vec2d acosh(Vec2d const x) {
+    return acosh_d(x);
+}
+
+#if MAX_VECTOR_SIZE >= 256
+static inline Vec4d acosh(Vec4d const x) {
+    return acosh_d(x);
+}
+#endif // MAX_VECTOR_SIZE >= 256
+
+#if MAX_VECTOR_SIZE >= 512
+static inline Vec8d acosh(Vec8d const x) {
+    return acosh_d(x);
+}
+#endif // MAX_VECTOR_SIZE >= 512
+
+
+// Template for acosh function, single precision
+// This function does not produce denormals
+// Template parameters:
+// VTYPE:  double vector type
+template<typename VTYPE>
+static inline VTYPE acosh_f(VTYPE const x0) {
+
+    // Coefficients
+    const float r0 =  1.4142135263E0f;
+    const float r1 = -1.1784741703E-1f;
+    const float r2 =  2.6454905019E-2f;
+    const float r3 = -7.5272886713E-3f;
+    const float r4 =  1.7596881071E-3f;
+
+    // data vectors
+    VTYPE  x1, y1, y2;
+
+    x1      = x0 - 1.0f;
+    auto undef   = x0 < 1.0f;                    // result is NAN
+    auto x_small = x1 < 0.49f;                   // use Pade approximation if abs(x-1) < 0.5
+    auto x_huge  = x1 > 1.E10f;                  // simple approximation, avoid overflow
+
+    if (horizontal_or(x_small)) {
+        // At least one element needs small method
+        y1 = sqrt(x1) * polynomial_4(x1, r0, r1, r2, r3, r4);
+        // x < 1 generates NAN
+        y1 = select(undef, nan_vec<VTYPE>(NAN_HYP), y1);
+    }
+    if (!horizontal_and(x_small)) {
+        // At least one element needs big method
+        y2 = log(x0 + sqrt(mul_sub(x0,x0,1.0)));
+        if (horizontal_or(x_huge)) {
+            // At least one element needs huge method to avoid overflow
+            y2 = select(x_huge, log(x0) + (float)VM_LN2, y2);
+        }
+    }
+    y1 = select(x_small, y1, y2);                // choose method
+    return y1;
+}
+
+// instances of acosh_f template
+static inline Vec4f acosh(Vec4f const x) {
+    return acosh_f(x);
+}
+
+#if MAX_VECTOR_SIZE >= 256
+static inline Vec8f acosh(Vec8f const x) {
+    return acosh_f(x);
+}
+#endif // MAX_VECTOR_SIZE >= 256
+
+#if MAX_VECTOR_SIZE >= 512
+static inline Vec16f acosh(Vec16f const x) {
+    return acosh_f(x);
+}
+#endif // MAX_VECTOR_SIZE >= 512
+
+
+// Template for atanh function, double precision
+// This function does not produce denormals
+// Template parameters:
+// VTYPE:  double vector type
+template<typename VTYPE>
+static inline VTYPE atanh_d(VTYPE const x0) {
+
+    // Coefficients
+    const double p0 = -3.09092539379866942570E1;
+    const double p1 =  6.54566728676544377376E1;
+    const double p2 = -4.61252884198732692637E1;
+    const double p3 =  1.20426861384072379242E1;
+    const double p4 = -8.54074331929669305196E-1;
+
+    const double q0 = -9.27277618139601130017E1;
+    const double q1 =  2.52006675691344555838E2;
+    const double q2 = -2.49839401325893582852E2;
+    const double q3 =  1.08938092147140262656E2;
+    const double q4 = -1.95638849376911654834E1;
+    const double q5 =  1.0;
+
+    // data vectors
+    VTYPE  x, x2, y1, y2, y3;
+
+    x  = abs(x0);
+    auto x_small = x < 0.5;                      // use Pade approximation if abs(x) < 0.5
+
+    if (horizontal_or(x_small)) {
+        // At least one element needs small method
+        x2 = x * x;
+        y1 = polynomial_4(x2, p0, p1, p2, p3, p4) / polynomial_5(x2, q0, q1, q2, q3, q4, q5);
+        y1 = mul_add(y1, x2*x, x);
+    }
+    if (!horizontal_and(x_small)) {
+        // At least one element needs big method
+        y2 = log((1.0+x)/(1.0-x)) * 0.5;
+        // check if out of range
+        y3 = select(x == 1.0, infinite_vec<VTYPE>(), nan_vec<VTYPE>(NAN_HYP));
+        y2 = select(x >= 1.0, y3, y2);
+    }
+    y1 = select(x_small, y1, y2);                // choose method
+    y1 = sign_combine(y1, x0);                   // get original sign
+    return y1;
+}
+
+// instances of atanh_d template
+static inline Vec2d atanh(Vec2d const x) {
+    return atanh_d(x);
+}
+
+#if MAX_VECTOR_SIZE >= 256
+static inline Vec4d atanh(Vec4d const x) {
+    return atanh_d(x);
+}
+#endif // MAX_VECTOR_SIZE >= 256
+
+#if MAX_VECTOR_SIZE >= 512
+static inline Vec8d atanh(Vec8d const x) {
+    return atanh_d(x);
+}
+#endif // MAX_VECTOR_SIZE >= 512
+
+
+// Template for atanh function, single precision
+// This function does not produce denormals
+// Template parameters:
+// VTYPE:  double vector type
+template<typename VTYPE>
+static inline VTYPE atanh_f(VTYPE const x0) {
+
+    // Coefficients
+    const float r0 = 3.33337300303E-1f;
+    const float r1 = 1.99782164500E-1f;
+    const float r2 = 1.46691431730E-1f;
+    const float r3 = 8.24370301058E-2f;
+    const float r4 = 1.81740078349E-1f;
+
+    // data vectors
+    VTYPE  x, x2, y1, y2, y3;
+
+    x  = abs(x0);
+    auto x_small = x < 0.5f;                     // use polynomial approximation if abs(x) < 0.5
+
+    if (horizontal_or(x_small)) {
+        // At least one element needs small method
+        x2 = x * x;
+        y1 = polynomial_4(x2, r0, r1, r2, r3, r4);
+        y1 = mul_add(y1, x2*x, x);
+    }
+    if (!horizontal_and(x_small)) {
+        // At least one element needs big method
+        y2 = log((1.0f+x)/(1.0f-x)) * 0.5f;
+        // check if out of range
+        y3 = select(x == 1.0f, infinite_vec<VTYPE>(), nan_vec<VTYPE>(NAN_HYP));
+        y2 = select(x >= 1.0f, y3, y2);
+    }
+    y1 = select(x_small, y1, y2);                // choose method
+    y1 = sign_combine(y1, x0);                   // get original sign
+    return y1;
+}
+
+// instances of atanh_f template
+static inline Vec4f atanh(Vec4f const x) {
+    return atanh_f(x);
+}
+
+#if MAX_VECTOR_SIZE >= 256
+static inline Vec8f atanh(Vec8f const x) {
+    return atanh_f(x);
+}
+#endif // MAX_VECTOR_SIZE >= 256
+
+#if MAX_VECTOR_SIZE >= 512
+static inline Vec16f atanh(Vec16f const x) {
+    return atanh_f(x);
+}
+#endif // MAX_VECTOR_SIZE >= 512
+
+#ifdef VCL_NAMESPACE
+}
+#endif
+
+#endif
diff --git a/EEDI3/vectorclass/vectormath_lib.h b/EEDI3/vectorclass/vectormath_lib.h
new file mode 100644
index 0000000..04be83c
--- /dev/null
+++ b/EEDI3/vectorclass/vectormath_lib.h
@@ -0,0 +1,2211 @@
+/****************************  vectormath_lib.h   *****************************
+* Author:        Agner Fog
+* Date created:  2012-05-30
+* Last modified: 2022-08-02
+* Version:       2.02.00
+* Project:       vector class library
+* Description:
+* Header file defining mathematical functions on floating point vectors
+* using Intel SVML (Short Vector Math Library)
+*
+* Include this file if you want to use SVML for math functions on vectors
+* See vcl_manual.pdf for details on how to obtain the SVML library and link to it.
+* Alternatively, use the inline math functions by including
+* vectormath_exp.h for power and exponential functions,
+* vectormath_trig.h for trigonometric functions,
+* vectormath_hyp.h for hyperbolic functions
+*
+* For detailed instructions, see vcl_manual.pdf
+*
+* (c) Copyright 2012-2022 Agner Fog.
+* Apache License version 2.0 or later.
+\*****************************************************************************/
+
+// check combination of header files
+#ifndef VECTORMATH_LIB_H
+#define VECTORMATH_LIB_H  202
+
+#ifdef VECTORMATH_COMMON_H
+#error conflicting header files. More than one implementation of mathematical functions included
+#else
+
+#include "vectorclass.h"     // make sure vector classes are defined first
+
+#ifdef   VCL_NAMESPACE
+namespace VCL_NAMESPACE {    // optional name space
+#endif
+
+#if defined(__INTEL_COMPILER) || defined(__INTEL_LLVM_COMPILER)
+#define USE_SVML_INTRINSICS  // Intel compilers have intrinsic functions of access to SVML library
+#endif
+
+#if !(defined(USE_SVML_INTRINSICS))
+// sinpi, cospi, and tanpi functions are included in SVML, but undocumented
+// (The "Classic" version of Intel compiler accepts the intrinsics of these functions even though they are not in the header files)
+#define TRIGPI_FUNCTIONS
+#endif
+
+#if defined(__clang__) || defined (__GNUC__)
+#define SINCOS_ASM  // sincos can be fixed with inline assembly
+#else
+    // MS compiler does not support inline assembly. sincos not available
+#endif
+
+
+
+#ifdef USE_SVML_INTRINSICS
+
+/*****************************************************************************
+*
+*      128-bit vector functions using Intel compiler intrinsic functions
+*
+*****************************************************************************/
+
+// exponential and power functions
+static inline Vec4f exp(Vec4f const x) {    // exponential function
+    return _mm_exp_ps(x);
+}
+static inline Vec2d exp(Vec2d const x) {    // exponential function
+    return _mm_exp_pd(x);
+}
+static inline Vec4f expm1(Vec4f const x) {  // exp(x)-1. Avoids loss of precision if x is close to 1
+    return _mm_expm1_ps(x);
+}
+static inline Vec2d expm1(Vec2d const x) {  // exp(x)-1. Avoids loss of precision if x is close to 1
+    return _mm_expm1_pd(x);
+}
+static inline Vec4f exp2(Vec4f const x) {   // pow(2,x)
+    return _mm_exp2_ps(x);
+}
+static inline Vec2d exp2(Vec2d const x) {   // pow(2,x)
+    return _mm_exp2_pd(x);
+}
+static inline Vec4f exp10(Vec4f const x) {  // pow(10,x)
+    return _mm_exp10_ps(x);
+}
+static inline Vec2d exp10(Vec2d const x) {  // pow(10,x)
+    return _mm_exp10_pd(x);
+}
+static inline Vec4f pow(Vec4f const a, Vec4f const b) {    // pow(a,b) = a to the power of b
+    return _mm_pow_ps(a, b);
+}
+static inline Vec4f pow(Vec4f const a, float const b) {    // pow(a,b) = a to the power of b
+    return _mm_pow_ps(a, Vec4f(b));
+}
+static inline Vec2d pow(Vec2d const a, Vec2d const b) {    // pow(a,b) = a to the power of b
+    return _mm_pow_pd(a, b);
+}
+static inline Vec2d pow(Vec2d const a, double const b) {   // pow(a,b) = a to the power of b
+    return _mm_pow_pd(a, Vec2d(b));
+}
+static inline Vec4f cbrt(Vec4f const x) {   // pow(x,1/3)
+    return _mm_cbrt_ps(x);
+}
+static inline Vec2d cbrt(Vec2d const x) {   // pow(x,1/3)
+    return _mm_cbrt_pd(x);
+}
+// logarithms
+static inline Vec4f log(Vec4f const x) {    // natural logarithm
+    return _mm_log_ps(x);
+}
+static inline Vec2d log(Vec2d const x) {    // natural logarithm
+    return _mm_log_pd(x);
+}
+static inline Vec4f log1p(Vec4f const x) {  // log(1+x). Avoids loss of precision if 1+x is close to 1
+    return _mm_log1p_ps(x);
+}
+static inline Vec2d log1p(Vec2d const x) {  // log(1+x). Avoids loss of precision if 1+x is close to 1
+    return _mm_log1p_pd(x);
+}
+static inline Vec4f log2(Vec4f const x) {   // logarithm base 2
+    return _mm_log2_ps(x);
+}
+static inline Vec2d log2(Vec2d const x) {   // logarithm base 2
+    return _mm_log2_pd(x);
+}
+static inline Vec4f log10(Vec4f const x) {  // logarithm base 10
+    return _mm_log10_ps(x);
+}
+static inline Vec2d log10(Vec2d const x) {  // logarithm base 10
+    return _mm_log10_pd(x);
+}
+
+// trigonometric functions
+static inline Vec4f sin(Vec4f const x) {    // sine
+    return _mm_sin_ps(x);
+}
+static inline Vec2d sin(Vec2d const x) {    // sine
+    return _mm_sin_pd(x);
+}
+static inline Vec4f cos(Vec4f const x) {    // cosine
+    return _mm_cos_ps(x);
+}
+static inline Vec2d cos(Vec2d const x) {    // cosine
+    return _mm_cos_pd(x);
+}
+static inline Vec4f sincos(Vec4f * pcos, Vec4f const x) {  // sine and cosine. sin(x) returned, cos(x) in pcos
+    __m128 r_sin, r_cos;
+    r_sin = _mm_sincos_ps(&r_cos, x);
+    *pcos = r_cos;
+    return r_sin;
+}
+static inline Vec2d sincos(Vec2d * pcos, Vec2d const x) {  // sine and cosine. sin(x) returned, cos(x) in pcos
+    __m128d r_sin, r_cos;
+    r_sin = _mm_sincos_pd(&r_cos, x);
+    *pcos = r_cos;
+    return r_sin;
+}
+static inline Vec4f tan(Vec4f const x) {    // tangent
+    return _mm_tan_ps(x);
+}
+static inline Vec2d tan(Vec2d const x) {    // tangent
+    return _mm_tan_pd(x);
+}
+
+#ifdef TRIGPI_FUNCTIONS
+static inline Vec4f sinpi(Vec4f const x) {    // sine
+    return _mm_sinpi_ps(x);
+}
+static inline Vec2d sinpi(Vec2d const x) {    // sine
+    return _mm_sinpi_pd(x);
+}
+static inline Vec4f cospi(Vec4f const x) {    // cosine
+    return _mm_cospi_ps(x);
+}
+static inline Vec2d cospi(Vec2d const x) {    // cosine
+    return _mm_cospi_pd(x);
+}
+static inline Vec4f tanpi(Vec4f const x) {    // tangent
+    return _mm_tanpi_ps(x);
+}
+static inline Vec2d tanpi(Vec2d const x) {    // tangent
+    return _mm_tanpi_pd(x);
+}
+#endif // TRIGPI_FUNCTIONS
+
+// inverse trigonometric functions
+static inline Vec4f asin(Vec4f const x) {   // inverse sine
+    return _mm_asin_ps(x);
+}
+static inline Vec2d asin(Vec2d const x) {   // inverse sine
+    return _mm_asin_pd(x);
+}
+
+static inline Vec4f acos(Vec4f const x) {   // inverse cosine
+    return _mm_acos_ps(x);
+}
+static inline Vec2d acos(Vec2d const x) {   // inverse cosine
+    return _mm_acos_pd(x);
+}
+
+static inline Vec4f atan(Vec4f const x) {   // inverse tangent
+    return _mm_atan_ps(x);
+}
+static inline Vec2d atan(Vec2d const x) {   // inverse tangent
+    return _mm_atan_pd(x);
+}
+static inline Vec4f atan2(Vec4f const a, Vec4f const b) {  // inverse tangent of a/b
+    return _mm_atan2_ps(a, b);
+}
+static inline Vec2d atan2(Vec2d const a, Vec2d const b) {  // inverse tangent of a/b
+    return _mm_atan2_pd(a, b);
+}
+
+// hyperbolic functions and inverse hyperbolic functions
+static inline Vec4f sinh(Vec4f const x) {   // hyperbolic sine
+    return _mm_sinh_ps(x);
+}
+static inline Vec2d sinh(Vec2d const x) {   // hyperbolic sine
+    return _mm_sinh_pd(x);
+}
+static inline Vec4f cosh(Vec4f const x) {   // hyperbolic cosine
+    return _mm_cosh_ps(x);
+}
+static inline Vec2d cosh(Vec2d const x) {   // hyperbolic cosine
+    return _mm_cosh_pd(x);
+}
+static inline Vec4f tanh(Vec4f const x) {   // hyperbolic tangent
+    return _mm_tanh_ps(x);
+}
+static inline Vec2d tanh(Vec2d const x) {   // hyperbolic tangent
+    return _mm_tanh_pd(x);
+}
+static inline Vec4f asinh(Vec4f const x) {  // inverse hyperbolic sine
+    return _mm_asinh_ps(x);
+}
+static inline Vec2d asinh(Vec2d const x) {  // inverse hyperbolic sine
+    return _mm_asinh_pd(x);
+}
+static inline Vec4f acosh(Vec4f const x) {  // inverse hyperbolic cosine
+    return _mm_acosh_ps(x);
+}
+static inline Vec2d acosh(Vec2d const x) {  // inverse hyperbolic cosine
+    return _mm_acosh_pd(x);
+}
+static inline Vec4f atanh(Vec4f const x) {  // inverse hyperbolic tangent
+    return _mm_atanh_ps(x);
+}
+static inline Vec2d atanh(Vec2d const x) {  // inverse hyperbolic tangent
+    return _mm_atanh_pd(x);
+}
+
+// error function
+static inline Vec4f erf(Vec4f const x) {    // error function
+    return _mm_erf_ps(x);
+}
+static inline Vec2d erf(Vec2d const x) {    // error function
+    return _mm_erf_pd(x);
+}
+static inline Vec4f erfc(Vec4f const x) {   // error function complement
+    return _mm_erfc_ps(x);
+}
+static inline Vec2d erfc(Vec2d const x) {   // error function complement
+    return _mm_erfc_pd(x);
+}
+static inline Vec4f erfinv(Vec4f const x) { // inverse error function
+    return _mm_erfinv_ps(x);
+}
+static inline Vec2d erfinv(Vec2d const x) { // inverse error function
+    return _mm_erfinv_pd(x);
+}
+
+static inline Vec4f cdfnorm(Vec4f const x) {     // cumulative normal distribution function
+    return _mm_cdfnorm_ps(x);
+}
+static inline Vec2d cdfnorm(Vec2d const x) {     // cumulative normal distribution function
+    return _mm_cdfnorm_pd(x);
+}
+static inline Vec4f cdfnorminv(Vec4f const x) {  // inverse cumulative normal distribution function
+    return _mm_cdfnorminv_ps(x);
+}
+static inline Vec2d cdfnorminv(Vec2d const x) {  // inverse cumulative normal distribution function
+    return _mm_cdfnorminv_pd(x);
+}
+
+#else
+/*************************************************************************************
+*
+*      128-bit vector functions using other compiler than Intel C++ compiler "Classic"
+*
+*************************************************************************************/
+
+#if (defined(_WIN64) && !defined(USE_SVML_INTRINSICS) )
+// (call with one parameter may work without __vectorcall because the parameter happens to be in zmm0, but that would be unsafe)
+#define V_VECTORCALL  __vectorcall  // fix calling convention, one parameter.
+#define V_VECTORCALL2 __vectorcall  // fix calling convention, two parameters or two returns
+#else
+#define V_VECTORCALL
+#define V_VECTORCALL2
+#endif
+
+
+// External function prototypes for SVML library, 128-bit vectors
+extern "C" {
+    extern __m128  V_VECTORCALL __svml_expf4       (__m128);
+    extern __m128d V_VECTORCALL __svml_exp2        (__m128d);
+    extern __m128  V_VECTORCALL __svml_expm1f4     (__m128);
+    extern __m128d V_VECTORCALL __svml_expm12      (__m128d);
+    extern __m128  V_VECTORCALL __svml_exp2f4      (__m128);
+    extern __m128d V_VECTORCALL __svml_exp22       (__m128d);
+    extern __m128  V_VECTORCALL __svml_exp10f4     (__m128);
+    extern __m128d V_VECTORCALL __svml_exp102      (__m128d);
+    extern __m128  V_VECTORCALL2 __svml_powf4      (__m128,  __m128);
+    extern __m128d V_VECTORCALL2 __svml_pow2       (__m128d, __m128d);
+    extern __m128  V_VECTORCALL __svml_cbrtf4      (__m128);
+    extern __m128d V_VECTORCALL __svml_cbrt2       (__m128d);
+    extern __m128  V_VECTORCALL __svml_invsqrtf4   (__m128);
+    extern __m128d V_VECTORCALL __svml_invsqrt2    (__m128d);
+    extern __m128  V_VECTORCALL __svml_logf4       (__m128);
+    extern __m128d V_VECTORCALL __svml_log2        (__m128d);
+    extern __m128  V_VECTORCALL __svml_log1pf4     (__m128);
+    extern __m128d V_VECTORCALL __svml_log1p2      (__m128d);
+    extern __m128  V_VECTORCALL __svml_log2f4      (__m128);
+    extern __m128d V_VECTORCALL __svml_log22       (__m128d);
+    extern __m128  V_VECTORCALL __svml_log10f4     (__m128);
+    extern __m128d V_VECTORCALL __svml_log102      (__m128d);
+    extern __m128  V_VECTORCALL __svml_sinf4       (__m128);
+    extern __m128d V_VECTORCALL __svml_sin2        (__m128d);
+    extern __m128  V_VECTORCALL __svml_cosf4       (__m128);
+    extern __m128d V_VECTORCALL __svml_cos2        (__m128d);
+    extern __m128  V_VECTORCALL2 __svml_sincosf4   (__m128);  // cos returned in xmm1
+    extern __m128d V_VECTORCALL2 __svml_sincos2    (__m128d); // cos returned in xmm1
+    extern __m128  V_VECTORCALL __svml_tanf4       (__m128);
+    extern __m128d V_VECTORCALL __svml_tan2        (__m128d);    
+    extern __m128  V_VECTORCALL __svml_sinpif4     (__m128);
+    extern __m128d V_VECTORCALL __svml_sinpi2      (__m128d);
+    extern __m128  V_VECTORCALL __svml_cospif4     (__m128);
+    extern __m128d V_VECTORCALL __svml_cospi2      (__m128d);
+    //extern __m128  V_VECTORCALL2 __svml_sincospif4 (__m128); // not in library
+    //extern __m128d V_VECTORCALL2 __svml_sincospi2  (__m128d);// not in library
+    extern __m128  V_VECTORCALL __svml_tanpif4     (__m128);
+    extern __m128d V_VECTORCALL __svml_tanpi2      (__m128d);
+    extern __m128  V_VECTORCALL __svml_asinf4      (__m128);
+    extern __m128d V_VECTORCALL __svml_asin2       (__m128d);
+    extern __m128  V_VECTORCALL __svml_acosf4      (__m128);
+    extern __m128d V_VECTORCALL __svml_acos2       (__m128d);
+    extern __m128  V_VECTORCALL __svml_atanf4      (__m128);
+    extern __m128d V_VECTORCALL __svml_atan2       (__m128d);
+    extern __m128  V_VECTORCALL2 __svml_atan2f4    (__m128,  __m128);
+    extern __m128d V_VECTORCALL2 __svml_atan22     (__m128d, __m128d);
+    extern __m128  V_VECTORCALL __svml_sinhf4      (__m128);
+    extern __m128d V_VECTORCALL __svml_sinh2       (__m128d);
+    extern __m128  V_VECTORCALL __svml_coshf4      (__m128);
+    extern __m128d V_VECTORCALL __svml_cosh2       (__m128d);
+    extern __m128  V_VECTORCALL __svml_tanhf4      (__m128);
+    extern __m128d V_VECTORCALL __svml_tanh2       (__m128d);
+    extern __m128  V_VECTORCALL __svml_asinhf4     (__m128);
+    extern __m128d V_VECTORCALL __svml_asinh2      (__m128d);
+    extern __m128  V_VECTORCALL __svml_acoshf4     (__m128);
+    extern __m128d V_VECTORCALL __svml_acosh2      (__m128d);
+    extern __m128  V_VECTORCALL __svml_atanhf4     (__m128);
+    extern __m128d V_VECTORCALL __svml_atanh2      (__m128d);
+    extern __m128  V_VECTORCALL __svml_erff4       (__m128);
+    extern __m128d V_VECTORCALL __svml_erf2        (__m128d);
+    extern __m128  V_VECTORCALL __svml_erfcf4      (__m128);
+    extern __m128d V_VECTORCALL __svml_erfc2       (__m128d);
+    extern __m128  V_VECTORCALL __svml_erfinvf4    (__m128);
+    extern __m128d V_VECTORCALL __svml_erfinv2     (__m128d);
+    extern __m128  V_VECTORCALL __svml_cdfnormf4   (__m128);
+    extern __m128d V_VECTORCALL __svml_cdfnorm2    (__m128d);
+    extern __m128  V_VECTORCALL __svml_cdfnorminvf4(__m128);
+    extern __m128d V_VECTORCALL __svml_cdfnorminv2 (__m128d);
+    extern __m128  V_VECTORCALL __svml_cexpf4      (__m128);
+    extern __m128d V_VECTORCALL __svml_cexp2       (__m128d);
+}
+
+
+/*****************************************************************************
+*
+*      Function definitions
+*
+*****************************************************************************/
+
+// exponential and power functions
+static inline Vec4f exp (Vec4f const x) {   // exponential function
+    return  __svml_expf4(x);
+}
+static inline Vec2d exp (Vec2d const x) {   // exponential function
+    return  __svml_exp2(x);
+}
+
+static inline Vec4f expm1 (Vec4f const x) { // exp(x)-1
+    return  __svml_expm1f4(x);
+}
+static inline Vec2d expm1 (Vec2d const x) { // exp(x)-1
+    return  __svml_expm12(x);
+}
+
+static inline Vec4f exp2 (Vec4f const x) {  // pow(2,x)
+    return  __svml_exp2f4(x);
+}
+static inline Vec2d exp2 (Vec2d const x) {  // pow(2,x)
+    return  __svml_exp22(x);
+}
+
+static inline Vec4f exp10 (Vec4f const x) { // pow(10,x)
+    return  __svml_exp10f4(x);
+}
+static inline Vec2d exp10 (Vec2d const x) { // pow(10,x)
+    return  __svml_exp102(x);
+}
+
+static inline Vec4f pow (Vec4f const a, Vec4f const b) {   // pow(a,b) = a to the power of b
+    return  __svml_powf4(a,b);
+}
+
+static inline Vec4f pow (Vec4f const a, float const b) {   // pow(a,b)
+    return  __svml_powf4(a,Vec4f(b));
+}
+static inline Vec2d pow (Vec2d const a, Vec2d const b) {   // pow(a,b)
+    return  __svml_pow2(a,b);
+}
+static inline Vec2d pow (Vec2d const a, double const b) {  // pow(a,b)
+    return  __svml_pow2(a,Vec2d(b));
+}
+
+static inline Vec4f cbrt (Vec4f const x) {  // pow(x,1/3)
+    return  __svml_cbrtf4(x);
+}
+static inline Vec2d cbrt (Vec2d const x) {  // pow(x,1/3)
+    return  __svml_cbrt2(x);
+}
+
+// logarithms
+static inline Vec4f log (Vec4f const x) {   // natural logarithm
+    return  __svml_logf4(x);
+}
+static inline Vec2d log (Vec2d const x) {   // natural logarithm
+    return  __svml_log2(x);
+}
+
+static inline Vec4f log1p (Vec4f const x) { // log(1+x)
+    return  __svml_log1pf4(x);
+}
+static inline Vec2d log1p (Vec2d const x) { // log(1+x)
+    return  __svml_log1p2(x);
+}
+
+static inline Vec4f log2 (Vec4f const x) {  // logarithm base 2
+    return  __svml_log2f4(x);
+}
+static inline Vec2d log2 (Vec2d const x) {  // logarithm base 2
+    return  __svml_log22(x);
+}
+
+static inline Vec4f log10 (Vec4f const x) { // logarithm base 10
+    return  __svml_log10f4(x);
+}
+static inline Vec2d log10 (Vec2d const x) { // logarithm base 10
+    return  __svml_log102(x);
+}
+
+// trigonometric functions (angles in radians)
+static inline Vec4f sin (Vec4f const x) {   // sine
+    return  __svml_sinf4(x);
+}
+static inline Vec2d sin (Vec2d const x) {   // sine
+    return  __svml_sin2(x);
+}
+
+static inline Vec4f cos (Vec4f const x) {   // cosine
+    return  __svml_cosf4(x);
+}
+static inline Vec2d cos (Vec2d const x) {   // cosine
+    return  __svml_cos2(x);
+}
+
+// sincos function. sin(x) returned, cos(x) in pcos
+
+#ifdef SINCOS_ASM  // sincos can be fixed with inline assembly
+
+static inline Vec4f sincos (Vec4f * pcos, Vec4f const x) {
+    __m128 r_sin, r_cos;
+    //   __asm__ ( "call __svml_sincosf4 \n movaps %%xmm0, %0 \n movaps %%xmm1, %1" : "=m"(r_sin), "=m"(r_cos) : "xmm0"(x) );
+    r_sin = __svml_sincosf4(x); // fix calling convention in windows and linux using assembly
+    __asm__ __volatile__ ( "movaps %%xmm1, %0":"=m"(r_cos));
+    *pcos = r_cos;
+    return r_sin;
+}
+
+static inline Vec2d sincos (Vec2d * pcos, Vec2d const x) {   // sine and cosine. sin(x) returned, cos(x) in pcos
+    __m128d r_sin, r_cos;
+    r_sin = __svml_sincos2(x);
+    __asm__ __volatile__ ( "movaps %%xmm1, %0":"=m"(r_cos));
+    *pcos = r_cos;
+    return r_sin;
+}
+#endif // inline assembly available
+
+static inline Vec4f tan (Vec4f const x) {   // tangent
+    return  __svml_tanf4(x);
+}
+static inline Vec2d tan (Vec2d const x) {   // tangent
+    return  __svml_tan2(x);
+}
+
+static inline Vec4f sinpi (Vec4f const x) {   // sine
+    return  __svml_sinpif4(x);
+}
+static inline Vec2d sinpi (Vec2d const x) {   // sine
+    return  __svml_sinpi2(x);
+}
+
+static inline Vec4f cospi (Vec4f const x) {   // cosine
+    return  __svml_cospif4(x);
+}
+static inline Vec2d cospi (Vec2d const x) {   // cosine
+    return  __svml_cospi2(x);
+}
+
+static inline Vec4f tanpi (Vec4f const x) {   // tangent
+    return  __svml_tanpif4(x);
+}
+static inline Vec2d tanpi (Vec2d const x) {   // tangent
+    return  __svml_tanpi2(x);
+}
+
+// inverse trigonometric functions
+static inline Vec4f asin (Vec4f const x) {  // inverse sine
+    return  __svml_asinf4(x);
+}
+static inline Vec2d asin (Vec2d const x) {  // inverse sine
+    return  __svml_asin2(x);
+}
+
+static inline Vec4f acos (Vec4f const x) {  // inverse cosine
+    return  __svml_acosf4(x);
+}
+static inline Vec2d acos (Vec2d const x) {  // inverse cosine
+    return  __svml_acos2(x);
+}
+
+static inline Vec4f atan (Vec4f const x) {  // inverse tangent
+    return  __svml_atanf4(x);
+}
+static inline Vec2d atan (Vec2d const x) {  // inverse tangent
+    return  __svml_atan2(x);
+}
+
+static inline Vec4f atan2 (Vec4f const a, Vec4f const b) { // inverse tangent of a/b
+    return  __svml_atan2f4(a,b);
+}
+static inline Vec2d atan2 (Vec2d const a, Vec2d const b) { // inverse tangent of a/b
+    return  __svml_atan22(a,b);
+}
+
+// hyperbolic functions and inverse hyperbolic functions
+static inline Vec4f sinh (Vec4f const x) {  // hyperbolic sine
+    return  __svml_sinhf4(x);
+}
+static inline Vec2d sinh (Vec2d const x) {  // hyperbolic sine
+    return  __svml_sinh2(x);
+}
+
+static inline Vec4f cosh (Vec4f const x) {  // hyperbolic cosine
+    return  __svml_coshf4(x);
+}
+static inline Vec2d cosh (Vec2d const x) {  // hyperbolic cosine
+    return  __svml_cosh2(x);
+}
+
+static inline Vec4f tanh (Vec4f const x) {  // hyperbolic tangent
+    return  __svml_tanhf4(x);
+}
+static inline Vec2d tanh (Vec2d const x) {  // hyperbolic tangent
+    return  __svml_tanh2(x);
+}
+
+static inline Vec4f asinh (Vec4f const x) { // inverse hyperbolic sine
+    return  __svml_asinhf4(x);
+}
+static inline Vec2d asinh (Vec2d const x) { // inverse hyperbolic sine
+    return  __svml_asinh2(x);
+}
+
+static inline Vec4f acosh (Vec4f const x) { // inverse hyperbolic cosine
+    return  __svml_acoshf4(x);
+}
+static inline Vec2d acosh (Vec2d const x) { // inverse hyperbolic cosine
+    return  __svml_acosh2(x);
+}
+
+static inline Vec4f atanh (Vec4f const x) { // inverse hyperbolic tangent
+    return  __svml_atanhf4(x);
+}
+static inline Vec2d atanh (Vec2d const x) { // inverse hyperbolic tangent
+    return  __svml_atanh2(x);
+}
+
+// error function
+static inline Vec4f erf (Vec4f const x) {   // error function
+    return  __svml_erff4(x);
+}
+static inline Vec2d erf (Vec2d const x) {   // error function
+    return  __svml_erf2(x);
+}
+
+static inline Vec4f erfc (Vec4f const x) {  // error function complement
+    return  __svml_erfcf4(x);
+}
+static inline Vec2d erfc (Vec2d const x) {  // error function complement
+    return  __svml_erfc2(x);
+}
+
+static inline Vec4f erfinv (Vec4f const x) {     // inverse error function
+    return  __svml_erfinvf4(x);
+}
+static inline Vec2d erfinv (Vec2d const x) {     // inverse error function
+    return  __svml_erfinv2(x);
+}
+
+static inline Vec4f cdfnorm (Vec4f const x) {    // cumulative normal distribution function
+    return  __svml_cdfnormf4(x);
+}
+static inline Vec2d cdfnorm (Vec2d const x) {    // cumulative normal distribution function
+    return  __svml_cdfnorm2(x);
+}
+
+static inline Vec4f cdfnorminv (Vec4f const x) { // inverse cumulative normal distribution function
+    return  __svml_cdfnorminvf4(x);
+}
+static inline Vec2d cdfnorminv (Vec2d const x) { // inverse cumulative normal distribution function
+    return  __svml_cdfnorminv2(x);
+}
+
+#endif   // USE_SVML_INTRINSICS
+
+
+
+#if defined (MAX_VECTOR_SIZE) && MAX_VECTOR_SIZE >= 256  // 256 bit vectors
+
+#if defined (VECTORF256_H)  // 256-bit vector registers supported
+
+#ifdef USE_SVML_INTRINSICS
+/*****************************************************************************
+*
+*      256-bit vector functions using Intel compiler intrinsic functions
+*
+*****************************************************************************/
+
+// exponential and power functions
+static inline Vec8f exp(Vec8f const x) {       // exponential function
+    return _mm256_exp_ps(x);
+}
+static inline Vec4d exp(Vec4d const x) {       // exponential function
+    return _mm256_exp_pd(x);
+}
+static inline Vec8f expm1(Vec8f const x) {     // exp(x)-1. Avoids loss of precision if x is close to 1
+    return _mm256_expm1_ps(x);
+}
+static inline Vec4d expm1(Vec4d const x) {     // exp(x)-1. Avoids loss of precision if x is close to 1
+    return _mm256_expm1_pd(x);
+}
+static inline Vec8f exp2(Vec8f const x) {      // pow(2,x)
+    return _mm256_exp2_ps(x);
+}
+static inline Vec4d exp2(Vec4d const x) {      // pow(2,x)
+    return _mm256_exp2_pd(x);
+}
+static inline Vec8f exp10(Vec8f const x) {     // pow(10,x)
+    return _mm256_exp10_ps(x);
+}
+static inline Vec4d exp10(Vec4d const x) {     // pow(10,x)
+    return _mm256_exp10_pd(x);
+}
+static inline Vec8f pow(Vec8f const a, Vec8f const b) { // pow(a,b) = a to the power of b
+    return _mm256_pow_ps(a, b);
+}
+static inline Vec8f pow(Vec8f const a, float const b) {  // pow(a,b) = a to the power of b
+    return _mm256_pow_ps(a, Vec8f(b));
+}
+static inline Vec4d pow(Vec4d const a, Vec4d const b) {  // pow(a,b) = a to the power of b
+    return _mm256_pow_pd(a, b);
+}
+static inline Vec4d pow(Vec4d const a, double const b) { // pow(a,b) = a to the power of b
+    return _mm256_pow_pd(a, Vec4d(b));
+}
+static inline Vec8f cbrt(Vec8f const x) {      // pow(x,1/3)
+    return _mm256_cbrt_ps(x);
+}
+static inline Vec4d cbrt(Vec4d const x) {      // pow(x,1/3)
+    return _mm256_cbrt_pd(x);
+}
+// logarithms
+static inline Vec8f log(Vec8f const x) {       // natural logarithm
+    return _mm256_log_ps(x);
+}
+static inline Vec4d log(Vec4d const x) {       // natural logarithm
+    return _mm256_log_pd(x);
+}
+static inline Vec8f log1p(Vec8f const x) {     // log(1+x). Avoids loss of precision if 1+x is close to 1
+    return _mm256_log1p_ps(x);
+}
+static inline Vec4d log1p(Vec4d const x) {     // log(1+x). Avoids loss of precision if 1+x is close to 1
+    return _mm256_log1p_pd(x);
+}
+static inline Vec8f log2(Vec8f const x) {      // logarithm base 2
+    return _mm256_log2_ps(x);
+}
+static inline Vec4d log2(Vec4d const x) {      // logarithm base 2
+    return _mm256_log2_pd(x);
+}
+static inline Vec8f log10(Vec8f const x) {     // logarithm base 10
+    return _mm256_log10_ps(x);
+}
+static inline Vec4d log10(Vec4d const x) {     // logarithm base 10
+    return _mm256_log10_pd(x);
+}
+
+// trigonometric functions
+static inline Vec8f sin(Vec8f const x) {       // sine
+    return _mm256_sin_ps(x);
+}
+static inline Vec4d sin(Vec4d const x) {       // sine
+    return _mm256_sin_pd(x);
+}
+static inline Vec8f cos(Vec8f const x) {       // cosine
+    return _mm256_cos_ps(x);
+}
+static inline Vec4d cos(Vec4d const x) {       // cosine
+    return _mm256_cos_pd(x);
+}
+static inline Vec8f sincos(Vec8f * pcos, Vec8f const x) { // sine and cosine. sin(x) returned, cos(x) in pcos
+    __m256 r_sin, r_cos;
+    r_sin = _mm256_sincos_ps(&r_cos, x);
+    *pcos = r_cos;
+    return r_sin;
+}
+static inline Vec4d sincos(Vec4d * pcos, Vec4d const x) {  // sine and cosine. sin(x) returned, cos(x) in pcos
+    __m256d r_sin, r_cos;
+    r_sin = _mm256_sincos_pd(&r_cos, x);
+    *pcos = r_cos;
+    return r_sin;
+}
+static inline Vec8f tan(Vec8f const x) {       // tangent
+    return _mm256_tan_ps(x);
+}
+static inline Vec4d tan(Vec4d const x) {       // tangent
+    return _mm256_tan_pd(x);
+}
+
+#ifdef TRIGPI_FUNCTIONS
+static inline Vec8f sinpi(Vec8f const x) {     // sine
+    return _mm256_sinpi_ps(x);
+}
+static inline Vec4d sinpi(Vec4d const x) {     // sine
+    return _mm256_sinpi_pd(x);
+}
+static inline Vec8f cospi(Vec8f const x) {     // cosine
+    return _mm256_cospi_ps(x);
+}
+static inline Vec4d cospi(Vec4d const x) {     // cosine
+    return _mm256_cospi_pd(x);
+}
+static inline Vec8f tanpi(Vec8f const x) {     // tangent
+    return _mm256_tanpi_ps(x);
+}
+static inline Vec4d tanpi(Vec4d const x) {     // tangent
+    return _mm256_tanpi_pd(x);
+}
+#endif // TRIGPI_FUNCTIONS
+
+// inverse trigonometric functions
+static inline Vec8f asin(Vec8f const x) {      // inverse sine
+    return _mm256_asin_ps(x);
+}
+static inline Vec4d asin(Vec4d const x) {      // inverse sine
+    return _mm256_asin_pd(x);
+}
+
+static inline Vec8f acos(Vec8f const x) {      // inverse cosine
+    return _mm256_acos_ps(x);
+}
+static inline Vec4d acos(Vec4d const x) {      // inverse cosine
+    return _mm256_acos_pd(x);
+}
+
+static inline Vec8f atan(Vec8f const x) {      // inverse tangent
+    return _mm256_atan_ps(x);
+}
+static inline Vec4d atan(Vec4d const x) {      // inverse tangent
+    return _mm256_atan_pd(x);
+}
+static inline Vec8f atan2(Vec8f const a, Vec8f const b) { // inverse tangent of a/b
+    return _mm256_atan2_ps(a, b);
+}
+static inline Vec4d atan2(Vec4d const a, Vec4d const b) { // inverse tangent of a/b
+    return _mm256_atan2_pd(a, b);
+}
+
+// hyperbolic functions and inverse hyperbolic functions
+static inline Vec8f sinh(Vec8f const x) {      // hyperbolic sine
+    return _mm256_sinh_ps(x);
+}
+static inline Vec4d sinh(Vec4d const x) {      // hyperbolic sine
+    return _mm256_sinh_pd(x);
+}
+static inline Vec8f cosh(Vec8f const x) {      // hyperbolic cosine
+    return _mm256_cosh_ps(x);
+}
+static inline Vec4d cosh(Vec4d const x) {      // hyperbolic cosine
+    return _mm256_cosh_pd(x);
+}
+static inline Vec8f tanh(Vec8f const x) {      // hyperbolic tangent
+    return _mm256_tanh_ps(x);
+}
+static inline Vec4d tanh(Vec4d const x) {      // hyperbolic tangent
+    return _mm256_tanh_pd(x);
+}
+static inline Vec8f asinh(Vec8f const x) {     // inverse hyperbolic sine
+    return _mm256_asinh_ps(x);
+}
+static inline Vec4d asinh(Vec4d const x) {     // inverse hyperbolic sine
+    return _mm256_asinh_pd(x);
+}
+static inline Vec8f acosh(Vec8f const x) {     // inverse hyperbolic cosine
+    return _mm256_acosh_ps(x);
+}
+static inline Vec4d acosh(Vec4d const x) {     // inverse hyperbolic cosine
+    return _mm256_acosh_pd(x);
+}
+static inline Vec8f atanh(Vec8f const x) {     // inverse hyperbolic tangent
+    return _mm256_atanh_ps(x);
+}
+static inline Vec4d atanh(Vec4d const x) {     // inverse hyperbolic tangent
+    return _mm256_atanh_pd(x);
+}
+
+// error function
+static inline Vec8f erf(Vec8f const x) {       // error function
+    return _mm256_erf_ps(x);
+}
+static inline Vec4d erf(Vec4d const x) {       // error function
+    return _mm256_erf_pd(x);
+}
+static inline Vec8f erfc(Vec8f const x) {      // error function complement
+    return _mm256_erfc_ps(x);
+}
+static inline Vec4d erfc(Vec4d const x) {      // error function complement
+    return _mm256_erfc_pd(x);
+}
+static inline Vec8f erfinv(Vec8f const x) {    // inverse error function
+    return _mm256_erfinv_ps(x);
+}
+static inline Vec4d erfinv(Vec4d const x) {    // inverse error function
+    return _mm256_erfinv_pd(x);
+}
+
+static inline Vec8f cdfnorm(Vec8f const x) {   // cumulative normal distribution function
+    return _mm256_cdfnorm_ps(x);
+}
+static inline Vec4d cdfnorm(Vec4d const x) {   // cumulative normal distribution function
+    return _mm256_cdfnorm_pd(x);
+}
+static inline Vec8f cdfnorminv(Vec8f const x) {// inverse cumulative normal distribution function
+    return _mm256_cdfnorminv_ps(x);
+}
+static inline Vec4d cdfnorminv(Vec4d const x) {// inverse cumulative normal distribution function
+    return _mm256_cdfnorminv_pd(x);
+}
+
+#else    // not USE_SVML_INTRINSICS
+/*****************************************************************************
+*
+*      256-bit vector functions using other compiler than Intel
+*
+*****************************************************************************/
+
+// External function prototypes for SVML library, 256-bit vectors
+extern "C" {
+    extern __m256  V_VECTORCALL __svml_expf8        (__m256);
+    extern __m256d V_VECTORCALL __svml_exp4         (__m256d);
+    extern __m256  V_VECTORCALL __svml_expm1f8      (__m256);
+    extern __m256d V_VECTORCALL __svml_expm14       (__m256d);
+    extern __m256  V_VECTORCALL __svml_exp2f8       (__m256);
+    extern __m256d V_VECTORCALL __svml_exp24        (__m256d);
+    extern __m256  V_VECTORCALL __svml_exp10f8      (__m256);
+    extern __m256d V_VECTORCALL __svml_exp104       (__m256d);
+    extern __m256  V_VECTORCALL2 __svml_powf8       (__m256,  __m256);
+    extern __m256d V_VECTORCALL2 __svml_pow4        (__m256d, __m256d);
+    extern __m256  V_VECTORCALL __svml_cbrtf8       (__m256);
+    extern __m256d V_VECTORCALL __svml_cbrt4        (__m256d);
+    extern __m256  V_VECTORCALL __svml_invsqrtf8    (__m256);
+    extern __m256d V_VECTORCALL __svml_invsqrt4     (__m256d);
+    extern __m256  V_VECTORCALL __svml_logf8        (__m256);
+    extern __m256d V_VECTORCALL __svml_log4         (__m256d);
+    extern __m256  V_VECTORCALL __svml_log1pf8      (__m256);
+    extern __m256d V_VECTORCALL __svml_log1p4       (__m256d);
+    extern __m256  V_VECTORCALL __svml_log2f8       (__m256);
+    extern __m256d V_VECTORCALL __svml_log24        (__m256d);
+    extern __m256  V_VECTORCALL __svml_log10f8      (__m256);
+    extern __m256d V_VECTORCALL __svml_log104       (__m256d);
+    extern __m256  V_VECTORCALL __svml_sinf8        (__m256);
+    extern __m256d V_VECTORCALL __svml_sin4         (__m256d);
+    extern __m256  V_VECTORCALL __svml_cosf8        (__m256);
+    extern __m256d V_VECTORCALL __svml_cos4         (__m256d);
+    extern __m256  V_VECTORCALL2 __svml_sincosf8    (__m256);  // cos returned in ymm1
+    extern __m256d V_VECTORCALL2 __svml_sincos4     (__m256d); // cos returned in ymm1
+    extern __m256  V_VECTORCALL __svml_tanf8        (__m256);
+    extern __m256d V_VECTORCALL __svml_tan4         (__m256d);
+    extern __m256  V_VECTORCALL __svml_sinpif8      (__m256);
+    extern __m256d V_VECTORCALL __svml_sinpi4       (__m256d);
+    extern __m256  V_VECTORCALL __svml_cospif8      (__m256);
+    extern __m256d V_VECTORCALL __svml_cospi4       (__m256d);
+    extern __m256  V_VECTORCALL __svml_tanpif8      (__m256);
+    extern __m256d V_VECTORCALL __svml_tanpi4       (__m256d);
+    extern __m256  V_VECTORCALL __svml_asinf8       (__m256);
+    extern __m256d V_VECTORCALL __svml_asin4        (__m256d);
+    extern __m256  V_VECTORCALL __svml_acosf8       (__m256);
+    extern __m256d V_VECTORCALL __svml_acos4        (__m256d);
+    extern __m256  V_VECTORCALL __svml_atanf8       (__m256);
+    extern __m256d V_VECTORCALL __svml_atan4        (__m256d);
+    extern __m256  V_VECTORCALL2 __svml_atan2f8     (__m256, __m256);
+    extern __m256d V_VECTORCALL2 __svml_atan24      (__m256d, __m256d);
+    extern __m256  V_VECTORCALL __svml_sinhf8       (__m256);
+    extern __m256d V_VECTORCALL __svml_sinh4        (__m256d);
+    extern __m256  V_VECTORCALL __svml_coshf8       (__m256);
+    extern __m256d V_VECTORCALL __svml_cosh4        (__m256d);
+    extern __m256  V_VECTORCALL __svml_tanhf8       (__m256);
+    extern __m256d V_VECTORCALL __svml_tanh4        (__m256d);
+    extern __m256  V_VECTORCALL __svml_asinhf8      (__m256);
+    extern __m256d V_VECTORCALL __svml_asinh4       (__m256d);
+    extern __m256  V_VECTORCALL __svml_acoshf8      (__m256);
+    extern __m256d V_VECTORCALL __svml_acosh4       (__m256d);
+    extern __m256  V_VECTORCALL __svml_atanhf8      (__m256);
+    extern __m256d V_VECTORCALL __svml_atanh4       (__m256d);
+    extern __m256  V_VECTORCALL __svml_erff8        (__m256);
+    extern __m256d V_VECTORCALL __svml_erf4         (__m256d);
+    extern __m256  V_VECTORCALL __svml_erfcf8       (__m256);
+    extern __m256d V_VECTORCALL __svml_erfc4        (__m256d);
+    extern __m256  V_VECTORCALL __svml_erfinvf8     (__m256);
+    extern __m256d V_VECTORCALL __svml_erfinv4      (__m256d);
+    extern __m256  V_VECTORCALL __svml_cdfnorminvf8(__m256);
+    extern __m256d V_VECTORCALL __svml_cdfnorminv4  (__m256d);
+    extern __m256  V_VECTORCALL __svml_cdfnormf8    (__m256);
+    extern __m256d V_VECTORCALL __svml_cdfnorm4     (__m256d);
+    //extern __m256  V_VECTORCALL __svml_cexpf8     (__m256);
+    //extern __m256d V_VECTORCALL __svml_cexp4      (__m256d);
+}
+
+
+// exponential and power functions
+static inline Vec8f exp (Vec8f const x) {      // exponential function
+    return  __svml_expf8(x);
+}
+static inline Vec4d exp (Vec4d const x) {      // exponential function
+    return  __svml_exp4(x);
+}
+static inline Vec8f expm1 (Vec8f const x) {    // exp(x)-1
+    return  __svml_expm1f8(x);
+}
+static inline Vec4d expm1 (Vec4d const x) {    // exp(x)-1
+    return  __svml_expm14(x);
+}
+static inline Vec8f exp2 (Vec8f const x) {     // pow(2,x)
+    return  __svml_exp2f8(x);
+}
+static inline Vec4d exp2 (Vec4d const x) {     // pow(2,x)
+    return  __svml_exp24(x);
+}
+static inline Vec8f exp10 (Vec8f const x) {    // pow(10,x)
+    return  __svml_exp10f8(x);
+}
+static inline Vec4d exp10 (Vec4d const x) {    // pow(10,x)
+    return  __svml_exp104(x);
+}
+static inline Vec8f pow (Vec8f const a, Vec8f const b) {  // pow(a,b) = a to the power of b
+    return  __svml_powf8(a,b);
+}
+static inline Vec8f pow (Vec8f const a, float const b) {  // pow(a,b)
+    return  __svml_powf8(a,Vec8f(b));
+}
+static inline Vec4d pow (Vec4d const a, Vec4d const b) {  // pow(a,b)
+    return  __svml_pow4(a,b);
+}
+static inline Vec4d pow (Vec4d const a, double const b) { // pow(a,b)
+    return  __svml_pow4(a,Vec4d(b));
+}
+static inline Vec8f cbrt (Vec8f const x) {     // pow(x,1/3)
+    return  __svml_cbrtf8(x);
+}
+static inline Vec4d cbrt (Vec4d const x) {     // pow(x,1/3)
+    return  __svml_cbrt4(x);
+}
+
+// logarithms
+static inline Vec8f log (Vec8f const x) {      // natural logarithm
+    return  __svml_logf8(x);
+}
+static inline Vec4d log (Vec4d const x) {      // natural logarithm
+    return  __svml_log4(x);
+}
+static inline Vec8f log1p (Vec8f const x) {    // log(1+x)
+    return  __svml_log1pf8(x);
+}
+static inline Vec4d log1p (Vec4d const x) {    // log(1+x)
+    return  __svml_log1p4(x);
+}
+static inline Vec8f log2 (Vec8f const x) {     // logarithm base 2
+    return  __svml_log2f8(x);
+}
+static inline Vec4d log2 (Vec4d const x) {     // logarithm base 2
+    return  __svml_log24(x);
+}
+static inline Vec8f log10 (Vec8f const x) {    // logarithm base 10
+    return  __svml_log10f8(x);
+}
+static inline Vec4d log10 (Vec4d const x) {    // logarithm base 10
+    return  __svml_log104(x);
+}
+
+// trigonometric functions (angles in radians)
+static inline Vec8f sin (Vec8f const x) {      // sine
+    return  __svml_sinf8(x);
+}
+static inline Vec4d sin (Vec4d const x) {      // sine
+    return  __svml_sin4(x);
+}
+static inline Vec8f cos (Vec8f const x) {      // cosine
+    return  __svml_cosf8(x);
+}
+static inline Vec4d cos (Vec4d const x) {      // cosine
+    return  __svml_cos4(x);
+}
+
+#ifdef SINCOS_ASM  // sincos can be fixed with inline assembly
+// no inline assembly in 64 bit MS compiler
+// sine and cosine. sin(x) returned, cos(x) in pcos
+static inline Vec8f sincos (Vec8f * pcos, Vec8f const x) {
+    __m256 r_sin, r_cos;
+    r_sin = __svml_sincosf8(x);
+    __asm__ __volatile__ ( "vmovaps %%ymm1, %0":"=m"(r_cos));
+    *pcos = r_cos;
+    return r_sin;
+}
+// sine and cosine. sin(x) returned, cos(x) in pcos
+static inline Vec4d sincos (Vec4d * pcos, Vec4d const x) {
+    __m256d r_sin, r_cos;
+    r_sin = __svml_sincos4(x);
+    __asm__ __volatile__ ( "vmovaps %%ymm1, %0":"=m"(r_cos));
+    *pcos = r_cos;
+    return r_sin;
+}
+#endif // sincos
+
+static inline Vec8f tan (Vec8f const x) {      // tangent
+    return  __svml_tanf8(x);
+}
+static inline Vec4d tan (Vec4d const x) {      // tangent
+    return  __svml_tan4(x);
+}
+
+static inline Vec8f sinpi (Vec8f const x) {    // sine
+    return  __svml_sinpif8(x);
+}
+static inline Vec4d sinpi (Vec4d const x) {    // sine
+    return  __svml_sinpi4(x);
+}
+static inline Vec8f cospi (Vec8f const x) {    // cosine
+    return  __svml_cospif8(x);
+}
+static inline Vec4d cospi (Vec4d const x) {    // cosine
+    return  __svml_cospi4(x);
+}
+static inline Vec8f tanpi (Vec8f const x) {    // tangent
+    return  __svml_tanpif8(x);
+}
+static inline Vec4d tanpi (Vec4d const x) {    // tangent
+    return  __svml_tanpi4(x);
+}
+
+// inverse trigonometric functions
+static inline Vec8f asin (Vec8f const x) {     // inverse sine
+    return  __svml_asinf8(x);
+}
+static inline Vec4d asin (Vec4d const x) {     // inverse sine
+    return  __svml_asin4(x);
+}
+static inline Vec8f acos (Vec8f const x) {     // inverse cosine
+    return  __svml_acosf8(x);
+}
+static inline Vec4d acos (Vec4d const x) {     // inverse cosine
+    return  __svml_acos4(x);
+}
+static inline Vec8f atan (Vec8f const x) {     // inverse tangent
+    return  __svml_atanf8(x);
+}
+static inline Vec4d atan (Vec4d const x) {     // inverse tangent
+    return  __svml_atan4(x);
+}
+static inline Vec8f atan2 (Vec8f const a, Vec8f const b) { // inverse tangent of a/b
+    return  __svml_atan2f8(a,b);
+}
+static inline Vec4d atan2 (Vec4d const a, Vec4d const b) { // inverse tangent of a/b
+    return  __svml_atan24(a,b);
+}
+
+// hyperbolic functions and inverse hyperbolic functions
+static inline Vec8f sinh (Vec8f const x) {     // hyperbolic sine
+    return  __svml_sinhf8(x);
+}
+static inline Vec4d sinh (Vec4d const x) {     // hyperbolic sine
+    return  __svml_sinh4(x);
+}
+static inline Vec8f cosh (Vec8f const x) {     // hyperbolic cosine
+    return  __svml_coshf8(x);
+}
+static inline Vec4d cosh (Vec4d const x) {     // hyperbolic cosine
+    return  __svml_cosh4(x);
+}
+static inline Vec8f tanh (Vec8f const x) {     // hyperbolic tangent
+    return  __svml_tanhf8(x);
+}
+static inline Vec4d tanh (Vec4d const x) {     // hyperbolic tangent
+    return  __svml_tanh4(x);
+}
+static inline Vec8f asinh (Vec8f const x) {    // inverse hyperbolic sine
+    return  __svml_asinhf8(x);
+}
+static inline Vec4d asinh (Vec4d const x) {    // inverse hyperbolic sine
+    return  __svml_asinh4(x);
+}
+static inline Vec8f acosh (Vec8f const x) {    // inverse hyperbolic cosine
+    return  __svml_acoshf8(x);
+}
+static inline Vec4d acosh (Vec4d const x) {    // inverse hyperbolic cosine
+    return  __svml_acosh4(x);
+}
+static inline Vec8f atanh (Vec8f const x) {    // inverse hyperbolic tangent
+    return  __svml_atanhf8(x);
+}
+static inline Vec4d atanh (Vec4d const x) {    // inverse hyperbolic tangent
+    return  __svml_atanh4(x);
+}
+
+// error function
+static inline Vec8f erf (Vec8f const x) {      // error function
+    return  __svml_erff8(x);
+}
+static inline Vec4d erf (Vec4d const x) {      // error function
+    return  __svml_erf4(x);
+}
+static inline Vec8f erfc (Vec8f const x) {     // error function complement
+    return  __svml_erfcf8(x);
+}
+static inline Vec4d erfc (Vec4d const x) {     // error function complement
+    return  __svml_erfc4(x);
+}
+static inline Vec8f erfinv (Vec8f const x) {   // inverse error function
+    return  __svml_erfinvf8(x);
+}
+static inline Vec4d erfinv (Vec4d const x) {   // inverse error function
+    return  __svml_erfinv4(x);
+}
+
+static inline Vec8f cdfnorm (Vec8f const x) {  // cumulative normal distribution function
+    return  __svml_cdfnormf8(x);
+}
+static inline Vec4d cdfnorm (Vec4d const x) {  // cumulative normal distribution function
+    return  __svml_cdfnorm4(x);
+}
+static inline Vec8f cdfnorminv (Vec8f const x) {  // inverse cumulative normal distribution function
+    return  __svml_cdfnorminvf8(x);
+}
+static inline Vec4d cdfnorminv (Vec4d const x) {  // inverse cumulative normal distribution function
+    return  __svml_cdfnorminv4(x);
+}
+
+#endif   // USE_SVML_INTRINSICS
+
+#else    // not VECTORF256_H
+
+/*****************************************************************************
+*
+*      256-bit vector functions emulated with 128-bit vectors
+*
+*****************************************************************************/
+
+// exponential and power functions
+static inline Vec8f exp (Vec8f const x) {      // exponential function
+    return Vec8f(exp(x.get_low()), exp(x.get_high()));
+}
+static inline Vec4d exp (Vec4d const x) {      // exponential function
+    return Vec4d(exp(x.get_low()), exp(x.get_high()));
+}
+static inline Vec8f expm1 (Vec8f const x) {    // exp(x)-1
+    return Vec8f(expm1(x.get_low()), expm1(x.get_high()));
+}
+static inline Vec4d expm1 (Vec4d const x) {    // exp(x)-1
+    return Vec4d(expm1(x.get_low()), expm1(x.get_high()));
+}
+static inline Vec8f exp2 (Vec8f const x) {     // pow(2,x)
+    return Vec8f(exp2(x.get_low()), exp2(x.get_high()));
+}
+static inline Vec4d exp2 (Vec4d const x) {     // pow(2,x)
+    return Vec4d(exp2(x.get_low()), exp2(x.get_high()));
+}
+static inline Vec8f exp10 (Vec8f const x) {    // pow(10,x)
+    return Vec8f(exp10(x.get_low()), exp10(x.get_high()));
+}
+static inline Vec4d exp10 (Vec4d const x) {    // pow(10,x)
+    return Vec4d(exp10(x.get_low()), exp10(x.get_high()));
+}
+static inline Vec8f pow (Vec8f const a, Vec8f const b) {  // pow(a,b) = a to the power of b
+    return Vec8f(pow(a.get_low(),b.get_low()), pow(a.get_high(),b.get_high()));
+}
+static inline Vec8f pow (Vec8f const a, float const b) {  // pow(a,b)
+    return Vec8f(pow(a.get_low(),b), pow(a.get_high(),b));
+}
+static inline Vec4d pow (Vec4d const a, Vec4d const b) {  // pow(a,b)
+    return Vec4d(pow(a.get_low(),b.get_low()), pow(a.get_high(),b.get_high()));
+}
+static inline Vec4d pow (Vec4d const a, double const b) { // pow(a,b)
+    return Vec4d(pow(a.get_low(),b), pow(a.get_high(),b));
+}
+static inline Vec8f cbrt (Vec8f const x) {     // pow(x,1/3)
+    return Vec8f(cbrt(x.get_low()), cbrt(x.get_high()));
+}
+static inline Vec4d cbrt (Vec4d const x) {     // pow(x,1/3)
+    return Vec4d(cbrt(x.get_low()), cbrt(x.get_high()));
+}
+
+// logarithms
+static inline Vec8f log (Vec8f const x) {      // natural logarithm
+    return Vec8f(log(x.get_low()), log(x.get_high()));
+}
+static inline Vec4d log (Vec4d const x) {      // natural logarithm
+    return Vec4d(log(x.get_low()), log(x.get_high()));
+}
+static inline Vec8f log1p (Vec8f const x) {    // log(1+x). Avoids loss of precision if 1+x is close to 1
+    return Vec8f(log1p(x.get_low()), log1p(x.get_high()));
+}
+static inline Vec4d log1p (Vec4d const x) {    // log(1+x). Avoids loss of precision if 1+x is close to 1
+    return Vec4d(log1p(x.get_low()), log1p(x.get_high()));
+}
+static inline Vec8f log2 (Vec8f const x) {     // logarithm base 2
+    return Vec8f(log2(x.get_low()), log2(x.get_high()));
+}
+static inline Vec4d log2 (Vec4d const x) {     // logarithm base 2
+    return Vec4d(log2(x.get_low()), log2(x.get_high()));
+}
+static inline Vec8f log10 (Vec8f const x) {    // logarithm base 10
+    return Vec8f(log10(x.get_low()), log10(x.get_high()));
+}
+static inline Vec4d log10 (Vec4d const x) {    // logarithm base 10
+    return Vec4d(log10(x.get_low()), log10(x.get_high()));
+}
+
+// trigonometric functions (angles in radians)
+static inline Vec8f sin (Vec8f const x) {      // sine
+    return Vec8f(sin(x.get_low()), sin(x.get_high()));
+}
+static inline Vec4d sin (Vec4d const x) {      // sine
+    return Vec4d(sin(x.get_low()), sin(x.get_high()));
+}
+static inline Vec8f cos (Vec8f const x) {      // cosine
+    return Vec8f(cos(x.get_low()), cos(x.get_high()));
+}
+static inline Vec4d cos (Vec4d const x) {      // cosine
+    return Vec4d(cos(x.get_low()), cos(x.get_high()));
+}
+#ifdef SINCOS_ASM  // sincos can be fixed with inline assembly
+static inline Vec8f sincos (Vec8f * pcos, Vec8f const x) { // sine and cosine. sin(x) returned, cos(x) in pcos
+    Vec4f r_cos0, r_cos1;
+    Vec8f r_sin = Vec8f(sincos(&r_cos0, x.get_low()), sincos(&r_cos1, x.get_high()));
+    *pcos = Vec8f(r_cos0, r_cos1);
+    return r_sin;
+}
+
+static inline Vec4d sincos (Vec4d * pcos, Vec4d const x) { // sine and cosine. sin(x) returned, cos(x) in pcos
+    Vec2d r_cos0, r_cos1;
+    Vec4d r_sin = Vec4d(sincos(&r_cos0, x.get_low()), sincos(&r_cos1, x.get_high()));
+    *pcos = Vec4d(r_cos0, r_cos1);
+    return r_sin;
+}
+#endif  // sincos
+
+static inline Vec8f tan (Vec8f const x) {      // tangent
+    return Vec8f(tan(x.get_low()), tan(x.get_high()));
+}
+static inline Vec4d tan (Vec4d const x) {      // tangent
+    return Vec4d(tan(x.get_low()), tan(x.get_high()));
+}
+
+#ifdef TRIGPI_FUNCTIONS
+static inline Vec8f sinpi (Vec8f const x) {    // sine
+    return Vec8f(sinpi(x.get_low()), sinpi(x.get_high()));
+}
+static inline Vec4d sinpi (Vec4d const x) {    // sine
+    return Vec4d(sinpi(x.get_low()), sinpi(x.get_high()));
+}
+static inline Vec8f cospi (Vec8f const x) {    // cosine
+    return Vec8f(cospi(x.get_low()), cospi(x.get_high()));
+}
+static inline Vec4d cospi (Vec4d const x) {    // cosine
+    return Vec4d(cospi(x.get_low()), cospi(x.get_high()));
+}
+static inline Vec8f tanpi (Vec8f const x) {    // tangent
+    return Vec8f(tanpi(x.get_low()), tanpi(x.get_high()));
+}
+static inline Vec4d tanpi (Vec4d const x) {    // tangent
+    return Vec4d(tanpi(x.get_low()), tanpi(x.get_high()));
+}
+#endif
+
+// inverse trigonometric functions
+static inline Vec8f asin (Vec8f const x) {     // inverse sine
+    return Vec8f(asin(x.get_low()), asin(x.get_high()));
+}
+static inline Vec4d asin (Vec4d const x) {     // inverse sine
+    return Vec4d(asin(x.get_low()), asin(x.get_high()));
+}
+static inline Vec8f acos (Vec8f const x) {     // inverse cosine
+    return Vec8f(acos(x.get_low()), acos(x.get_high()));
+}
+static inline Vec4d acos (Vec4d const x) {     // inverse cosine
+    return Vec4d(acos(x.get_low()), acos(x.get_high()));
+}
+static inline Vec8f atan (Vec8f const x) {     // inverse tangent
+    return Vec8f(atan(x.get_low()), atan(x.get_high()));
+}
+static inline Vec4d atan (Vec4d const x) {     // inverse tangent
+    return Vec4d(atan(x.get_low()), atan(x.get_high()));
+}
+static inline Vec8f atan2 (Vec8f const a, Vec8f const b) {  // inverse tangent of a/b
+    return Vec8f(atan2(a.get_low(),b.get_low()), atan2(a.get_high(),b.get_high()));
+}
+static inline Vec4d atan2 (Vec4d const a, Vec4d const b) {  // inverse tangent of a/b
+    return Vec4d(atan2(a.get_low(),b.get_low()), atan2(a.get_high(),b.get_high()));
+}
+
+// hyperbolic functions
+static inline Vec8f sinh (Vec8f const x) {     // hyperbolic sine
+    return Vec8f(sinh(x.get_low()), sinh(x.get_high()));
+}
+static inline Vec4d sinh (Vec4d const x) {     // hyperbolic sine
+    return Vec4d(sinh(x.get_low()), sinh(x.get_high()));
+}
+static inline Vec8f cosh (Vec8f const x) {     // hyperbolic cosine
+    return Vec8f(cosh(x.get_low()), cosh(x.get_high()));
+}
+static inline Vec4d cosh (Vec4d const x) {     // hyperbolic cosine
+    return Vec4d(cosh(x.get_low()), cosh(x.get_high()));
+}
+static inline Vec8f tanh (Vec8f const x) {     // hyperbolic tangent
+    return Vec8f(tanh(x.get_low()), tanh(x.get_high()));
+}
+static inline Vec4d tanh (Vec4d const x) {     // hyperbolic tangent
+    return Vec4d(tanh(x.get_low()), tanh(x.get_high()));
+}
+
+// inverse hyperbolic functions
+static inline Vec8f asinh (Vec8f const x) {    // inverse hyperbolic sine
+    return Vec8f(asinh(x.get_low()), asinh(x.get_high()));
+}
+static inline Vec4d asinh (Vec4d const x) {    // inverse hyperbolic sine
+    return Vec4d(asinh(x.get_low()), asinh(x.get_high()));
+}
+static inline Vec8f acosh (Vec8f const x) {    // inverse hyperbolic cosine
+    return Vec8f(acosh(x.get_low()), acosh(x.get_high()));
+}
+static inline Vec4d acosh (Vec4d const x) {    // inverse hyperbolic cosine
+    return Vec4d(acosh(x.get_low()), acosh(x.get_high()));
+}
+static inline Vec8f atanh (Vec8f const x) {    // inverse hyperbolic tangent
+    return Vec8f(atanh(x.get_low()), atanh(x.get_high()));
+}
+static inline Vec4d atanh (Vec4d const x) {    // inverse hyperbolic tangent
+    return Vec4d(atanh(x.get_low()), atanh(x.get_high()));
+}
+
+// error function
+static inline Vec8f erf (Vec8f const x) {      // error function
+    return Vec8f(erf(x.get_low()), erf(x.get_high()));
+}
+static inline Vec4d erf (Vec4d const x) {      // error function
+    return Vec4d(erf(x.get_low()), erf(x.get_high()));
+}
+static inline Vec8f erfc (Vec8f const x) {     // error function complement
+    return Vec8f(erfc(x.get_low()), erfc(x.get_high()));
+}
+static inline Vec4d erfc (Vec4d const x) {     // error function complement
+    return Vec4d(erfc(x.get_low()), erfc(x.get_high()));
+}
+static inline Vec8f erfinv (Vec8f const x) {   // inverse error function
+    return Vec8f(erfinv(x.get_low()), erfinv(x.get_high()));
+}
+static inline Vec4d erfinv (Vec4d const x) {   // inverse error function
+    return Vec4d(erfinv(x.get_low()), erfinv(x.get_high()));
+}
+
+static inline Vec8f cdfnorm (Vec8f const x) {  // cumulative normal distribution function
+    return Vec8f(cdfnorm(x.get_low()), cdfnorm(x.get_high()));
+}
+static inline Vec4d cdfnorm (Vec4d const x) {  // cumulative normal distribution function
+    return Vec4d(cdfnorm(x.get_low()), cdfnorm(x.get_high()));
+}
+static inline Vec8f cdfnorminv (Vec8f const x) { // inverse cumulative normal distribution function
+    return Vec8f(cdfnorminv(x.get_low()), cdfnorminv(x.get_high()));
+}
+static inline Vec4d cdfnorminv (Vec4d const x) { // inverse cumulative normal distribution function
+    return Vec4d(cdfnorminv(x.get_low()), cdfnorminv(x.get_high()));
+}
+
+#endif   // VECTORF256_H
+
+#endif  // 256 bits
+
+
+#if defined (MAX_VECTOR_SIZE) && MAX_VECTOR_SIZE >= 512    // 512 bit vectors
+
+#if defined (VECTORF512_H)  // 512-bit vector registers supported
+
+#ifdef USE_SVML_INTRINSICS
+/*****************************************************************************
+*
+*      512-bit vector functions using Intel compiler intrinsic functions
+*
+*****************************************************************************/
+
+// exponential and power functions
+static inline Vec16f exp(Vec16f const x) {       // exponential function
+    return _mm512_exp_ps(x);
+}
+static inline Vec8d exp(Vec8d const x) {         // exponential function
+    return _mm512_exp_pd(x);
+}
+static inline Vec16f expm1(Vec16f const x) {     // exp(x)-1
+    return _mm512_expm1_ps(x);
+}
+static inline Vec8d expm1(Vec8d const x) {       // exp(x)-1
+    return _mm512_expm1_pd(x);
+}
+static inline Vec16f exp2(Vec16f const x) {      // pow(2,x)
+    return _mm512_exp2_ps(x);
+}
+static inline Vec8d exp2(Vec8d const x) {        // pow(2,x)
+    return _mm512_exp2_pd(x);
+}
+static inline Vec16f exp10(Vec16f const x) {     // pow(10,x)
+    return _mm512_exp10_ps(x);
+}
+static inline Vec8d exp10(Vec8d const x) {       // pow(10,x)
+    return _mm512_exp10_pd(x);
+}
+static inline Vec16f pow(Vec16f const a, Vec16f const b) { // pow(a,b) = a to the power of b
+    return _mm512_pow_ps(a, b);
+}
+static inline Vec16f pow(Vec16f const a, float const b) {  // pow(a,b)
+    return _mm512_pow_ps(a, Vec16f(b));
+}
+static inline Vec8d pow(Vec8d const a, Vec8d const b) {    // pow(a,b)
+    return _mm512_pow_pd(a, b);
+}
+static inline Vec8d pow(Vec8d const a, double const b) {   // pow(a,b)
+    return _mm512_pow_pd(a, Vec8d(b));
+}
+static inline Vec16f cbrt(Vec16f const x) {      // pow(x,1/3)
+    return _mm512_cbrt_ps(x);
+}
+static inline Vec8d cbrt(Vec8d const x) {        // pow(x,1/3)
+    return _mm512_cbrt_pd(x);
+}
+// logarithms
+static inline Vec16f log(Vec16f const x) {       // natural logarithm
+    return _mm512_log_ps(x);
+}
+static inline Vec8d log(Vec8d const x) {         // natural logarithm
+    return _mm512_log_pd(x);
+}
+static inline Vec16f log1p(Vec16f const x) {     // log(1+x)
+    return _mm512_log1p_ps(x);
+}
+static inline Vec8d log1p(Vec8d const x) {       // log(1+x)
+    return _mm512_log1p_pd(x);
+}
+static inline Vec16f log2(Vec16f const x) {      // logarithm base 2
+    return _mm512_log2_ps(x);
+}
+static inline Vec8d log2(Vec8d const x) {        // logarithm base 2
+    return _mm512_log2_pd(x);
+}
+static inline Vec16f log10(Vec16f const x) {     // logarithm base 10
+    return _mm512_log10_ps(x);
+}
+static inline Vec8d log10(Vec8d const x) {       // logarithm base 10
+    return _mm512_log10_pd(x);
+}
+
+// trigonometric functions
+static inline Vec16f sin(Vec16f const x) {       // sine
+    return _mm512_sin_ps(x);
+}
+static inline Vec8d sin(Vec8d const x) {         // sine
+    return _mm512_sin_pd(x);
+}
+static inline Vec16f cos(Vec16f const x) {       // cosine
+    return _mm512_cos_ps(x);
+}
+static inline Vec8d cos(Vec8d const x) {         // cosine
+    return _mm512_cos_pd(x);
+}
+static inline Vec16f sincos(Vec16f * pcos, Vec16f const x) { // sine and cosine. sin(x) returned, cos(x) in pcos
+    __m512 r_sin, r_cos;
+    r_sin = _mm512_sincos_ps(&r_cos, x);
+    *pcos = r_cos;
+    return r_sin;
+}
+static inline Vec8d sincos(Vec8d * pcos, Vec8d const x) {    // sine and cosine. sin(x) returned, cos(x) in pcos
+    __m512d r_sin, r_cos;
+    r_sin = _mm512_sincos_pd(&r_cos, x);
+    *pcos = r_cos;
+    return r_sin;
+}
+static inline Vec16f tan(Vec16f const x) {       // tangent
+    return _mm512_tan_ps(x);
+}
+static inline Vec8d tan(Vec8d const x) {         // tangent
+    return _mm512_tan_pd(x);
+}
+
+#ifdef TRIGPI_FUNCTIONS
+
+static inline Vec16f sinpi(Vec16f const x) {     // sine
+    return _mm512_sinpi_ps(x);
+}
+static inline Vec8d sinpi(Vec8d const x) {       // sine
+    return _mm512_sinpi_pd(x);
+}
+static inline Vec16f cospi(Vec16f const x) {     // cosine
+    return _mm512_cospi_ps(x);
+}
+static inline Vec8d cospi(Vec8d const x) {       // cosine
+    return _mm512_cospi_pd(x);
+}
+static inline Vec16f tanpi(Vec16f const x) {     // tangent
+    return _mm512_tanpi_ps(x);
+}
+
+static inline Vec8d tanpi(Vec8d const x) {       // tangent
+#ifdef __INTEL_COMPILER
+    // see https://community.intel.com/t5/Intel-C-Compiler/mm512-tanpi-pd-wrong-declaration/m-p/1404627
+    return _mm512_castps_pd(_mm512_tanpi_pd(_mm512_castpd_ps(x)));
+#else
+    return _mm512_tanpi_pd(x);
+#endif
+}
+#endif  // TRIGPI_FUNCTIONS
+
+// inverse trigonometric functions
+static inline Vec16f asin(Vec16f const x) {      // inverse sine
+    return _mm512_asin_ps(x);
+}
+static inline Vec8d asin(Vec8d const x) {        // inverse sine
+    return _mm512_asin_pd(x);
+}
+
+static inline Vec16f acos(Vec16f const x) {      // inverse cosine
+    return _mm512_acos_ps(x);
+}
+static inline Vec8d acos(Vec8d const x) {        // inverse cosine
+    return _mm512_acos_pd(x);
+}
+
+static inline Vec16f atan(Vec16f const x) {      // inverse tangent
+    return _mm512_atan_ps(x);
+}
+static inline Vec8d atan(Vec8d const x) {        // inverse tangent
+    return _mm512_atan_pd(x);
+}
+static inline Vec16f atan2(Vec16f const a, Vec16f const b) { // inverse tangent of a/b
+    return _mm512_atan2_ps(a, b);
+}
+static inline Vec8d atan2(Vec8d const a, Vec8d const b) {    // inverse tangent of a/b
+    return _mm512_atan2_pd(a, b);
+}
+
+// hyperbolic functions and inverse hyperbolic functions
+static inline Vec16f sinh(Vec16f const x) {      // hyperbolic sine
+    return _mm512_sinh_ps(x);
+}
+static inline Vec8d sinh(Vec8d const x) {        // hyperbolic sine
+    return _mm512_sinh_pd(x);
+}
+static inline Vec16f cosh(Vec16f const x) {      // hyperbolic cosine
+    return _mm512_cosh_ps(x);
+}
+static inline Vec8d cosh(Vec8d const x) {        // hyperbolic cosine
+    return _mm512_cosh_pd(x);
+}
+static inline Vec16f tanh(Vec16f const x) {      // hyperbolic tangent
+    return _mm512_tanh_ps(x);
+}
+static inline Vec8d tanh(Vec8d const x) {        // hyperbolic tangent
+    return _mm512_tanh_pd(x);
+}
+static inline Vec16f asinh(Vec16f const x) {     // inverse hyperbolic sine
+    return _mm512_asinh_ps(x);
+}
+static inline Vec8d asinh(Vec8d const x) {       // inverse hyperbolic sine
+    return _mm512_asinh_pd(x);
+}
+static inline Vec16f acosh(Vec16f const x) {     // inverse hyperbolic cosine
+    return _mm512_acosh_ps(x);
+}
+static inline Vec8d acosh(Vec8d const x) {       // inverse hyperbolic cosine
+    return _mm512_acosh_pd(x);
+}
+static inline Vec16f atanh(Vec16f const x) {     // inverse hyperbolic tangent
+    return _mm512_atanh_ps(x);
+}
+static inline Vec8d atanh(Vec8d const x) {       // inverse hyperbolic tangent
+    return _mm512_atanh_pd(x);
+}
+
+// error function
+static inline Vec16f erf(Vec16f const x) {       // error function
+    return _mm512_erf_ps(x);
+}
+static inline Vec8d erf(Vec8d const x) {         // error function
+    return _mm512_erf_pd(x);
+}
+static inline Vec16f erfc(Vec16f const x) {      // error function complement
+    return _mm512_erfc_ps(x);
+}
+static inline Vec8d erfc(Vec8d const x) {        // error function complement
+    return _mm512_erfc_pd(x);
+}
+static inline Vec16f erfinv(Vec16f const x) {    // inverse error function
+    return _mm512_erfinv_ps(x);
+}
+static inline Vec8d erfinv(Vec8d const x) {      // inverse error function
+    return _mm512_erfinv_pd(x);
+}
+
+static inline Vec16f cdfnorm(Vec16f const x) {   // cumulative normal distribution function
+    return _mm512_cdfnorm_ps(x);
+}
+static inline Vec8d cdfnorm(Vec8d const x) {     // cumulative normal distribution function
+    return _mm512_cdfnorm_pd(x);
+}
+static inline Vec16f cdfnorminv(Vec16f const x) {// inverse cumulative normal distribution function
+    return _mm512_cdfnorminv_ps(x);
+}
+static inline Vec8d cdfnorminv(Vec8d const x) {  // inverse cumulative normal distribution function
+    return _mm512_cdfnorminv_pd(x);
+}
+
+#else    // USE_SVML_INTRINSICS
+/*****************************************************************************
+*
+*      512-bit vector functions using other compiler than Intel
+*
+*****************************************************************************/
+
+// External function prototypes for SVML library, 512-bit vectors
+extern "C" {
+    extern __m512  V_VECTORCALL __svml_expf16       (__m512);
+    extern __m512d V_VECTORCALL __svml_exp8         (__m512d);
+    extern __m512  V_VECTORCALL __svml_expm1f16     (__m512);
+    extern __m512d V_VECTORCALL __svml_expm18       (__m512d);
+    extern __m512  V_VECTORCALL __svml_exp2f16      (__m512);
+    extern __m512d V_VECTORCALL __svml_exp28        (__m512d);
+    extern __m512  V_VECTORCALL __svml_exp10f16     (__m512);
+    extern __m512d V_VECTORCALL __svml_exp108       (__m512d);
+    extern __m512  V_VECTORCALL2 __svml_powf16      (__m512,  __m512);
+    extern __m512d V_VECTORCALL2 __svml_pow8        (__m512d, __m512d);
+    extern __m512  V_VECTORCALL __svml_cbrtf16      (__m512);
+    extern __m512d V_VECTORCALL __svml_cbrt8        (__m512d);
+    extern __m512  V_VECTORCALL __svml_invsqrtf16   (__m512);
+    extern __m512d V_VECTORCALL __svml_invsqrt8     (__m512d);
+    extern __m512  V_VECTORCALL __svml_logf16       (__m512);
+    extern __m512d V_VECTORCALL __svml_log8         (__m512d);
+    extern __m512  V_VECTORCALL __svml_log1pf16     (__m512);
+    extern __m512d V_VECTORCALL __svml_log1p8       (__m512d);
+    extern __m512  V_VECTORCALL __svml_log2f16      (__m512);
+    extern __m512d V_VECTORCALL __svml_log28        (__m512d);
+    extern __m512  V_VECTORCALL __svml_log10f16     (__m512);
+    extern __m512d V_VECTORCALL __svml_log108       (__m512d);
+    extern __m512  V_VECTORCALL __svml_sinf16       (__m512);
+    extern __m512d V_VECTORCALL __svml_sin8         (__m512d);
+    extern __m512  V_VECTORCALL __svml_cosf16       (__m512);
+    extern __m512d V_VECTORCALL __svml_cos8         (__m512d);
+    extern __m512  V_VECTORCALL2 __svml_sincosf16   (__m512);  // cos returned in ymm1
+    extern __m512d V_VECTORCALL2 __svml_sincos8     (__m512d); // cos returned in ymm1
+    extern __m512  V_VECTORCALL __svml_tanf16       (__m512);
+    extern __m512d V_VECTORCALL __svml_tan8         (__m512d);
+    extern __m512  V_VECTORCALL __svml_sinpif16     (__m512);
+    extern __m512d V_VECTORCALL __svml_sinpi8       (__m512d);
+    extern __m512  V_VECTORCALL __svml_cospif16     (__m512);
+    extern __m512d V_VECTORCALL __svml_cospi8       (__m512d);
+    extern __m512  V_VECTORCALL __svml_tanpif16     (__m512);
+    extern __m512d V_VECTORCALL __svml_tanpi8       (__m512d);
+    extern __m512  V_VECTORCALL __svml_asinf16      (__m512);
+    extern __m512d V_VECTORCALL __svml_asin8        (__m512d);
+    extern __m512  V_VECTORCALL __svml_acosf16      (__m512);
+    extern __m512d V_VECTORCALL __svml_acos8        (__m512d);
+    extern __m512  V_VECTORCALL __svml_atanf16      (__m512);
+    extern __m512d V_VECTORCALL __svml_atan8        (__m512d);
+    extern __m512  V_VECTORCALL2 __svml_atan2f16    (__m512, __m512);
+    extern __m512d V_VECTORCALL2 __svml_atan28      (__m512d, __m512d);
+    extern __m512  V_VECTORCALL __svml_sinhf16      (__m512);
+    extern __m512d V_VECTORCALL __svml_sinh8        (__m512d);
+    extern __m512  V_VECTORCALL __svml_coshf16      (__m512);
+    extern __m512d V_VECTORCALL __svml_cosh8        (__m512d);
+    extern __m512  V_VECTORCALL __svml_tanhf16      (__m512);
+    extern __m512d V_VECTORCALL __svml_tanh8        (__m512d);
+    extern __m512  V_VECTORCALL __svml_asinhf16     (__m512);
+    extern __m512d V_VECTORCALL __svml_asinh8       (__m512d);
+    extern __m512  V_VECTORCALL __svml_acoshf16     (__m512);
+    extern __m512d V_VECTORCALL __svml_acosh8       (__m512d);
+    extern __m512  V_VECTORCALL __svml_atanhf16     (__m512);
+    extern __m512d V_VECTORCALL __svml_atanh8       (__m512d);
+    extern __m512  V_VECTORCALL __svml_erff16       (__m512);
+    extern __m512d V_VECTORCALL __svml_erf8         (__m512d);
+    extern __m512  V_VECTORCALL __svml_erfcf16      (__m512);
+    extern __m512d V_VECTORCALL __svml_erfc8        (__m512d);
+    extern __m512  V_VECTORCALL __svml_erfinvf16    (__m512);
+    extern __m512d V_VECTORCALL __svml_erfinv8      (__m512d);
+    extern __m512  V_VECTORCALL __svml_cdfnorminvf16(__m512);
+    extern __m512d V_VECTORCALL __svml_cdfnorminv8  (__m512d);
+    extern __m512  V_VECTORCALL __svml_cdfnormf16   (__m512);
+    extern __m512d V_VECTORCALL __svml_cdfnorm8     (__m512d);
+    //extern __m512  V_VECTORCALL __svml_cexpf16    (__m512);
+    //extern __m512d V_VECTORCALL __svml_cexp8      (__m512d);
+}
+
+
+// exponential and power functions
+static inline Vec16f exp (Vec16f const x) {      // exponential function
+    return  __svml_expf16(x);
+}
+static inline Vec8d exp (Vec8d const x) {        // exponential function
+    return  __svml_exp8(x);
+}
+static inline Vec16f expm1 (Vec16f const x) {    // exp(x)-1
+    return  __svml_expm1f16(x);
+}
+static inline Vec8d expm1 (Vec8d const x) {      // exp(x)-1
+    return  __svml_expm18(x);
+}
+static inline Vec16f exp2 (Vec16f const x) {     // pow(2,x)
+    return  __svml_exp2f16(x);
+}
+static inline Vec8d exp2 (Vec8d const x) {       // pow(2,x)
+    return  __svml_exp28(x);
+}
+static inline Vec16f exp10 (Vec16f const x) {    // pow(10,x)
+    return  __svml_exp10f16(x);
+}
+static inline Vec8d exp10 (Vec8d const x) {      // pow(10,x)
+    return  __svml_exp108(x);
+}
+static inline Vec16f pow (Vec16f const a, Vec16f const b) {  // pow(a,b) = a to the power of b
+    return  __svml_powf16(a,b);
+}
+static inline Vec16f pow (Vec16f const a, float const b) {   // pow(a,b)
+    return  __svml_powf16(a,Vec16f(b));
+}
+static inline Vec8d pow (Vec8d const a, Vec8d const b) {     // pow(a,b)
+    return  __svml_pow8(a,b);
+}
+static inline Vec8d pow (Vec8d const a, double const b) {    // pow(a,b)
+    return  __svml_pow8(a,Vec8d(b));
+}
+static inline Vec16f cbrt (Vec16f const x) {     // pow(x,1/3)
+    return  __svml_cbrtf16(x);
+}
+static inline Vec8d cbrt (Vec8d const x) {       // pow(x,1/3)
+    return  __svml_cbrt8(x);
+}
+
+// logarithms
+static inline Vec16f log (Vec16f const x) {      // natural logarithm
+    return  __svml_logf16(x);
+}
+static inline Vec8d log (Vec8d const x) {        // natural logarithm
+    return  __svml_log8(x);
+}
+static inline Vec16f log1p (Vec16f const x) {    // log(1+x)
+    return  __svml_log1pf16(x);
+}
+static inline Vec8d log1p (Vec8d const x) {      // log(1+x)
+    return  __svml_log1p8(x);
+}
+static inline Vec16f log2 (Vec16f const x) {     // logarithm base 2
+    return  __svml_log2f16(x);
+}
+static inline Vec8d log2 (Vec8d const x) {       // logarithm base 2
+    return  __svml_log28(x);
+}
+static inline Vec16f log10 (Vec16f const x) {    // logarithm base 10
+    return  __svml_log10f16(x);
+}
+static inline Vec8d log10 (Vec8d const x) {      // logarithm base 10
+    return  __svml_log108(x);
+}
+
+// trigonometric functions (angles in radians)
+static inline Vec16f sin (Vec16f const x) {      // sine
+    return  __svml_sinf16(x);
+}
+static inline Vec8d sin (Vec8d const x) {        // sine
+    return  __svml_sin8(x);
+}
+static inline Vec16f cos (Vec16f const x) {      // cosine
+    return  __svml_cosf16(x);
+}
+static inline Vec8d cos (Vec8d const x) {        // cosine
+    return  __svml_cos8(x);
+}
+
+#ifdef SINCOS_ASM  // sincos can be fixed with inline assembly
+// no inline assembly in 64 bit MS compiler
+// sine and cosine. sin(x) returned, cos(x) in pcos
+static inline Vec16f sincos (Vec16f * pcos, Vec16f const x) {
+    __m512 r_sin, r_cos;
+    r_sin = __svml_sincosf16(x);
+    __asm__ __volatile__ ( "vmovaps %%zmm1, %0":"=m"(r_cos));
+    *pcos = r_cos;
+    return r_sin;
+}
+// sine and cosine. sin(x) returned, cos(x) in pcos
+static inline Vec8d sincos (Vec8d * pcos, Vec8d const x) {
+    __m512d r_sin, r_cos;
+    r_sin = __svml_sincos8(x);
+    __asm__ __volatile__ ( "vmovaps %%zmm1, %0":"=m"(r_cos));
+    *pcos = r_cos;
+    return r_sin;
+}
+#endif // sincos
+
+static inline Vec16f tan (Vec16f const x) {      // tangent
+    return  __svml_tanf16(x);
+}
+static inline Vec8d tan (Vec8d const x) {        // tangent
+    return  __svml_tan8(x);
+}
+
+static inline Vec16f sinpi (Vec16f const x) {    // sine
+    return  __svml_sinpif16(x);
+}
+static inline Vec8d sinpi (Vec8d const x) {      // sine
+    return  __svml_sinpi8(x);
+}
+static inline Vec16f cospi (Vec16f const x) {    // cosine
+    return  __svml_cospif16(x);
+}
+static inline Vec8d cospi (Vec8d const x) {      // cosine
+    return  __svml_cospi8(x);
+}
+static inline Vec16f tanpi (Vec16f const x) {    // tangent
+    return  __svml_tanpif16(x);
+}
+static inline Vec8d tanpi (Vec8d const x) {      // tangent
+    return  __svml_tanpi8(x);
+}
+
+// inverse trigonometric functions
+static inline Vec16f asin (Vec16f const x) {     // inverse sine
+    return  __svml_asinf16(x);
+}
+static inline Vec8d asin (Vec8d const x) {       // inverse sine
+    return  __svml_asin8(x);
+}
+static inline Vec16f acos (Vec16f const x) {     // inverse cosine
+    return  __svml_acosf16(x);
+}
+static inline Vec8d acos (Vec8d const x) {       // inverse cosine
+    return  __svml_acos8(x);
+}
+static inline Vec16f atan (Vec16f const x) {     // inverse tangent
+    return  __svml_atanf16(x);
+}
+static inline Vec8d atan (Vec8d const x) {       // inverse tangent
+    return  __svml_atan8(x);
+}
+static inline Vec16f atan2 (Vec16f const a, Vec16f const b) {// inverse tangent of a/b
+    return  __svml_atan2f16(a,b);
+}
+static inline Vec8d atan2 (Vec8d const a, Vec8d const b) {   // inverse tangent of a/b
+    return  __svml_atan28(a,b);
+}
+
+// hyperbolic functions and inverse hyperbolic functions
+static inline Vec16f sinh (Vec16f const x) {     // hyperbolic sine
+    return  __svml_sinhf16(x);
+}
+static inline Vec8d sinh (Vec8d const x) {       // hyperbolic sine
+    return  __svml_sinh8(x);
+}
+static inline Vec16f cosh (Vec16f const x) {     // hyperbolic cosine
+    return  __svml_coshf16(x);
+}
+static inline Vec8d cosh (Vec8d const x) {       // hyperbolic cosine
+    return  __svml_cosh8(x);
+}
+static inline Vec16f tanh (Vec16f const x) {     // hyperbolic tangent
+    return  __svml_tanhf16(x);
+}
+static inline Vec8d tanh (Vec8d const x) {       // hyperbolic tangent
+    return  __svml_tanh8(x);
+}
+static inline Vec16f asinh (Vec16f const x) {    // inverse hyperbolic sine
+    return  __svml_asinhf16(x);
+}
+static inline Vec8d asinh (Vec8d const x) {      // inverse hyperbolic sine
+    return  __svml_asinh8(x);
+}
+static inline Vec16f acosh (Vec16f const x) {    // inverse hyperbolic cosine
+    return  __svml_acoshf16(x);
+}
+static inline Vec8d acosh (Vec8d const x) {      // inverse hyperbolic cosine
+    return  __svml_acosh8(x);
+}
+static inline Vec16f atanh (Vec16f const x) {    // inverse hyperbolic tangent
+    return  __svml_atanhf16(x);
+}
+static inline Vec8d atanh (Vec8d const x) {      // inverse hyperbolic tangent
+    return  __svml_atanh8(x);
+}
+
+// error function
+static inline Vec16f erf (Vec16f const x) {      // error function
+    return  __svml_erff16(x);
+}
+static inline Vec8d erf (Vec8d const x) {        // error function
+    return  __svml_erf8(x);
+}
+static inline Vec16f erfc (Vec16f const x) {     // error function complement
+    return  __svml_erfcf16(x);
+}
+static inline Vec8d erfc (Vec8d const x) {       // error function complement
+    return  __svml_erfc8(x);
+}
+static inline Vec16f erfinv (Vec16f const x) {   // inverse error function
+    return  __svml_erfinvf16(x);
+}
+static inline Vec8d erfinv (Vec8d const x) {     // inverse error function
+    return  __svml_erfinv8(x);
+}
+
+static inline Vec16f cdfnorm (Vec16f const x) {  // cumulative normal distribution function
+    return  __svml_cdfnormf16(x);
+}
+static inline Vec8d cdfnorm (Vec8d const x) {    // cumulative normal distribution function
+    return  __svml_cdfnorm8(x);
+}
+static inline Vec16f cdfnorminv (Vec16f const x) {  // inverse cumulative normal distribution function
+    return  __svml_cdfnorminvf16(x);
+}
+static inline Vec8d cdfnorminv (Vec8d const x) {    // inverse cumulative normal distribution function
+    return  __svml_cdfnorminv8(x);
+}
+
+#endif   // USE_SVML_INTRINSICS
+
+#else    // VECTORF512_H
+
+/*****************************************************************************
+*
+*      512-bit vector functions emulated with 256-bit vectors
+*
+*****************************************************************************/
+
+// exponential and power functions
+static inline Vec16f exp (Vec16f const x) {      // exponential function
+    return Vec16f(exp(x.get_low()), exp(x.get_high()));
+}
+static inline Vec8d exp (Vec8d const x) {        // exponential function
+    return Vec8d(exp(x.get_low()), exp(x.get_high()));
+}
+static inline Vec16f expm1 (Vec16f const x) {    // exp(x)-1
+    return Vec16f(expm1(x.get_low()), expm1(x.get_high()));
+}
+static inline Vec8d expm1 (Vec8d const x) {      // exp(x)-1
+    return Vec8d(expm1(x.get_low()), expm1(x.get_high()));
+}
+static inline Vec16f exp2 (Vec16f const x) {     // pow(2,x)
+    return Vec16f(exp2(x.get_low()), exp2(x.get_high()));
+}
+static inline Vec8d exp2 (Vec8d const x) {       // pow(2,x)
+    return Vec8d(exp2(x.get_low()), exp2(x.get_high()));
+}
+static inline Vec16f exp10 (Vec16f const x) {    // pow(10,x)
+    return Vec16f(exp10(x.get_low()), exp10(x.get_high()));
+}
+static inline Vec8d exp10 (Vec8d const x) {      // pow(10,x)
+    return Vec8d(exp10(x.get_low()), exp10(x.get_high()));
+}
+static inline Vec16f pow (Vec16f const a, Vec16f const b) {  // pow(a,b) = a to the power of b
+    return Vec16f(pow(a.get_low(),b.get_low()), pow(a.get_high(),b.get_high()));
+}
+static inline Vec16f pow (Vec16f const a, float const b) {   // pow(a,b)
+    return Vec16f(pow(a.get_low(),b), pow(a.get_high(),b));
+}
+static inline Vec8d pow (Vec8d const a, Vec8d const b) {     // pow(a,b)
+    return Vec8d(pow(a.get_low(),b.get_low()), pow(a.get_high(),b.get_high()));
+}
+static inline Vec8d pow (Vec8d const a, double const b) {    // pow(a,b)
+    return Vec8d(pow(a.get_low(),b), pow(a.get_high(),b));
+}
+static inline Vec16f cbrt (Vec16f const x) {     // pow(x,1/3)
+    return Vec16f(cbrt(x.get_low()), cbrt(x.get_high()));
+}
+static inline Vec8d cbrt (Vec8d const x) {       // pow(x,1/3)
+    return Vec8d(cbrt(x.get_low()), cbrt(x.get_high()));
+}
+
+// logarithms
+static inline Vec16f log (Vec16f const x) {      // natural logarithm
+    return Vec16f(log(x.get_low()), log(x.get_high()));
+}
+static inline Vec8d log (Vec8d const x) {        // natural logarithm
+    return Vec8d(log(x.get_low()), log(x.get_high()));
+}
+static inline Vec16f log1p (Vec16f const x) {    // log(1+x)
+    return Vec16f(log1p(x.get_low()), log1p(x.get_high()));
+}
+static inline Vec8d log1p (Vec8d const x) {      // log(1+x)
+    return Vec8d(log1p(x.get_low()), log1p(x.get_high()));
+}
+static inline Vec16f log2 (Vec16f const x) {     // logarithm base 2
+    return Vec16f(log2(x.get_low()), log2(x.get_high()));
+}
+static inline Vec8d log2 (Vec8d const x) {       // logarithm base 2
+    return Vec8d(log2(x.get_low()), log2(x.get_high()));
+}
+static inline Vec16f log10 (Vec16f const x) {    // logarithm base 10
+    return Vec16f(log10(x.get_low()), log10(x.get_high()));
+}
+static inline Vec8d log10 (Vec8d const x) {      // logarithm base 10
+    return Vec8d(log10(x.get_low()), log10(x.get_high()));
+}
+
+// trigonometric functions (angles in radians)
+static inline Vec16f sin (Vec16f const x) {      // sine
+    return Vec16f(sin(x.get_low()), sin(x.get_high()));
+}
+static inline Vec8d sin (Vec8d const x) {        // sine
+    return Vec8d(sin(x.get_low()), sin(x.get_high()));
+}
+static inline Vec16f cos (Vec16f const x) {      // cosine
+    return Vec16f(cos(x.get_low()), cos(x.get_high()));
+}
+static inline Vec8d cos (Vec8d const x) {        // cosine
+    return Vec8d(cos(x.get_low()), cos(x.get_high()));
+}
+#ifdef SINCOS_ASM  // sincos can be fixed with inline assembly
+static inline Vec16f sincos (Vec16f * pcos, Vec16f const x) {  // sine and cosine. sin(x) returned, cos(x) in pcos
+    Vec8f r_cos0, r_cos1;
+    Vec16f r_sin = Vec16f(sincos(&r_cos0, x.get_low()), sincos(&r_cos1, x.get_high()));
+    *pcos = Vec16f(r_cos0, r_cos1);
+    return r_sin;
+}
+
+static inline Vec8d sincos (Vec8d * pcos, Vec8d const x) {     // sine and cosine. sin(x) returned, cos(x) in pcos
+    Vec4d r_cos0, r_cos1;
+    Vec8d r_sin = Vec8d(sincos(&r_cos0, x.get_low()), sincos(&r_cos1, x.get_high()));
+    *pcos = Vec8d(r_cos0, r_cos1);
+    return r_sin;
+}
+#endif  // sincos
+
+static inline Vec16f tan (Vec16f const x) {      // tangent
+    return Vec16f(tan(x.get_low()), tan(x.get_high()));
+}
+static inline Vec8d tan (Vec8d const x) {        // tangent
+    return Vec8d(tan(x.get_low()), tan(x.get_high()));
+}
+
+#ifdef TRIGPI_FUNCTIONS
+static inline Vec16f sinpi (Vec16f const x) {    // sine
+    return Vec16f(sinpi(x.get_low()), sinpi(x.get_high()));
+}
+static inline Vec8d sinpi (Vec8d const x) {      // sine
+    return Vec8d(sinpi(x.get_low()), sinpi(x.get_high()));
+}
+static inline Vec16f cospi (Vec16f const x) {    // cosine
+    return Vec16f(cospi(x.get_low()), cospi(x.get_high()));
+}
+static inline Vec8d cospi (Vec8d const x) {      // cosine
+    return Vec8d(cospi(x.get_low()), cospi(x.get_high()));
+}
+static inline Vec16f tanpi (Vec16f const x) {    // tangent
+    return Vec16f(tanpi(x.get_low()), tanpi(x.get_high()));
+}
+static inline Vec8d tanpi (Vec8d const x) {      // tangent
+    return Vec8d(tanpi(x.get_low()), tanpi(x.get_high()));
+}
+#endif
+
+// inverse trigonometric functions
+static inline Vec16f asin (Vec16f const x) {     // inverse sine
+    return Vec16f(asin(x.get_low()), asin(x.get_high()));
+}
+static inline Vec8d asin (Vec8d const x) {       // inverse sine
+    return Vec8d(asin(x.get_low()), asin(x.get_high()));
+}
+static inline Vec16f acos (Vec16f const x) {     // inverse cosine
+    return Vec16f(acos(x.get_low()), acos(x.get_high()));
+}
+static inline Vec8d acos (Vec8d const x) {       // inverse cosine
+    return Vec8d(acos(x.get_low()), acos(x.get_high()));
+}
+static inline Vec16f atan (Vec16f const x) {     // inverse tangent
+    return Vec16f(atan(x.get_low()), atan(x.get_high()));
+}
+static inline Vec8d atan (Vec8d const x) {       // inverse tangent
+    return Vec8d(atan(x.get_low()), atan(x.get_high()));
+}
+static inline Vec16f atan2 (Vec16f const a, Vec16f const b) {  // inverse tangent of a/b
+    return Vec16f(atan2(a.get_low(),b.get_low()), atan2(a.get_high(),b.get_high()));
+}
+static inline Vec8d atan2 (Vec8d const a, Vec8d const b) {     // inverse tangent of a/b
+    return Vec8d(atan2(a.get_low(),b.get_low()), atan2(a.get_high(),b.get_high()));
+}
+
+// hyperbolic functions
+static inline Vec16f sinh (Vec16f const x) {     // hyperbolic sine
+    return Vec16f(sinh(x.get_low()), sinh(x.get_high()));
+}
+static inline Vec8d sinh (Vec8d const x) {       // hyperbolic sine
+    return Vec8d(sinh(x.get_low()), sinh(x.get_high()));
+}
+static inline Vec16f cosh (Vec16f const x) {     // hyperbolic cosine
+    return Vec16f(cosh(x.get_low()), cosh(x.get_high()));
+}
+static inline Vec8d cosh (Vec8d const x) {       // hyperbolic cosine
+    return Vec8d(cosh(x.get_low()), cosh(x.get_high()));
+}
+static inline Vec16f tanh (Vec16f const x) {     // hyperbolic tangent
+    return Vec16f(tanh(x.get_low()), tanh(x.get_high()));
+}
+static inline Vec8d tanh (Vec8d const x) {       // hyperbolic tangent
+    return Vec8d(tanh(x.get_low()), tanh(x.get_high()));
+}
+
+// inverse hyperbolic functions
+static inline Vec16f asinh (Vec16f const x) {    // inverse hyperbolic sine
+    return Vec16f(asinh(x.get_low()), asinh(x.get_high()));
+}
+static inline Vec8d asinh (Vec8d const x) {      // inverse hyperbolic sine
+    return Vec8d(asinh(x.get_low()), asinh(x.get_high()));
+}
+static inline Vec16f acosh (Vec16f const x) {    // inverse hyperbolic cosine
+    return Vec16f(acosh(x.get_low()), acosh(x.get_high()));
+}
+static inline Vec8d acosh (Vec8d const x) {      // inverse hyperbolic cosine
+    return Vec8d(acosh(x.get_low()), acosh(x.get_high()));
+}
+static inline Vec16f atanh (Vec16f const x) {    // inverse hyperbolic tangent
+    return Vec16f(atanh(x.get_low()), atanh(x.get_high()));
+}
+static inline Vec8d atanh (Vec8d const x) {      // inverse hyperbolic tangent
+    return Vec8d(atanh(x.get_low()), atanh(x.get_high()));
+}
+
+// error function
+static inline Vec16f erf (Vec16f const x) {      // error function
+    return Vec16f(erf(x.get_low()), erf(x.get_high()));
+}
+static inline Vec8d erf (Vec8d const x) {        // error function
+    return Vec8d(erf(x.get_low()), erf(x.get_high()));
+}
+static inline Vec16f erfc (Vec16f const x) {     // error function complement
+    return Vec16f(erfc(x.get_low()), erfc(x.get_high()));
+}
+static inline Vec8d erfc (Vec8d const x) {       // error function complement
+    return Vec8d(erfc(x.get_low()), erfc(x.get_high()));
+}
+static inline Vec16f erfinv (Vec16f const x) {   // inverse error function
+    return Vec16f(erfinv(x.get_low()), erfinv(x.get_high()));
+}
+static inline Vec8d erfinv (Vec8d const x) {     // inverse error function
+    return Vec8d(erfinv(x.get_low()), erfinv(x.get_high()));
+}
+
+static inline Vec16f cdfnorm (Vec16f const x) {  // cumulative normal distribution function
+    return Vec16f(cdfnorm(x.get_low()), cdfnorm(x.get_high()));
+}
+static inline Vec8d cdfnorm (Vec8d const x) {    // cumulative normal distribution function
+    return Vec8d(cdfnorm(x.get_low()), cdfnorm(x.get_high()));
+}
+static inline Vec16f cdfnorminv (Vec16f const x) {// inverse cumulative normal distribution function
+    return Vec16f(cdfnorminv(x.get_low()), cdfnorminv(x.get_high()));
+}
+static inline Vec8d cdfnorminv (Vec8d const x) {  // inverse cumulative normal distribution function
+    return Vec8d(cdfnorminv(x.get_low()), cdfnorminv(x.get_high()));
+}
+
+#endif   // VECTORF512_H
+
+#endif   // MAX_VECTOR_SIZE >= 512
+
+#ifdef   VCL_NAMESPACE
+}
+#endif   // VCL_NAMESPACE
+
+#endif   // VECTORMATH_COMMON_H
+
+#endif   // VECTORMATH_LIB_H
diff --git a/EEDI3/vectorclass/vectormath_trig.h b/EEDI3/vectorclass/vectormath_trig.h
new file mode 100644
index 0000000..72d5c57
--- /dev/null
+++ b/EEDI3/vectorclass/vectormath_trig.h
@@ -0,0 +1,1040 @@
+/****************************  vectormath_trig.h   ******************************
+* Author:        Agner Fog
+* Date created:  2014-04-18
+* Last modified: 2022-07-26
+* Version:       2.02.00
+* Project:       vector class library
+* Description:
+* Header file containing inline version of trigonometric functions
+* and inverse trigonometric functions
+* sin, cos, sincos, tan
+* asin, acos, atan, atan2
+*
+* Theory, methods, and inspiration based partially on these sources:
+* > Moshier, Stephen Lloyd Baluk: Methods and programs for mathematical functions.
+*   Ellis Horwood, 1989.
+* > VDT library developed on CERN by Danilo Piparo, Thomas Hauth and
+*   Vincenzo Innocente, 2012, https://svnweb.cern.ch/trac/vdt
+* > Cephes math library by Stephen L. Moshier 1992,
+*   http://www.netlib.org/cephes/
+*
+* For detailed instructions, see vectormath_common.h and vcl_manual.pdf
+*
+* (c) Copyright 2014-2022 Agner Fog.
+* Apache License version 2.0 or later.
+******************************************************************************/
+
+#ifndef VECTORMATH_TRIG_H
+#define VECTORMATH_TRIG_H  202
+
+#include "vectormath_common.h"
+
+#ifdef VCL_NAMESPACE
+namespace VCL_NAMESPACE {
+#endif
+
+
+// *************************************************************
+//             sin/cos template, double precision
+// *************************************************************
+// Template parameters:
+// VTYPE:  f.p. vector type
+// SC:     1 = sin, 2 = cos, 3 = sincos, 4 = tan, 8 = multiply by pi
+// Parameters:
+// xx = input x (radians)
+// cosret = return pointer (only if SC = 3)
+template<typename VTYPE, int SC>
+static inline VTYPE sincos_d(VTYPE * cosret, VTYPE const xx) {
+
+    // define constants
+    const double P0sin = -1.66666666666666307295E-1;
+    const double P1sin = 8.33333333332211858878E-3;
+    const double P2sin = -1.98412698295895385996E-4;
+    const double P3sin = 2.75573136213857245213E-6;
+    const double P4sin = -2.50507477628578072866E-8;
+    const double P5sin = 1.58962301576546568060E-10;
+
+    const double P0cos = 4.16666666666665929218E-2;
+    const double P1cos = -1.38888888888730564116E-3;
+    const double P2cos = 2.48015872888517045348E-5;
+    const double P3cos = -2.75573141792967388112E-7;
+    const double P4cos = 2.08757008419747316778E-9;
+    const double P5cos = -1.13585365213876817300E-11;
+
+    const double DP1 = 7.853981554508209228515625E-1 * 2.;
+    const double DP2 = 7.94662735614792836714E-9 * 2.;
+    const double DP3 = 3.06161699786838294307E-17 * 2.;
+
+    typedef decltype(roundi(xx)) ITYPE;          // integer vector type
+    //typedef decltype(nan_code(xx)) UITYPE;       // unsigned integer vector type
+    typedef decltype(xx < xx) BVTYPE;            // boolean vector type
+
+    VTYPE  xa, x, y, x2, s, c, sin1, cos1;       // data vectors
+    ITYPE  q, signsin, signcos;                  // integer vectors, 64 bit
+
+    BVTYPE swap;                                 // boolean vector
+
+#if INSTRSET < 8  // no FMA
+    const double input_limit = 1.E13;            // lower overflow limit without FMA
+#else
+    const double input_limit = 1.E15;
+#endif
+
+    xa = abs(xx);
+
+    // Find quadrant
+    if constexpr ((SC & 8) != 0) {
+        y = round(xa * 2.0);
+    }
+    else {
+        xa = select(xa > VTYPE(input_limit), VTYPE(0.f), xa); // overflow limit
+        y = round(xa * (double)(2. / VM_PI));    // quadrant, as float
+    }
+    q = roundi(y);                               // quadrant, as integer
+    // Quadrant:
+    //      0 -   pi/4 => 0
+    //   pi/4 - 3*pi/4 => 1
+    // 3*pi/4 - 5*pi/4 => 2
+    // 5*pi/4 - 7*pi/4 => 3
+    // 7*pi/4 - 8*pi/4 => 4
+
+    if constexpr ((SC & 8) != 0) {
+        x = nmul_add(y, 0.5, xa) * (VM_PI);
+    }
+    else {
+        // Reduce by extended precision modular arithmetic
+#if INSTRSET < 8  // no FMA
+        x = ((xa - y * DP1) - y * DP2) - y * DP3;
+#else
+        x = nmul_add(y, DP3, nmul_add(y, DP2 + DP1, xa));
+#endif
+    }
+    // Expansion of sin and cos, valid for -pi/4 <= x <= pi/4
+    x2 = x * x;
+    s = polynomial_5(x2, P0sin, P1sin, P2sin, P3sin, P4sin, P5sin);
+    c = polynomial_5(x2, P0cos, P1cos, P2cos, P3cos, P4cos, P5cos);
+    s = mul_add(x * x2, s, x);                                       // s = x + (x * x2) * s;
+    c = mul_add(x2 * x2, c, nmul_add(x2, 0.5, 1.0));                 // c = 1.0 - x2 * 0.5 + (x2 * x2) * c;
+
+    // swap sin and cos if odd quadrant
+    swap = BVTYPE((q & 1) != 0);
+
+    if constexpr ((SC & 1) != 0) {  // calculate sin
+        sin1 = select(swap, c, s);
+        signsin = ((q << 62) ^ ITYPE(reinterpret_i(xx)));
+        sin1 = sign_combine(sin1, reinterpret_d(signsin));
+    }
+    if constexpr ((SC & 2) != 0) {  // calculate cos
+        cos1 = select(swap, s, c);
+        signcos = ((q + 1) & 2) << 62;
+        cos1 ^= reinterpret_d(signcos);
+    }
+    if constexpr ((SC & 7) == 3) {  // calculate both. cos returned through pointer
+        *cosret = cos1;
+    }
+    if constexpr ((SC & 1) != 0) return sin1; else return cos1;
+}
+
+// instantiations of sincos_d template:
+
+static inline Vec2d sin(Vec2d const x) {
+    return sincos_d<Vec2d, 1>(0, x);
+}
+
+static inline Vec2d cos(Vec2d const x) {
+    return sincos_d<Vec2d, 2>(0, x);
+}
+
+static inline Vec2d sincos(Vec2d * cosret, Vec2d const x) {
+    return sincos_d<Vec2d, 3>(cosret, x);
+}
+
+static inline Vec2d sinpi(Vec2d const x) {
+    return sincos_d<Vec2d, 9>(0, x);
+}
+
+static inline Vec2d cospi(Vec2d const x) {
+    return sincos_d<Vec2d, 10>(0, x);
+}
+
+static inline Vec2d sincospi(Vec2d * cosret, Vec2d const x) {
+    return sincos_d<Vec2d, 11>(cosret, x);
+}
+
+
+#if MAX_VECTOR_SIZE >= 256
+static inline Vec4d sin(Vec4d const x) {
+    return sincos_d<Vec4d, 1>(0, x);
+}
+
+static inline Vec4d cos(Vec4d const x) {
+    return sincos_d<Vec4d, 2>(0, x);
+}
+
+static inline Vec4d sincos(Vec4d * cosret, Vec4d const x) {
+    return sincos_d<Vec4d, 3>(cosret, x);
+}
+
+static inline Vec4d sinpi(Vec4d const x) {
+    return sincos_d<Vec4d, 9>(0, x);
+}
+
+static inline Vec4d cospi(Vec4d const x) {
+    return sincos_d<Vec4d, 10>(0, x);
+}
+
+static inline Vec4d sincospi(Vec4d * cosret, Vec4d const x) {
+    return sincos_d<Vec4d, 11>(cosret, x);
+}
+
+#endif // MAX_VECTOR_SIZE >= 256
+
+#if MAX_VECTOR_SIZE >= 512
+static inline Vec8d sin(Vec8d const x) {
+    return sincos_d<Vec8d, 1>(0, x);
+}
+
+static inline Vec8d cos(Vec8d const x) {
+    return sincos_d<Vec8d, 2>(0, x);
+}
+
+static inline Vec8d sincos(Vec8d * cosret, Vec8d const x) {
+    return sincos_d<Vec8d, 3>(cosret, x);
+}
+
+static inline Vec8d sinpi(Vec8d const x) {
+    return sincos_d<Vec8d, 9>(0, x);
+}
+
+static inline Vec8d cospi(Vec8d const x) {
+    return sincos_d<Vec8d, 10>(0, x);
+}
+
+static inline Vec8d sincospi(Vec8d * cosret, Vec8d const x) {
+    return sincos_d<Vec8d, 11>(cosret, x);
+}
+
+#endif // MAX_VECTOR_SIZE >= 512
+
+
+// *************************************************************
+//             sincos template, single precision
+// *************************************************************
+// Template parameters:
+// VTYPE:  f.p. vector type
+// SC:     1 = sin, 2 = cos, 3 = sincos, 4 = tan, 8 = multiply by pi
+// Parameters:
+// xx = input x (radians)
+// cosret = return pointer (only if SC = 3)
+template<typename VTYPE, int SC>
+static inline VTYPE sincos_f(VTYPE* cosret, VTYPE const xx) {
+
+    // define constants
+    const float DP1F = 0.78515625f * 2.f;
+    const float DP2F = 2.4187564849853515625E-4f * 2.f;
+    const float DP3F = 3.77489497744594108E-8f * 2.f;
+
+    const float P0sinf = -1.6666654611E-1f;
+    const float P1sinf = 8.3321608736E-3f;
+    const float P2sinf = -1.9515295891E-4f;
+
+    const float P0cosf = 4.166664568298827E-2f;
+    const float P1cosf = -1.388731625493765E-3f;
+    const float P2cosf = 2.443315711809948E-5f;
+
+    typedef decltype(roundi(xx)) ITYPE;          // integer vector type
+    //typedef decltype(nan_code(xx)) UITYPE;       // unsigned integer vector type
+    typedef decltype(xx < xx) BVTYPE;            // boolean vector type
+    
+#if INSTRSET < 8  // no FMA
+    const float input_limit = 1.E5f;             // lower overflow limit without FMA
+#else
+    const float input_limit = 1.E7f;
+#endif
+
+    VTYPE  xa, x, y, x2, s, c, sin1, cos1;       // data vectors
+    ITYPE  q, signsin, signcos;                  // integer vectors
+    BVTYPE swap;                                 // boolean vector
+
+    xa = abs(xx);
+
+    // Find quadrant
+    if constexpr ((SC & 8) != 0) {
+        y = round(xa * 2.0f);
+    }
+    else {
+        xa = select(xa > VTYPE(input_limit), VTYPE(0.f), xa); // overflow limit
+        y = round(xa * (float)(2. / VM_PI));     // quadrant, as float
+    }
+    q = roundi(y);                               // quadrant, as integer
+    // Quadrant:
+    //      0 -   pi/4 => 0
+    //   pi/4 - 3*pi/4 => 1
+    // 3*pi/4 - 5*pi/4 => 2
+    // 5*pi/4 - 7*pi/4 => 3
+    // 7*pi/4 - 8*pi/4 => 4
+
+    if constexpr ((SC & 8) != 0) {
+        x = nmul_add(y, 0.5f, xa)*float(VM_PI);
+    }
+    else {
+        // Reduce by extended precision modular arithmetic
+#if INSTRSET < 8  // no FMA
+        x = ((xa - y * DP1F) - y * DP2F) - y * DP3F;
+#else
+        x = nmul_add(y, DP3F, nmul_add(y, DP2F + DP1F, xa));
+#endif
+    }
+    // Taylor expansion of sin and cos, valid for -pi/4 <= x <= pi/4
+    x2 = x * x;
+    s = polynomial_2(x2, P0sinf, P1sinf, P2sinf) * (x*x2) + x;
+    c = polynomial_2(x2, P0cosf, P1cosf, P2cosf) * (x2*x2) + nmul_add(0.5f, x2, 1.0f);
+
+    // swap sin and cos if odd quadrant
+    swap = BVTYPE((q & 1) != 0);
+
+    if constexpr ((SC & 5) != 0) {  // calculate sin
+        sin1 = select(swap, c, s);
+        signsin = ((q << 30) ^ ITYPE(reinterpret_i(xx)));
+        sin1 = sign_combine(sin1, reinterpret_f(signsin));
+    }
+    if constexpr ((SC & 6) != 0) {  // calculate cos
+        cos1 = select(swap, s, c);
+        signcos = ((q + 1) & 2) << 30;
+        cos1 ^= reinterpret_f(signcos);
+    }
+    if constexpr ((SC & 7) == 1) return sin1;
+    else if constexpr ((SC & 7) == 2) return cos1;
+    else if constexpr ((SC & 7) == 3) {  // calculate both. cos returned through pointer
+        *cosret = cos1;
+        return sin1;
+    }
+    else {  // SC == 4. tan
+        if constexpr (SC == 12) {
+            // tanpi can give INF result, tan cannot. Get the right sign of INF result according to IEEE 754-2019
+            cos1 = select(cos1 == 0.f, 0.f, cos1); // remove sign of 0
+            // the sign of zero output is arbitrary. fixing it would be a waste of code
+        }
+        return sin1 / cos1;
+    }
+}
+
+// instantiations of sincos_f template:
+
+static inline Vec4f sin(Vec4f const x) {
+    return sincos_f<Vec4f, 1>(0, x);
+}
+
+static inline Vec4f cos(Vec4f const x) {
+    return sincos_f<Vec4f, 2>(0, x);
+}
+
+static inline Vec4f sincos(Vec4f * cosret, Vec4f const x) {
+    return sincos_f<Vec4f, 3>(cosret, x);
+}
+
+static inline Vec4f tan(Vec4f const x) {
+    return sincos_f<Vec4f, 4>(0, x);
+}
+
+static inline Vec4f sinpi(Vec4f const x) {
+    return sincos_f<Vec4f, 9>(0, x);
+}
+
+static inline Vec4f cospi(Vec4f const x) {
+    return sincos_f<Vec4f, 10>(0, x);
+}
+
+static inline Vec4f sincospi(Vec4f * cosret, Vec4f const x) {
+    return sincos_f<Vec4f, 11>(cosret, x);
+}
+
+static inline Vec4f tanpi(Vec4f const x) {
+    return sincos_f<Vec4f, 12>(0, x);
+}
+
+#if MAX_VECTOR_SIZE >= 256
+static inline Vec8f sin(Vec8f const x) {
+    return sincos_f<Vec8f, 1>(0, x);
+}
+
+static inline Vec8f cos(Vec8f const x) {
+    return sincos_f<Vec8f, 2>(0, x);
+}
+
+static inline Vec8f sincos(Vec8f * cosret, Vec8f const x) {
+    return sincos_f<Vec8f, 3>(cosret, x);
+}
+
+static inline Vec8f tan(Vec8f const x) {
+    return sincos_f<Vec8f, 4>(0, x);
+} 
+
+static inline Vec8f sinpi(Vec8f const x) {
+    return sincos_f<Vec8f, 9>(0, x);
+}
+
+static inline Vec8f cospi(Vec8f const x) {
+    return sincos_f<Vec8f, 10>(0, x);
+}
+
+static inline Vec8f sincospi(Vec8f * cosret, Vec8f const x) {
+    return sincos_f<Vec8f, 11>(cosret, x);
+}
+
+static inline Vec8f tanpi(Vec8f const x) {
+    return sincos_f<Vec8f, 12>(0, x);
+}
+
+#endif // MAX_VECTOR_SIZE >= 256
+
+#if MAX_VECTOR_SIZE >= 512
+static inline Vec16f sin(Vec16f const x) {
+    return sincos_f<Vec16f, 1>(0, x);
+}
+
+static inline Vec16f cos(Vec16f const x) {
+    return sincos_f<Vec16f, 2>(0, x);
+}
+
+static inline Vec16f sincos(Vec16f * cosret, Vec16f const x) {
+    return sincos_f<Vec16f, 3>(cosret, x);
+}
+
+static inline Vec16f tan(Vec16f const x) {
+    return sincos_f<Vec16f, 4>(0, x);
+}
+
+static inline Vec16f sinpi(Vec16f const x) {
+    return sincos_f<Vec16f, 9>(0, x);
+}
+
+static inline Vec16f cospi(Vec16f const x) {
+    return sincos_f<Vec16f, 10>(0, x);
+}
+
+static inline Vec16f sincospi(Vec16f * cosret, Vec16f const x) {
+    return sincos_f<Vec16f, 11>(cosret, x);
+}
+
+static inline Vec16f tanpi(Vec16f const x) {
+    return sincos_f<Vec16f, 12>(0, x);
+}
+
+#endif // MAX_VECTOR_SIZE >= 512
+
+
+// *************************************************************
+//             tan template, double precision
+// *************************************************************
+// Template parameters:
+// VTYPE:  f.p. vector type
+// Template parameters:
+// SC:  0 = tan, 8 = multiply by pi
+// Parameters:
+// x = input x (radians)
+template<typename VTYPE, int SC>
+static inline VTYPE tan_d(VTYPE const x) {
+
+    // define constants
+    const double DP1 = 7.853981554508209228515625E-1 * 2.;
+    const double DP2 = 7.94662735614792836714E-9 * 2.;
+    const double DP3 = 3.06161699786838294307E-17 * 2.;
+
+    const double P2tan = -1.30936939181383777646E4;
+    const double P1tan = 1.15351664838587416140E6;
+    const double P0tan = -1.79565251976484877988E7;
+
+    const double Q3tan = 1.36812963470692954678E4;
+    const double Q2tan = -1.32089234440210967447E6;
+    const double Q1tan = 2.50083801823357915839E7;
+    const double Q0tan = -5.38695755929454629881E7;
+
+    typedef decltype(x > x) BVTYPE;         // boolean vector type
+    VTYPE  xa, y, z, zz, px, qx, tn, recip; // data vectors
+    BVTYPE doinvert;                        // boolean vector
+    typedef decltype(nan_code(x)) UITYPE;   // unsigned integer vector type
+
+    xa = abs(x);
+
+    // Find quadrant
+    if constexpr ((SC & 8) != 0) {
+        y = round(xa * 2.0);
+    }
+    else {
+        xa = select(xa > VTYPE(1.E15), VTYPE(0.), xa); // overflow limit
+        y = round(xa * (double)(2. / VM_PI));    // quadrant, as float
+    }
+    auto q = roundi(y);                          // quadrant, as integer
+    // Quadrant:
+    //      0 -   pi/4 => 0
+    //   pi/4 - 3*pi/4 => 1
+    // 3*pi/4 - 5*pi/4 => 2
+    // 5*pi/4 - 7*pi/4 => 3
+    // 7*pi/4 - 8*pi/4 => 4
+
+    if constexpr ((SC & 8) != 0) {
+        z = nmul_add(y, 0.5, xa) * (VM_PI);
+    }
+    else {
+        // Reduce by extended precision modular arithmetic
+#if INSTRSET < 8  // no FMA
+        z = ((xa - y * DP1) - y * DP2) - y * DP3;
+#else
+        z = nmul_add(y, DP3, nmul_add(y, DP2 + DP1, xa));
+#endif
+    }
+    // Pade approximation of tan, valid for -pi/4 <= x <= pi/4
+    zz = z * z;
+    px = polynomial_2(zz, P0tan, P1tan, P2tan);
+    qx = polynomial_4n(zz, Q0tan, Q1tan, Q2tan, Q3tan);
+
+    // qx cannot be 0 for x <= pi/4
+    tn = mul_add(px / qx, z * zz, z);            // tn = z + z * zz * px / qx;
+
+    // if (q&2) tn = -1/tn
+    doinvert = BVTYPE((q & 1) != 0);
+
+    if constexpr ((SC & 8) != 0) {
+        // tan cannot give infinity because the input cannot be exactly pi/2.
+        // tanpi can generate infinity. Get the right sign of infinity:
+        UITYPE infsign = UITYPE(q) << 62;   // get bit 1 into the sign bit position
+        VTYPE  zsign = sign_combine(VTYPE(-0.), reinterpret_d(infsign));
+        tn = select(tn == 0., zsign, tn);   // get INF with the right sign when tn == 0
+        // the sign of zero output is arbitrary. fixing it would be a waste of code
+    }
+    recip = -1. / tn;
+    tn = select(doinvert, recip, tn);
+    tn = sign_combine(tn, x);       // combine with original sign of x
+    return tn;
+}
+
+// instantiations of tan_d template:
+
+static inline Vec2d tan(Vec2d const x) {
+    return tan_d<Vec2d, 0>(x);
+}
+
+static inline Vec2d tanpi(Vec2d const x) {
+    return tan_d<Vec2d, 8>(x);
+}
+
+#if MAX_VECTOR_SIZE >= 256
+static inline Vec4d tan(Vec4d const x) {
+    return tan_d<Vec4d, 0>(x);
+}
+
+static inline Vec4d tanpi(Vec4d const x) {
+    return tan_d<Vec4d, 8>(x);
+}
+#endif // MAX_VECTOR_SIZE >= 256
+
+#if MAX_VECTOR_SIZE >= 512
+static inline Vec8d tan(Vec8d const x) {
+    return tan_d<Vec8d, 0>(x);
+}
+
+static inline Vec8d tanpi(Vec8d const x) {
+    return tan_d<Vec8d, 8>(x);
+}
+#endif // MAX_VECTOR_SIZE >= 512
+
+
+// *************************************************************
+//             tan template, single precision
+// *************************************************************
+// This is removed for the single precision version.
+// It is faster to use tan(x) = sin(x)/cos(x)
+
+
+
+// *************************************************************
+//             asin/acos template, double precision
+// *************************************************************
+// Template parameters:
+// VTYPE:  f.p. vector type
+// AC: 0 = asin, 1 = acos
+// Parameters:
+// x = input x
+template<typename VTYPE, int AC>
+static inline VTYPE asin_d(VTYPE const x) {
+
+    // define constants
+    const double R4asin = 2.967721961301243206100E-3;
+    const double R3asin = -5.634242780008963776856E-1;
+    const double R2asin = 6.968710824104713396794E0;
+    const double R1asin = -2.556901049652824852289E1;
+    const double R0asin = 2.853665548261061424989E1;
+
+    const double S3asin = -2.194779531642920639778E1;
+    const double S2asin = 1.470656354026814941758E2;
+    const double S1asin = -3.838770957603691357202E2;
+    const double S0asin = 3.424398657913078477438E2;
+
+    const double P5asin = 4.253011369004428248960E-3;
+    const double P4asin = -6.019598008014123785661E-1;
+    const double P3asin = 5.444622390564711410273E0;
+    const double P2asin = -1.626247967210700244449E1;
+    const double P1asin = 1.956261983317594739197E1;
+    const double P0asin = -8.198089802484824371615E0;
+
+    const double Q4asin = -1.474091372988853791896E1;
+    const double Q3asin = 7.049610280856842141659E1;
+    const double Q2asin = -1.471791292232726029859E2;
+    const double Q1asin = 1.395105614657485689735E2;
+    const double Q0asin = -4.918853881490881290097E1;
+
+    VTYPE  xa, xb, x1, x2, x3, x4, x5, px, qx, rx, sx, vx, wx, y1, z, z1, z2;
+    bool   dobig, dosmall;
+
+    xa = abs(x);
+    auto big = xa >= 0.625;  // boolean vector
+
+    /*
+    Small: xa < 0.625
+    ------------------
+    x = xa * xa;
+    px = PX(x);
+    qx = QX(x);
+    y1 = x*px/qx;
+    y1 = xa * y1 + xa;
+
+    Big: xa >= 0.625
+    ------------------
+    x = 1.0 - xa;
+    rx = RX(x);
+    sx = SX(x);
+    y1 = x * rx/sx;
+    x3 = sqrt(x+x);
+    y3 = x3 * y1 - MOREBITS;
+    z = pi/2 - x3 - y3
+    */
+
+    // select a common x for all polynomials
+    // This allows sharing of powers of x through common subexpression elimination
+    x1 = select(big, 1.0 - xa, xa * xa);
+
+    // calculate powers of x1 outside branches to make sure they are only calculated once
+    x2 = x1 * x1;
+    x4 = x2 * x2;
+    x5 = x4 * x1;
+    x3 = x2 * x1;
+
+    dosmall = !horizontal_and(big);    // at least one element is small
+    dobig = horizontal_or(big);        // at least one element is big
+
+    // calculate polynomials (reuse powers of x)
+    if (dosmall) {
+        // px = polynomial_5 (x1, P0asin, P1asin, P2asin, P3asin, P4asin, P5asin);
+        // qx = polynomial_5n(x1, Q0asin, Q1asin, Q2asin, Q3asin, Q4asin);
+        px = mul_add(x3, P3asin, P0asin) + mul_add(x4, P4asin, x1*P1asin) + mul_add(x5, P5asin, x2*P2asin);
+        qx = mul_add(x4, Q4asin, x5) + mul_add(x3, Q3asin, x1*Q1asin) + mul_add(x2, Q2asin, Q0asin);
+    }
+    if (dobig) {
+        // rx = polynomial_4 (x1, R0asin, R1asin, R2asin, R3asin, R4asin);
+        // sx = polynomial_4n(x1, S0asin, S1asin, S2asin, S3asin);
+        rx = mul_add(x3, R3asin, x2*R2asin) + mul_add(x4, R4asin, mul_add(x1, R1asin, R0asin));
+        sx = mul_add(x3, S3asin, x4) + mul_add(x2, S2asin, mul_add(x1, S1asin, S0asin));
+    }
+
+    // select and divide outside branches to avoid dividing twice
+    vx = select(big, rx, px);
+    wx = select(big, sx, qx);
+    y1 = vx / wx * x1;
+
+    // results for big
+    if (dobig) {                                 // avoid square root if all are small
+        xb = sqrt(x1 + x1);                      // this produces NAN if xa > 1 so we don't need a special case for xa > 1
+        z1 = mul_add(xb, y1, xb);                // yb = xb * y1; z1 = xb + yb;
+    }
+
+    // results for small
+    z2 = mul_add(xa, y1, xa);                    // z2 = xa * y1 + xa;
+
+    // correct for sign
+    if constexpr (AC == 1) {                     // acos
+        z1 = select(x < 0., VM_PI - z1, z1);
+        z2 = VM_PI_2 - sign_combine(z2, x);
+        z = select(big, z1, z2);
+    }
+    else {     // asin
+        z1 = VM_PI_2 - z1;
+        z = select(big, z1, z2);
+        z = sign_combine(z, x);
+    }
+    return z;
+}
+
+// instantiations of asin_d template:
+
+static inline Vec2d asin(Vec2d const x) {
+    return asin_d<Vec2d, 0>(x);
+}
+
+static inline Vec2d acos(Vec2d const x) {
+    return asin_d<Vec2d, 1>(x);
+}
+
+#if MAX_VECTOR_SIZE >= 256
+static inline Vec4d asin(Vec4d const x) {
+    return asin_d<Vec4d, 0>(x);
+}
+
+static inline Vec4d acos(Vec4d const x) {
+    return asin_d<Vec4d, 1>(x);
+}
+#endif // MAX_VECTOR_SIZE >= 256
+
+#if MAX_VECTOR_SIZE >= 512
+static inline Vec8d asin(Vec8d const x) {
+    return asin_d<Vec8d, 0>(x);
+}
+
+static inline Vec8d acos(Vec8d const x) {
+    return asin_d<Vec8d, 1>(x);
+}
+#endif // MAX_VECTOR_SIZE >= 512
+
+
+// *************************************************************
+//             asin/acos template, single precision
+// *************************************************************
+// Template parameters:
+// VTYPE:  f.p. vector type
+// AC: 0 = asin, 1 = acos
+// Parameters:
+// x = input x
+template<typename VTYPE, int AC>
+static inline VTYPE asin_f(VTYPE const x) {
+
+    // define constants
+    const float P4asinf = 4.2163199048E-2f;
+    const float P3asinf = 2.4181311049E-2f;
+    const float P2asinf = 4.5470025998E-2f;
+    const float P1asinf = 7.4953002686E-2f;
+    const float P0asinf = 1.6666752422E-1f;
+
+    VTYPE  xa, x1, x2, x3, x4, xb, z, z1, z2;
+
+    xa = abs(x);
+    auto big = xa > 0.5f;                        // boolean vector
+
+    x1 = 0.5f * (1.0f - xa);
+    x2 = xa * xa;
+    x3 = select(big, x1, x2);
+
+    //if (horizontal_or(big))
+    {
+        xb = sqrt(x1);
+    }
+    x4 = select(big, xb, xa);
+
+    z = polynomial_4(x3, P0asinf, P1asinf, P2asinf, P3asinf, P4asinf);
+    z = mul_add(z, x3*x4, x4);                   // z = z * (x3*x4) + x4;
+    z1 = z + z;
+
+    // correct for sign
+    if constexpr (AC == 1) {                     // acos
+        z1 = select(x < 0., float(VM_PI) - z1, z1);
+        z2 = float(VM_PI_2) - sign_combine(z, x);
+        z = select(big, z1, z2);
+    }
+    else {     // asin
+        z1 = float(VM_PI_2) - z1;
+        z = select(big, z1, z);
+        z = sign_combine(z, x);
+    }
+
+    return z;
+}
+
+// instantiations of asin_f template:
+
+static inline Vec4f asin(Vec4f const x) {
+    return asin_f<Vec4f, 0>(x);
+}
+
+static inline Vec4f acos(Vec4f const x) {
+    return asin_f<Vec4f, 1>(x);
+}
+
+#if MAX_VECTOR_SIZE >= 256
+static inline Vec8f asin(Vec8f const x) {
+    return asin_f<Vec8f, 0>(x);
+}
+static inline Vec8f acos(Vec8f const x) {
+    return asin_f<Vec8f, 1>(x);
+}
+#endif // MAX_VECTOR_SIZE >= 256
+
+#if MAX_VECTOR_SIZE >= 512
+static inline Vec16f asin(Vec16f const x) {
+    return asin_f<Vec16f, 0>(x);
+}
+static inline Vec16f acos(Vec16f const x) {
+    return asin_f<Vec16f, 1>(x);
+}
+#endif // MAX_VECTOR_SIZE >= 512
+
+
+// *************************************************************
+//             atan template, double precision
+// *************************************************************
+// Template parameters:
+// VTYPE:  f.p. vector type
+// T2:     0 = atan, 1 = atan2
+// Parameters:
+// y, x. calculate tan(y/x)
+// result is between -pi/2 and +pi/2 when x > 0
+// result is between -pi and -pi/2 or between pi/2 and pi when x < 0 for atan2
+template<typename VTYPE, int T2>
+static inline VTYPE atan_d(VTYPE const y, VTYPE const x) {
+
+    // define constants
+    //const double ONEOPIO4 = 4./VM_PI;
+    const double MOREBITS = 6.123233995736765886130E-17;
+    const double MOREBITSO2 = MOREBITS * 0.5;
+    const double T3PO8 = VM_SQRT2 + 1.; // 2.41421356237309504880;
+
+    const double P4atan = -8.750608600031904122785E-1;
+    const double P3atan = -1.615753718733365076637E1;
+    const double P2atan = -7.500855792314704667340E1;
+    const double P1atan = -1.228866684490136173410E2;
+    const double P0atan = -6.485021904942025371773E1;
+
+    const double Q4atan = 2.485846490142306297962E1;
+    const double Q3atan = 1.650270098316988542046E2;
+    const double Q2atan = 4.328810604912902668951E2;
+    const double Q1atan = 4.853903996359136964868E2;
+    const double Q0atan = 1.945506571482613964425E2;
+
+    typedef decltype (x > x) BVTYPE;                            // boolean vector type
+    VTYPE  t, x1, x2, y1, y2, s, fac, a, b, z, zz, px, qx, re;  // data vectors
+    BVTYPE swapxy, notbig, notsmal;                             // boolean vectors
+
+    if constexpr (T2 == 1) {  // atan2(y,x)
+        // move in first octant
+        x1 = abs(x);
+        y1 = abs(y);
+        swapxy = (y1 > x1);
+        // swap x and y if y1 > x1
+        x2 = select(swapxy, y1, x1);
+        y2 = select(swapxy, x1, y1);
+
+        // check for special case: x and y are both +/- INF
+        BVTYPE both_infinite = is_inf(x) & is_inf(y);   // x and Y are both infinite
+        if (horizontal_or(both_infinite)) {             // at least one element has both infinite
+            VTYPE mone = VTYPE(-1.0);
+            x2 = select(both_infinite, x2 & mone, x2);  // get 1.0 with the sign of x
+            y2 = select(both_infinite, y2 & mone, y2);  // get 1.0 with the sign of y
+        }
+
+        t = y2 / x2;                  // x = y = 0 gives NAN here
+    }
+    else {    // atan(y)
+        t = abs(y);
+    }
+
+    // small:  t < 0.66
+    // medium: 0.66 <= t <= 2.4142 (1+sqrt(2))
+    // big:    t > 2.4142
+    notbig  = t <= T3PO8;  // t <= 2.4142
+    notsmal = t >= 0.66;   // t >= 0.66
+
+    s   = select(notbig, VTYPE(VM_PI_4), VTYPE(VM_PI_2));
+    s   = notsmal & s;                   // select(notsmal, s, 0.);
+    fac = select(notbig, VTYPE(MOREBITSO2), VTYPE(MOREBITS));
+    fac = notsmal & fac;  //select(notsmal, fac, 0.);
+
+    // small:  z = t / 1.0;
+    // medium: z = (t-1.0) / (t+1.0);
+    // big:    z = -1.0 / t;
+    a = notbig & t;                    // select(notbig, t, 0.);
+    a = if_add(notsmal, a, -1.);
+    b = notbig & VTYPE(1.);            //  select(notbig, 1., 0.);
+    b = if_add(notsmal, b, t);
+    z = a / b;                         // division by 0 will not occur unless x and y are both 0
+
+    zz = z * z;
+
+    px = polynomial_4(zz, P0atan, P1atan, P2atan, P3atan, P4atan);
+    qx = polynomial_5n(zz, Q0atan, Q1atan, Q2atan, Q3atan, Q4atan);
+
+    re = mul_add(px / qx, z * zz, z);  // re = (px / qx) * (z * zz) + z;
+    re += s + fac;
+
+    if constexpr (T2 == 1) {           // atan2(y,x)
+        // move back in place
+        re = select(swapxy, VM_PI_2 - re, re);
+        re = select((x | y) == 0., 0., re);      // atan2(0,0) = 0 by convention
+        re = select(sign_bit(x), VM_PI - re, re);// also for x = -0.
+    }
+    // get sign bit
+    re = sign_combine(re, y);
+
+    return re;
+}
+
+// instantiations of atan_d template:
+
+static inline Vec2d atan2(Vec2d const y, Vec2d const x) {
+    return atan_d<Vec2d, 1>(y, x);
+}
+
+static inline Vec2d atan(Vec2d const y) {
+    return atan_d<Vec2d, 0>(y, 0.);
+}
+
+#if MAX_VECTOR_SIZE >= 256
+static inline Vec4d atan2(Vec4d const y, Vec4d const x) {
+    return atan_d<Vec4d, 1>(y, x);
+}
+
+static inline Vec4d atan(Vec4d const y) {
+    return atan_d<Vec4d, 0>(y, 0.);
+}
+#endif // MAX_VECTOR_SIZE >= 256
+
+#if MAX_VECTOR_SIZE >= 512
+static inline Vec8d atan2(Vec8d const y, Vec8d const x) {
+    return atan_d<Vec8d, 1>(y, x);
+}
+
+static inline Vec8d atan(Vec8d const y) {
+    return atan_d<Vec8d, 0>(y, 0.);
+}
+#endif // MAX_VECTOR_SIZE >= 512
+
+
+
+// *************************************************************
+//             atan template, single precision
+// *************************************************************
+// Template parameters:
+// VTYPE:  f.p. vector type
+// T2:     0 = atan, 1 = atan2
+// Parameters:
+// y, x. calculate tan(y/x)
+// result is between -pi/2 and +pi/2 when x > 0
+// result is between -pi and -pi/2 or between pi/2 and pi when x < 0 for atan2
+template<typename VTYPE, int T2>
+static inline VTYPE atan_f(VTYPE const y, VTYPE const x) {
+
+    // define constants
+    const float P3atanf = 8.05374449538E-2f;
+    const float P2atanf = -1.38776856032E-1f;
+    const float P1atanf = 1.99777106478E-1f;
+    const float P0atanf = -3.33329491539E-1f;
+
+    typedef decltype (x > x) BVTYPE;             // boolean vector type
+    VTYPE  t, x1, x2, y1, y2, s, a, b, z, zz, re;// data vectors
+    BVTYPE swapxy, notbig, notsmal;              // boolean vectors
+
+    if constexpr (T2 == 1) {  // atan2(y,x)
+        // move in first octant
+        x1 = abs(x);
+        y1 = abs(y);
+        swapxy = (y1 > x1);
+        // swap x and y if y1 > x1
+        x2 = select(swapxy, y1, x1);
+        y2 = select(swapxy, x1, y1);
+
+        // check for special case: x and y are both +/- INF
+        BVTYPE both_infinite = is_inf(x) & is_inf(y);   // x and Y are both infinite
+        if (horizontal_or(both_infinite)) {             // at least one element has both infinite
+            VTYPE mone = VTYPE(-1.0f);
+            x2 = select(both_infinite, x2 & mone, x2);  // get 1.0 with the sign of x
+            y2 = select(both_infinite, y2 & mone, y2);  // get 1.0 with the sign of y
+        }
+
+        // x = y = 0 will produce NAN. No problem, fixed below
+        t = y2 / x2;
+    }
+    else {    // atan(y)
+        t = abs(y);
+    }
+
+    // small:  t < 0.4142
+    // medium: 0.4142 <= t <= 2.4142
+    // big:    t > 2.4142  (not for atan2)
+    if constexpr (T2 == 0) {  // atan(y)
+        notsmal = t >= float(VM_SQRT2 - 1.);     // t >= tan  pi/8
+        notbig = t <= float(VM_SQRT2 + 1.);      // t <= tan 3pi/8
+
+        s = select(notbig, VTYPE(float(VM_PI_4)), VTYPE(float(VM_PI_2)));
+        s = notsmal & s;                         // select(notsmal, s, 0.);
+
+        // small:  z = t / 1.0;
+        // medium: z = (t-1.0) / (t+1.0);
+        // big:    z = -1.0 / t;
+        a = notbig & t;                // select(notbig, t, 0.);
+        a = if_add(notsmal, a, -1.f);
+        b = notbig & VTYPE(1.f);       //  select(notbig, 1., 0.);
+        b = if_add(notsmal, b, t);
+        z = a / b;                     // division by 0 will not occur unless x and y are both 0
+    }
+    else {  // atan2(y,x)
+        // small:  z = t / 1.0;
+        // medium: z = (t-1.0) / (t+1.0);
+        notsmal = t >= float(VM_SQRT2 - 1.);
+        a = if_add(notsmal, t, -1.f);
+        b = if_add(notsmal, 1.f, t);
+        s = notsmal & VTYPE(float(VM_PI_4));
+        z = a / b;
+    }
+
+    zz = z * z;
+
+    // Taylor expansion
+    re = polynomial_3(zz, P0atanf, P1atanf, P2atanf, P3atanf);
+    re = mul_add(re, zz * z, z) + s;
+
+    if constexpr (T2 == 1) {                               // atan2(y,x)
+        // move back in place
+        re = select(swapxy, float(VM_PI_2) - re, re);
+        re = select((x | y) == 0.f, 0.f, re);              // atan2(0,+0) = 0 by convention
+        re = select(sign_bit(x), float(VM_PI) - re, re);   // also for x = -0.
+    }
+    // get sign bit
+    re = sign_combine(re, y);
+
+    return re;
+}
+
+// instantiations of atan_f template:
+
+static inline Vec4f atan2(Vec4f const y, Vec4f const x) {
+    return atan_f<Vec4f, 1>(y, x);
+}
+
+static inline Vec4f atan(Vec4f const y) {
+    return atan_f<Vec4f, 0>(y, 0.);
+}
+
+#if MAX_VECTOR_SIZE >= 256
+static inline Vec8f atan2(Vec8f const y, Vec8f const x) {
+    return atan_f<Vec8f, 1>(y, x);
+}
+
+static inline Vec8f atan(Vec8f const y) {
+    return atan_f<Vec8f, 0>(y, 0.);
+}
+
+#endif // MAX_VECTOR_SIZE >= 256
+
+#if MAX_VECTOR_SIZE >= 512
+static inline Vec16f atan2(Vec16f const y, Vec16f const x) {
+    return atan_f<Vec16f, 1>(y, x);
+}
+
+static inline Vec16f atan(Vec16f const y) {
+    return atan_f<Vec16f, 0>(y, 0.);
+}
+
+#endif // MAX_VECTOR_SIZE >= 512
+
+#ifdef VCL_NAMESPACE
+}
+#endif
+
+#endif
diff --git a/meson.build b/meson.build
index 0a50bb3..99b20d5 100644
--- a/meson.build
+++ b/meson.build
@@ -4,12 +4,25 @@ project('EEDI3', 'cpp',
   version : '5'
 )
 
-add_project_arguments('-Wno-unused-variable', '-ffast-math', language : 'cpp')
+opencl_dep = dependency('OpenCL', required : false)
+if not opencl_dep.found()
+  opencl_dep = meson.get_compiler('cpp').find_library('OpenCL')
+endif
+
+boost_dep = dependency('boost', modules : ['filesystem', 'system'])
+
+add_project_arguments('-Wno-unused-variable', '-ffast-math', '-std=c++17', language : 'cpp')
+add_project_arguments('-DHAVE_OPENCL', language : 'cpp')
+
+
 
 sources = [
   'EEDI3/EEDI3.cpp',
   'EEDI3/EEDI3.hpp',
   'EEDI3/shared.hpp',
+  'EEDI3/EEDI3CL.cpp',
+  'EEDI3/EEDI3CL.hpp',
+  'EEDI3/EEDI3CL_SSE2.cpp',
   'EEDI3/vectorclass/instrset.h',
   'EEDI3/vectorclass/instrset_detect.cpp'
 ]
@@ -17,7 +30,7 @@ sources = [
 vapoursynth_dep = dependency('vapoursynth').partial_dependency(compile_args : true, includes : true)
 
 deps = [vapoursynth_dep]
-
+deps += [opencl_dep, boost_dep]
 libs = []
 
 if host_machine.cpu_family().startswith('x86')
